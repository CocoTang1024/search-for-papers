{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed DOI: 10.48550/arXiv.2504.13460v3 (no explicit DOI in metadata)\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    # Encode the title for URL safety and construct the query\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1'\n",
    "    \n",
    "    try:\n",
    "        # Send request to arXiv API\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(data)\n",
    "        \n",
    "        # Define namespaces\n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "        \n",
    "        # Find the first entry\n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return \"No articles found for the given title.\"\n",
    "        \n",
    "        # Extract arXiv ID\n",
    "        id_element = entry.find('atom:id', ns)\n",
    "        if id_element is None or not id_element.text:\n",
    "            return \"Could not retrieve arXiv ID.\"\n",
    "        arxiv_id = id_element.text.split('/')[-1]  # e.g., 2504.13460v3\n",
    "        \n",
    "        # Check for existing DOI\n",
    "        doi_element = entry.find('arxiv:doi', ns)\n",
    "        if doi_element is not None and doi_element.text:\n",
    "            return f\"DOI: {doi_element.text}\"\n",
    "        \n",
    "        # Construct DOI from arXiv ID\n",
    "        constructed_doi = f\"10.48550/arXiv.{arxiv_id}\"\n",
    "        return f\"Constructed DOI: {constructed_doi} (no explicit DOI in metadata)\"\n",
    "            \n",
    "    except urllib.error.URLError as e:\n",
    "        return f\"Error fetching data: {e}\"\n",
    "    except ET.ParseError:\n",
    "        return \"Error parsing XML response.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with the provided title\n",
    "    title = \"Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization\"\n",
    "    result = get_arxiv_doi(title)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取RIS文件: D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal.ris\n",
      "输入文件中找到 1647 个条目。\n",
      "\n",
      "处理标题: Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2504.13460v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Attention in Diffusion Model: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FDDet: Frequency-Decoupling for Boundary Refinement in Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2007.06866v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning\n",
      "构造的DOI: 10.48550/arXiv.2504.00527v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs\n",
      "构造的DOI: 10.48550/arXiv.2504.00072v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Precise Action Spotting: Addressing Temporal Misalignment in Labels with Dynamic Label Assignment\n",
      "构造的DOI: 10.48550/arXiv.2504.00149v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks\n",
      "构造的DOI: 10.48550/arXiv.2503.22405v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention\n",
      "构造的DOI: 10.48550/arXiv.2504.09738v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing\n",
      "构造的DOI: 10.48550/arXiv.2503.21541v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Autoregressive Adaptive Hypergraph Transformer for Skeleton-based Activity Recognition\n",
      "构造的DOI: 10.48550/arXiv.2411.05692v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MambaBEV: An efficient 3D detection model with Mamba2\n",
      "构造的DOI: 10.48550/arXiv.2410.12673v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2309.03167v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning\n",
      "构造的DOI: 10.48550/arXiv.2503.21055v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-Enhanced Memory-Refined Transformer for Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2208.14209v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset\n",
      "构造的DOI: 10.48550/arXiv.2503.18553v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2503.18943v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Anomize: Better Open Vocabulary Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2503.18094v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Collaborative Temporal Consistency Learning for Point-supervised Natural Language Video Localization\n",
      "构造的DOI: 10.48550/arXiv.2503.17651v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Detection Model Compression by Progressive Block Drop\n",
      "构造的DOI: 10.48550/arXiv.2503.16916v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FMDConv: Fast Multi-Attention Dynamic Convolution via Speed-Accuracy Trade-off\n",
      "构造的DOI: 10.48550/arXiv.2201.00392v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Probabilistic Prompt Distribution Learning for Animal Pose Estimation\n",
      "构造的DOI: 10.48550/arXiv.2503.16120v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Visual Position Prompt for MLLM based Visual Grounding\n",
      "构造的DOI: 10.48550/arXiv.2503.15426v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Challenges and Trends in Egocentric Vision: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2011.00786v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Squeeze Out Tokens from Sample for Finer-Grained Data Governance\n",
      "构造的DOI: 10.48550/arXiv.2503.14559v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation\n",
      "构造的DOI: 10.48550/arXiv.2503.13957v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2503.14343v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long-VMNet: Accelerating Long-Form Video Understanding via Fixed Memory\n",
      "构造的DOI: 10.48550/arXiv.2503.13707v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action tube generation by person query matching for spatio-temporal action detection\n",
      "DOI: 10.5220/0013089500003912\n",
      "\n",
      "处理标题: ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation\n",
      "构造的DOI: 10.48550/arXiv.2503.12348v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: United we stand, Divided we fall: Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition in Valence-Arousal Space\n",
      "构造的DOI: 10.48550/arXiv.2503.12261v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Comprehensive Survey on Knowledge Distillation\n",
      "构造的DOI: 10.48550/arXiv.2302.14643v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation\n",
      "构造的DOI: 10.48550/arXiv.2503.11081v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment\n",
      "构造的DOI: 10.48550/arXiv.2503.09081v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Measure Twice, Cut Once: Grasping Video Structures and Event Semantics with LLMs for Video Temporal Localization\n",
      "构造的DOI: 10.48550/arXiv.2503.09027v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VIDEO ACTION DIFFERENCING\n",
      "构造的DOI: 10.48550/arXiv.2503.07860v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TimeLoc: A Unified End-to-End Framework for Precise Timestamp Localization in Long Videos\n",
      "构造的DOI: 10.48550/arXiv.2503.06526v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-End Action Segmentation Transformer\n",
      "构造的DOI: 10.48550/arXiv.2302.13074v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OSCAR: Object Status and Contextual Awareness for Recipes to Support Non-Visual Cooking\n",
      "构造的DOI: 10.48550/arXiv.2503.05962v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatial-Temporal Perception with Causal Inference for Naturalistic Driving Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2503.04078v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MODELING FINE-GRAINED HAND-OBJECT DYNAMICS FOR EGOCENTRIC VIDEO REPRESENTATION LEARNING\n",
      "构造的DOI: 10.48550/arXiv.2503.00986v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CRUPL: A Semi-Supervised Cyber Attack Detection with Consistency Regularization and Uncertainty-aware Pseudo-Labeling in Smart Grid\n",
      "构造的DOI: 10.48550/arXiv.2503.00358v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Precise Event Spotting in Sports Videos: Solving Long-Range Dependency and Class Imbalance\n",
      "构造的DOI: 10.48550/arXiv.1109.6811v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OpenTAD: A Unified Framework and Comprehensive Study of Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2409.18478v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating and Attributing in Videos\n",
      "构造的DOI: 10.48550/arXiv.2503.00049v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Data Augmentation for Instruction Following Policies via Trajectory Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2503.01871v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Task-Oriented 6-DoF Grasp Pose Detection in Clutters\n",
      "构造的DOI: 10.48550/arXiv.2502.16976v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks\n",
      "构造的DOI: 10.48550/arXiv.2502.15462v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WEAKLY SUPERVISED VIDEO SCENE GRAPH GENERATION VIA NATURAL LANGUAGE SUPERVISION\n",
      "构造的DOI: 10.48550/arXiv.2502.15370v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical Context Transformer for Multi-level Semantic Scene Understanding\n",
      "构造的DOI: 10.48550/arXiv.2502.15184v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contrast-Unity for Partially-Supervised Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.1910.14303v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild\n",
      "构造的DOI: 10.48550/arXiv.2502.14892v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: When do they StOP?: A First Step Towards Automatically Identifying Team Communication in the Operating Room\n",
      "构造的DOI: 10.48550/arXiv.2502.08299v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2502.08544v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Mamba Architecture for Vision Applications\n",
      "构造的DOI: 10.48550/arXiv.2502.07161v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: From Image to Video: An Empirical Study of Diffusion Representations\n",
      "构造的DOI: 10.48550/arXiv.2502.07001v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploiting Ensemble Learning for Cross-View Isolated Sign Language Recognition\n",
      "构造的DOI: 10.48550/arXiv.2502.02196v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse Task Perspectives\n",
      "构造的DOI: 10.48550/arXiv.2502.02487v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: IMDPROMPTER: ADAPTING SAM TO IMAGE MANIPULATION DETECTION BY CROSS-VIEW AUTOMATED PROMPT LEARNING\n",
      "构造的DOI: 10.48550/arXiv.2502.02454v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Minimalistic Video Saliency Prediction via Efficient Decoder & Spatio Temporal Action Cues\n",
      "构造的DOI: 10.48550/arXiv.2502.00397v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: XRF V2: A Dataset for Action Summarization with Wi-Fi Signals, and IMUs in Phones, Watches, Earbuds, and Glasses\n",
      "构造的DOI: 10.48550/arXiv.2501.19034v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ALBAR: ADVERSARIAL LEARNING APPROACH TO MITIGATE BIASES IN ACTION RECOGNITION\n",
      "构造的DOI: 10.48550/arXiv.2502.00156v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem Solving & a Novel Optimization Strategy\n",
      "构造的DOI: 10.48550/arXiv.2501.18509v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fusion of Millimeter-wave Radar and Pulse Oximeter Data for Low-burden Diagnosis of Obstructive Sleep Apnea-Hypopnea Syndrome\n",
      "构造的DOI: 10.48550/arXiv.2501.15264v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BAICHUAN-OMNI-1.5 TECHNICAL REPORT\n",
      "构造的DOI: 10.48550/arXiv.1707.08679v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Parameter-Efficient Fine-Tuning for Foundation Models\n",
      "构造的DOI: 10.48550/arXiv.2305.02176v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Meta-Feature Adapter: Integrating Environmental Metadata for Enhanced Animal Re-identification\n",
      "构造的DOI: 10.48550/arXiv.2501.13368v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SMART-VISION: SURVEY OF MODERN ACTION RECOGNITION TECHNIQUES IN VISION\n",
      "DOI: 10.1109/TNNLS.2023.3321432\n",
      "\n",
      "处理标题: Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction\n",
      "构造的DOI: 10.48550/arXiv.2501.11124v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FINDING THE TRIGGER: CAUSAL ABDUCTIVE REASONING ON VIDEO EVENTS\n",
      "构造的DOI: 10.48550/arXiv.2501.09304v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A New Teacher-Reviewer-Student Framework for Semi-supervised 2D Human Pose Estimation\n",
      "构造的DOI: 10.48550/arXiv.2107.03000v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2501.07810v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion\n",
      "构造的DOI: 10.48550/arXiv.1704.01047v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: The Devil is in the Spurious Correlations: Boosting Moment Retrieval with Dynamic Learning\n",
      "构造的DOI: 10.48550/arXiv.2501.07305v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: COMMON SENSE IS ALL YOU NEED\n",
      "构造的DOI: 10.48550/arXiv.2501.06642v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MS-Temba: Multi-Scale Temporal Mamba for Efficient Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2501.06138v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance\n",
      "构造的DOI: 10.48550/arXiv.2412.15246v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TCPFormer: Learning Temporal Correlation with Implicit Pose Proxy for 3D Human Pose Estimation\n",
      "构造的DOI: 10.48550/arXiv.2501.01770v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization\n",
      "构造的DOI: 10.48550/arXiv.1909.06500v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multimodal Large Models Are Effective Action Anticipators\n",
      "构造的DOI: 10.48550/arXiv.2501.00795v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CAREBENCH: A Fine-Grained Benchmark for Video Captioning and Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2501.00513v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Vinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model\n",
      "构造的DOI: 10.48550/arXiv.2412.21080v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action-Agnostic Point-Level Supervision for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems\n",
      "构造的DOI: 10.48550/arXiv.2412.20201v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DAVE: Diverse Atomic Visual Elements Dataset with High Representation of Vulnerable Road Users in Complex and Unpredictable Environments\n",
      "构造的DOI: 10.48550/arXiv.2412.20042v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Novel Method for Pignistic Information Fusion in the View of Z-number\n",
      "构造的DOI: 10.48550/arXiv.2501.06201v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2412.19108v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Residual Feature-Reutilization Inception Network for Image Classification\n",
      "构造的DOI: 10.48550/arXiv.1608.02201v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A novel framework for MCDM based on Z numbers and soft likelihood function\n",
      "构造的DOI: 10.48550/arXiv.2412.19321v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Underwater Image Restoration via Polymorphic Large Kernel CNNs\n",
      "构造的DOI: 10.48550/arXiv.2412.18459v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical Vector Quantization for Unsupervised Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2412.17640v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Adaptive Framework for Multi-View Clustering Leveraging Conditional Entropy Optimization\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets\n",
      "构造的DOI: 10.48550/arXiv.2407.12642v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Group Interactions and Semantic Intentions for Multi-Object Trajectory Prediction\n",
      "构造的DOI: 10.48550/arXiv.2412.15673v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WiFi CSI Based Temporal Activity Detection via Dual Pyramid Network\n",
      "构造的DOI: 10.48550/arXiv.2412.16233v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: JoVALE: Detecting Human Actions in Video Using Audiovisual and Language Contexts\n",
      "构造的DOI: 10.48550/arXiv.2412.13708v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense Audio-Visual Event Localization under Cross-Modal Consistency and Multi-Temporal Granularity Collaboration\n",
      "构造的DOI: 10.48550/arXiv.2412.12628v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning\n",
      "构造的DOI: 10.48550/arXiv.2412.12791v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ShotVL: Human-centric Highlight Frame Retrieval via Language Queries\n",
      "构造的DOI: 10.48550/arXiv.2412.12675v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Localization with Cross Layer Task Decoupling and Refinement\n",
      "构造的DOI: 10.48550/arXiv.2412.09202v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis\n",
      "DOI: 10.24963/ijcai.2024/46\n",
      "\n",
      "处理标题: TIMEREFINE: Temporal Grounding with Time Refining Video LLM\n",
      "构造的DOI: 10.48550/arXiv.2412.09601v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Repurposing from User Generated Content: A Large-scale Dataset and Benchmark\n",
      "构造的DOI: 10.48550/arXiv.1903.10412v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D Reconstruction from Unposed Sparse Views\n",
      "构造的DOI: 10.48550/arXiv.2412.08412v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Repetitive Action Counting with Hybrid Temporal Relation Modeling\n",
      "构造的DOI: 10.48550/arXiv.2412.07233v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Stable Mean Teacher for Semi-supervised Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2412.07072v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Long Video Understanding via Fine-detailed Video Story Generation\n",
      "构造的DOI: 10.48550/arXiv.2412.06182v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity\n",
      "构造的DOI: 10.48550/arXiv.2412.06171v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LinVT: Empower Your Image-level Large Language Model to Understand Videos\n",
      "构造的DOI: 10.48550/arXiv.2311.18445v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Streaming Detection of Queried Event Start\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Stain-aware Domain Alignment for Imbalance Blood Cell Classification\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability\n",
      "构造的DOI: 10.48550/arXiv.2411.18211v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Benchmarking the Robustness of Optical Flow Estimation to Corruptions\n",
      "构造的DOI: 10.48550/arXiv.2411.14865v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: About Time: Advances, Challenges, and Outlooks of Action Understanding\n",
      "构造的DOI: 10.48550/arXiv.2411.15106v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LLaVA-MR: Large Language-and-Vision Assistant for Video Moment Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2411.14505v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: What’s in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning\n",
      "构造的DOI: 10.48550/arXiv.2411.14688v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization\n",
      "构造的DOI: 10.48550/arXiv.2411.12525v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploiting VLM Localizability and Semantics for Open Vocabulary Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2411.10922v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Number it: Temporal Grounding Videos like Flipping Manga\n",
      "构造的DOI: 10.48550/arXiv.2411.10332v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Can MLLMs Guide Weakly-Supervised Temporal Action Localization Tasks?\n",
      "构造的DOI: 10.48550/arXiv.2411.08466v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Vision Language Models are In-Context Value Learners\n",
      "构造的DOI: 10.48550/arXiv.2406.06973v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Beyond Model Adaptation at Test Time: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos\n",
      "构造的DOI: 10.48550/arXiv.2411.02570v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Activating Self-Attention for Multi-Scene Absolute Pose Regression\n",
      "构造的DOI: 10.48550/arXiv.2411.01443v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ROAD-Waymo: Action Awareness at Scale for Autonomous Driving\n",
      "构造的DOI: 10.48550/arXiv.2302.00673v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OnlineTAS: An Online Baseline for Temporal Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2411.01122v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Technical Report for SoccerNet Challenge 2022 - Replay Grounding Task\n",
      "构造的DOI: 10.48550/arXiv.2308.16651v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Technical Report for ActivityNet Challenge 2022 - Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2106.06138v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Recovering Complete Actions for Cross-dataset Skeleton Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2410.23641v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MarDini: Masked Autoregressive Diffusion for Video Generation at Scale\n",
      "构造的DOI: 10.48550/arXiv.2212.05199v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes\n",
      "构造的DOI: 10.48550/arXiv.2111.04260v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset\n",
      "构造的DOI: 10.48550/arXiv.2410.19488v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VARS: Vision-based Assessment of Risk in Security Systems\n",
      "构造的DOI: 10.48550/arXiv.2307.14114v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TIMESUITE: IMPROVING MLLMS FOR LONG VIDEO UNDERSTANDING VIA GROUNDED TUNING\n",
      "构造的DOI: 10.48550/arXiv.2410.19702v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PESFormer: Boosting Macro- and Micro-expression Spotting with Direct Timestamp Encoding\n",
      "构造的DOI: 10.48550/arXiv.2410.18695v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Optimal Adapter Placement for Efficient Transfer Learning\n",
      "构造的DOI: 10.48550/arXiv.2410.15858v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ContextDet: Temporal Action Detection with Adaptive Context Aggregation\n",
      "构造的DOI: 10.48550/arXiv.2410.15279v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zero-shot Action Localization via the Confidence of Large Vision-Language Models\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GEM-VPC: A dual Graph-Enhanced Multimodal integration for Video Paragraph Captioning\n",
      "构造的DOI: 10.48550/arXiv.2410.09377v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models\n",
      "构造的DOI: 10.48550/arXiv.2410.08611v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2410.08593v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with Heterogeneous Graph Adapter\n",
      "构造的DOI: 10.48550/arXiv.2410.07854v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Understanding Spatio-Temporal Relations in Human-Object Interaction using Pyramid Graph Convolutional Network\n",
      "构造的DOI: 10.48550/arXiv.2410.07912v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model\n",
      "构造的DOI: 10.48550/arXiv.2405.15413v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2410.05771v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024\n",
      "构造的DOI: 10.48550/arXiv.2410.09088v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FGCL: FINE-GRAINED CONTRASTIVE LEARNING FOR MANDARIN STUTTERING EVENT DETECTION\n",
      "构造的DOI: 10.48550/arXiv.2410.05647v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EGOOOPS: A DATASET FOR MISTAKE ACTION DETECTION FROM EGOCENTRIC VIDEOS REFERRING TO PROCEDURAL TEXTS\n",
      "构造的DOI: 10.48550/arXiv.2410.05343v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep learning for action spotting in association football videos\n",
      "构造的DOI: 10.48550/arXiv.2410.01304v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting Essential and Nonessential Settings of Evidential Deep Learning\n",
      "构造的DOI: 10.48550/arXiv.2410.00393v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VIDAS: VISION-BASED DANGER ASSESSMENT AND SCORING\n",
      "DOI: 10.1145/3702250.3702279\n",
      "\n",
      "处理标题: RADGAZEGEN: RADIOMICS AND GAZE-GUIDED MEDICAL IMAGE GENERATION USING DIFFUSION MODELS\n",
      "未找到标题为 'RADIOMICS AND GAZE-GUIDED MEDICAL IMAGE GENERATION USING DIFFUSION MODELS' 的文章。\n",
      "\n",
      "处理标题: Solution for Temporal Sound Localisation Task of ECCV Second Perception Test Challenge 2024\n",
      "构造的DOI: 10.48550/arXiv.2409.19595v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Detection of Sleep Apnea-Hypopnea Events Using Millimeter-wave Radar and Pulse Oximeter\n",
      "构造的DOI: 10.48550/arXiv.2409.19217v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal2Seq: A Unified Framework for Temporal Video Understanding Tasks\n",
      "构造的DOI: 10.48550/arXiv.2409.18478v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Query matching for spatio-temporal action detection with query-based object detector\n",
      "构造的DOI: 10.48550/arXiv.2409.18408v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EAGLE: Egocentric AGgregated Language-video Engine\n",
      "DOI: 10.1145/3664647.3681618\n",
      "\n",
      "处理标题: Shape-intensity knowledge distillation for robust medical image segmentation\n",
      "构造的DOI: 10.48550/arXiv.2412.13742v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EDSNET: EFFICIENT-DSNET FOR VIDEO SUMMARIZATION\n",
      "构造的DOI: 10.48550/arXiv.2404.12353v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text Alignment\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference\n",
      "构造的DOI: 10.48550/arXiv.2409.12467v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HAVANA: Hierarchical stochastic neighbor embedding for Accelerated Video ANnotAtions\n",
      "构造的DOI: 10.48550/arXiv.2409.10641v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Uncertainty-Guided Appearance-Motion Association Network for Out-of-Distribution Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1703.10664v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM\n",
      "构造的DOI: 10.48550/arXiv.1810.08831v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization\n",
      "构造的DOI: 10.48550/arXiv.2409.07967v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Recent Trends of Multimodal Affective Computing: A Survey from an NLP Perspective\n",
      "构造的DOI: 10.48550/arXiv.2502.00837v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2409.05122v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Comprehensive Survey on Evidential Deep Learning and Its Applications\n",
      "构造的DOI: 10.48550/arXiv.2409.04720v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Introducing Gating and Context into Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2409.04205v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MultiCounter: Multiple Action Agnostic Repetition Counting in Untrimmed Videos\n",
      "构造的DOI: 10.48550/arXiv.2409.04035v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SELF-SUPERVISED CONTRASTIVE LEARNING FOR VIDEOS USING DIFFERENTIABLE LOCAL ALIGNMENT\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RuleExplorer: A Scalable Matrix Visualization for Understanding Tree Ensemble Classifiers\n",
      "构造的DOI: 10.48550/arXiv.2409.03164v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Non-target Divergence Hypothesis: Toward Understanding Domain Gaps in Cross-Modal Knowledge Distillation\n",
      "构造的DOI: 10.48550/arXiv.2409.02438v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified Framework with Consistency across Modalities for Human Activity Recognition\n",
      "构造的DOI: 10.48550/arXiv.2409.02385v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification\n",
      "构造的DOI: 10.48550/arXiv.2102.02051v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Student Actions in Classroom Scenes: New Dataset and Baseline\n",
      "构造的DOI: 10.48550/arXiv.2304.03897v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open-Vocabulary Action Localization with Iterative Visual Prompting\n",
      "构造的DOI: 10.48550/arXiv.2402.07872v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Box2Flow: Instance-based Action Flow Graphs from Videos\n",
      "构造的DOI: 10.48550/arXiv.2409.00295v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Prediction-Feedback DETR for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2408.16729v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-Temporal Context Prompting for Zero-Shot Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2408.15996v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Online pre-training with long-form videos\n",
      "构造的DOI: 10.48550/arXiv.1802.08091v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Completeness: A Generalizable Action Proposal Generator for Zero-Shot Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2408.13777v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FMI-TAL: Few-shot Multiple Instances Temporal Action Localization by Probability Distribution Learning and Interval Cluster Refinement\n",
      "构造的DOI: 10.48550/arXiv.2408.13765v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long-term Pre-traning for Temporal Action Detection with Transformers\n",
      "构造的DOI: 10.48550/arXiv.2302.03561v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models\n",
      "构造的DOI: 10.48550/arXiv.2408.11318v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Just a Hint: Point-Supervised Camouflaged Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2308.06701v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2408.08066v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary-Recovering Network for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Domain-invariant Representation Learning via Segment Anything Model for Blood Cell Classification\n",
      "构造的DOI: 10.48550/arXiv.2408.07467v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dynamic and Compressive Adaptation of Transformers From Images to Videos\n",
      "构造的DOI: 10.48550/arXiv.2408.06840v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2408.05955v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2408.06437v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts\n",
      "构造的DOI: 10.48550/arXiv.2408.05905v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EFFICIENT TEST-TIME PROMPT TUNING FOR VISION-LANGUAGE MODELS\n",
      "构造的DOI: 10.48550/arXiv.2410.08020v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: JARViS: Detecting Actions in Video Using Unified Actor-Scene Context Relation Modeling\n",
      "构造的DOI: 10.48550/arXiv.2408.03612v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Online Temporal Action Localization with Memory-Augmented Transformer\n",
      "构造的DOI: 10.48550/arXiv.2408.02957v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition\n",
      "构造的DOI: 10.48550/arXiv.2408.02623v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DisCoM-KD: Cross-Modal Knowledge Distillation via Disentanglement Representation and Adversarial Learning\n",
      "构造的DOI: 10.48550/arXiv.2408.07080v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Faster Diffusion Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2408.02024v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Source EEG Emotion Recognition via Dynamic Contrastive Domain Adaptation\n",
      "DOI: 10.1016/j.bspc.2024.107337\n",
      "\n",
      "处理标题: Segment Anything for Videos: A Systematic Survey\n",
      "构造的DOI: 10.48550/arXiv.2211.11956v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SpotFormer: Multi-Scale Spatio-Temporal Transformer for Facial Expression Spotting\n",
      "构造的DOI: 10.48550/arXiv.2407.20799v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Classification Matters: Improving Video Action Detection with Class-Specific Attention\n",
      "构造的DOI: 10.48550/arXiv.2407.19698v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DIFFUSION FEEDBACK HELPS CLIP SEE BETTER\n",
      "构造的DOI: 10.48550/arXiv.2407.20171v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Inverse Partial Optimal Transport Framework for Music-guided Movie Trailer Generation\n",
      "构造的DOI: 10.48550/arXiv.2407.19456v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment\n",
      "构造的DOI: 10.48550/arXiv.1904.04346v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos\n",
      "构造的DOI: 10.48550/arXiv.2407.18289v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Trajectory-aligned Space-time Tokens for Few-shot Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2305.02673v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Harnessing Temporal Causality for Advanced Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2407.17792v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semi-Supervised Pipe Video Temporal Defect Interval Localization\n",
      "DOI: 10.1111/mice.13403\n",
      "\n",
      "处理标题: Dynamic Color Assignment for Hierarchical Data\n",
      "构造的DOI: 10.48550/arXiv.2407.14742v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Enhancing Temporal Action Localization: Advanced S6 Modeling with Recurrent Mechanism\n",
      "构造的DOI: 10.48550/arXiv.2407.13078v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos\n",
      "构造的DOI: 10.48550/arXiv.2407.12987v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking the Architecture Design for Efficient Generic Event Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2407.12622v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects\n",
      "构造的DOI: 10.48550/arXiv.2407.12371v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Causality-inspired Discriminative Feature Learning in Triple Domains for Gait Recognition\n",
      "构造的DOI: 10.48550/arXiv.2407.12519v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation\n",
      "构造的DOI: 10.48550/arXiv.2407.11954v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporally Grounding Instructional Diagrams in Unconstrained Videos\n",
      "构造的DOI: 10.48550/arXiv.2407.12066v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models\n",
      "构造的DOI: 10.48550/arXiv.2305.17331v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PAFUSE: Part-based Diffusion for 3D Whole-Body Pose Estimation\n",
      "构造的DOI: 10.48550/arXiv.2401.03914v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open Vocabulary Multi-Label Video Classification\n",
      "构造的DOI: 10.48550/arXiv.2407.09073v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images\n",
      "构造的DOI: 10.48550/arXiv.2407.09694v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semi-Supervised Object Detection: A Survey on Progress from CNN to Transformer\n",
      "DOI: 10.1109/JBHI.2023.3348436\n",
      "\n",
      "处理标题: Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2407.08971v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2404.13311v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2407.08150v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2106.13014v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OneDiff: A Generalist Model for Image Difference Captioning\n",
      "构造的DOI: 10.48550/arXiv.2407.05645v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool\n",
      "构造的DOI: 10.48550/arXiv.2407.05355v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MMAD: Multi-label Micro-Action Detection in Videos\n",
      "DOI: 10.1109/PERVASIVE.2015.7087093\n",
      "\n",
      "处理标题: SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding\n",
      "构造的DOI: 10.48550/arXiv.2407.05118v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained Dynamic Network for Generic Event Boundary Detection\n",
      "DOI: 10.1609/aaai.v39i12.33445\n",
      "\n",
      "处理标题: Micro-gesture Online Recognition using Learnable Query Points\n",
      "构造的DOI: 10.48550/arXiv.2407.04490v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation\n",
      "构造的DOI: 10.48550/arXiv.2407.04603v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DyFADet: Dynamic Feature Aggregation for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2407.03197v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2010.09803v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VFIMamba: Video Frame Interpolation with State Space Models\n",
      "构造的DOI: 10.48550/arXiv.2111.13817v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: The Solution for Temporal Sound Localisation Task of ICCV 1st Perception Test Challenge 2023\n",
      "构造的DOI: 10.48550/arXiv.2407.02318v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Referring Atomic Video Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2407.01872v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tarsier: Recipes for Training and Evaluating Large Video Description Models\n",
      "构造的DOI: 10.48550/arXiv.2407.00634v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GIM: A Million-scale Benchmark for Generative Image Manipulation Detection and Localization\n",
      "构造的DOI: 10.48550/arXiv.2504.18361v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MVOC: a training-free multiple video object composition method with diffusion models\n",
      "构造的DOI: 10.48550/arXiv.2406.15829v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open-Vocabulary Temporal Action Localization using Multimodal Guidance\n",
      "构造的DOI: 10.48550/arXiv.2406.15556v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2503.24008v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions\n",
      "构造的DOI: 10.48550/arXiv.2406.14367v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised Multi-actor Social Activity Understanding in Streaming Videos\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ViLCo-Bench: VIdeo Language COntinual learning Benchmark\n",
      "构造的DOI: 10.48550/arXiv.2406.11816v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PrAViC: Probabilistic Adaptation Framework for Real-Time Video Classification\n",
      "构造的DOI: 10.48550/arXiv.2406.11443v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM\n",
      "构造的DOI: 10.48550/arXiv.2406.12235v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FCA-RAC: First Cycle Annotated Repetitive Action Counting\n",
      "构造的DOI: 10.48550/arXiv.2406.12178v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-Supervised Representation Learning with Spatial-Temporal Consistency for Sign Language Recognition\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding\n",
      "构造的DOI: 10.48550/arXiv.2406.08877v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance\n",
      "构造的DOI: 10.48550/arXiv.2406.08294v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YOLOv12 to Its Genesis: A Decadal and Comprehensive Review of The You Only Look Once (YOLO) Series\n",
      "构造的DOI: 10.48550/arXiv.2406.19407v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MEMSVD: LONG-RANGE TEMPORAL STRUCTURE CAPTURING USING INCREMENTAL SVD\n",
      "DOI: 10.1145/3269206.3271682\n",
      "\n",
      "处理标题: Teaching with Uncertainty: Unleashing the Potential of Knowledge Distillation in Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2406.06999v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AutoTVG: A New Vision-language Pre-training Paradigm for Temporal Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2406.07091v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Effective-Efficient Approach for Dense Multi-Label Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2406.06187v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Incomplete Multi-label Learning: Recent Advances and Future Trends\n",
      "构造的DOI: 10.48550/arXiv.1311.3144v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SMC++: Masked Learning of Unsupervised Video Semantic Compression\n",
      "构造的DOI: 10.48550/arXiv.2406.04765v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SMART: Scene-motion-aware human action recognition framework for mental disorder group\n",
      "构造的DOI: 10.48550/arXiv.2406.04649v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GrootVL: Tree Topology is All You Need in State Space Model\n",
      "构造的DOI: 10.48550/arXiv.2406.02395v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object Aware Egocentric Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2406.01079v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Advancing Weakly-Supervised Audio-Visual Video Parsing via Segment-wise Pseudo Labeling\n",
      "构造的DOI: 10.48550/arXiv.2406.00919v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HENASY: Learning to Assemble Scene-Entities for Egocentric Video-Language Model\n",
      "构造的DOI: 10.48550/arXiv.2408.03567v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Diversifying Query: Region-Guided Transformer for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2212.13163v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MALT: Multi-scale Action Learning Transformer for Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2006.03732v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YotoR-You Only Transform One Representation\n",
      "构造的DOI: 10.48550/arXiv.2206.11057v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LOOKING BACKWARD: STREAMING VIDEO-TO-VIDEO TRANSLATION WITH FEATURE BANKS\n",
      "构造的DOI: 10.48550/arXiv.2405.15757v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views\n",
      "构造的DOI: 10.48550/arXiv.2405.13659v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: What Makes Good Few-shot Examples for Vision-Language Models\n",
      "构造的DOI: 10.48550/arXiv.2406.03666v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Text-Video Retrieval with Global-Local Semantic Consistent Learning\n",
      "DOI: 10.1109/TGRS.2021.3060705\n",
      "\n",
      "处理标题: Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition\n",
      "构造的DOI: 10.48550/arXiv.2504.09156v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open-Vocabulary Spatio-Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1807.00486v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2405.08458v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: No Time to Waste: Squeeze Time into Channel for Mobile Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2405.08344v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection\n",
      "DOI: 10.1109/TPAMI.2024.3377192\n",
      "\n",
      "处理标题: FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment\n",
      "构造的DOI: 10.48550/arXiv.2405.06887v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning\n",
      "构造的DOI: 10.48550/arXiv.2405.04533v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation\n",
      "构造的DOI: 10.48550/arXiv.2405.04405v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-view Action Recognition via Directed Gromov-Wasserstein Discrepancy\n",
      "构造的DOI: 10.48550/arXiv.2405.01337v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features\n",
      "构造的DOI: 10.48550/arXiv.2404.19542v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SFMVIT: SLOWFAST MEET VIT IN CHAOTIC WORLD\n",
      "构造的DOI: 10.48550/arXiv.2404.16609v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2012.03457v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TEST-TIME ADAPTATION FOR COMBATING MISSING MODALITIES IN EGOCENTRIC VIDEOS\n",
      "构造的DOI: 10.48550/arXiv.2410.08020v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network\n",
      "构造的DOI: 10.48550/arXiv.2303.13763v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: STAT: Towards Generalizable Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2404.13311v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DeepLocalization: Using change point detection for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2404.12258v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CorrNet+: Sign Language Recognition and Translation via Spatial-Temporal Correlation\n",
      "构造的DOI: 10.48550/arXiv.2503.03287v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mixed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2404.10717v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unifying Global and Local Scene Entities Modelling for Precise Action Spotting\n",
      "构造的DOI: 10.48550/arXiv.2404.09951v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Clothes-Changing Person Re-Identification with Feasibility-Aware Intermediary Matching\n",
      "构造的DOI: 10.48550/arXiv.2404.09507v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: STMixer: A One-Stage Sparse Action Detector\n",
      "构造的DOI: 10.48550/arXiv.2404.09842v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection\n",
      "构造的DOI: 10.48550/arXiv.2404.09263v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: IDD-X: A Multi-View Dataset for Ego-relative Important Object Localization and Explanation in Dense and Unstructured Traffic\n",
      "构造的DOI: 10.48550/arXiv.2404.08561v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Test-Time Zero-Shot Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2410.08020v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LOCALIZING MOMENTS OF ACTIONS IN UNTRIMMED VIDEOS OF INFANTS WITH AUTISM SPECTRUM DISORDER\n",
      "构造的DOI: 10.48550/arXiv.2404.05849v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TIM: A Time Interval Machine for Audio-Visual Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2404.05559v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2404.04933v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Koala: Key frame-conditioned long video-LLM\n",
      "DOI: 10.1109/TIT.2014.2317312\n",
      "\n",
      "处理标题: UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization\n",
      "构造的DOI: 10.48550/arXiv.2404.03179v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression\n",
      "构造的DOI: 10.48550/arXiv.2404.02405v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SnAG: Scalable and Accurate Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2404.02257v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Precise and Robust Sidewalk Detection: Leveraging Ensemble Learning to Surpass LLM Limitations in Urban Environments\n",
      "构造的DOI: 10.48550/arXiv.2405.14876v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Language Model Guided Interpretable Video Action Reasoning\n",
      "构造的DOI: 10.48550/arXiv.2404.01591v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2309.09060v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action Detection via an Image Diffusion Process\n",
      "构造的DOI: 10.48550/arXiv.2404.01051v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoDistill: Language-aware Visual Semantic Distillation for Video Question Answering\n",
      "构造的DOI: 10.48550/arXiv.2403.06679v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Streaming Dense Video Captioning\n",
      "构造的DOI: 10.48550/arXiv.2404.01297v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dual DETRs for Multi-Label Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2404.00653v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions\n",
      "构造的DOI: 10.48550/arXiv.2403.20254v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CROSS-ATTENTION IS NOT ALWAYS NEEDED: DYNAMIC CROSS-ATTENTION FOR AUDIO-VISUAL DIMENSIONAL EMOTION RECOGNITION\n",
      "构造的DOI: 10.48550/arXiv.1311.6007v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PLOT-TAL - Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization\n",
      "DOI: 10.1007/s11263-023-01846-2\n",
      "\n",
      "处理标题: Multi-scale Unified Network for Image Classification\n",
      "构造的DOI: 10.48550/arXiv.1911.09389v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OmniViD: A Generative Framework for Universal Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2405.03770v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Every Shot Counts: Using Exemplars for Repetition Counting in Videos\n",
      "构造的DOI: 10.48550/arXiv.2403.18074v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Invertible Diffusion Models for Compressed Sensing\n",
      "构造的DOI: 10.48550/arXiv.2403.17006v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CHAIN-OF-ACTION: FAITHFUL AND MULTIMODAL QUESTION ANSWERING THROUGH LARGE LANGUAGE MODELS\n",
      "构造的DOI: 10.48550/arXiv.2403.17359v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding\n",
      "构造的DOI: 10.48550/arXiv.2311.18445v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object Detectors in the Open Environment: Challenges, Solutions, and Outlook\n",
      "DOI: 10.1142/S0217751X05021683\n",
      "\n",
      "处理标题: Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey\n",
      "构造的DOI: 10.48550/arXiv.2205.00299v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting\n",
      "构造的DOI: 10.48550/arXiv.2403.14240v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2403.14174v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2406.11303v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting\n",
      "构造的DOI: 10.48550/arXiv.2403.11959v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting\n",
      "DOI: 10.1109/TPAMI.2024.3490777\n",
      "\n",
      "处理标题: Boosting Semi-Supervised Temporal Action Localization by Learning from Non-Target Classes\n",
      "构造的DOI: 10.48550/arXiv.1610.02483v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Audio-Visual Segmentation via Unlabeled Frame Exploitation\n",
      "构造的DOI: 10.48550/arXiv.2403.11074v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Computer User Interface Understanding. A New Dataset and a Learning Framework\n",
      "构造的DOI: 10.48550/arXiv.2403.10170v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning\n",
      "DOI: 10.1109/TIP.2024.3372469\n",
      "\n",
      "处理标题: Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2403.09626v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Training\n",
      "构造的DOI: 10.48550/arXiv.2212.11078v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoMamba: State Space Model for Efficient Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2403.06977v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Density-Guided Label Smoothing for Temporal Localization of Driving Actions\n",
      "DOI: 10.1109/CVPRW56347.2022.00358\n",
      "\n",
      "处理标题: GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2403.06154v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Benchmarking Micro-action Recognition: Dataset, Methods, and Applications\n",
      "DOI: 10.1007/978-3-031-31435-3_7\n",
      "\n",
      "处理标题: Dynamic Cross Attention for Audio-Visual Person Verification\n",
      "构造的DOI: 10.48550/arXiv.2403.04661v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives\n",
      "构造的DOI: 10.48550/arXiv.2212.06301v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spectrum AUC Difference (SAUCD): Human-aligned 3D Shape Evaluation\n",
      "DOI: 10.1145/3072959.3073608\n",
      "\n",
      "处理标题: Efficient Action Counting with Dynamic Queries\n",
      "构造的DOI: 10.48550/arXiv.2403.01543v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YOLO-MED: MULTI-TASK INTERACTION NETWORK FOR BIOMEDICAL IMAGES\n",
      "构造的DOI: 10.48550/arXiv.2406.12925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deployment Prior Injection for Run-time Calibratable Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2402.17207v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis\n",
      "构造的DOI: 10.48550/arXiv.2402.15952v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoPrism: A Foundational Visual Encoder for Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2402.13217v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: 3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection\n",
      "构造的DOI: 10.48550/arXiv.2402.12128v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos\n",
      "构造的DOI: 10.48550/arXiv.2402.11057v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization\n",
      "构造的DOI: 10.48550/arXiv.2402.08249v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Editing for Video Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2402.02335v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multimodal Action Quality Assessment\n",
      "DOI: 10.1109/TIP.2024.3362135\n",
      "\n",
      "处理标题: Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model\n",
      "构造的DOI: 10.48550/arXiv.2401.16280v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Computer Vision for Primate Behavior Analysis in the Wild\n",
      "构造的DOI: 10.48550/arXiv.2401.16424v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-model learning by sequential reading of untrimmed videos for action recognition\n",
      "构造的DOI: 10.48550/arXiv.2401.14675v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2401.11654v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring Missing Modality in Multimodal Egocentric Datasets\n",
      "构造的DOI: 10.48550/arXiv.2401.11470v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping\n",
      "构造的DOI: 10.48550/arXiv.2401.08787v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning and Uncertainty Estimation\n",
      "构造的DOI: 10.48550/arXiv.2401.07271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition\n",
      "构造的DOI: 10.48550/arXiv.2401.07061v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NODI: Out-Of-Distribution Detection with Noise from Diffusion\n",
      "构造的DOI: 10.48550/arXiv.2308.00303v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge\n",
      "构造的DOI: 10.48550/arXiv.2401.06659v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Online Continuous Sign Language Recognition and Translation\n",
      "构造的DOI: 10.48550/arXiv.2401.05336v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning\n",
      "构造的DOI: 10.48550/arXiv.2401.04105v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TWO-STREAM JOINT MATCHING BASED ON CONTRASTIVE LEARNING FOR FEW-SHOT ACTION RECOGNITION\n",
      "构造的DOI: 10.48550/arXiv.2401.04150v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Text-Driven Traffic Anomaly Detection with Temporal High-Frequency Modeling in Driving Videos\n",
      "构造的DOI: 10.48550/arXiv.2401.03522v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Detours for Navigating Instructional Videos\n",
      "构造的DOI: 10.48550/arXiv.2401.01823v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports\n",
      "构造的DOI: 10.48550/arXiv.2401.01505v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering\n",
      "构造的DOI: 10.48550/arXiv.1905.13540v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Glance and Focus: Memory Prompting for Multi-Event Video Question Answering\n",
      "构造的DOI: 10.48550/arXiv.2205.01652v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Holistic Autonomous Driving Understanding by Bird’s-Eye-View Injected Multi-Modal Large Models\n",
      "构造的DOI: 10.48550/arXiv.2401.00988v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hyperspectral Image Denoising via Spatial-Spectral Recurrent Transformer\n",
      "DOI: 10.1109/TGRS.2024.3374953\n",
      "\n",
      "处理标题: Multiscale Vision Transformers meet Bipartite Matching for efficient single-stage Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2312.17686v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bridging Modality Gap for Visual Grounding with Effecitve Cross-modal Distillation\n",
      "构造的DOI: 10.48550/arXiv.2312.17648v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Multi-axis Representation in Frequency Domain for Medical Image Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2306.07783v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Simple LLM Framework for Long-Range Video Question-Answering\n",
      "构造的DOI: 10.48550/arXiv.2505.05467v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TOWARDS ROBUST MULTIMODAL PROMPTING WITH MISSING MODALITIES\n",
      "构造的DOI: 10.48550/arXiv.2312.15890v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TinySAM: Pushing the Envelope for Efficient Segment Anything Model\n",
      "构造的DOI: 10.48550/arXiv.2312.13789v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Perception Test 2023: A Summary of the First Challenge And Outcome\n",
      "构造的DOI: 10.48550/arXiv.2208.13265v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SADA: Semantic adversarial unsupervised domain adaptation for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2312.13377v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Collaborative Weakly Supervised Video Correlation Learning for Procedure-Aware Instructional Video Analysis\n",
      "构造的DOI: 10.48550/arXiv.2312.11024v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Generative Model-based Feature Knowledge Distillation for Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2312.08644v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semi-supervised Active Learning for Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised Temporal Action Localization via Self-paced Incremental Learning\n",
      "构造的DOI: 10.48550/arXiv.2312.07384v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models\n",
      "构造的DOI: 10.48550/arXiv.2305.17331v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-modal Knowledge Transfer\n",
      "构造的DOI: 10.48550/arXiv.2312.07378v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatiotemporal Event Graphs for Dynamic Scene Understanding\n",
      "构造的DOI: 10.48550/arXiv.2312.07621v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Enhancing Single-Frame Supervision for Better Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2312.05178v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding\n",
      "构造的DOI: 10.48550/arXiv.2007.10937v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Low-power, Continuous Remote Behavioral Localization with Event Cameras\n",
      "构造的DOI: 10.48550/arXiv.2312.03799v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RotaTR: Detection Transformer for Dense and Rotated Object\n",
      "构造的DOI: 10.48550/arXiv.2312.02821v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model\n",
      "构造的DOI: 10.48550/arXiv.2312.02483v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2312.01764v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Adapting Short-Term Transformers for Action Detection in Untrimmed Videos\n",
      "构造的DOI: 10.48550/arXiv.2312.01897v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network\n",
      "构造的DOI: 10.48550/arXiv.2312.02224v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent Video Internet of Things\n",
      "构造的DOI: 10.48550/arXiv.2312.00401v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2109.08333v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos\n",
      "构造的DOI: 10.48550/arXiv.2212.13163v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2107.07089v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Weakly Supervised End-to-end Learning for Long-video Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.1910.11285v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object-based (yet Class-agnostic) Video Domain Adaptation\n",
      "构造的DOI: 10.48550/arXiv.2208.05187v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Scalable Label Distribution Learning for Multi-Label Classification\n",
      "构造的DOI: 10.48550/arXiv.2311.16556v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames\n",
      "构造的DOI: 10.48550/arXiv.2311.17241v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Localization for Inertial-based Human Activity Recognition\n",
      "DOI: 10.1145/3206025.3206029\n",
      "\n",
      "处理标题: Efficient Pre-training for Localized Instruction Generation of Procedural Videos\n",
      "构造的DOI: 10.48550/arXiv.2311.15964v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ADM-Loc: Actionness Distribution Modeling for Point-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2311.15916v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models\n",
      "构造的DOI: 10.48550/arXiv.2311.16103v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection\n",
      "构造的DOI: 10.48550/arXiv.2311.16464v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Centre Stage: Centricity-based Audio-Visual Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\n",
      "构造的DOI: 10.48550/arXiv.2101.01456v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation\n",
      "构造的DOI: 10.48550/arXiv.2303.16635v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring Time Granularity on Temporal Graphs for Dynamic Link Prediction in Real-world Networks\n",
      "构造的DOI: 10.48550/arXiv.2311.12255v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SURGPLAN: SURGICAL PHASE LOCALIZATION NETWORK FOR PHASE RECOGNITION\n",
      "构造的DOI: 10.48550/arXiv.2311.09965v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open-Vocabulary Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2404.04986v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Meta-Adapter: An Online Few-shot Learner for Vision-Language Model\n",
      "构造的DOI: 10.48550/arXiv.2206.02742v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos\n",
      "构造的DOI: 10.48550/arXiv.2311.03550v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols\n",
      "构造的DOI: 10.48550/arXiv.2008.10774v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MODELING THE UNCERTAINTY WITH MAXIMUM DISCREPANT STUDENTS FOR SEMI-SUPERVISED 2D POSE ESTIMATION\n",
      "构造的DOI: 10.48550/arXiv.2311.01770v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2311.00729v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mask Propagation for Efficient Video Semantic Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2310.18954v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning\n",
      "构造的DOI: 10.48550/arXiv.2310.17177v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Temporal Sentence Grounding From Narrated EgoVideos\n",
      "构造的DOI: 10.48550/arXiv.2310.17395v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Hybrid Graph Network for Complex Activity Detection in Video\n",
      "构造的DOI: 10.48550/arXiv.2310.17493v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FloCoDe: Unbiased Dynamic Scene Graph Generation with Temporal Consistency and Correlation Debiasing\n",
      "构造的DOI: 10.48550/arXiv.2310.16073v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2110.04820v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding\n",
      "构造的DOI: 10.48550/arXiv.2310.13347v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Query-aware Long Video Localization and Relation Discrimination for Deep Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2310.12724v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Video Diffusion Models\n",
      "构造的DOI: 10.48550/arXiv.2405.03150v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: On the Relevance of Temporal Features for Medical Ultrasound Video Recognition\n",
      "DOI: 10.1007/978-3-031-43895-0_70\n",
      "\n",
      "处理标题: Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model\n",
      "构造的DOI: 10.48550/arXiv.2502.13447v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary Discretization and Reliable Classification Network for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2310.06403v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation\n",
      "构造的DOI: 10.48550/arXiv.2310.05720v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Proposal-based Temporal Action Localization with Point-level Supervision\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval\n",
      "DOI: 10.1109/ICCV51070.2023.01249\n",
      "\n",
      "处理标题: Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization\n",
      "DOI: 10.1109/CVPRW59228.2023.00576\n",
      "\n",
      "处理标题: A Grammatical Compositional Model for Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2310.02887v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video\n",
      "构造的DOI: 10.48550/arXiv.2310.01324v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NEUCORE: Neural Concept Reasoning for Composed Image Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2310.01358v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Training a Large Video Model on a Single Machine in a Day\n",
      "构造的DOI: 10.48550/arXiv.1904.12843v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention\n",
      "构造的DOI: 10.48550/arXiv.2309.16309v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-End Streaming Video Temporal Action Segmentation with Reinforcement Learning\n",
      "构造的DOI: 10.48550/arXiv.2309.15683v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Local Compressed Video Stream Learning for Generic Event Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2309.15431v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ENIGMA-51: Towards a Fine-Grained Understanding of Human-Object Interactions in Industrial Scenarios\n",
      "构造的DOI: 10.48550/arXiv.2309.14809v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VidChapters-7M: Video Chapters at Scale\n",
      "构造的DOI: 10.48550/arXiv.2309.13952v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Automatic Animation of Hair Blowing in Still Portrait Photos\n",
      "构造的DOI: 10.48550/arXiv.2309.14207v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Chop & Learn: Recognizing and Generating Object-State Compositions\n",
      "构造的DOI: 10.48550/arXiv.1811.04433v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary-Aware Proposal Generation Method for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1806.02964v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SURVEY OF ACTION RECOGNITION, SPOTTING AND SPATIO-TEMPORAL LOCALIZATION IN SOCCER - CURRENT TRENDS AND RESEARCH PERSPECTIVES\n",
      "未找到标题为 'SURVEY OF ACTION RECOGNITION, SPOTTING AND SPATIO-TEMPORAL LOCALIZATION IN SOCCER - CURRENT TRENDS AND RESEARCH PERSPECTIVES' 的文章。\n",
      "\n",
      "处理标题: SkeleTR: Towrads Skeleton-based Action Recognition in the Wild\n",
      "构造的DOI: 10.48550/arXiv.2309.11445v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2309.11569v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2309.09060v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Human Action Representations from Temporal Context in Lifestyle Vlogs\n",
      "DOI: 10.18653/v1/P19-1643\n",
      "\n",
      "处理标题: JOADAA: joint online action detection and action anticipation\n",
      "构造的DOI: 10.48550/arXiv.2309.06130v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Localization with Enhanced Instant Discriminability\n",
      "构造的DOI: 10.48550/arXiv.2309.05590v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization\n",
      "构造的DOI: 10.48550/arXiv.2309.01331v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AAN: Attributes-Aware Network for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Human trajectory prediction using LSTM with Attention mechanism\n",
      "构造的DOI: 10.48550/arXiv.2309.00331v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation\n",
      "构造的DOI: 10.48550/arXiv.2309.00096v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DiffusionVMR: Diffusion Model for Joint Video Moment Retrieval and Highlight Detection\n",
      "构造的DOI: 10.48550/arXiv.2308.15109v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization\n",
      "DOI: 10.1145/3581783.3613767\n",
      "\n",
      "处理标题: Joint Gaze-Location and Gaze-Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2006.14937v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Computation-efficient Deep Learning for Computer Vision: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Prompting Visual-Language Models for Dynamic Facial Expression Recognition\n",
      "构造的DOI: 10.48550/arXiv.2308.13382v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation\n",
      "构造的DOI: 10.48550/arXiv.2308.12608v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2308.12609v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models\n",
      "构造的DOI: 10.48550/arXiv.2308.13082v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods\n",
      "构造的DOI: 10.48550/arXiv.2204.04360v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection\n",
      "DOI: 10.1109/LSP.2022.3226411\n",
      "\n",
      "处理标题: How Much Temporal Long-Term Context is Needed for Action Segmentation?\n",
      "构造的DOI: 10.48550/arXiv.2308.11358v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UnLoc: A Unified Framework for Video Localization Tasks\n",
      "构造的DOI: 10.48550/arXiv.2403.17935v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-Feedback DETR for Temporal Action Detection\n",
      "DOI: 10.1103/PhysRevE.94.042226\n",
      "\n",
      "处理标题: Weakly-Supervised Action Localization by Hierarchically-structured Latent Attention Modeling\n",
      "构造的DOI: 10.48550/arXiv.2112.04684v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatial-Temporal Alignment Network for Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.1907.09021v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long-range Multimodal Pretraining for Movie Understanding\n",
      "构造的DOI: 10.48550/arXiv.2308.09775v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Audio-Visual Glance Network for Efficient Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.2308.09322v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progression-Guided Temporal Action Detection in Videos\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Event-Guided Procedure Planning from Instructional Videos with Text Supervision\n",
      "构造的DOI: 10.48550/arXiv.2308.08885v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DSFNet: Dual-GCN and Location-fused Self-attention with Weighted Fast Normalized Fusion for Polyps Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2308.07946v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Helping Hands: An Object-Aware Ego-Centric Video Recognition Model\n",
      "构造的DOI: 10.48550/arXiv.2504.05040v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Memory-and-Anticipation Transformer for Online Action Understanding\n",
      "构造的DOI: 10.48550/arXiv.2308.07893v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Survey on video anomaly detection in dynamic scenes with moving cameras\n",
      "构造的DOI: 10.48550/arXiv.2308.07050v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Sentence Grounding in Streaming Videos\n",
      "DOI: 10.1145/3581783.3612120\n",
      "\n",
      "处理标题: DiffSED: Sound Event Detection with Denoising Diffusion\n",
      "构造的DOI: 10.48550/arXiv.2308.07293v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ViGT: Proposal-free Video Grounding with Learnable Token in Transformer\n",
      "DOI: 10.1007/s11432-022-3783-3\n",
      "\n",
      "处理标题: Temporally-Adaptive Models for Efficient Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2403.09626v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Is there progress in activity progress prediction?\n",
      "构造的DOI: 10.48550/arXiv.2308.05533v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization\n",
      "构造的DOI: 10.48550/arXiv.2308.05648v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PAT: Position-Aware Transformer for Dense Multi-Label Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2406.06187v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation\n",
      "构造的DOI: 10.48550/arXiv.2308.04549v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Improving Scene Graph Generation with Superpixel-Based Interaction Learning\n",
      "构造的DOI: 10.48550/arXiv.2308.02339v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Deep Learning-based Spatio-temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoPro: A Visual Analytics Approach for Interactive Video Programming\n",
      "DOI: 10.1109/TVCG.2023.3326586\n",
      "\n",
      "处理标题: Towards Imbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation\n",
      "DOI: 10.1007/s11432-023-4030-y\n",
      "\n",
      "处理标题: UniVTG: Towards Unified Video-Language Temporal Grounding\n",
      "构造的DOI: 10.48550/arXiv.2307.16715v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2008.01432v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory\n",
      "构造的DOI: 10.48550/arXiv.2307.14277v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: KEFA: A Knowledge Enhanced and Fine-grained Aligned Speaker for Navigation Instruction Generation\n",
      "构造的DOI: 10.48550/arXiv.2307.13368v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities\n",
      "构造的DOI: 10.48550/arXiv.2307.12128v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor-agnostic Multi-label Action Recognition with Multi-modal Query\n",
      "构造的DOI: 10.48550/arXiv.2409.18408v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization\n",
      "构造的DOI: 10.48550/arXiv.1506.02588v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone\n",
      "构造的DOI: 10.48550/arXiv.2212.06301v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition\n",
      "构造的DOI: 10.48550/arXiv.2307.05541v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoGLUE: Video General Understanding Evaluation of Foundation Models\n",
      "构造的DOI: 10.48550/arXiv.2503.14378v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NMS Threshold matters for Ego4D Moment Queries - 2nd place solution to the Ego4D Moment Queries Challenge 2023\n",
      "构造的DOI: 10.48550/arXiv.2211.09558v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multimodal Imbalance-Aware Gradient Modulation for Weakly-supervised Audio-Visual Video Parsing\n",
      "构造的DOI: 10.48550/arXiv.2307.02041v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast Segment Anything\n",
      "构造的DOI: 10.48550/arXiv.2306.12156v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relating tSNE and UMAP to Classical Dimensionality Reduction\n",
      "构造的DOI: 10.48550/arXiv.2306.11898v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition\n",
      "构造的DOI: 10.48550/arXiv.2306.11546v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Granularity Hand Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2306.10858v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action Sensitivity Learning for the Ego4D Episodic Memory Challenge 2023\n",
      "构造的DOI: 10.48550/arXiv.2306.09172v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Long Form Audio-visual Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2106.11310v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PaReprop: Fast Parallelized Reversible Backpropagation\n",
      "构造的DOI: 10.48550/arXiv.2306.09342v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: E2E-LOAD: End-to-End Long-form Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2006.03732v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Video Moment Localization\n",
      "构造的DOI: 10.48550/arXiv.2306.07515v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised information extraction from inscrutable handwritten document images\n",
      "构造的DOI: 10.48550/arXiv.2306.06823v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos\n",
      "构造的DOI: 10.48550/arXiv.2112.14958v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Optimizing ViViT Training: Time and Memory Reduction for Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2306.04822v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Background-aware Moment Detection for Video Moment Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2306.02728v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoComposer: Compositional Video Synthesis with Motion Controllability\n",
      "构造的DOI: 10.48550/arXiv.2306.02018v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding\n",
      "构造的DOI: 10.48550/arXiv.2306.00576v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Multi-Modal Transformer Network for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction\n",
      "构造的DOI: 10.48550/arXiv.2211.09066v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic Space\n",
      "构造的DOI: 10.48550/arXiv.2409.19252v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Z-GMOT: Zero-shot Generic Multiple Object Tracking\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Proposal-based Multiple Instance Learning for Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1807.02800v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-label Video Classification for Underwater Ship Inspection\n",
      "构造的DOI: 10.48550/arXiv.2305.17338v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mindstorms in Natural Language-Based Societies of Mind\n",
      "构造的DOI: 10.48550/arXiv.2305.17066v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action Sensitivity Learning for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2305.15701v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SoundSieve: Seconds-Long Audio Event Recognition on Intermittently-Powered Systems\n",
      "构造的DOI: 10.48550/arXiv.2306.09126v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep Neural Networks in Video Human Action Recognition: A Review\n",
      "构造的DOI: 10.48550/arXiv.0912.3953v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Faster Video Moment Retrieval with Point-Level Supervision\n",
      "构造的DOI: 10.48550/arXiv.2305.14017v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Real-Time Idling Vehicles Detection Using Combined Audio-Visual Deep Learning\n",
      "构造的DOI: 10.48550/arXiv.2412.10892v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Lightweight Delivery Detection on Doorbell Cameras\n",
      "构造的DOI: 10.48550/arXiv.2305.07812v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language\n",
      "构造的DOI: 10.48550/arXiv.2305.05662v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2205.09956v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Category-Oriented Representation Learning for Image to Multi-Modal Retrieval\n",
      "构造的DOI: 10.48550/arXiv.2309.13885v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Transform-Equivariant Consistency Learning for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2308.04197v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency\n",
      "构造的DOI: 10.48550/arXiv.2305.02734v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Modelling Spatio-Temporal Interactions For Compositional Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2305.02673v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Glitch in the Matrix: A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization\n",
      "构造的DOI: 10.48550/arXiv.2305.01979v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boosting Weakly-Supervised Temporal Action Localization with Text Information\n",
      "构造的DOI: 10.48550/arXiv.2305.00607v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer\n",
      "构造的DOI: 10.48550/arXiv.2412.09513v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense Hybrid Proposal Modulation for Lane Detection\n",
      "构造的DOI: 10.48550/arXiv.2304.14874v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SeeHow: Workflow Extraction from Programming Screencasts through Action-Aware Video Analytics\n",
      "构造的DOI: 10.48550/arXiv.2304.14042v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Temporal Action Localization with Bidirectional Semantic Consistency Constraint\n",
      "构造的DOI: 10.48550/arXiv.2304.12616v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MRSN: MULTI-RELATION SUPPORT NETWORK FOR VIDEO ACTION DETECTION\n",
      "构造的DOI: 10.48550/arXiv.2310.03377v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-End Spatio-Temporal Action Localisation with Video Transformers\n",
      "构造的DOI: 10.48550/arXiv.2304.12160v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Large-capacity and Flexible Video Steganography via Invertible Neural Network\n",
      "构造的DOI: 10.48550/arXiv.2304.12300v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multidimensional Uncertainty Quantification for Deep Neural Networks\n",
      "构造的DOI: 10.48550/arXiv.2304.10527v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ACTIVITY CLASSIFICATION USING UNSUPERVISED DOMAIN TRANSFER FROM BODY WORN SENSORS\n",
      "构造的DOI: 10.48550/arXiv.2304.10643v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Implicit Temporal Modeling with Learnable Alignment for Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.2304.10465v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progressive Visual Prompt Learning with Contrastive Feature Re-formation\n",
      "构造的DOI: 10.48550/arXiv.2304.08386v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Efficient Video Action Detection with Token Dropout and Context Refinement\n",
      "构造的DOI: 10.48550/arXiv.2304.08451v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels\n",
      "构造的DOI: 10.48550/arXiv.2304.07978v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DeepSegmenter: Temporal Action Localization for Detecting Anomalies in Untrimmed Naturalistic Driving Videos\n",
      "构造的DOI: 10.48550/arXiv.2304.08261v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications\n",
      "DOI: 10.1007/s11633-023-1385-0\n",
      "\n",
      "处理标题: WEAR: An Outdoor Sports Dataset for Wearable and Egocentric Activity Recognition\n",
      "构造的DOI: 10.48550/arXiv.2304.05088v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary-Denoising for Video Activity Localization\n",
      "构造的DOI: 10.48550/arXiv.1904.09936v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FedBEVT: Federated Learning Bird’s Eye View Perception Transformer in Road Traffic Systems\n",
      "DOI: 10.1109/TIV.2023.3310674\n",
      "\n",
      "处理标题: Unbiased Scene Graph Generation in Videos\n",
      "构造的DOI: 10.48550/arXiv.2304.00733v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2008.00188v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: On the Benefits of 3D Pose and Tracking for Human Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2304.01199v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DOAD: Decoupled One Stage Action Detection Network\n",
      "构造的DOI: 10.48550/arXiv.2304.00254v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: JCDNet: Joint of Common and Definite phases Network for Weakly Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2303.17294v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2303.17285v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions\n",
      "构造的DOI: 10.48550/arXiv.2303.16990v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking\n",
      "构造的DOI: 10.48550/arXiv.2303.16727v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: STRUCTURED VIDEO-LANGUAGE MODELING WITH TEMPORAL GROUPING AND SPATIAL GROUNDING\n",
      "构造的DOI: 10.48550/arXiv.2303.16341v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video\n",
      "构造的DOI: 10.48550/arXiv.2303.16053v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CycleACR: Cycle Modeling of Actor-Context Relations for Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1904.07846v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling\n",
      "构造的DOI: 10.48550/arXiv.2303.15270v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion\n",
      "构造的DOI: 10.48550/arXiv.2303.14863v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Scalable Neural Representation for Diverse Videos\n",
      "构造的DOI: 10.48550/arXiv.2303.14124v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature\n",
      "构造的DOI: 10.48550/arXiv.2303.12332v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline\n",
      "构造的DOI: 10.48550/arXiv.1903.10412v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ANEDL: Adaptive Negative Evidential Deep Learning for Open-Set Semi-supervised Learning\n",
      "构造的DOI: 10.48550/arXiv.2303.12091v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BoxSnake: Polygonal Instance Segmentation with Box Supervision\n",
      "构造的DOI: 10.48550/arXiv.2303.11630v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-modal Prompting for Low-Shot Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Decomposed Prototype Learning for Few-Shot Scene Graph Generation\n",
      "构造的DOI: 10.48550/arXiv.2303.10863v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization\n",
      "构造的DOI: 10.48550/arXiv.1611.00201v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery\n",
      "构造的DOI: 10.48550/arXiv.2303.09813v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Co-Occurrence Matters: Learning Action Relation for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1912.03612v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2303.09055v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Align and Attend: Multimodal Summarization with Dual Contrastive Losses\n",
      "构造的DOI: 10.48550/arXiv.2303.07284v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TriDet: Temporal Action Detection with Relative Boundary Modeling\n",
      "构造的DOI: 10.48550/arXiv.2309.05590v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos\n",
      "构造的DOI: 10.48550/arXiv.2303.06378v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Diverse Temporal Grounding under Single Positive Labels\n",
      "构造的DOI: 10.48550/arXiv.2303.06545v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TAEC: Unsupervised Action Segmentation with Temporal-Aware Embedding and Clustering\n",
      "构造的DOI: 10.48550/arXiv.2303.05166v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Faster Learning of Temporal Action Proposal via Sparse Multilevel Boundary Generator\n",
      "构造的DOI: 10.48550/arXiv.2303.03166v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Improving Audio-Visual Video Parsing with Pseudo Visual Labels\n",
      "构造的DOI: 10.48550/arXiv.2303.02344v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Uncertainty Estimation by Fisher Information-based Evidential Deep Learning\n",
      "构造的DOI: 10.48550/arXiv.2303.02045v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open Set Action Recognition via Multi-Label Evidential Learning\n",
      "构造的DOI: 10.48550/arXiv.2303.12698v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning\n",
      "构造的DOI: 10.48550/arXiv.1801.06267v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Localizing Moments in Long Video Via Multimodal Guidance\n",
      "构造的DOI: 10.48550/arXiv.2302.13372v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Segment Transformer for Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2302.13074v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Constraint and Union for Partially-Supervised Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2302.09850v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MINOTAUR: Multi-task Video Grounding From Multimodal Queries\n",
      "构造的DOI: 10.48550/arXiv.2406.12925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2302.06848v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2302.05160v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Reversible Vision Transformers\n",
      "构造的DOI: 10.48550/arXiv.2302.04869v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WEAKLY-SUPERVISED REPRESENTATION LEARNING FOR VIDEO ALIGNMENT AND ANALYSIS\n",
      "构造的DOI: 10.48550/arXiv.2409.04607v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer\n",
      "构造的DOI: 10.48550/arXiv.2302.03548v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EPIC-SOUNDS: A Large-Scale Dataset of Actions that Sound\n",
      "构造的DOI: 10.48550/arXiv.1903.10412v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Skeleton-based Human Action Recognition via Convolutional Neural Networks (CNN)\n",
      "构造的DOI: 10.48550/arXiv.2301.13360v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Champion Solution for the WSDM2023 Toloka VQA Challenge\n",
      "构造的DOI: 10.48550/arXiv.2301.09045v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Source-free Subject Adaptation for EEG-based Visual Recognition\n",
      "构造的DOI: 10.48550/arXiv.2202.02901v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Perceiving Video-Language Pre-training\n",
      "构造的DOI: 10.48550/arXiv.1912.03612v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploiting Auxiliary Caption for Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2301.05997v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Generic Event Boundary Detection in Video with Pyramid Features\n",
      "构造的DOI: 10.48550/arXiv.2301.04288v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dynamic Grained Encoder for Vision Transformers\n",
      "构造的DOI: 10.48550/arXiv.2301.03831v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network\n",
      "构造的DOI: 10.48550/arXiv.2001.00335v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2301.01970v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Ego-Only: Egocentric Action Detection without Exocentric Transferring\n",
      "构造的DOI: 10.48550/arXiv.2301.01380v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An End-to-End Multi-Scale Network for Action Prediction in Videos\n",
      "构造的DOI: 10.48550/arXiv.1907.08895v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features\n",
      "构造的DOI: 10.48550/arXiv.2007.06866v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2212.11078v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2212.09335v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries\n",
      "构造的DOI: 10.48550/arXiv.2212.06969v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contextual Explainable Video Representation: Human Perception-based Understanding\n",
      "构造的DOI: 10.48550/arXiv.2503.16502v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dilation-Erosion for Single-Frame Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1807.02800v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Egocentric Video Task Translation\n",
      "构造的DOI: 10.48550/arXiv.2212.06301v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CLIP-TSA: CLIP-ASSISTED TEMPORAL SELF-ATTENTION FOR WEAKLY-SUPERVISED VIDEO ANOMALY DETECTION\n",
      "构造的DOI: 10.48550/arXiv.2006.09654v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection\n",
      "构造的DOI: 10.48550/arXiv.2212.04090v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data\n",
      "构造的DOI: 10.48550/arXiv.2406.12925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: InternVideo: General Video Foundation Models via Generative and Discriminative Learning\n",
      "构造的DOI: 10.48550/arXiv.2212.03191v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs\n",
      "构造的DOI: 10.48550/arXiv.2406.12925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Kinematic-aware Hierarchical Attention Network for Human Pose Estimation in Videos\n",
      "构造的DOI: 10.48550/arXiv.2211.15868v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Modal Few-Shot Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Post-Processing Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2401.14625v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2211.14053v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Task Learning of Object State Changes from Uncurated Videos\n",
      "构造的DOI: 10.48550/arXiv.2406.12925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground\n",
      "构造的DOI: 10.48550/arXiv.2211.12883v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TransVCL: Attention-enhanced Video Copy Localization Network with Flexible Supervision\n",
      "构造的DOI: 10.48550/arXiv.2211.13090v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SVFormer: Semi-supervised Video Transformer for Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2407.12805v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FORECASTING UNOBSERVED NODE STATES WITH SPATIO-TEMPORAL GRAPH NEURAL NETWORKS\n",
      "构造的DOI: 10.48550/arXiv.2211.11596v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Slow Motion Matters: A Slow Motion Enhanced Network for Weakly Supervised Temporal Action Localization\n",
      "DOI: 10.1109/TCSVT.2022.3201540\n",
      "\n",
      "处理标题: A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset\n",
      "构造的DOI: 10.48550/arXiv.2211.10624v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contrastive Positive Sample Propagation along the Audio-Visual Event Line\n",
      "构造的DOI: 10.48550/arXiv.2211.09980v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ReLER@ZJU Submission to the Ego4D Moment Queries Challenge 2022\n",
      "构造的DOI: 10.48550/arXiv.2211.09558v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Where a Strong Backbone Meets Strong Features - ActionFormer for Ego4D Moment Queries Challenge\n",
      "构造的DOI: 10.48550/arXiv.2306.09172v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring State Change Capture of Heterogeneous Backbones @ Ego4D Hands and Objects Challenge 2022\n",
      "构造的DOI: 10.48550/arXiv.2211.08728v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Simple Transformer-Based Model for Ego4D Natural Language Queries Challenge\n",
      "构造的DOI: 10.48550/arXiv.2211.09529v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Clinically Plausible Pathology-Anatomy Disentanglement in Patient Brain MRI with Structured Variational Priors\n",
      "构造的DOI: 10.48550/arXiv.2211.07820v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks\n",
      "构造的DOI: 10.48550/arXiv.2211.06023v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PRIOR-ENHANCED TEMPORAL ACTION LOCALIZATION USING SUBJECT-AWARE SPATIAL ATTENTION\n",
      "DOI: 10.1007/s11263-022-01649-x\n",
      "\n",
      "处理标题: Dynamic loss balancing and sequential enhancement for road-safety assessment and traffic scene classification\n",
      "DOI: 10.1109/TITS.2024.3456214\n",
      "\n",
      "处理标题: SimOn: A Simple Framework for Online Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2211.04905v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Facial Tic Detection in Untrimmed Videos of Tourette Syndrome Patients\n",
      "DOI: 10.1109/ICPR56361.2022.9956140\n",
      "\n",
      "处理标题: Bringing Online Egocentric Action Recognition into the wild\n",
      "DOI: 10.1109/LRA.2023.3251843\n",
      "\n",
      "处理标题: Distill and Collect for Semi-Supervised Temporal Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2211.01311v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild\n",
      "构造的DOI: 10.48550/arXiv.2211.00794v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LONGSHORTNET: EXPLORING TEMPORAL AND SEMANTIC FEATURES FUSION IN STREAMING PERCEPTION\n",
      "构造的DOI: 10.48550/arXiv.2210.15518v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Refining Action Boundaries for One-stage Detection\n",
      "构造的DOI: 10.48550/arXiv.2210.14284v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Holistic Interaction Transformer Network for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2210.12686v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YOWO-Plus: An Incremental Improvement\n",
      "构造的DOI: 10.48550/arXiv.1804.02767v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cyclical Self-Supervision for Semi-Supervised Ejection Fraction Prediction from Echocardiogram Videos\n",
      "构造的DOI: 10.48550/arXiv.2210.11291v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points\n",
      "构造的DOI: 10.48550/arXiv.2210.11035v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking Learning Approaches for Long-Term Action Anticipation\n",
      "构造的DOI: 10.48550/arXiv.2210.11566v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Segmentation: An Analysis of Modern Techniques\n",
      "构造的DOI: 10.48550/arXiv.2212.00505v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SEMANTIC VIDEO MOMENTS RETRIEVAL AT SCALE: A NEW TASK AND A BASELINE\n",
      "构造的DOI: 10.48550/arXiv.2211.04872v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LAD: A Hybrid Deep Learning System for Benign Paroxysmal Positional Vertigo Disorders Diagnostic\n",
      "构造的DOI: 10.48550/arXiv.2210.08282v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation\n",
      "构造的DOI: 10.48550/arXiv.2302.06195v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LINEAR VIDEO TRANSFORMER WITH FEATURE FIXATION\n",
      "构造的DOI: 10.48550/arXiv.2210.08164v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Masked Motion Encoding for Self-Supervised Video Representation Learning\n",
      "构造的DOI: 10.48550/arXiv.2305.14344v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Robust Action Segmentation from Timestamp Supervision\n",
      "构造的DOI: 10.48550/arXiv.2210.06501v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning\n",
      "构造的DOI: 10.48550/arXiv.2212.00986v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LiveSeg: Unsupervised Multimodal Temporal Segmentation of Long Livestream Videos\n",
      "构造的DOI: 10.48550/arXiv.2210.05840v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization\n",
      "DOI: 10.1109/TMM.2023.3324498\n",
      "\n",
      "处理标题: Motion Aware Self-Supervision for Generic Event Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2012.11717v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Referring Expression Comprehension via Transformer with Content-aware Query\n",
      "构造的DOI: 10.48550/arXiv.2310.16402v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SoccerNet 2022 Challenges Results\n",
      "DOI: 10.1145/3552437.3558545\n",
      "\n",
      "处理标题: AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation\n",
      "构造的DOI: 10.48550/arXiv.2210.02578v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ROAD-R: The Autonomous Driving Dataset with Logical Requirements\n",
      "DOI: 10.1007/s10994-023-06322-z\n",
      "\n",
      "处理标题: An In-depth Study of Stochastic Backpropagation\n",
      "构造的DOI: 10.48550/arXiv.2210.00129v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Streaming Video Temporal Action Segmentation In Real Time\n",
      "构造的DOI: 10.48550/arXiv.2209.13808v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoSpeed-Net: Forecasting Speed-Control in Driver Behavior from Egocentric Video Data\n",
      "DOI: 10.1145/3557915.3560946\n",
      "\n",
      "处理标题: A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective\n",
      "构造的DOI: 10.48550/arXiv.1806.04237v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2111.04321v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-modal Video Chapter Generation\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hand Hygiene Assessment via Joint Step Segmentation and Key Action Scorer\n",
      "构造的DOI: 10.48550/arXiv.2209.12221v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding\n",
      "构造的DOI: 10.48550/arXiv.2209.10918v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks\n",
      "构造的DOI: 10.48550/arXiv.2504.09574v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RGB-Event Fusion for Moving Object Detection in Autonomous Driving\n",
      "构造的DOI: 10.48550/arXiv.2108.03004v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SDFE-LV: A Large-Scale, Multi-Source, and Unconstrained Database for Spotting Dynamic Facial Expressions in Long Videos\n",
      "构造的DOI: 10.48550/arXiv.2209.08445v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action Segmentation in Videos\n",
      "构造的DOI: 10.48550/arXiv.2408.00365v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Frame-Subtitle Self-Supervision for Multi-Modal Video Question Answering\n",
      "DOI: 10.1145/3077136.3080655\n",
      "\n",
      "处理标题: Spatio-Temporal Action Detection Under Large Motion\n",
      "构造的DOI: 10.48550/arXiv.2209.02250v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Real-Time Cattle Interaction Recognition via Triple-stream Network\n",
      "构造的DOI: 10.48550/arXiv.2209.02241v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation\n",
      "构造的DOI: 10.48550/arXiv.2209.00638v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical Local-Global Transformer for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2208.14882v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Active Learning with Effective Scoring Functions for Semi-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2208.14856v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Circular Window-based Cascade Transformer for Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2208.14209v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Survey: Exploiting Data Redundancy for Optimization of Deep Learning\n",
      "构造的DOI: 10.48550/arXiv.2208.13363v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor-identified Spatiotemporal Action Detection - Detecting Who Is Doing What in Videos\n",
      "构造的DOI: 10.48550/arXiv.2003.04865v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Adaptive Perception Transformer for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2208.11908v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream\n",
      "构造的DOI: 10.48550/arXiv.2208.12673v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Review on Action Recognition for Accident Detection in Smart City Transportation Systems\n",
      "构造的DOI: 10.48550/arXiv.2307.12128v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion Prediction\n",
      "构造的DOI: 10.48550/arXiv.2501.16551v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Localization with Multi-temporal Scales\n",
      "构造的DOI: 10.48550/arXiv.2208.07493v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Semantic Correspondence with Sparse Annotations\n",
      "构造的DOI: 10.48550/arXiv.2208.06974v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Automated Key-Point Detection in Images with Partial Pool View\n",
      "DOI: 10.1145/3552437.3555705\n",
      "\n",
      "处理标题: Generative Action Description Prompts for Skeleton-based Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2208.05318v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring Anchor-based Detection for Ego4D Natural Language Query\n",
      "构造的DOI: 10.48550/arXiv.2208.05375v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Consistency-based Self-supervised Learning for Temporal Anomaly Localization\n",
      "DOI: 10.1109/LRA.2021.3058909\n",
      "\n",
      "处理标题: Exploiting Shape Cues for Weakly Supervised Semantic Segmentation\n",
      "DOI: 10.1016/j.patcog.2022.108953\n",
      "\n",
      "处理标题: Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos\n",
      "构造的DOI: 10.48550/arXiv.2208.01954v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Two-Stream Transformer Architecture for Long Form Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2106.11310v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition\n",
      "DOI: 10.1587/transinf.2022EDP7138\n",
      "\n",
      "处理标题: Reducing the Vision and Language Bias for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2207.13457v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: P2ANet: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos\n",
      "构造的DOI: 10.48550/arXiv.2207.12730v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering\n",
      "构造的DOI: 10.48550/arXiv.2207.12647v8 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions\n",
      "构造的DOI: 10.48550/arXiv.2207.11805v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DBQ-SSD: DYNAMIC BALL QUERY FOR EFFICIENT 3D OBJECT DETECTION\n",
      "构造的DOI: 10.48550/arXiv.2207.10909v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EgoEnv: Human-centric environment representations from egocentric video\n",
      "构造的DOI: 10.48550/arXiv.2212.04636v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Efficient Spatio-Temporal Pyramid Transformer for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2207.10448v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Correspondence Matters for Video Referring Expression Comprehension\n",
      "构造的DOI: 10.48550/arXiv.2207.10400v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LocVTP: Video-Text Pre-training for Temporal Localization\n",
      "构造的DOI: 10.48550/arXiv.2411.09849v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning\n",
      "构造的DOI: 10.48550/arXiv.2207.10158v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers\n",
      "构造的DOI: 10.48550/arXiv.2207.09662v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AN EFFICIENT FRAMEWORK FOR FEW-SHOT SKELETON-BASED TEMPORAL ACTION SEGMENTATION\n",
      "构造的DOI: 10.48550/arXiv.2207.09925v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Is an Object-Centric Video Representation Beneficial for Transfer?\n",
      "构造的DOI: 10.48550/arXiv.2207.10075v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2207.10137v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spotting Temporally Precise, Fine-Grained Events in Video\n",
      "构造的DOI: 10.48550/arXiv.2207.10213v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semantic Novelty Detection via Relational Reasoning\n",
      "构造的DOI: 10.48550/arXiv.2207.08699v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unifying Event Detection and Captioning as Sequence Generation via Pre-Training\n",
      "构造的DOI: 10.48550/arXiv.2207.08625v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zero-Shot Temporal Action Detection via Vision-Language Prompting\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: You Should Look at All Objects\n",
      "构造的DOI: 10.48550/arXiv.2207.07889v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SVGRAPH: LEARNING SEMANTIC GRAPHS FROM INSTRUCTIONAL VIDEOS\n",
      "构造的DOI: 10.48550/arXiv.2207.08001v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semi-Supervised Temporal Action Detection with Proposal-Free Masking\n",
      "构造的DOI: 10.48550/arXiv.2207.06580v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ReAct: Temporal Action Detection with Relational Queries\n",
      "构造的DOI: 10.48550/arXiv.2407.17792v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2207.06659v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Point-to-Box Network for Accurate Object Detection via Single Point Supervision\n",
      "构造的DOI: 10.48550/arXiv.2207.06827v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning\n",
      "构造的DOI: 10.48550/arXiv.2207.06580v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Entry-Flipped Transformer for Inference and Prediction of Participant Behavior\n",
      "构造的DOI: 10.48550/arXiv.2207.06235v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained Activities of People Worldwide\n",
      "构造的DOI: 10.48550/arXiv.2410.05323v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Human-centric Spatio-Temporal Video Grounding via the Combination of Mutual Matching Network and TubeDETR\n",
      "构造的DOI: 10.48550/arXiv.2207.04201v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Beyond Transfer Learning: Co-finetuning for Action Localisation\n",
      "构造的DOI: 10.48550/arXiv.2208.06281v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Dialog as Conversation about Objects Living in Space-Time\n",
      "构造的DOI: 10.48550/arXiv.2207.03656v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Federated Self-supervised Learning for Video Understanding\n",
      "构造的DOI: 10.48550/arXiv.2207.01975v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MVP: Robust Multi-View Practice for Driving Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2207.02042v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A LARGE-SCALE ROBUSTNESS ANALYSIS OF VIDEO ACTION RECOGNITION MODELS\n",
      "构造的DOI: 10.48550/arXiv.1806.11230v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OS-MSL: One Stage Multimodal Sequential Link Framework for Scene Segmentation and Classification\n",
      "构造的DOI: 10.48550/arXiv.2207.01241v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Egocentric Video-Language Pretraining @ Ego4D Challenge 2022\n",
      "构造的DOI: 10.48550/arXiv.2206.01670v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: How Far Can I Go ?: A Self-Supervised Approach for Deterministic Video Depth Forecasting\n",
      "构造的DOI: 10.48550/arXiv.2207.00506v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Timestamp-Supervised Action Segmentation with Graph Convolutional Networks\n",
      "DOI: 10.1109/TETC.2022.3230912\n",
      "\n",
      "处理标题: Exploring Temporally Dynamic Data Augmentation for Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.2206.15015v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Submission to Generic Event Boundary Detection Challenge@CVPR 2022: Local Context Modeling and Global Boundary Decoding Approach\n",
      "构造的DOI: 10.48550/arXiv.2206.15268v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RankSEG: A Consistent Ranking-based Framework for Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2311.18537v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning\n",
      "构造的DOI: 10.48550/arXiv.2305.02176v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Programmatic Concept Learning for Human Motion Description and Synthesis\n",
      "构造的DOI: 10.48550/arXiv.2206.13502v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Activity Localisation with Uncertainties in Temporal Boundary\n",
      "构造的DOI: 10.48550/arXiv.2206.12923v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2206.11493v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space\n",
      "构造的DOI: 10.48550/arXiv.2206.11895v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning\n",
      "构造的DOI: 10.48550/arXiv.2107.12618v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification\n",
      "构造的DOI: 10.48550/arXiv.2203.12081v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline\n",
      "构造的DOI: 10.48550/arXiv.2207.09639v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Probing Visual-Audio Representation for Video Highlight Detection via Hard-Pairs Guided Contrastive Learning\n",
      "构造的DOI: 10.48550/arXiv.2206.10157v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation\n",
      "构造的DOI: 10.48550/arXiv.2206.10095v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware Proposal Network for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action Spotting using Dense Detection Anchors Revisited: Submission to the SoccerNet Challenge 2022\n",
      "构造的DOI: 10.48550/arXiv.2206.07846v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks\n",
      "构造的DOI: 10.48550/arXiv.2206.06637v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TadML: A fast temporal action detection with Mechanics-MLP\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector\n",
      "构造的DOI: 10.48550/arXiv.2206.03064v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Structured Context Transformer for Generic Event Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2206.02985v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video-based Human-Object Interaction Detection from Tubelet Tokens\n",
      "构造的DOI: 10.48550/arXiv.2206.01908v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Egocentric Video-Language Pretraining\n",
      "构造的DOI: 10.48550/arXiv.2206.01670v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications\n",
      "DOI: 10.1007/978-3-031-31435-3_7\n",
      "\n",
      "处理标题: Future Transformer for Long-term Action Anticipation\n",
      "构造的DOI: 10.48550/arXiv.2205.14022v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Do we really need temporal convolutions in action segmentation?\n",
      "构造的DOI: 10.48550/arXiv.2205.13425v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Architecture Self-supervised Video Representation Learning\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Social Interpretable Tree for Pedestrian Trajectory Prediction\n",
      "构造的DOI: 10.48550/arXiv.2205.13296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos\n",
      "DOI: 10.1145/3477495.3532083\n",
      "\n",
      "处理标题: TEMPORALLY PRECISE ACTION SPOTTING in SOCCER VIDEOS USING DENSE DETECTION ANCHORS\n",
      "构造的DOI: 10.48550/arXiv.2205.10450v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Structured Attention Composition for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2205.09956v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action parsing using context features\n",
      "DOI: 10.1109/DICTA.2017.8227399\n",
      "\n",
      "处理标题: Masked Autoencoders As Spatiotemporal Learners\n",
      "构造的DOI: 10.48550/arXiv.2205.09113v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ETAD: Training Action Detection End to End on a Laptop\n",
      "构造的DOI: 10.48550/arXiv.2205.07134v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-Supervised Masking for Unsupervised Anomaly Detection and Localization\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Action Detection Guided by Audio Narration\n",
      "构造的DOI: 10.48550/arXiv.2205.05895v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2205.02717v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CONVEX COMBINATION CONSISTENCY BETWEEN NEIGHBORS FOR WEAKLY-SUPERVISED ACTION LOCALIZATION\n",
      "构造的DOI: 10.48550/arXiv.2205.00400v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RADNet: A Deep Neural Network Model for Robust Perception in Moving Autonomous Systems\n",
      "构造的DOI: 10.48550/arXiv.2205.00364v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos\n",
      "构造的DOI: 10.48550/arXiv.2204.13548v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hybrid Relation Guided Set Matching for Few-shot Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2204.13423v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contrastive Language-Action Pre-training for Temporal Localization\n",
      "构造的DOI: 10.48550/arXiv.2203.03978v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Estimation of Reliable Proposal Quality for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2204.11695v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing\n",
      "构造的DOI: 10.48550/arXiv.2412.19563v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Multi-Person Video Dataset Annotation Method of Spatio-Temporally Actions\n",
      "构造的DOI: 10.48550/arXiv.2204.10160v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Moment Retrieval from Text Queries via Single Frame Annotation\n",
      "DOI: 10.1145/3477495.3532078\n",
      "\n",
      "处理标题: End-to-end Dense Video Captioning as Sequence Generation\n",
      "构造的DOI: 10.48550/arXiv.2204.08121v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Action Detection: Analysing Limitations and Challenges\n",
      "构造的DOI: 10.48550/arXiv.2203.10960v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization\n",
      "构造的DOI: 10.48550/arXiv.2204.06228v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Position-aware Location Regression Network for Temporal Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2104.05606v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: E2TAD: An Energy-Efficient Tracking-based Action Detector\n",
      "构造的DOI: 10.48550/arXiv.2210.11219v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Probabilistic Representations for Video Contrastive Learning\n",
      "构造的DOI: 10.48550/arXiv.2204.03946v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Frequency Selective Augmentation for Video Representation Learning\n",
      "构造的DOI: 10.48550/arXiv.2204.03865v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MHMS: Multimodal Hierarchical Multimedia Summarization\n",
      "构造的DOI: 10.48550/arXiv.2204.03734v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment\n",
      "构造的DOI: 10.48550/arXiv.2307.02730v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MM-SEAL: A Large-scale Video Dataset of Multi-person Multi-grained Spatio-temporally Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2105.11107v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network\n",
      "构造的DOI: 10.48550/arXiv.2204.02674v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Alignment Networks for Long-term Video\n",
      "构造的DOI: 10.48550/arXiv.1904.07846v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Empirical Study of End-to-End Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2204.02932v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TallFormer: Temporal Action Localization with a Long-memory Transformer\n",
      "构造的DOI: 10.48550/arXiv.2208.07493v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2410.05323v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fair Contrastive Learning for Facial Attribute Classification\n",
      "构造的DOI: 10.48550/arXiv.2203.16209v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models\n",
      "构造的DOI: 10.48550/arXiv.2203.16755v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2308.11358v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning\n",
      "构造的DOI: 10.48550/arXiv.2203.14957v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-End Active Speaker Detection\n",
      "构造的DOI: 10.48550/arXiv.2411.13849v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos\n",
      "构造的DOI: 10.48550/arXiv.2203.14104v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised Pre-training for Temporal Action Localization Tasks\n",
      "构造的DOI: 10.48550/arXiv.2203.13609v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs\n",
      "构造的DOI: 10.48550/arXiv.2203.12344v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos\n",
      "构造的DOI: 10.48550/arXiv.2203.11637v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Point3D: tracking actions as moving points with 3D CNNs\n",
      "构造的DOI: 10.48550/arXiv.2203.10584v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LocATe: End-to-end Localization of Actions in 3D with Transformers\n",
      "构造的DOI: 10.48550/arXiv.2203.10719v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation\n",
      "构造的DOI: 10.48550/arXiv.1806.02964v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: On the Pitfalls of Batch Normalization for End-to-End Video Learning: A Study on Surgical Workflow Analysis\n",
      "构造的DOI: 10.48550/arXiv.2406.14576v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RCL: Recurrent Continuous Localization for Temporal Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2203.07112v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: EventFormer: AU Event Transformer for Facial Action Unit Event Detection\n",
      "构造的DOI: 10.48550/arXiv.2203.06355v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OpenTAL: Towards Open Set Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2203.05114v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Universal Prototype Transport for Zero-Shot Action Recognition and Localization\n",
      "DOI: 10.1007/s11263-023-01846-2\n",
      "\n",
      "处理标题: End-to-End Semi-Supervised Learning for Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2203.03838v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Panoramic Human Activity Recognition\n",
      "构造的DOI: 10.48550/arXiv.2203.03806v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PAMI-AD: AN ACTIVITY DETECTOR EXPLOITING PART-ATTENTION AND MOTION INFORMATION IN SURVEILLANCE VIDEOS\n",
      "未找到标题为 'AN ACTIVITY DETECTOR EXPLOITING PART-ATTENTION AND MOTION INFORMATION IN SURVEILLANCE VIDEOS' 的文章。\n",
      "\n",
      "处理标题: Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2201.00457v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation\n",
      "构造的DOI: 10.48550/arXiv.2203.02925v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SegTAD: Precise Temporal Action Detection via Semantic Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2203.01542v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Colar: Effective and Efficient Online Action Detection by Consulting Exemplars\n",
      "构造的DOI: 10.48550/arXiv.2203.01057v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing\n",
      "构造的DOI: 10.48550/arXiv.2203.00387v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2203.00307v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video\n",
      "构造的DOI: 10.48550/arXiv.2203.00859v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An End-to-End Transformer Model for Crowd Localization\n",
      "构造的DOI: 10.48550/arXiv.2202.13065v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs\n",
      "DOI: 10.1145/3495211\n",
      "\n",
      "处理标题: Less is More: Surgical Phase Recognition from Timestamp Supervision\n",
      "构造的DOI: 10.48550/arXiv.2202.08199v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ActionFormer: Localizing Moments of Actions with Transformers\n",
      "构造的DOI: 10.48550/arXiv.2008.13705v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Generic Self-Supervised Framework of Learning Invariant Discriminative Features\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos\n",
      "构造的DOI: 10.48550/arXiv.2202.04947v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint-bone Fusion Graph Convolutional Network for Semi-supervised Skeleton Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2109.12946v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Untrimmed Action Anticipation\n",
      "构造的DOI: 10.48550/arXiv.2202.04132v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Recent Trends in 2D Object Detection and Applications in Video Event Recognition\n",
      "构造的DOI: 10.48550/arXiv.2202.03206v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Explore-And-Match: Bridging Proposal-Based and Proposal-Free With Transformer for Sentence Grounding in Videos\n",
      "构造的DOI: 10.48550/arXiv.2201.10168v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rich Action-semantic Consistent Knowledge for Early Action Prediction\n",
      "构造的DOI: 10.48550/arXiv.2201.09169v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LTC-GIF: Attracting More Clicks on Feature-length Sports Videos\n",
      "构造的DOI: 10.48550/arXiv.2201.09077v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning\n",
      "构造的DOI: 10.48550/arXiv.2201.08051v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.2401.04023v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Sentence Grounding in Videos: A Survey and Future Directions\n",
      "构造的DOI: 10.48550/arXiv.1805.09018v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CONTINUAL TRANSFORMERS: REDUNDANCY-FREE ATTENTION FOR ONLINE INFERENCE\n",
      "构造的DOI: 10.48550/arXiv.2212.11385v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Network Level Spatial Temporal Traffic State Forecasting with Hierarchical-Attention-LSTM (HierAttnLSTM)\n",
      "DOI: 10.48130/dts-0024-0021\n",
      "\n",
      "处理标题: Unsupervised Temporal Video Grounding with Deep Semantic Clustering\n",
      "构造的DOI: 10.48550/arXiv.2201.05307v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hand-Object Interaction Reasoning\n",
      "构造的DOI: 10.48550/arXiv.2408.16331v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Smart director: An event-driven directing system for live broadcasting\n",
      "构造的DOI: 10.48550/arXiv.2201.04024v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning from Synthetic InSAR with Vision Transformers: The case of volcanic unrest detection\n",
      "构造的DOI: 10.48550/arXiv.2201.03016v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TVNet: Temporal Voting Network for Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2201.00434v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Memory-Guided Semantic Learning Network for Temporal Sentence Grounding\n",
      "构造的DOI: 10.48550/arXiv.2106.16136v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Background-aware classification activation map for weakly supervised object localization\n",
      "构造的DOI: 10.48550/arXiv.2207.07818v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Joint Modelling Based on Hierarchical Transformer for Co-summarization\n",
      "DOI: 10.1109/TPAMI.2022.3186506\n",
      "\n",
      "处理标题: Fine-grained Multi-Modal Self-Supervised Learning\n",
      "构造的DOI: 10.48550/arXiv.2410.05323v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ACGNet: Action complement graph network forweakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2112.10977v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UnweaveNet: Unweaving Activity Stories\n",
      "构造的DOI: 10.48550/arXiv.2112.10194v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks\n",
      "构造的DOI: 10.48550/arXiv.2112.09791v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Masked Feature Prediction for Self-Supervised Visual Pre-Training\n",
      "构造的DOI: 10.48550/arXiv.2305.14344v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action proposal generation with background constraint\n",
      "构造的DOI: 10.48550/arXiv.2112.07984v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SeqFormer: Sequential Transformer for Video Instance Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2112.08275v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SVIP: Sequence VerIfication for Procedures in Videos\n",
      "构造的DOI: 10.48550/arXiv.2112.06447v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Anomaly Crossing: New Horizons for Video Anomaly Detection as Cross-domain Few-shot Learning\n",
      "构造的DOI: 10.48550/arXiv.2112.06320v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection\n",
      "构造的DOI: 10.48550/arXiv.2112.04771v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Hierarchical Spatio-Temporal Graph Convolutional Neural Network for anomaly detection in videos\n",
      "DOI: 10.1109/TCSVT.2021.3134410\n",
      "\n",
      "处理标题: Exploring temporal granularity in self-supervised video representation learning\n",
      "构造的DOI: 10.48550/arXiv.2112.04480v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Prompting Visual-Language Models for Efficient Video Understanding\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: DCAN: Improving temporal action detection via dual context aggregation\n",
      "构造的DOI: 10.48550/arXiv.2112.03612v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GaTector: A Unified Framework for Gaze Object Prediction\n",
      "构造的DOI: 10.48550/arXiv.2112.03549v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation\n",
      "构造的DOI: 10.48550/arXiv.2112.04011v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2112.03902v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Regularity learning via explicit distribution modeling for skeletal video anomaly detection\n",
      "构造的DOI: 10.48550/arXiv.2112.03649v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MViTv2: Improved Multiscale Vision Transformers for Classification and Detection\n",
      "构造的DOI: 10.48550/arXiv.2112.01526v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Iterative frame-level representation learning and classification for semi-supervised temporal action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2112.01402v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph convolutional module for temporal action localization in videos\n",
      "构造的DOI: 10.48550/arXiv.2110.05904v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions\n",
      "构造的DOI: 10.48550/arXiv.2112.00431v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UBoCo: Unsupervised boundary contrastive learning for generic event boundary detection\n",
      "构造的DOI: 10.48550/arXiv.2109.15170v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-guided Self-supervised Pretraining for Temporal Activity Detection\n",
      "构造的DOI: 10.48550/arXiv.2111.13675v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SWAT: Spatial Structure Within and Among Tokens\n",
      "构造的DOI: 10.48550/arXiv.2111.13677v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Background-click supervision for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1807.02800v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MM-Pyramid: Multimodal Pyramid Attentional Network for Audio-Visual Event Localization and Video Parsing\n",
      "构造的DOI: 10.48550/arXiv.2111.12374v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer\n",
      "构造的DOI: 10.48550/arXiv.2111.12082v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense uncertainty estimation via an ensemble-based conditional latent variable model\n",
      "构造的DOI: 10.48550/arXiv.2110.06427v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Generalized Visual Odometry Using Position-Aware Optical Flow and Geometric Bundle Adjustment\n",
      "DOI: 10.1016/j.patcog.2022.109262\n",
      "\n",
      "处理标题: Exploring Segment-level Semantics for Online Phase Recognition from Surgical Videos\n",
      "构造的DOI: 10.48550/arXiv.2111.11044v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised dense action anticipation\n",
      "构造的DOI: 10.48550/arXiv.2309.06130v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised action localization crop in video retargeting for 3D ConvNets\n",
      "构造的DOI: 10.48550/arXiv.2111.07426v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action recognition using transfer learning and majority voting for CSGO\n",
      "DOI: 10.1109/ICTS52701.2021.9608407\n",
      "\n",
      "处理标题: KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1903.09868v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-scale 2D representation learning for weakly-supervised moment retrieval\n",
      "构造的DOI: 10.48550/arXiv.1912.03590v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A critical study on the recent deep learning based semi-supervised video anomaly detection methods\n",
      "构造的DOI: 10.48550/arXiv.2111.01604v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting spatio-temporal layouts for compositional action recognition\n",
      "构造的DOI: 10.48550/arXiv.2111.01936v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: With a little help from my temporal context: Multimodal egocentric action recognition\n",
      "构造的DOI: 10.48550/arXiv.2207.07077v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical deep residual reasoning for temporal moment localization\n",
      "DOI: 10.1145/3469877.3490595\n",
      "\n",
      "处理标题: Domain adaptation in multi-view embedding for cross-modal video retrieval\n",
      "构造的DOI: 10.48550/arXiv.2110.12812v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-denoising neural networks for few shot learning\n",
      "构造的DOI: 10.48550/arXiv.2009.09172v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AEI: Actors-environment interaction with adaptive attention for temporal action proposals generation\n",
      "构造的DOI: 10.48550/arXiv.2110.11474v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Few-shot temporal action localization with query adaptive transformer\n",
      "构造的DOI: 10.48550/arXiv.2110.10552v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Discovery-and-Selection: Towards Optimal Multiple Instance Learning for Weakly Supervised Object Detection\n",
      "构造的DOI: 10.48550/arXiv.2110.09060v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised shot boundary detection for temporal segmentation of long capsule endoscopy videos\n",
      "构造的DOI: 10.48550/arXiv.2110.09067v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph convolution neural network for weakly supervised abnormality localization in long capsule endoscopy videos\n",
      "构造的DOI: 10.48550/arXiv.2110.09110v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos\n",
      "构造的DOI: 10.48550/arXiv.2110.08708v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ASFormer: Transformer for action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2302.13074v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Knights: First place submission for VIPriors21 action recognition challenge at ICCV 2021\n",
      "构造的DOI: 10.48550/arXiv.2110.07758v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Ego4D: Around the World in 3,000 Hours of Egocentric Video\n",
      "构造的DOI: 10.48550/arXiv.2110.07058v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object-Region Video Transformers\n",
      "DOI: 10.1109/TPAMI.2022.3186506\n",
      "\n",
      "处理标题: Multiple style transfer via Variational AutoEncoder\n",
      "构造的DOI: 10.48550/arXiv.2110.07375v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TADA! TEMPORALLY-ADAPTIVE CONVOLUTIONS FOR VIDEO UNDERSTANDING\n",
      "构造的DOI: 10.48550/arXiv.2110.06178v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Modal interaction graph convolutional network for temporal language localization in videos\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical modeling for task recognition and action segmentation in weakly-labeled instructional videos\n",
      "构造的DOI: 10.48550/arXiv.2110.05697v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Efficient modelling across time of human actions and interactions\n",
      "DOI: 10.33540/789\n",
      "\n",
      "处理标题: Spatio-temporal video representation learning for AI based video playback style prediction\n",
      "构造的DOI: 10.48550/arXiv.2110.01015v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep learning-based action detection in untrimmed videos: A survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: IntentVizor: Towards Generic Query Guided Interactive Video Summarization\n",
      "构造的DOI: 10.48550/arXiv.2109.14834v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Information elevation network for fast online action detection\n",
      "构造的DOI: 10.48550/arXiv.2109.13572v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-Supervised Learning for Semi-Supervised Temporal Language Grounding\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards high-quality temporal action detection with sparse proposals\n",
      "构造的DOI: 10.48550/arXiv.2109.08847v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A survey on temporal sentence grounding in videos\n",
      "构造的DOI: 10.48550/arXiv.2109.08039v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: METEOR: A Dense, Heterogeneous, and Unstructured Traffic Dataset with Rare Behaviors\n",
      "构造的DOI: 10.48550/arXiv.2109.07648v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: On Pursuit of Designing Multi-modal Transformer for Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2109.06085v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progressively guide to attend: An iterative alignment framework for temporal sentence grounding\n",
      "构造的DOI: 10.48550/arXiv.2109.06400v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding\n",
      "构造的DOI: 10.48550/arXiv.2109.04872v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to Discriminate Information for Online Action Detection: Analysis and Application\n",
      "构造的DOI: 10.48550/arXiv.1003.1117v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WHYACT: Identifying action reasons in lifestyle vlogs\n",
      "构造的DOI: 10.48550/arXiv.2109.02747v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Efficient Action Recognition Using Confidence Distillation\n",
      "构造的DOI: 10.48550/arXiv.2109.02137v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Efficient visual recognition with deep neural networks: A survey on recent advances and new directions\n",
      "构造的DOI: 10.48550/arXiv.2409.18538v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Modulation Network for audio-visual event localization\n",
      "构造的DOI: 10.48550/arXiv.1710.04344v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep reinforcement learning in computer vision: A comprehensive survey\n",
      "构造的DOI: 10.48550/arXiv.2205.00299v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Identity-aware graph memory network for action detection\n",
      "DOI: 10.1145/3474085.3475503\n",
      "\n",
      "处理标题: Support-set based cross-supervision for video grounding\n",
      "构造的DOI: 10.48550/arXiv.2407.04519v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph2Pix: A graph-based image to image translation framework\n",
      "构造的DOI: 10.48550/arXiv.2111.13105v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised joint anomaly detection and classification\n",
      "构造的DOI: 10.48550/arXiv.2108.08996v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Target adaptive context aggregation for video scene graph generation\n",
      "构造的DOI: 10.48550/arXiv.2108.08121v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-end Dense video captioning with parallel decoding\n",
      "构造的DOI: 10.48550/arXiv.2108.07781v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Group-aware Contrastive Regression for action quality assessment\n",
      "构造的DOI: 10.48550/arXiv.2401.02841v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MSR-GCN: Multi-scale residual graph convolution networks for human motion prediction\n",
      "构造的DOI: 10.48550/arXiv.2108.07152v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised temporal anomaly segmentation with dynamic time warping\n",
      "构造的DOI: 10.48550/arXiv.2108.06816v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction\n",
      "构造的DOI: 10.48550/arXiv.2108.06852v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Segmentation with High-level Complex Activity Labels\n",
      "构造的DOI: 10.48550/arXiv.2108.06706v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Foreground-action consistency network forweakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2112.07338v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Modal Graph with Meta Concepts for Video Captioning\n",
      "DOI: 10.1109/TIP.2022.3192709\n",
      "\n",
      "处理标题: Deep Motion Prior for Weakly-Supervised Temporal Action Localization\n",
      "DOI: 10.1109/TIP.2022.3193752\n",
      "\n",
      "处理标题: Multi-source fusion and automatic predictor selection for zero-shot video object segmentation\n",
      "构造的DOI: 10.48550/arXiv.2108.05076v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning action completeness from points for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2108.05029v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Localization Using Gated Recurrent Units\n",
      "构造的DOI: 10.48550/arXiv.2108.03375v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dual Graph Convolutional Networks with transformer and curriculum learning for image captioning\n",
      "构造的DOI: 10.48550/arXiv.2108.02366v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UniCon: Unified context network for robust active speaker detection\n",
      "DOI: 10.1145/3474085.3475275\n",
      "\n",
      "处理标题: Deep Image-based Illumination Harmonization\n",
      "构造的DOI: 10.48550/arXiv.2108.00150v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CRD-CGAN: Category-consistent and relativistic constraints for diverse text-to-image generation\n",
      "构造的DOI: 10.48550/arXiv.2107.13516v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Predicting the future from first person (egocentric) vision: A survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spot what matters: Learning context using graph convolutional networks for weakly-supervised action detection\n",
      "DOI: 10.1007/978-3-030-68799-1_9\n",
      "\n",
      "处理标题: Cross-modal consensus network for weakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2010.11594v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Enriching local and global contexts for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2107.12960v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Transferable knowledge-based multi-granularity aggregation network for temporal action localization: Submission to activitynet challenge 2021\n",
      "构造的DOI: 10.48550/arXiv.1806.04391v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Sentence temporal and semantic relations in video activity localisation\n",
      "构造的DOI: 10.48550/arXiv.2107.11443v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: QVHighlights: Detecting moments and highlights in videos via natural language queries\n",
      "构造的DOI: 10.48550/arXiv.2107.09609v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AGENT-ENVIRONMENT NETWORK FOR TEMPORAL ACTION PROPOSAL GENERATION\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RGB stream is enough for temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.2107.04362v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long Short-Term Transformer for Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2107.03377v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: When video classification meets incremental classes\n",
      "构造的DOI: 10.48550/arXiv.2106.15827v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised temporal adjacent network for language grounding\n",
      "构造的DOI: 10.48550/arXiv.2106.16136v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SRF-NET: Selective receptive field network for anchor-free temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.2106.15258v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal context for action detection\n",
      "构造的DOI: 10.48550/arXiv.2410.15279v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised discovery of actions in instructional videos\n",
      "构造的DOI: 10.48550/arXiv.2106.14733v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring temporal context and human movement dynamics for online action detection in videos\n",
      "构造的DOI: 10.48550/arXiv.2106.13967v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hear me out: Fusional approaches for audio augmented temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2106.14118v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical object-oriented spatio-temporal reasoning for video question answering\n",
      "构造的DOI: 10.48550/arXiv.2106.13432v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring Stronger Feature for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2106.13014v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A survey on human-aware robot navigation\n",
      "构造的DOI: 10.48550/arXiv.2504.15643v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Winning the CVPR'2021 kinetics-GEBD challenge: Contrastive learning approach\n",
      "构造的DOI: 10.48550/arXiv.2103.14005v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: OadTR: Online action detection with transformers\n",
      "DOI: 10.1177/10692509241308069\n",
      "\n",
      "处理标题: Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track\n",
      "构造的DOI: 10.48550/arXiv.2106.10829v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Proposal relation network for temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.2106.11812v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised temporal action localization through local-global background modeling\n",
      "构造的DOI: 10.48550/arXiv.2207.06659v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Equivariance-bridged SO(2)-Invariant representation learning using graph convolutional network\n",
      "构造的DOI: 10.48550/arXiv.1812.03813v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Discerning generic event boundaries in long-form wild videos\n",
      "构造的DOI: 10.48550/arXiv.2106.10090v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-end Temporal Action Detection with Transformer\n",
      "构造的DOI: 10.48550/arXiv.2406.06187v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BABEL: Bodies, action and behavior with English labels\n",
      "构造的DOI: 10.48550/arXiv.2106.09696v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: C3: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues\n",
      "构造的DOI: 10.48550/arXiv.2106.08914v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: JRDB-Act: A large-scale dataset for spatio-temporal action, social group and activity detection\n",
      "构造的DOI: 10.48550/arXiv.2106.08827v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relation modeling in spatio-temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1807.10982v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Stronger Baseline for Ego-Centric Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2106.06942v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NeRF in detail: Learning to sample for view synthesis\n",
      "构造的DOI: 10.48550/arXiv.2204.03476v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards training stronger video vision transformers for EPIC-KITCHENS-100 action recognition\n",
      "构造的DOI: 10.48550/arXiv.2106.05058v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Keeping your eye on the ball: Trajectory attention in video transformers\n",
      "构造的DOI: 10.48550/arXiv.2501.08329v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Few-shot action localization without knowing boundaries\n",
      "构造的DOI: 10.48550/arXiv.2004.06971v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Transferring Knowledge from Text to Video: Zero-Shot Anticipation for Procedural Actions\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TSI: Temporal saliency integration for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2105.04213v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Continual 3D Convolutional Neural Networks for Real-time Processing of Videos\n",
      "DOI: 10.1007/978-3-031-19772-7_22\n",
      "\n",
      "处理标题: Rethinking the constraints of multimodal fusion: Case study in weakly-supervised audio-visual video parsing\n",
      "构造的DOI: 10.48550/arXiv.2105.14430v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards diverse paragraph captioning for untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2105.14477v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised action segmentation with self-supervised feature learning and co-occurrence parsing\n",
      "构造的DOI: 10.48550/arXiv.2105.14158v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering\n",
      "构造的DOI: 10.48550/arXiv.2105.13353v7 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge\n",
      "DOI: 10.1109/TKDE.2022.3153060\n",
      "\n",
      "处理标题: Temporal action proposal generation with transformers\n",
      "构造的DOI: 10.48550/arXiv.2105.12043v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-level attentive convoluntional neural network for crowd counting\n",
      "构造的DOI: 10.48550/arXiv.2105.11422v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FineAction: A Fine-Grained Video Dataset for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2105.11107v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Coarse to fine multi-resolution temporal convolutional network\n",
      "构造的DOI: 10.48550/arXiv.2105.10859v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Sharing Pain: Using Pain Domain Transfer for Video Recognition of Low Grade Orthopedic Pain in Horses\n",
      "DOI: 10.1371/journal.pone.0263854\n",
      "\n",
      "处理标题: Parallel attention network with sequence matching for video grounding\n",
      "DOI: 10.18653/v1/2021.findings-acl.69\n",
      "\n",
      "处理标题: Weakly supervised dense video captioning via jointly usage of knowledge distillation and cross-modal matching\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Finding a needle in a haystack: Tiny flying object detection in 4K videos using a joint detection-and-tracking approach\n",
      "构造的DOI: 10.48550/arXiv.2105.08253v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MultiSports: A multi-person video dataset of spatio-temporally localized sports actions\n",
      "构造的DOI: 10.48550/arXiv.2206.01038v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations\n",
      "构造的DOI: 10.48550/arXiv.2105.07085v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action shuffling for weakly supervised temporal localization\n",
      "DOI: 10.1109/TIP.2022.3185485\n",
      "\n",
      "处理标题: Reconstructive sequence-graph network for video summarization\n",
      "DOI: 10.1109/TPAMI.2021.3072117\n",
      "\n",
      "处理标题: Weakly supervised action selection learning in video\n",
      "构造的DOI: 10.48550/arXiv.2105.02439v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action unit memory network for weakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2104.14135v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor-centered Representations for Action Localization in Streaming Videos\n",
      "构造的DOI: 10.48550/arXiv.2104.14131v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relevance detection in cataract surgery videos by spatio-temporal action localization\n",
      "DOI: 10.1109/ICPR48806.2021.9412525\n",
      "\n",
      "处理标题: Multimodal clustering networks for self-supervised learning from unlabeled videos\n",
      "构造的DOI: 10.48550/arXiv.2104.12671v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Swimmer stroke rate estimation from overhead race video\n",
      "构造的DOI: 10.48550/arXiv.2104.12056v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Modeling long-term interactions to enhance action recognition\n",
      "构造的DOI: 10.48550/arXiv.2104.11520v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks\n",
      "构造的DOI: 10.48550/arXiv.1906.05948v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Low pass filter for anti-aliasing in temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2104.11403v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly Supervised Object Localization and Detection: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatiotemporal Deformable Scene Graphs for Complex Activity Detection\n",
      "构造的DOI: 10.48550/arXiv.2312.07621v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action segmentation with mixed temporal domain adaptation\n",
      "构造的DOI: 10.48550/arXiv.2104.07461v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object-centric representation learning for video question answering\n",
      "DOI: 10.1145/3077136.3080655\n",
      "\n",
      "处理标题: Object priors for classifying and localizing unseen actions\n",
      "构造的DOI: 10.48550/arXiv.2104.04715v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ACM-Net: Action context modeling network for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2008.13705v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised learning for semi-supervised temporal action proposal\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Few-shot transformation of common actions into time and space\n",
      "构造的DOI: 10.48550/arXiv.2104.02439v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Local metrics for multi-object tracking\n",
      "构造的DOI: 10.48550/arXiv.2308.02724v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zeus: Efficiently Localizing Actions in Videos using Reinforcement Learning\n",
      "DOI: 10.1145/3514221.3526181\n",
      "\n",
      "处理标题: Adaptive mutual supervision for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2104.02357v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Anchor-constrained viterbi for set-supervised action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2002.11925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SGCN:Sparse graph convolution network for pedestrian trajectory prediction\n",
      "构造的DOI: 10.48550/arXiv.2104.01528v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fingerspelling detection in American sign language\n",
      "构造的DOI: 10.48550/arXiv.2104.01291v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TubeR: Tubelet Transformer for Video Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2104.00969v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised motion learning from static images\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A survey on natural language video localization\n",
      "构造的DOI: 10.48550/arXiv.2104.00234v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CoLA: Weakly-supervised temporal action localization with snippet contrastive learning\n",
      "构造的DOI: 10.48550/arXiv.2103.16392v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised temporal action localization through learning explicit subspaces for action and context\n",
      "构造的DOI: 10.48550/arXiv.2103.16155v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified graph structured models for video understanding\n",
      "构造的DOI: 10.48550/arXiv.2103.15662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Augmented transformer with adaptive graph for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.2103.16024v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Low-fidelity end-to-end video encoder pre-training for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2204.12293v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ACSNet: Action-Context Separation Network for weakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2106.11811v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: The blessings of unlabeled background in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2103.13183v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal context aggregation network for temporal action proposal refinement\n",
      "获取数据时出错: HTTP Error 502: Bad Gateway\n",
      "\n",
      "处理标题: Learning salient boundary feature for anchor-free temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2103.13137v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Repetitive activity counting by sight and sound\n",
      "构造的DOI: 10.48550/arXiv.2103.13096v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PGT: A progressive method for training models on long videos\n",
      "构造的DOI: 10.48550/arXiv.2103.11313v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware biaffine localizing network for temporal sentence grounding\n",
      "构造的DOI: 10.48550/arXiv.2103.11555v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NAS-TC: Neural architecture search on temporal convolutions for complex action recognition\n",
      "构造的DOI: 10.48550/arXiv.2104.01110v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Information maximization clustering via multi-view self-labelling\n",
      "构造的DOI: 10.48550/arXiv.1112.0611v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action segmentation from timestamp supervision\n",
      "构造的DOI: 10.48550/arXiv.2103.06669v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PcmNet: Position-sensitive context modeling network for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2308.11358v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contrastive neural architecture search with neural architecture comparators\n",
      "构造的DOI: 10.48550/arXiv.2103.05471v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Time and frequency network for human action detection in videos\n",
      "构造的DOI: 10.48550/arXiv.2103.04680v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Modeling Multi-Label Action Dependencies for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2103.03027v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Coarse-fine networks for temporal activity detection in videos\n",
      "构造的DOI: 10.48550/arXiv.2003.05583v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ROAD: The ROad event Awareness Dataset for Autonomous Driving\n",
      "DOI: 10.1109/TPAMI.2022.3150906\n",
      "\n",
      "处理标题: RMS-Net: Regression and Masking for Soccer Event Spotting\n",
      "构造的DOI: 10.48550/arXiv.2102.07624v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dynamic neural networks: A survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Machine learning applications on neuroimaging for diagnosis and prognosis of epilepsy: A review\n",
      "构造的DOI: 10.48550/arXiv.0912.3953v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relaxed Transformer decoders for direct action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.2102.01894v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progressive Localization Networks for Language-based Moment Localization\n",
      "构造的DOI: 10.48550/arXiv.2102.01282v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video transformer network\n",
      "构造的DOI: 10.48550/arXiv.1812.02707v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Forecasting action through contact representations from first person video\n",
      "DOI: 10.1109/TPAMI.2021.3055233\n",
      "\n",
      "处理标题: Generic event boundary detection: A benchmark for event segmentation\n",
      "构造的DOI: 10.48550/arXiv.2302.06301v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A closer look at temporal sentence grounding in videos: Dataset and metric\n",
      "构造的DOI: 10.48550/arXiv.2311.03566v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical graph-RNNs for action detection of multiple activities\n",
      "构造的DOI: 10.48550/arXiv.2101.08581v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Discovering multi-label actor-action association in a weakly supervised setting\n",
      "构造的DOI: 10.48550/arXiv.1707.09593v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bridging the gap between human action recognition and online action detection\n",
      "构造的DOI: 10.48550/arXiv.2101.08851v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Activity graph transformer for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2101.08540v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Neural networks behave as hash encoders: An empirical study\n",
      "构造的DOI: 10.48550/arXiv.2006.11421v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware image denoising with auto-threshold canny edge detection to suppress adversarial perturbation\n",
      "构造的DOI: 10.48550/arXiv.2101.05833v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Single-path Bit Sharing for Automatic Loss-aware Model Compression\n",
      "构造的DOI: 10.48550/arXiv.2101.04935v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MAAS: Multi-modal assignation for active speaker detection\n",
      "构造的DOI: 10.48550/arXiv.2311.01886v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LAEO-Net++: Revisiting people looking at each other in videos\n",
      "构造的DOI: 10.48550/arXiv.1906.05261v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Personal privacy protection via irrelevant faces tracking and pixelation in video live streaming\n",
      "DOI: 10.1109/TIFS.2020.3029913\n",
      "\n",
      "处理标题: Global2local: Efficient structure search for video action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2101.00910v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A hybrid attention mechanism for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2101.00545v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking the heatmap regression for bottom-up human pose estimation\n",
      "构造的DOI: 10.48550/arXiv.2012.15175v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-shot temporal event localization: A benchmark\n",
      "构造的DOI: 10.48550/arXiv.2302.04752v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Action Localization, and Action Recognition using Global-Local Attention of 3D CNN\n",
      "DOI: 10.1007/s11263-022-01649-x\n",
      "\n",
      "处理标题: FLAVR: Flow-agnostic video representations for fast frame interpolation\n",
      "构造的DOI: 10.48550/arXiv.2012.08512v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Point-level temporal action localization: Bridging fully-supervised proposals to weakly-supervised losses\n",
      "构造的DOI: 10.48550/arXiv.2304.00917v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: D2-Net: Weakly-supervised action localization via discriminative embeddings and denoised activations\n",
      "构造的DOI: 10.48550/arXiv.2012.06440v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A comprehensive study of deep video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2012.06567v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Understanding Action Sequences based on Video Captioning for Learning-from-Observation\n",
      "构造的DOI: 10.48550/arXiv.2101.05061v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware graph convolution network for target re-identification\n",
      "DOI: 10.1109/BigData47090.2019.9006004\n",
      "\n",
      "处理标题: Rethinking learnable tree filter for generic feature transform\n",
      "构造的DOI: 10.48550/arXiv.2012.03482v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained dynamic head for object detection\n",
      "DOI: 10.1609/aaai.v39i12.33445\n",
      "\n",
      "处理标题: End-to-end object detection with fully convolutional network\n",
      "构造的DOI: 10.48550/arXiv.1906.09806v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VideoMix: Rethinking data augmentation for video classification\n",
      "构造的DOI: 10.48550/arXiv.2012.03457v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-scale 2D temporal adjacent networks for moment localization with natural language\n",
      "构造的DOI: 10.48550/arXiv.1912.03590v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatial-temporal alignment network for action recognition and detection\n",
      "构造的DOI: 10.48550/arXiv.2012.02426v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Comprehensive review on recent methods and challenges of video description\n",
      "构造的DOI: 10.48550/arXiv.2011.14752v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Self-Stitching Graph Network for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2110.05904v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Annotation-Efficient untrimmed video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A3D: Adaptive 3D networks for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2011.08652v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CLAWS: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection\n",
      "构造的DOI: 10.48550/arXiv.2011.12077v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TSP: Temporally-sensitive pretraining of video encoders for localization tasks\n",
      "构造的DOI: 10.48550/arXiv.2011.11479v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action detection with multi-level supervision\n",
      "构造的DOI: 10.48550/arXiv.1910.01286v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary-sensitive pre-training for temporal localization in videos\n",
      "构造的DOI: 10.48550/arXiv.2212.00986v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: From recognition to prediction: Analysis of human action and trajectory prediction in video\n",
      "构造的DOI: 10.48550/arXiv.2011.10670v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Stressnet: Deep learning to predict stress with fracture propagation in brittle materials\n",
      "构造的DOI: 10.48550/arXiv.2011.10227v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action duration prediction for segment-level alignment of weakly-labeled videos\n",
      "构造的DOI: 10.48550/arXiv.2404.19542v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VLG-Net: Video-language graph matching network for video grounding\n",
      "构造的DOI: 10.48550/arXiv.2011.10132v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Privileged knowledge distillation for online action detection\n",
      "构造的DOI: 10.48550/arXiv.2011.09158v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TRAT: Tracking by attention Using spatio-Temporal Features\n",
      "构造的DOI: 10.48550/arXiv.2103.15436v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: 3D CNNs with adaptive temporal feature resolutions\n",
      "构造的DOI: 10.48550/arXiv.2011.08652v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LAP-Net: Adaptive features sampling via learning action progression for online action detection\n",
      "构造的DOI: 10.48550/arXiv.2011.07915v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SALAD: Self-assessment learning for action detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Human-centric spatio-temporal video grounding with visual transformers\n",
      "构造的DOI: 10.48550/arXiv.1807.00486v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TMMF: Temporal Multi-modal Fusion for single-stage continuous gesture recognition\n",
      "构造的DOI: 10.48550/arXiv.2001.05833v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Improved soccer action spotting using both audio and video streams\n",
      "构造的DOI: 10.48550/arXiv.2011.04258v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor and Action Modular Network for Text-based Video Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2011.00786v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RSPNet: Relative speed perception for unsupervised video representation learning\n",
      "构造的DOI: 10.48550/arXiv.2011.07949v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Two-stream consensus network for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2107.12589v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep analysis of CNN-based Spatio-temporal representations for action recognition\n",
      "构造的DOI: 10.48550/arXiv.1901.10799v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Robust two-stream multi-feature network for driver drowsiness detection\n",
      "构造的DOI: 10.48550/arXiv.2102.12555v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DORi: Discovering object relationship for moment localization of a natural-language query in video\n",
      "构造的DOI: 10.48550/arXiv.2010.06260v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Online action detection in streaming videos with time buffers\n",
      "构造的DOI: 10.48550/arXiv.2010.03016v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Leveraging tacit information embedded in CNN layers for visual tracking\n",
      "构造的DOI: 10.48550/arXiv.2010.01204v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Adversarial semi-supervised multi-domain tracking\n",
      "构造的DOI: 10.48550/arXiv.1808.01976v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dissected 3D CNNs: Temporal skip connections for efficient online video processing\n",
      "构造的DOI: 10.48550/arXiv.2009.14639v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: In-sample contrastive learning and consistent attention for weakly supervised object localization\n",
      "构造的DOI: 10.48550/arXiv.2009.12063v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Frame-wise cross-modal matching for video moment retrieval\n",
      "构造的DOI: 10.48550/arXiv.2402.13576v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dual semantic fusion network for video object detection\n",
      "DOI: 10.1145/3394171.3413583\n",
      "\n",
      "处理标题: BSN++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.2009.07641v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-label activity recognition using activity-specific features and activity correlations\n",
      "构造的DOI: 10.48550/arXiv.2009.07420v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Collaborative distillation in the parameter and spectrum domains for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2009.06902v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Removing the background by adding the background: Towards background robust self-supervised video representation learning\n",
      "构造的DOI: 10.48550/arXiv.2009.05769v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HAA500: Human-centric atomic action dataset with curated videos\n",
      "构造的DOI: 10.48550/arXiv.2009.05224v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Survey on Machine Learning from Few Samples\n",
      "构造的DOI: 10.48550/arXiv.2009.02653v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Data mining and time series segmentation via extrema: preliminary investigations\n",
      "构造的DOI: 10.48550/arXiv.0706.1410v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to localize actions from moments\n",
      "构造的DOI: 10.48550/arXiv.2008.13705v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised video representation learning by uncovering spatio-temporal statistics\n",
      "构造的DOI: 10.48550/arXiv.2403.15790v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Online spatiotemporal action detection and prediction via causal representations\n",
      "构造的DOI: 10.48550/arXiv.2008.13759v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Finding action tubes with a sparse-to-dense framework\n",
      "构造的DOI: 10.48550/arXiv.2008.13196v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary uncertainty in a single-stage temporal action localization network\n",
      "构造的DOI: 10.48550/arXiv.2008.11170v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action localization with variance-aware networks\n",
      "构造的DOI: 10.48550/arXiv.1708.03280v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LC-NAS: Latency constrained neural architecture search for point cloud networks\n",
      "构造的DOI: 10.48550/arXiv.2008.10309v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: VLANet: Video-language alignment network for weakly-supervised video moment retrieval\n",
      "构造的DOI: 10.48550/arXiv.1907.12763v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting anchor mechanisms for temporal action localization\n",
      "DOI: 10.1109/TIP.2020.3016486\n",
      "\n",
      "处理标题: Text-based localization of moments in a video corpus\n",
      "DOI: 10.1109/TIP.2021.3120038\n",
      "\n",
      "处理标题: Dynamic Adjacency Matrix for Video Relocalization\n",
      "构造的DOI: 10.48550/arXiv.2008.08977v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CFAD: Coarse-to-fine action detector for spatiotemporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1804.01824v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Regularized two-branch proposal networks for weakly-supervised moment retrieval in videos\n",
      "构造的DOI: 10.48550/arXiv.2008.08257v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Equivalent classification mapping for weakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2008.07728v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Poet: Product-oriented video captioner for E-commerce\n",
      "DOI: 10.1145/3394171.3413880\n",
      "\n",
      "处理标题: Localizing the common action among a few videos\n",
      "构造的DOI: 10.48550/arXiv.2008.05826v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A unified framework for shot type classification based on subject centric lens\n",
      "构造的DOI: 10.48550/arXiv.2008.03548v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-Level temporal pyramid network for action detection\n",
      "构造的DOI: 10.48550/arXiv.2410.07912v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Location-aware graph convolutional networks for video question answering\n",
      "构造的DOI: 10.48550/arXiv.2008.09105v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained iterative attention network for temporal language localization in videos\n",
      "DOI: 10.1609/aaai.v39i12.33445\n",
      "\n",
      "处理标题: Polysemy deciphering network for robust human-object interaction detection\n",
      "构造的DOI: 10.48550/arXiv.2008.02918v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Jointly cross- and self-modal graph attention network for query-based moment localization\n",
      "构造的DOI: 10.48550/arXiv.2008.01403v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Boundary Content Graph Neural Network for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.2008.01432v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020\n",
      "构造的DOI: 10.48550/arXiv.2006.11693v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph Neural Network for Video Relocalization\n",
      "构造的DOI: 10.48550/arXiv.2007.09877v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing\n",
      "构造的DOI: 10.48550/arXiv.2111.03225v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware RCNN: a baseline for action detection in videos\n",
      "构造的DOI: 10.48550/arXiv.2007.09861v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SumGraph: Video summarization via recursive graph modeling\n",
      "构造的DOI: 10.48550/arXiv.2007.08809v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Anisotropic Mesh Adaptation for Image Segmentation Based on Mumford-Shah Functional\n",
      "构造的DOI: 10.48550/arXiv.2007.08696v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Alleviating Over-segmentation Errors by Detecting Action Boundaries\n",
      "构造的DOI: 10.48550/arXiv.2007.06866v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Generating visually aligned sound from videos\n",
      "DOI: 10.1109/TIP.2020.3009820\n",
      "\n",
      "处理标题: Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization\n",
      "构造的DOI: 10.48550/arXiv.2007.06643v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Universal-to-Specific Framework for Complex Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.1909.03466v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision\n",
      "构造的DOI: 10.48550/arXiv.2007.04687v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Evaluation for Weakly Supervised Object Localization: Protocol, Metrics, and Datasets\n",
      "DOI: 10.1109/DSN.2018.00056\n",
      "\n",
      "处理标题: Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search\n",
      "构造的DOI: 10.48550/arXiv.2007.07197v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised temporal action localization with segment-level labels\n",
      "构造的DOI: 10.48550/arXiv.1908.06552v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep Learning for Vision-based Prediction: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast Training of Deep Networks with One-Class CNNs\n",
      "DOI: 10.1109/ICASSP49660.2025.10889238\n",
      "\n",
      "处理标题: Counting Out Time: Class Agnostic Video Repetition Counting in the Wild\n",
      "构造的DOI: 10.48550/arXiv.2006.15418v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning\n",
      "构造的DOI: 10.48550/arXiv.2006.14262v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Comprehensive Information Integration Modeling Framework for Video Titling\n",
      "DOI: 10.1145/3394486.3403325\n",
      "\n",
      "处理标题: Weak Supervision and Referring Attention for Temporal-Textual Association Learning\n",
      "构造的DOI: 10.48550/arXiv.2006.11747v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation\n",
      "DOI: 10.1109/TETC.2022.3230912\n",
      "\n",
      "处理标题: Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization 1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020\n",
      "构造的DOI: 10.48550/arXiv.2006.09116v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor-context-actor relation network for spatio-temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1909.03252v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CBR-Net: Cascade Boundary Refinement Network for Action Detection: Submission to ActivityNet Challenge 2020 (Task 1)\n",
      "构造的DOI: 10.48550/arXiv.2006.07526v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization\n",
      "DOI: 10.3847/1538-4357/ac6875\n",
      "\n",
      "处理标题: ESAD: Endoscopic surgeon action detection dataset\n",
      "构造的DOI: 10.48550/arXiv.2006.07164v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Fusion Network for Temporal Action Localization: Submission to ActivityNet Challenge 2020 (Task E)\n",
      "构造的DOI: 10.48550/arXiv.2006.11693v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WOAD: Weakly supervised online action detection in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2006.03732v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Egocentric Object Manipulation Graphs\n",
      "构造的DOI: 10.48550/arXiv.2006.03201v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: In the eye of the beholder: Gaze and actions in first person video\n",
      "构造的DOI: 10.48550/arXiv.1612.03094v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Complex Sequential Understanding through the Awareness of Spatial and Temporal Concepts\n",
      "DOI: 10.1038/s42256-020-0168-3\n",
      "\n",
      "处理标题: Deep graph learning for semi-supervised classification\n",
      "构造的DOI: 10.48550/arXiv.1911.08538v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Intra- and inter-action understanding via temporal action parsing\n",
      "构造的DOI: 10.48550/arXiv.2005.10229v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Context-aware and scale-insensitive temporal repetition counting\n",
      "构造的DOI: 10.48550/arXiv.2305.13778v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\n",
      "构造的DOI: 10.48550/arXiv.1904.03870v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: New frontiers in IoT: Networking, systems, reliability, and security challenges\n",
      "构造的DOI: 10.48550/arXiv.2005.07338v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA\n",
      "构造的DOI: 10.48550/arXiv.2404.04007v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HiEve: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events\n",
      "DOI: 10.1007/s11263-023-01842-6\n",
      "\n",
      "处理标题: STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction\n",
      "构造的DOI: 10.48550/arXiv.2402.19002v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to Segment Actions from Observation and Narration\n",
      "构造的DOI: 10.48550/arXiv.2005.03684v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hierarchical attention network for action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2005.03209v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Inferring temporal compositions of actions using probabilistic automata\n",
      "构造的DOI: 10.48550/arXiv.2004.13217v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action recognition in real-world videos\n",
      "构造的DOI: 10.48550/arXiv.1512.00795v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TAEN: Temporal aware embedding network for few-shot action recognition\n",
      "构造的DOI: 10.48550/arXiv.1904.07846v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Group activity detection from trajectory and video data in soccer\n",
      "构造的DOI: 10.48550/arXiv.2004.10299v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Local-global video-text interactions for temporal grounding\n",
      "构造的DOI: 10.48550/arXiv.2411.07945v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Asynchronous interaction aggregation for action detection\n",
      "构造的DOI: 10.48550/arXiv.2004.07485v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ActionSpotter: Deep reinforcement learning framework for temporal action spotting in videos\n",
      "构造的DOI: 10.48550/arXiv.2004.06971v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FineGym: A hierarchical video dataset for fine-grained action understanding\n",
      "构造的DOI: 10.48550/arXiv.2405.17729v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Event detection in coarsely annotated sports videos via parallel multi receptive field 1D convolutions\n",
      "构造的DOI: 10.48550/arXiv.2004.06172v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: X3D: Expanding architectures for efficient video recognition\n",
      "构造的DOI: 10.48550/arXiv.2004.04730v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dense Regression Network for Video Grounding\n",
      "构造的DOI: 10.48550/arXiv.2004.03545v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Two-Stream AMTnet for Action Detection\n",
      "构造的DOI: 10.48550/arXiv.2004.01494v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal Tubelet Feature Aggregation and Object Linking in Videos\n",
      "构造的DOI: 10.48550/arXiv.1801.09823v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting few-shot activity detection with class similarity control\n",
      "构造的DOI: 10.48550/arXiv.2004.00137v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation\n",
      "构造的DOI: 10.48550/arXiv.2003.14266v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long short-term relation networks for video action detection\n",
      "构造的DOI: 10.48550/arXiv.2003.14065v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal action detection with multi-object interaction\n",
      "构造的DOI: 10.48550/arXiv.2004.00180v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Interactions and Relationships between Movie Characters\n",
      "构造的DOI: 10.48550/arXiv.2003.13158v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised action localization by generative attention modeling\n",
      "构造的DOI: 10.48550/arXiv.2003.12424v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking online action detection in untrimmed videos: A novel online evaluation protocol\n",
      "构造的DOI: 10.48550/arXiv.2003.12041v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action localization through continual predictive learning\n",
      "构造的DOI: 10.48550/arXiv.2003.12185v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: The instantaneous accuracy: A novel metric for the problem of online human behaviour recognition in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2003.09970v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation\n",
      "DOI: 10.1109/TPAMI.2020.2980824\n",
      "\n",
      "处理标题: A NOVEL ONLINE ACTION DETECTION FRAMEWORK FROM UNTRIMMED VIDEO STREAMS\n",
      "构造的DOI: 10.48550/arXiv.2003.07734v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos\n",
      "构造的DOI: 10.48550/arXiv.2003.07048v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Closed-loop matters: Dual regression networks for single image super-resolution\n",
      "构造的DOI: 10.48550/arXiv.2412.03017v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spectral Graph Attention Network with fast eigen-approximation\n",
      "构造的DOI: 10.48550/arXiv.2111.07602v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SF-Net: Single-frame supervision for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2009.11859v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Edc3: Ensemble of deep-classifiers using class-specific copula functions to improve semantic image segmentation\n",
      "构造的DOI: 10.48550/arXiv.2003.05710v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ZSTAD: Zero-shot temporal activity detection\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Accurate temporal action proposal generation with relation-aware pyramid network\n",
      "构造的DOI: 10.48550/arXiv.2003.04145v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Transferring cross-domain knowledge for video sign language recognition\n",
      "构造的DOI: 10.48550/arXiv.2003.03703v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cost-sensitive portfolio selection via deep reinforcement learning\n",
      "DOI: 10.1109/INISTA59065.2023.10310402\n",
      "\n",
      "处理标题: Intelligent home 3D: Automatic 3D-house design from linguistic descriptions only\n",
      "构造的DOI: 10.48550/arXiv.2003.00397v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Set-constrained viterbi for set-supervised action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2002.11925v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Automatic gesture recognition in robot-assisted surgery with reinforcement learning and tree search\n",
      "构造的DOI: 10.48550/arXiv.2002.08718v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Bottom-up temporal action localization with mutual regularization\n",
      "构造的DOI: 10.48550/arXiv.2407.02367v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action graphs: weakly-supervised action localization with graph convolution networks\n",
      "构造的DOI: 10.48550/arXiv.1904.12659v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences\n",
      "构造的DOI: 10.48550/arXiv.2001.11122v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Audiovisual SlowFast networks for video recognition\n",
      "构造的DOI: 10.48550/arXiv.2001.08740v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Evaluating weakly supervised object localization methods right\n",
      "构造的DOI: 10.48550/arXiv.2001.07437v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A comprehensive study on temporal modeling for online action detection\n",
      "构造的DOI: 10.48550/arXiv.2001.07501v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised temporal action localization using deep metric learning\n",
      "构造的DOI: 10.48550/arXiv.2001.07793v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning spatiotemporal features via video and text pair discrimination\n",
      "构造的DOI: 10.48550/arXiv.2001.05691v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised visual semantic parsing\n",
      "构造的DOI: 10.48550/arXiv.2001.02359v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Discrimination-aware network pruning for deep model compression\n",
      "DOI: 10.1109/LSP.2021.3088323\n",
      "\n",
      "处理标题: Grab: Fast and accurate sensor processing for cashier-free shopping\n",
      "构造的DOI: 10.48550/arXiv.2001.01033v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Mimetics: Towards understanding human actions out of context\n",
      "构造的DOI: 10.48550/arXiv.1912.07249v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action genome: Actions as composition of spatio-temporal scene graphs\n",
      "构造的DOI: 10.48550/arXiv.1912.06992v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SPIN: A high speed, high resolution vision dataset for tracking and action recognition in ping pong\n",
      "构造的DOI: 10.48550/arXiv.1912.06640v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action modifiers: Learning from adverbs in instructional videos\n",
      "构造的DOI: 10.48550/arXiv.1912.06617v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Why can't I dance in the mall? Learning to mitigate scene bias in action recognition\n",
      "构造的DOI: 10.48550/arXiv.1912.05534v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Forecasting future action sequences with attention: a new approach to weakly supervised action forecasting\n",
      "DOI: 10.1109/TIP.2020.3021497\n",
      "\n",
      "处理标题: Learning to Discriminate Information for Online Action Detection\n",
      "构造的DOI: 10.48550/arXiv.1912.04461v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Car pose in context: Accurate pose estimation with ground plane constraints\n",
      "构造的DOI: 10.48550/arXiv.1912.04363v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Listen to look: Action recognition by previewing audio\n",
      "构造的DOI: 10.48550/arXiv.1912.04487v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video action detection by learning graph-based spatio-temporal interactions\n",
      "构造的DOI: 10.48550/arXiv.2411.05636v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Comprehensive soccer video understanding: Towards human-comparable video understanding system in constrained environment\n",
      "构造的DOI: 10.48550/arXiv.2106.11310v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning Sparse 2D Temporal Adjacent Networks for Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.1912.03612v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning 2D temporal adjacent networks for moment localization with natural language\n",
      "构造的DOI: 10.48550/arXiv.1912.03590v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal pyramid graph convolutions for human action recognition and postural assessment\n",
      "构造的DOI: 10.48550/arXiv.1912.03442v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zero-shot recognition of complex action sequences\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Compositional Temporal Visual Grounding of Natural Language Event Descriptions\n",
      "构造的DOI: 10.48550/arXiv.1912.02256v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A context-aware loss function for action spotting in soccer videos\n",
      "构造的DOI: 10.48550/arXiv.1912.01326v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Probing the state of the art: A critical look at visual representation evaluation\n",
      "构造的DOI: 10.48550/arXiv.2409.08202v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AdapNet: Adaptability decomposing encoder-decoder network for weakly supervised action recognition and localization\n",
      "构造的DOI: 10.48550/arXiv.1911.11961v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: G-TAD: Sub-graph localization for temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.2004.00180v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SRG: Snippet Relatedness-based Temporal Action Proposal Generator\n",
      "DOI: 10.1109/TCSVT.2019.2953187\n",
      "\n",
      "处理标题: Zero-Shot Imitating Collaborative Manipulation Plans from YouTube Cooking Videos\n",
      "构造的DOI: 10.48550/arXiv.1911.10686v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Background suppression network for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2112.07984v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TEINet: Towards an Efficient Architecture for Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.2007.04356v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised video moment retrieval via semantic completion network\n",
      "构造的DOI: 10.48550/arXiv.1911.08199v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-class relevance learning for temporal concept localization\n",
      "构造的DOI: 10.48550/arXiv.1911.08548v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-attention networks for temporal localization of video-level labels\n",
      "构造的DOI: 10.48550/arXiv.2312.07117v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: You only watch once: A unified CNN architecture for real-time spatiotemporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1911.06644v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CMSN: Continuous multi-stage network and variable margin cosine loss for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.1911.06080v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast learning of temporal action proposal via dense boundary generator\n",
      "构造的DOI: 10.48550/arXiv.1911.04127v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A proposed artificial intelligence model for real-time human action localization and tracking\n",
      "构造的DOI: 10.48550/arXiv.1911.04469v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action localization using long short-term dependency\n",
      "构造的DOI: 10.48550/arXiv.2208.07493v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NAT: Neural architecture transformer for accurate and compact architectures\n",
      "构造的DOI: 10.48550/arXiv.1910.14488v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Semantic conditioned dynamic modulation for temporal sentence grounding in videos\n",
      "构造的DOI: 10.48550/arXiv.1910.14303v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to track any object\n",
      "构造的DOI: 10.48550/arXiv.1910.11844v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to localize temporal events in large-scale video data\n",
      "构造的DOI: 10.48550/arXiv.1910.11631v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: LPAT: Learning to Predict Adaptive Threshold for Weakly-supervised Temporal Action Localization\n",
      "构造的DOI: 10.48550/arXiv.2309.09060v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-Supervised Completion Moment Detection using Temporal Attention\n",
      "构造的DOI: 10.48550/arXiv.1910.09920v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AFO-TAD: Anchor-free one-stage detector for temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.1910.08250v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tiny video networks\n",
      "构造的DOI: 10.48550/arXiv.1910.06961v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TruNet: Short videos generation from long videos via story-preserving truncation\n",
      "构造的DOI: 10.48550/arXiv.1910.05899v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning temporal action proposalswith fewer labels\n",
      "构造的DOI: 10.48550/arXiv.1910.01286v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised energy-based learning for action segmentation\n",
      "构造的DOI: 10.48550/arXiv.2504.05700v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WMAN: Weakly-supervised moment alignment network for text-based video segment retrieval\n",
      "DOI: 10.1109/TIP.2021.3120038\n",
      "\n",
      "处理标题: Coupled generative adversarial network for continuous fine-grained action segmentation\n",
      "构造的DOI: 10.48550/arXiv.1909.09283v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Making the invisible visible: Action recognition through walls and occlusions\n",
      "构造的DOI: 10.48550/arXiv.1909.09300v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fine-grained action segmentation using the semi-supervised action GAN\n",
      "构造的DOI: 10.48550/arXiv.2410.05323v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multiple human tracking using multi-cues including primitive action features\n",
      "构造的DOI: 10.48550/arXiv.1909.08171v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep point-wise prediction for action temporal proposal\n",
      "构造的DOI: 10.48550/arXiv.1909.07725v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progression modelling for online and early gesture detection\n",
      "构造的DOI: 10.48550/arXiv.1909.06672v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Gaussian temporal awareness networks for action localization\n",
      "构造的DOI: 10.48550/arXiv.1909.03877v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph convolutional networks for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2110.05904v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: WSLLN: Weakly supervised natural language localization networks\n",
      "构造的DOI: 10.48550/arXiv.1909.00239v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Out the window: A crowd-sourced dataset for activity classification in security video\n",
      "构造的DOI: 10.48550/arXiv.1908.10899v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep concept-wise temporal convolutional networks for action localization\n",
      "构造的DOI: 10.48550/arXiv.1908.09442v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: 3C-Net: Category count and center loss for weakly-supervised action localization\n",
      "构造的DOI: 10.48550/arXiv.1908.08216v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MULTI-STREAM SINGLE SHOT SPATIAL-TEMPORAL ACTION DETECTION\n",
      "构造的DOI: 10.48550/arXiv.1908.08178v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Proposal-free temporal moment localization of a natural-language query in video using guided attention\n",
      "构造的DOI: 10.48550/arXiv.1908.07236v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised action localization with background modeling\n",
      "构造的DOI: 10.48550/arXiv.2207.06659v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Three branches: Detecting actions with richer features\n",
      "构造的DOI: 10.48550/arXiv.1908.04519v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Adversarial seeded sequence growing for weakly-supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1908.02422v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Report of 2017 NSF workshop on multimedia challenges, opportunities and research roadmaps\n",
      "构造的DOI: 10.48550/arXiv.1908.02308v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Scale matters: Temporal scale aggregation network for precise action localization in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.1908.00707v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-granularity fusion network for proposal and activity localization: Submission to activitynet challenge 2019 task 1 and task 2\n",
      "构造的DOI: 10.48550/arXiv.1907.12223v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Submission to activitynet challenge 2019: Task B spatio-temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.2405.09059v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BMN: Boundary-matching network for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.1811.08496v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Attention filtering for multi-person spatiotemporal action detection on deep two-stream CNN architectures\n",
      "构造的DOI: 10.48550/arXiv.1907.12919v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An Efficient 3D CNN for Action/Object Segmentation in Video\n",
      "构造的DOI: 10.48550/arXiv.1907.08895v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Understanding teacher gaze patterns for robot learning\n",
      "构造的DOI: 10.48550/arXiv.1907.07202v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deformable tube network for action detection in videos\n",
      "构造的DOI: 10.48550/arXiv.1907.01847v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Localizing unseen activities in video via image query\n",
      "构造的DOI: 10.48550/arXiv.1906.12165v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: vireoJD-MM at Activity Detection in Extended Videos\n",
      "构造的DOI: 10.48550/arXiv.1906.08547v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Delving into 3D Action Anticipation from Streaming Videos\n",
      "构造的DOI: 10.48550/arXiv.1906.06521v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Accelerating temporal action proposal generation via high performance computing\n",
      "DOI: 10.1007/s11704-021-0173-7\n",
      "\n",
      "处理标题: Trimmed action recognition, dense-captioning events in videos, and spatio-temporal action localization with focus on activitynet challenge 2019\n",
      "构造的DOI: 10.48550/arXiv.1906.07016v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning spatiooral representation with local and global diffusion\n",
      "构造的DOI: 10.48550/arXiv.1906.05571v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: FASTER recurrent networks for video classification\n",
      "构造的DOI: 10.48550/arXiv.1906.04226v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Detecting the starting frame of actions in video\n",
      "构造的DOI: 10.48550/arXiv.1903.09868v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Risky action recognition in lane change video clips using deep spatiotemporal networks with segmentation mask transfer\n",
      "DOI: 10.1109/ITSC.2019.8917362\n",
      "\n",
      "处理标题: Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos\n",
      "DOI: 10.1007/978-3-031-20059-5_11\n",
      "\n",
      "处理标题: Two-stream region convolutional 3D network for temporal activity detection\n",
      "构造的DOI: 10.48550/arXiv.1801.09184v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation\n",
      "DOI: 10.1109/TPAMI.2018.2884469\n",
      "\n",
      "处理标题: TACNet: Transition-aware context network for spatio-temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.1905.13417v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Improving action localization by progressive cross-stream cooperation\n",
      "构造的DOI: 10.48550/arXiv.1905.11575v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring feature representation and training strategies in temporal\n",
      "构造的DOI: 10.48550/arXiv.1905.10608v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Marginalized average attentional network for weakly-supervised learning\n",
      "构造的DOI: 10.48550/arXiv.1810.03773v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised temporal localization via occurrence count learning\n",
      "构造的DOI: 10.48550/arXiv.1905.07293v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Neural message passing on hybrid spatio-temporal visual and symbolic graphs for video understanding\n",
      "构造的DOI: 10.48550/arXiv.1904.05582v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Domain adversarial reinforcement learning for partial domain adaptation\n",
      "构造的DOI: 10.48550/arXiv.1905.04094v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Follow the attention: Combining partial pose and object motion for fine-grained action detection\n",
      "构造的DOI: 10.48550/arXiv.1905.04430v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal video re-localization by warp LSTM\n",
      "构造的DOI: 10.48550/arXiv.1905.03922v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks\n",
      "构造的DOI: 10.48550/arXiv.1905.02419v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-Temporal Action Localization in a Weakly Supervised Setting\n",
      "构造的DOI: 10.48550/arXiv.2107.12618v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Recurrent convolutional strategies for face manipulation detection in videos\n",
      "构造的DOI: 10.48550/arXiv.1905.00582v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A study on action detection in the wild\n",
      "构造的DOI: 10.48550/arXiv.2103.15792v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tripping through time: Efficient localization of activities in videos\n",
      "构造的DOI: 10.48550/arXiv.1904.09936v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: STEP: Spatio-temporal progressive learning for video action detection\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal unet: Sample level human action recognition usingwifi\n",
      "构造的DOI: 10.48550/arXiv.1904.11953v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Early Detection of Injuries in MLB Pitchers from Video\n",
      "构造的DOI: 10.48550/arXiv.1904.08916v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DDLSTM: Dual-Domain LSTM for cross-dataset action recognition\n",
      "构造的DOI: 10.48550/arXiv.1603.07772v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal cycle-consistency learning\n",
      "构造的DOI: 10.48550/arXiv.2301.04126v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised gaussian networks for action detection\n",
      "构造的DOI: 10.48550/arXiv.1904.07774v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Decoupling localization and classification in single shot temporal action detection\n",
      "构造的DOI: 10.48550/arXiv.1904.07442v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action recognition from single timestamp supervision in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.1904.04689v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards Analyzing Semantic Robustness of Deep Neural Networks\n",
      "DOI: 10.1007/978-3-030-66415-2_2\n",
      "\n",
      "处理标题: Unsupervised learning of action classes with continuous temporal embedding\n",
      "构造的DOI: 10.48550/arXiv.1904.04189v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SCSampler: Sampling salient clips from video for efficient action recognition\n",
      "构造的DOI: 10.48550/arXiv.1904.04289v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relational action forecasting\n",
      "构造的DOI: 10.48550/arXiv.1904.04231v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics\n",
      "构造的DOI: 10.48550/arXiv.2312.08662v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Relation-aware global attention for person re-identification\n",
      "构造的DOI: 10.48550/arXiv.2109.06057v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast weakly supervised action segmentation using mutual consistency\n",
      "构造的DOI: 10.48550/arXiv.1904.03116v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised video moment retrieval from text queries\n",
      "构造的DOI: 10.48550/arXiv.1904.03282v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Paying more attention to motion: Attention distillation for learning video representations\n",
      "构造的DOI: 10.48550/arXiv.1904.03249v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Activity driven weakly supervised object detection\n",
      "构造的DOI: 10.48550/arXiv.1904.01665v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dance with flow: Two-in-one stream action detection\n",
      "构造的DOI: 10.48550/arXiv.1802.08362v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RefineLoc: Iterative refinement forweakly-supervised action localization\n",
      "构造的DOI: 10.48550/arXiv.1904.00227v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Social behavioral phenotyping of drosophila with a 2D-3D hybrid CNN framework\n",
      "构造的DOI: 10.48550/arXiv.1903.11421v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: StartNet: Online detection of action start in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.1903.09868v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection\n",
      "构造的DOI: 10.48550/arXiv.1903.07256v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: GolfDB: A video database for golf swing sequencing\n",
      "构造的DOI: 10.48550/arXiv.1903.06528v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Investigation on combining 3d convolution of image data and optical flow to generate temporal action proposals\n",
      "构造的DOI: 10.48550/arXiv.1903.04176v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: COIN: A large-scale dataset for comprehensive instructional video analysis\n",
      "构造的DOI: 10.48550/arXiv.1903.02874v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MS-TCN: Multi-stage temporal convolutional network for action segmentation\n",
      "DOI: 10.1109/TETC.2022.3230912\n",
      "\n",
      "处理标题: Collaborative spatiotemporal feature learning for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.2002.03152v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatiotemporal pyramid network for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.1903.01038v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Progress regression RNN for online spatial-temporal action localization in unconstrained videos\n",
      "构造的DOI: 10.48550/arXiv.1903.00304v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video summarization via actionness ranking\n",
      "构造的DOI: 10.48550/arXiv.1903.00110v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards real-time eyeblink detection in the wild: Dataset, theory and practices\n",
      "构造的DOI: 10.48550/arXiv.2405.18347v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision\n",
      "构造的DOI: 10.48550/arXiv.1902.07370v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring frame segmentation networks for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1708.03280v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Registration-Free Face-SSD: single shot analysis of smiles, facial attributes, and affect in the wild\n",
      "DOI: 10.1016/j.cviu.2019.01.006\n",
      "\n",
      "处理标题: Skeleton-based online action prediction using scale selection network\n",
      "构造的DOI: 10.48550/arXiv.1902.03084v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TrackNet: Simultaneous Object Detection and Tracking and Its Application in Traffic Video Analysis\n",
      "构造的DOI: 10.48550/arXiv.1902.01466v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Anomaly locality in video surveillance\n",
      "构造的DOI: 10.48550/arXiv.1901.10364v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal Action Recognition: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Read, watch, and move: Reinforcement learning for temporally grounding natural language descriptions in videos\n",
      "构造的DOI: 10.48550/arXiv.1901.06829v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cricket stroke extraction: Towards creation of a large-scale cricket actions dataset\n",
      "构造的DOI: 10.48550/arXiv.1901.03107v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DMC-Net: Generating discriminative motion cues for fast compressed video action recognition\n",
      "构造的DOI: 10.48550/arXiv.1901.03460v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Actor conditioned attention maps for video action detection\n",
      "构造的DOI: 10.48550/arXiv.2006.07976v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Similarity R-C3D for few-shot temporal activity detection\n",
      "构造的DOI: 10.48550/arXiv.1812.10000v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AU R-CNN: Encoding expert prior knowledge into R-CNN for action unit detection\n",
      "DOI: 10.1016/j.neucom.2019.03.082\n",
      "\n",
      "处理标题: TAN: Temporal Aggregation Network for Dense Multi-label Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.1812.06203v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Long-term feature banks for detailed video understanding\n",
      "构造的DOI: 10.48550/arXiv.2302.03561v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SlowFast Networks for Video Recognition\n",
      "构造的DOI: 10.48550/arXiv.1812.03982v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A structured model for action detection\n",
      "构造的DOI: 10.48550/arXiv.2310.02887v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video Action Transformer Network\n",
      "构造的DOI: 10.48550/arXiv.1812.02707v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Zero-shot anticipation for instructional activities\n",
      "构造的DOI: 10.48550/arXiv.2501.15271v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An empirical study towards understanding how deep convolutional nets recognize falls\n",
      "构造的DOI: 10.48550/arXiv.1812.01923v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Local Temporal Bilinear Pooling for Fine-Grained Action Parsing\n",
      "构造的DOI: 10.48550/arXiv.1812.01922v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-modal capsule routing for actor and action video segmentation conditioned on natural language queries\n",
      "构造的DOI: 10.48550/arXiv.1812.00303v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment\n",
      "构造的DOI: 10.48550/arXiv.1812.00087v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Discovering Spatio-Temporal Action Tubes\n",
      "构造的DOI: 10.48550/arXiv.1811.12248v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Optimized Skeleton-based Action Recognition via Sparsified Graph Regression\n",
      "构造的DOI: 10.48550/arXiv.1811.12013v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MIST multiple instance spatial transformers\n",
      "构造的DOI: 10.48550/arXiv.1811.10725v5 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep RNN framework for visual sequential applications\n",
      "构造的DOI: 10.48550/arXiv.1811.09961v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: MAC: Mining activity concepts for language-based temporal localization\n",
      "构造的DOI: 10.48550/arXiv.1811.08925v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A proposal-based solution to spatio-temporal action detection in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal recurrent networks for online action detection\n",
      "构造的DOI: 10.48550/arXiv.1811.07391v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Recurrent Convolutions for Causal 3D CNNs\n",
      "构造的DOI: 10.48550/arXiv.1811.07157v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A perceptual prediction framework for self supervised event segmentation\n",
      "构造的DOI: 10.48550/arXiv.1811.04869v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning\n",
      "构造的DOI: 10.48550/arXiv.1811.02307v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond\n",
      "构造的DOI: 10.48550/arXiv.1811.02545v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BLP-Boundary likelihood pinpointing networks for accurate temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1811.02189v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cascaded pyramid mining network for weakly supervised temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1810.11794v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: American sign language fingerspelling recognition in the wild\n",
      "构造的DOI: 10.48550/arXiv.2203.13291v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Action Detection by Joint Identification-Verification\n",
      "构造的DOI: 10.48550/arXiv.1810.08375v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Embarrassingly simple model for early action proposal\n",
      "构造的DOI: 10.48550/arXiv.1810.07420v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cross-Modal and Hierarchical Modeling of Video and Text\n",
      "构造的DOI: 10.48550/arXiv.2003.00392v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: 3D Feature Pyramid Attention Module for Robust Visual Speech Recognition\n",
      "构造的DOI: 10.48550/arXiv.1810.06178v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Every pixel counts ++: Joint learning of geometry and motion with 3D holistic understanding\n",
      "构造的DOI: 10.48550/arXiv.1810.06125v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Interpretable spatio-temporal attention for video action recognition\n",
      "构造的DOI: 10.48550/arXiv.1810.04511v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Attentive sequence to sequence translation for localizing clips of interest by natural language descriptions\n",
      "构造的DOI: 10.48550/arXiv.1808.08803v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Predicting action tubes\n",
      "构造的DOI: 10.48550/arXiv.1808.07712v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Fast video shot transition localization with deep structured models\n",
      "构造的DOI: 10.48550/arXiv.1808.04234v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Time perception machine: Temporal point processes for the when, where and what of activity prediction\n",
      "DOI: 10.1016/j.physrep.2012.03.001\n",
      "\n",
      "处理标题: Pixel objectness: Learning to segment generic objects automatically in images and videos\n",
      "构造的DOI: 10.48550/arXiv.1808.04702v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Dynamic temporal pyramid network: A closer look at multi-scale modeling for activity detection\n",
      "构造的DOI: 10.48550/arXiv.1808.02536v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Video re-localization\n",
      "构造的DOI: 10.48550/arXiv.1808.06601v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TraMNet - Transition matrix network for efficient action tube proposals\n",
      "构造的DOI: 10.48550/arXiv.1907.01847v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs\n",
      "构造的DOI: 10.48550/arXiv.1808.00079v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Understanding human-human interactions: A survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multi-fiber networks for video recognition\n",
      "构造的DOI: 10.48550/arXiv.2105.14195v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action detection from a robot-car perspective\n",
      "DOI: 10.1103/PhysRevE.105.045315\n",
      "\n",
      "处理标题: Actor-centric relation network\n",
      "DOI: 10.1016/j.joi.2009.06.004\n",
      "\n",
      "处理标题: Diagnosing error in temporal action detectors\n",
      "构造的DOI: 10.48550/arXiv.1807.10706v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: W-TALC: Weakly-supervised temporal activity localization and classification\n",
      "构造的DOI: 10.48550/arXiv.1601.02129v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Move forward and tell: A progressive generator of video descriptions\n",
      "构造的DOI: 10.48550/arXiv.1807.10018v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AutoLoc: Weakly-supervised Temporal action localization in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2110.00111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: S3D: Single shot multi-span detector via fully 3D convolutional network\n",
      "构造的DOI: 10.48550/arXiv.1807.08069v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CTAP: Complementary temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.1807.04821v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal instance learning: Action tubes from class supervision\n",
      "构造的DOI: 10.48550/arXiv.1807.02800v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Step-by-step erasion, one-by-one collection: Aweakly supervised temporal action detector\n",
      "DOI: 10.1103/PhysRevLett.125.173901\n",
      "\n",
      "处理标题: A flexible model for training action localization with varying levels of supervision\n",
      "构造的DOI: 10.48550/arXiv.1806.11328v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: YH technologies at ActivityNet Challenge 2018\n",
      "构造的DOI: 10.48550/arXiv.1807.00686v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Human Action Recognition and Prediction: A Survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Modeling spatio-temporal human track structure for action localization\n",
      "构造的DOI: 10.48550/arXiv.1806.11008v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Every Pixel Counts: Unsupervised geometry learning with holistic 3d motion understanding\n",
      "构造的DOI: 10.48550/arXiv.1806.10556v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Best Vision Technologies Submission to ActivityNet Challenge 2018-Task: Dense-Captioning Events in Videos\n",
      "构造的DOI: 10.48550/arXiv.2407.07478v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Deep reinforcement learning for surgical gesture segmentation and classification\n",
      "构造的DOI: 10.48550/arXiv.1806.08089v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: BSN: Boundary sensitive network for temporal action proposal generation\n",
      "构造的DOI: 10.48550/arXiv.1806.02964v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action4D: Real-time action recognition in the Crowd and clutter\n",
      "构造的DOI: 10.48550/arXiv.2105.09188v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Pointly-supervised action localization\n",
      "DOI: 10.1016/j.aim.2011.04.007\n",
      "\n",
      "处理标题: VideoCapsuleNet: A simplified network for action detection\n",
      "构造的DOI: 10.48550/arXiv.1805.08162v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action completion: A temporal model for moment detection\n",
      "构造的DOI: 10.48550/arXiv.1805.06749v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NeuralNetwork-Viterbi: A framework for weakly supervised video learning\n",
      "构造的DOI: 10.48550/arXiv.2003.13141v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Visual attribute-augmented three-dimensional convolutional neural network for enhanced human action recognition\n",
      "构造的DOI: 10.48550/arXiv.1912.04217v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Low-latency human action recognition with weighted multi-region convolutional neural network\n",
      "构造的DOI: 10.48550/arXiv.1805.02877v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly-supervised visual instrument-playing action detection in videos\n",
      "构造的DOI: 10.48550/arXiv.2207.11805v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Revisiting temporal modeling for video-based person ReID\n",
      "构造的DOI: 10.48550/arXiv.1805.02104v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Jointly localizing and describing events for dense video captioning\n",
      "构造的DOI: 10.48550/arXiv.1804.08274v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking the faster R-CNN architecture for temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1804.07667v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: To find where you talk: Temporal sentence localization in video with attention based location regression\n",
      "构造的DOI: 10.48550/arXiv.1804.07014v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Precise Temporal Action Localization by Evolving Temporal Proposals\n",
      "DOI: 10.1145/3206025.3206029\n",
      "\n",
      "处理标题: Mutual suppression network for video prediction using disentangled features\n",
      "构造的DOI: 10.48550/arXiv.1804.04810v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Multilevel language and vision integration for text-to-clip retrieval\n",
      "构造的DOI: 10.48550/arXiv.1804.05113v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: SoccerNet: A scalable dataset for action spotting in soccer videos\n",
      "DOI: 10.1109/CVPRW.2018.00223\n",
      "\n",
      "处理标题: Fine-grained activity recognition in baseball videos\n",
      "构造的DOI: 10.48550/arXiv.1804.03247v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Layout-induced video representation for recognizing agent-in-place actions\n",
      "DOI: 10.1109/VCIP56404.2022.10008833\n",
      "\n",
      "处理标题: End-to-end dense video captioning with masked transformer\n",
      "构造的DOI: 10.48550/arXiv.1804.00819v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: When will you do what? Anticipating temporal occurrences of activities\n",
      "构造的DOI: 10.48550/arXiv.1804.00892v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning to anonymize faces for privacy preserving action detection\n",
      "构造的DOI: 10.48550/arXiv.1803.11556v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Motion-appearance co-memory networks for video question answering\n",
      "DOI: 10.1145/3077136.3080655\n",
      "\n",
      "处理标题: Unsupervised learning and segmentation of complex activities from video\n",
      "构造的DOI: 10.48550/arXiv.1803.09490v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: DYAN: A dynamical atoms-based network for video prediction?\n",
      "构造的DOI: 10.48550/arXiv.1907.06835v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Featureless: Bypassing feature extraction in action categorization\n",
      "构造的DOI: 10.48550/arXiv.1803.06962v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal gaussian mixture layer for videos\n",
      "构造的DOI: 10.48550/arXiv.1803.06316v6 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal human action segmentation via dynamic clustering\n",
      "构造的DOI: 10.48550/arXiv.1803.05790v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint event detection and description in continuous video streams\n",
      "构造的DOI: 10.48550/arXiv.1802.10250v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rehar: Robust and efficient human activity recognition\n",
      "构造的DOI: 10.48550/arXiv.1802.09745v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Real-time end-to-end action detection with two-stream networks\n",
      "构造的DOI: 10.48550/arXiv.2412.10892v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Online detection of action start in untrimmed, streaming videos\n",
      "构造的DOI: 10.48550/arXiv.1903.09868v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning representative temporal features for action recognition\n",
      "构造的DOI: 10.48550/arXiv.2109.11593v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Object detection in videos by high quality object linking\n",
      "构造的DOI: 10.48550/arXiv.1801.09823v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: End-to-end fine-grained action segmentation and recognition using conditional random field models and discriminative sparse coding\n",
      "构造的DOI: 10.48550/arXiv.1801.09571v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Contextual multi-scale region convolutional 3D network for activity detection\n",
      "构造的DOI: 10.48550/arXiv.1801.09184v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatial temporal graph convolutional networks for skeleton-based action recognition\n",
      "构造的DOI: 10.48550/arXiv.1801.07455v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: HACS: Human action clips and segments dataset for recognition and temporal localization\n",
      "构造的DOI: 10.48550/arXiv.1712.09374v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly supervised action localization by sparse temporal pooling network\n",
      "构造的DOI: 10.48550/arXiv.1712.05080v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification\n",
      "构造的DOI: 10.48550/arXiv.2503.13777v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Learning latent super-events to detect multiple activities in videos\n",
      "构造的DOI: 10.48550/arXiv.1712.01938v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Budget-aware activity detection with a recurrent policy network\n",
      "构造的DOI: 10.48550/arXiv.1712.00097v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Graph distillation for action detection with privileged modalities\n",
      "构造的DOI: 10.48550/arXiv.1712.00108v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: An end-to-end 3D convolutional neural network for action detection and segmentation in videos\n",
      "构造的DOI: 10.48550/arXiv.1712.01111v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Predictive learning: Using future representation learning variantial autoencoder for human action prediction\n",
      "构造的DOI: 10.48550/arXiv.1711.09265v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Excitation backprop for RNNs\n",
      "构造的DOI: 10.48550/arXiv.1711.06778v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: RGB-D-based human motion recognition with deep learning: A survey\n",
      "构造的DOI: 10.48550/arXiv.1708.05296v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Single shot temporal action detection\n",
      "DOI: 10.1145/3123266.3123343\n",
      "\n",
      "处理标题: Differentiating objects by motion: Joint detection and tracking of small flying objects\n",
      "构造的DOI: 10.48550/arXiv.1709.04666v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A comprehensive survey of deep learning in remote sensing: Theories, tools and challenges for the community\n",
      "DOI: 10.1038/s41467-024-54341-8\n",
      "\n",
      "处理标题: Action classification and highlighting in videos\n",
      "构造的DOI: 10.48550/arXiv.1612.00558v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ConvNet architecture search for spatiotemporal feature learning\n",
      "构造的DOI: 10.48550/arXiv.1708.05038v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Exploring temporal preservation networks for precise temporal action localization\n",
      "构造的DOI: 10.48550/arXiv.1708.03280v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal context network for activity localization in videos\n",
      "构造的DOI: 10.48550/arXiv.2103.13141v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal action detection with cascade proposal and location anticipation 2017\n",
      "构造的DOI: 10.48550/arXiv.1708.00042v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Localizing Actions from Video Labels and Pseudo-Annotations\n",
      "构造的DOI: 10.48550/arXiv.2008.13705v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatial-aware object embeddings for zero-shot localization and classification of actions\n",
      "构造的DOI: 10.48550/arXiv.1707.09145v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Spatio-temporal human action localisation and instance segmentation in temporally untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.1707.07213v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal Convolution Based Action Proposal: Submission to ActivityNet 2017\n",
      "构造的DOI: 10.48550/arXiv.1704.03503v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600 Papers Survey\n",
      "构造的DOI: 10.48550/arXiv.1707.06436v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Detecting parts for action localization\n",
      "构造的DOI: 10.48550/arXiv.1707.06005v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Red: Reinforced encoder-decoder networks for action anticipation\n",
      "构造的DOI: 10.48550/arXiv.1707.04818v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A self-adaptive proposal model for temporal action detection based on reinforcement learning\n",
      "构造的DOI: 10.48550/arXiv.1706.07251v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Joint max margin and semantic features for continuous event detection in complex scenes\n",
      "构造的DOI: 10.48550/arXiv.1706.04122v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action search: Spotting actions in videos and its application to temporal action localization\n",
      "DOI: 10.1109/cvprw59228.2023.00538\n",
      "\n",
      "处理标题: Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition\n",
      "构造的DOI: 10.48550/arXiv.2103.15232v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action sets: Weakly supervised action segmentation without ordering constraints\n",
      "构造的DOI: 10.48550/arXiv.1706.00699v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Generic tubelet proposals for action localization\n",
      "构造的DOI: 10.48550/arXiv.1705.10861v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Continuous video to simple signals for swimming stroke detection with convolutional neural networks\n",
      "构造的DOI: 10.48550/arXiv.1705.09894v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Extraction and classification of diving clips from continuous video footage\n",
      "构造的DOI: 10.48550/arXiv.1705.09003v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AVA: A video dataset of spatio-temporally localized atomic visual actions\n",
      "构造的DOI: 10.48550/arXiv.1705.08421v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation\n",
      "构造的DOI: 10.48550/arXiv.1705.07818v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: TALL: Temporal activity localization via language query\n",
      "构造的DOI: 10.48550/arXiv.1705.02101v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Am I done? Predicting action progress in video\n",
      "构造的DOI: 10.48550/arXiv.1705.01781v4 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Cascaded boundary regression for temporal action detection Y 201\n",
      "构造的DOI: 10.48550/arXiv.1705.01180v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Action understanding with multiple classes of actors\n",
      "构造的DOI: 10.48550/arXiv.1704.08723v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Satiooral Person retrieval via natural language queries\n",
      "构造的DOI: 10.48550/arXiv.1704.07945v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action detection with structured segment networks\n",
      "构造的DOI: 10.48550/arXiv.2308.09268v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: AmTNet: Action-micro-tube regression by end-to-end trainable deep architecture\n",
      "构造的DOI: 10.48550/arXiv.1704.04952v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Temporal action localization by structured maximal sums\n",
      "构造的DOI: 10.48550/arXiv.1704.04671v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization\n",
      "构造的DOI: 10.48550/arXiv.1704.04232v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Predictive-corrective networks for action detection\n",
      "构造的DOI: 10.48550/arXiv.1810.08375v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Forecasting human dynamics from static images\n",
      "构造的DOI: 10.48550/arXiv.1704.03432v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: ActionVLAD: Learning spatio-temporal aggregation for action classification\n",
      "构造的DOI: 10.48550/arXiv.1704.02895v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Incremental tube construction for human action detection\n",
      "构造的DOI: 10.48550/arXiv.1704.01358v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Two stream LSTM:A deep fusion framework for human action recognition\n",
      "构造的DOI: 10.48550/arXiv.1704.01194v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection\n",
      "构造的DOI: 10.48550/arXiv.1704.00616v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Unsupervised action proposal ranking through proposal recombination\n",
      "构造的DOI: 10.48550/arXiv.1704.00758v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos\n",
      "构造的DOI: 10.48550/arXiv.1703.10664v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Towards automatic learning of procedures from web instructional videos\n",
      "构造的DOI: 10.48550/arXiv.1703.09788v3 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling\n",
      "构造的DOI: 10.48550/arXiv.2201.05675v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: R-C3D: Region convolutional 3D network for temporal activity detection\n",
      "构造的DOI: 10.48550/arXiv.1801.09184v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: Turn tap: Temporal unit regression network for temporal action proposals\n",
      "构造的DOI: 10.48550/arXiv.1703.06189v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: UntrimmedNets for weakly supervised action recognition and detection\n",
      "构造的DOI: 10.48550/arXiv.1703.03329v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: A pursuit of temporal accuracy in general activity detection\n",
      "构造的DOI: 10.48550/arXiv.1703.02716v1 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: PathTrack: Fast trajectory annotation with path supervision\n",
      "构造的DOI: 10.48550/arXiv.1703.02437v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: NoScope: Optimizing neural network queries over video at scale\n",
      "构造的DOI: 10.48550/arXiv.1805.01046v2 (元数据中无显式DOI)\n",
      "\n",
      "处理标题: CDC: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos\n",
      "构造的DOI: 10.48550/arXiv.2107.12618v1 (元数据中无显式DOI)\n",
      "\n",
      "处理了 1552 个arXiv条目和 95 个非arXiv条目。\n",
      "将 1647 个条目写入输出文件: D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal_updated.ris\n",
      "处理完成。\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    \"\"\"使用标题查询arXiv API，获取arXiv ID并构造DOI。\"\"\"\n",
    "    # 如果标题包含冒号，只使用冒号后面的部分\n",
    "    if ':' in title:\n",
    "        title = title.split(':', 1)[1].strip()\n",
    "    \n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1'\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        \n",
    "        root = ET.fromstring(data)\n",
    "        ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}\n",
    "        \n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return None, f\"未找到标题为 '{title}' 的文章。\"\n",
    "        \n",
    "        id_element = entry.find('atom:id', ns)\n",
    "        if id_element is None or not id_element.text:\n",
    "            return None, \"无法获取arXiv ID。\"\n",
    "        \n",
    "        arxiv_id = id_element.text.split('/')[-1]  # 例如：2504.13460v3\n",
    "        doi_element = entry.find('arxiv:doi', ns)\n",
    "        if doi_element is not None and doi_element.text:\n",
    "            return doi_element.text, f\"DOI: {doi_element.text}\"\n",
    "        \n",
    "        constructed_doi = f\"10.48550/arXiv.{arxiv_id}\"\n",
    "        return constructed_doi, f\"构造的DOI: {constructed_doi} (元数据中无显式DOI)\"\n",
    "            \n",
    "    except urllib.error.URLError as e:\n",
    "        return None, f\"获取数据时出错: {e}\"\n",
    "    except ET.ParseError:\n",
    "        return None, \"解析XML响应时出错。\"\n",
    "\n",
    "def parse_ris_file(file_path):\n",
    "    \"\"\"解析RIS文件，将其分为条目列表，每个条目为行列表。\"\"\"\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                current_entry.append(line)\n",
    "            if line.startswith('ER  -'):\n",
    "                entries.append(current_entry)\n",
    "                current_entry = []\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def process_ris_entries(entries):\n",
    "    \"\"\"处理RIS条目，为arXiv条目添加DOI，并分类条目。\"\"\"\n",
    "    arxiv_entries = []\n",
    "    other_entries = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        publisher = None\n",
    "        title = None\n",
    "        \n",
    "        # 提取PB和TI\n",
    "        for line in entry:\n",
    "            if line.startswith('PB  -'):\n",
    "                publisher = line[6:].strip()\n",
    "            elif line.startswith('TI  -'):\n",
    "                title = line[6:].strip()\n",
    "        \n",
    "        if publisher == 'arXiv' and title:\n",
    "            print(f\"\\n处理标题: {title}\")\n",
    "            doi, message = get_arxiv_doi(title)\n",
    "            print(message)\n",
    "            \n",
    "            if doi:\n",
    "                # 在ER之前插入DO字段\n",
    "                new_entry = [line for line in entry if not line.startswith('ER  -')]\n",
    "                new_entry.append(f'DO  - {doi}')\n",
    "                new_entry.append('ER  -')\n",
    "                arxiv_entries.append(new_entry)\n",
    "            else:\n",
    "                # 如果DOI获取失败，保持条目不变\n",
    "                arxiv_entries.append(entry)\n",
    "            \n",
    "            # 遵守arXiv API速率限制\n",
    "            time.sleep(0)\n",
    "        else:\n",
    "            other_entries.append(entry)\n",
    "    \n",
    "    return arxiv_entries, other_entries\n",
    "\n",
    "def write_ris_file(entries, output_path):\n",
    "    \"\"\"将条目写入新的RIS文件。\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in entries:\n",
    "            for line in entry:\n",
    "                f.write(line + '\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "def main(input_path, output_path):\n",
    "    print(f\"读取RIS文件: {input_path}\")\n",
    "    entries = parse_ris_file(input_path)\n",
    "    print(f\"输入文件中找到 {len(entries)} 个条目。\")\n",
    "    \n",
    "    arxiv_entries, other_entries = process_ris_entries(entries)\n",
    "    print(f\"\\n处理了 {len(arxiv_entries)} 个arXiv条目和 {len(other_entries)} 个非arXiv条目。\")\n",
    "    \n",
    "    # 合并条目：先arXiv，再其他\n",
    "    all_entries = arxiv_entries + other_entries\n",
    "    print(f\"将 {len(all_entries)} 个条目写入输出文件: {output_path}\")\n",
    "    write_ris_file(all_entries, output_path)\n",
    "    print(\"处理完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal.ris\"\n",
    "    output_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal_updated.ris\"\n",
    "    main(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xiong, T.\n",
      "AU  - Wei, W.\n",
      "AU  - Xu, K.\n",
      "AU  - Chen, D.\n",
      "TI  - SA-DETR:Span Aware Detection Transformer for Moment Retrieval\n",
      "PY  - 2025\n",
      "T2  - Proceedings - International Conference on Computational Linguistics, COLING\n",
      "VL  - Part F206484-1\n",
      "SP  - 7634\n",
      "EP  - 7647\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218506386&partnerID=40&md5=a00effe39f4c900caf8b1435095241e5\n",
      "AD  - Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, China\n",
      "AD  - Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL), China\n",
      "AD  - Ping An Property & Casualty Insurance company of China, Ltd., China\n",
      "AB  - Moment Retrieval aims to locate specific video segments related to the given text. Recently, DETR-based methods, originating from Object Detection, have emerged as effective solutions for Moment Retrieval. These approaches focus on multimodal feature fusion and refining Queries composed of span anchor and content embedding. Despite the success, they often overlook the video-text instance related information in Query Initialization and the crucial guidance role of span anchors in Query Refinement, leading to inaccurate predictions. To address this, we propose a novel Span Aware DEtection TRansformer (SA-DETR) that leverages the importance of instance related span anchors. To fully leverage the instance related information, we generate span anchors based on video-text pair rather than using learnable parameters, as is common in conventional DETR-based methods, and supervise them with GT labels. To effectively exploit the correspondence between span anchors and video clips, we enhance content embedding guided by textual features and generate Gaussian mask to modulate the interaction between content embedding and fusion features. Furthermore, we explore the feature alignment across various stages and granularities and apply denoise learning to boost the span awareness of the model. Extensive experiments on QVHighlights, Charades-STA, and TACoS demonstrate the effectiveness of our approach. © 2025 Association for Computational Linguistics.\n",
      "KW  - Computational linguistics\n",
      "KW  - Content based retrieval\n",
      "KW  - Embeddings\n",
      "KW  - Query processing\n",
      "KW  - Video analysis\n",
      "KW  - Effective solution\n",
      "KW  - Embeddings\n",
      "KW  - Fusion features\n",
      "KW  - Gaussian masks\n",
      "KW  - Multimodal feature fusions\n",
      "KW  - Objects detection\n",
      "KW  - Query refinement\n",
      "KW  - Textual features\n",
      "KW  - Video segments\n",
      "KW  - Video-clips\n",
      "A2  - Rambow O.\n",
      "A2  - Wanner L.\n",
      "A2  - Apidianaki M.\n",
      "A2  - Al-Khalifa H.\n",
      "A2  - Di Eugenio B.\n",
      "A2  - Schockaert S.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 29512093 (ISSN); 979-889176196-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc. Main Conf. Int. Conf. Comput. Linguist., COLING\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: W. Wei; Cognitive Computing and Intelligent Information Processing (CCIIP) Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, China; email: weiw@hust.edu.cn; Conference name: 31st International Conference on Computational Linguistics, COLING 2025; Conference date: 19 January 2025 through 24 January 2025; Conference code: 206484\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 2 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wu, Z.\n",
      "AU  - Cheng, B.\n",
      "AU  - Han, J.\n",
      "AU  - Ma, J.\n",
      "AU  - Zhang, S.\n",
      "AU  - Chen, Y.\n",
      "AU  - Li, C.\n",
      "TI  - VideoQA-TA: Temporal-Aware Multi-Modal Video Question Answering\n",
      "PY  - 2025\n",
      "T2  - Proceedings - International Conference on Computational Linguistics, COLING\n",
      "VL  - Part F206484-1\n",
      "SP  - 7239\n",
      "EP  - 7252\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218487795&partnerID=40&md5=b0cac6c876c66c74f8a5ad9d508ef491\n",
      "AD  - State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China\n",
      "AD  - Hong Kong University of Science and Technology, Hong Kong\n",
      "AD  - North China Institute of Computing Technology, China\n",
      "AB  - Video question answering (VideoQA) has recently gained considerable attention in the field of computer vision, aiming to generate answers rely on both linguistic and visual reasoning. However, existing methods often align visual or textual features directly with large language models, which limits the deep semantic association between modalities and hinders a comprehensive understanding of the interactions within spatial and temporal contexts, ultimately leading to sub-optimal reasoning performance. To address this issue, we propose a novel temporal-aware framework for multi-modal video question answering, dubbed VideoQA-TA, which enhances reasoning ability and accuracy of VideoQA by aligning videos and questions at fine-grained levels. Specifically, an effective Spatial-Temporal Attention mechanism (STA) is designed for video aggregation, transforming video features into spatial and temporal representations while attending to information at different levels. Furthermore, a Temporal Object Injection strategy (TOI) is proposed to align object-level and frame-level information within videos, which further improves the accuracy by injecting explicit temporal information. Experimental results on MSVD-QA, MSRVTT-QA, and ActivityNet-QA datasets demonstrate the superior performance of our proposed method compared with the current SOTAs, meanwhile, visualization analysis further verifies the effectiveness of incorporating temporal information to videos. © 2025 Association for Computational Linguistics.\n",
      "KW  - Computational linguistics\n",
      "KW  - Modeling languages\n",
      "KW  - Semantics\n",
      "KW  - Video analysis\n",
      "KW  - Visual languages\n",
      "KW  - Language model\n",
      "KW  - Linguistic reasonings\n",
      "KW  - Multi-modal\n",
      "KW  - Question Answering\n",
      "KW  - Reasoning performance\n",
      "KW  - Semantic associations\n",
      "KW  - Temporal information\n",
      "KW  - Textual features\n",
      "KW  - Visual feature\n",
      "KW  - Visual reasoning\n",
      "KW  - Question answering\n",
      "A2  - Rambow O.\n",
      "A2  - Wanner L.\n",
      "A2  - Apidianaki M.\n",
      "A2  - Al-Khalifa H.\n",
      "A2  - Di Eugenio B.\n",
      "A2  - Schockaert S.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 29512093 (ISSN); 979-889176196-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc. Main Conf. Int. Conf. Comput. Linguist., COLING\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: B. Cheng; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China; email: chengbo@bupt.edu.cn; Conference name: 31st International Conference on Computational Linguistics, COLING 2025; Conference date: 19 January 2025 through 24 January 2025; Conference code: 206484\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 3 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15061 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209929118&partnerID=40&md5=5dcdf2a5b4cd59526cedae3e19541d73\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172645-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 4 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15068 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209364057&partnerID=40&md5=cbba743c71b4db9b7489b96e434a729a\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172683-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 5 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15099 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210182049&partnerID=40&md5=9f293d9c63bc930902b3b22d2340897d\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172939-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 6 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15125 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209593294&partnerID=40&md5=4ff3e105b6642a25289b8dda9a728a05\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172854-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 7 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15129 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210032746&partnerID=40&md5=80ae0dd01b8ed951209c421d988867e1\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173208-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 8 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15141 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210445131&partnerID=40&md5=1f7fc718579faa345443677551eeb75a\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173009-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 9 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15074 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209824439&partnerID=40&md5=6f2847e10dcc291491daa721fb2fd8fe\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172639-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 10 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15085 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209385687&partnerID=40&md5=2bdbdc8172e1877c52e684900643f185\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173382-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 11 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15108 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209930371&partnerID=40&md5=1d54eebe874e7b97ea7b008ab53a6eb1\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172972-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 12 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15109 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209943183&partnerID=40&md5=6dae08ae60322efb6fa211e953bdac26\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172982-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 13 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15133 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209946695&partnerID=40&md5=7bf27e99eb56b0f02921bcd9c52cd73b\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173225-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 14 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15087 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209200935&partnerID=40&md5=aac735f0824eabac382c8bb299cf01ea\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173396-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 15 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15139 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209924783&partnerID=40&md5=946c5e0ec7968c1d7f0a648bcd81e4a2\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173003-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 16 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15146 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209561256&partnerID=40&md5=fa59ab1c7a515b0082e857ec08e884b2\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173222-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 17 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15116 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209208705&partnerID=40&md5=82a03d75de20d5f7230d2097a2acaeb0\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173635-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 18 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15066 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209951471&partnerID=40&md5=1f59d717694d08cc308a77f0f070bb3d\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173241-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 19 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15084 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209778824&partnerID=40&md5=95ad421fd385d12f401706a6865bc06c\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173346-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 20 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15069 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209999410&partnerID=40&md5=80f9c41541668bd425bdc2ef0057bba0\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173246-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 21 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15107 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209374872&partnerID=40&md5=8a16d0f4988088213583bc765d2f6525\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172966-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 22 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15130 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209363345&partnerID=40&md5=bf71c91f91e822efa5c3d7249020985d\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173219-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 23 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15076 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206353431&partnerID=40&md5=5d32315cda029d77213fd6d6e04d6d87\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172648-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 24 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15081 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208568046&partnerID=40&md5=20054db0e3aa8ffa413fd2ab92ae44e4\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173336-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 25 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15092 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208584810&partnerID=40&md5=70da409236f3dbcbf8e9908af38f01fc\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172753-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 26 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15063 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208563157&partnerID=40&md5=6d6614a14ddc3017e63801f27b276870\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172651-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 27 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15134 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208560052&partnerID=40&md5=f3ec980b307de31b411a92110a1c15c0\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173115-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 28 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15114 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208561022&partnerID=40&md5=57ffe6efb198f9a0a1d1ec1daecd3b4e\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172991-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 29 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15096 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206368620&partnerID=40&md5=b598ab5e022c4425de3bcbef73a976a7\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172919-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 30 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15100 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206244860&partnerID=40&md5=35c429d78f04e611c901e9192403e07b\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172945-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 31 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15062 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206379356&partnerID=40&md5=8bbf8a62874656c0f971ca107998219b\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173234-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 32 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15140 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206389714&partnerID=40&md5=7cc5f43d3965322e0a5f4fd5ff697cea\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173006-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 33 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15060 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208216152&partnerID=40&md5=1165717a45a0f96af1ee2215cf92ba2d\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172626-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 34 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15080 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206369969&partnerID=40&md5=dd1c09bf4f5fcae8291edcd86b83840b\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172669-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 35 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science\n",
      "VL  - 15059 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003535358&partnerID=40&md5=f5e6d29952b041385c86f758fff44fc3\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173231-7 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 36 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15112 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208574070&partnerID=40&md5=3c3d191cf0bb95c31a1168fdec95dee1\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172948-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 37 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15093 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206371635&partnerID=40&md5=b0ca174be38d735fd1e61c0c618096fd\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172760-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 38 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15115 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206370132&partnerID=40&md5=f7a220ae2192a45dd5638be1efb52248\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172997-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 39 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15088 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208588011&partnerID=40&md5=e288be1c8e8d3239d99f4bf460d37128\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173403-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 40 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15120 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208576313&partnerID=40&md5=ea1a095805e3a77b5c379a0f1fd9970f\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173032-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 41 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15104 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206380738&partnerID=40&md5=4f915320ebc7646401bdd2aa3a780907\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172951-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 42 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15137 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208579704&partnerID=40&md5=e3178502007223e2fe7e902c88fb089a\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172985-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 43 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15142 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208545919&partnerID=40&md5=82d6955d1c8bdaf35841a9e0714883e8\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172906-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 44 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15082 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208578689&partnerID=40&md5=22ce8625f0b130f5852f79d9e104b454\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172690-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 45 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15122 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208580926&partnerID=40&md5=c6974acfa8c6466e513cea314d93ef56\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173038-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 46 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15101 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206387075&partnerID=40&md5=5d883d05084f953c81c7ff99e3cd05d4\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172774-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 47 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15102 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206382736&partnerID=40&md5=7df2a866fe55588063eb3ecbbd2662b8\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172783-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 48 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15086 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208562840&partnerID=40&md5=b3ce2cedb07ff5e59bfe99c91bb06fe6\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303173389-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 49 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15067 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208270353&partnerID=40&md5=292e3d43060773aa96561f8387b47e6b\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172672-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 50 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2025\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 15065 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206185005&partnerID=40&md5=b01bdcdcaf1d273c9a8d6ca7e6e79cc2\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172666-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 51 without DOI:\n",
      "TY  - CONF\n",
      "TI  - 14th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2025\n",
      "PY  - 2025\n",
      "T2  - International Conference on Pattern Recognition Applications and Methods\n",
      "VL  - 1\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002379932&partnerID=40&md5=ef5bbe2e792b83ce1f856162003cfae9\n",
      "AB  - The proceedings contain 89 papers. The special focus in this conference is on Pattern Recognition Applications and Methods. The topics include: Assessment of Training Progression on a Surgical Simulator Using Machine Learning and Explainable Artificial Intelligence Techniques; an Easy-to-Use System for Tracking Robotic Platforms Using Time-of-Flight Sensors in Lab Environments; rethinking Model Selection Beyond ImageNet Accuracy for Waste Classification; computation of 2D Discrete Geometric Moments on Quadtrees; hyperspectral Image Compression Using Implicit Neural Representation and Meta-Learned Based Network; multi-View Skeleton Analysis for Human Action Segmentation Tasks; stance Detection in Twitter Conversations Using Reply Support Classification; LAST: Utilizing Synthetic Image Style Transfer to Tackle Domain Shift in Aerial Image Segmentation; A Comparative Analysis of Hyperparameter Effects on CNN Architectures for Facial Emotion Recognition; non Contact Stress Assessment Based on Deep Tabular Method; action Recognition in Law Enforcement: A Novel Dataset from Body Worn Cameras; anomalous Event Detection in Traffic Audio; silhouette Segmentation for Near-Fall Detection Through Analysis of Human Movements in Surveillance Videos; single Hyperspectral Image Super-Resolution Utilizing Implicit Neural Representations; game State and Spatio-Temporal Action Detection in Soccer Using Graph Neural Networks and 3D Convolutional Networks; distribution Controlled Clustering of Time Series Segments by Reduced Embeddings; partition Tree Ensembles for Improving Multi-Class Classification; FFAD: Fixed-Position Few-Shot Anomaly Detection for Wire Harness Utilizing Vision-Language Models; DNN Layers Features Reduction for Out-of-Distribution Detection; occlusion Detection for Face Image Quality Assessment; lifelong Learning Needs Sleep: Few-Shot Incremental Learning Enhanced by Sleep; Improving Floating Wind Turbine Stability with Evolutionary Computation for TMD Optimization.\n",
      "A2  - Castrillon-Santana M.\n",
      "A2  - De Marsico M.\n",
      "A2  - Fred A.\n",
      "PB  - Science and Technology Publications, Lda\n",
      "SN  - 21844313 (ISSN); 978-989758730-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Int. Conf. Pattern. Recognit. Appl. Method.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 14th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2025; Conference date: 23 February 2025 through 25 February 2025; Conference code: 328979\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 52 without DOI:\n",
      "TY  - CONF\n",
      "TI  - 2024 7th International Conference on Mechatronics, Robotics and Automation, ICMRA 2024\n",
      "PY  - 2024\n",
      "T2  - 2024 7th International Conference on Mechatronics, Robotics and Automation, ICMRA 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216089034&partnerID=40&md5=ea193a78bc1fd85ee8364b190a4c8515\n",
      "AB  - The proceedings contain 38 papers. The topics discussed include: research on propeller blades sanding degree recognition algorithm based on improved deeplabv3+ network; research on dynamic load balancing based on artificial bee colony algorithm; pump-valve coordinated control of robotic arm driven by electro-hydraulic system; hybrid steganography: leveraging chaotic encryption and CNN for robust image hiding; temporal action detection with frequency attention mechanism; spider arm: design and fabrication of a cost-effective six-axis robotic arm featuring a novel double-ring load transfer mechanism; and research on salt pool ion concentration prediction model based on LSTM and attention mechanism.\n",
      "PB  - Institute of Electrical and Electronics Engineers Inc.\n",
      "SN  - 979-835035247-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Int. Conf. Mechatronics, Robot. Autom., ICMRA\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 7th International Conference on Mechatronics, Robotics and Automation, ICMRA 2024; Conference date: 20 September 2024 through 22 September 2024; Conference code: 205554\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 53 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, P.\n",
      "AU  - Wang, F.\n",
      "AU  - Li, K.\n",
      "AU  - Chen, G.\n",
      "AU  - Wei, Y.\n",
      "AU  - Tang, S.\n",
      "AU  - Wu, Z.\n",
      "AU  - Guo, D.\n",
      "TI  - Micro-gesture Online Recognition using Learnable Query Points\n",
      "PY  - 2024\n",
      "T2  - CEUR Workshop Proceedings\n",
      "VL  - 3848\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212411224&partnerID=40&md5=9e30e7b5512640ba165c9f70a7f33ced\n",
      "AD  - School of Computer Science and Information Engineering, School of Artificial Intelligence, Hefei University of Technology (HFUT), China\n",
      "AD  - Key Laboratory of Knowledge Engineering with Big Data (HFUT), Ministry of Education, China\n",
      "AD  - Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China\n",
      "AD  - CCAI, Zhejiang University, China\n",
      "AD  - Anhui Zhonghuitong Technology Co., Ltd., China\n",
      "AB  - In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the Micro-gesture Online Recognition track in the MiGA challenge at IJCAI 2024. The Micro-gesture Online Recognition task involves identifying the category and locating the start and end times of micro-gestures in video clips. Compared to the typical Temporal Action Detection task, the Micro-gesture Online Recognition task focuses more on distinguishing between micro-gestures and pinpointing the start and end times of actions. Our solution ranks 2nd in the Micro-gesture Online Recognition track. © 2022 Copyright for this paper by its authors.\n",
      "KW  - action online recognition\n",
      "KW  - Mamba\n",
      "KW  - Micro-gesture\n",
      "KW  - video understanding\n",
      "KW  - Action online recognition\n",
      "KW  - Detection tasks\n",
      "KW  - Mamba\n",
      "KW  - Micro-gesture\n",
      "KW  - On-line recognition\n",
      "KW  - Query points\n",
      "KW  - Video understanding\n",
      "KW  - Video-clips\n",
      "KW  - Gesture recognition\n",
      "A2  - Chen H.\n",
      "A2  - Schuller B.W.\n",
      "A2  - Schuller B.W.\n",
      "A2  - Adeli E.\n",
      "A2  - Zhao G.\n",
      "PB  - CEUR-WS\n",
      "SN  - 16130073 (ISSN)\n",
      "LA  - English\n",
      "J2  - CEUR Workshop Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: K. Li; CCAI, Zhejiang University, China; email: kunli.hfut@gmail.com; D. Guo; School of Computer Science and Information Engineering, School of Artificial Intelligence, Hefei University of Technology (HFUT), China; email: guodan@hfut.edu.cn; Conference name: 2024 IJCAI Workshop and Challenge on Micro-Gesture Analysis for Hidden Emotion Understanding, MiGA 2024; Conference code: 204366\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 54 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Henrich, J.\n",
      "AU  - Post, C.\n",
      "AU  - Kneib, T.\n",
      "AU  - Yahyapour, R.\n",
      "AU  - Bingert, S.\n",
      "AU  - Traulsen, I.\n",
      "TI  - Development of a versatile and sophisticated benchmark dataset for the detection of pigs in images\n",
      "ST  - Entwicklung eines vielfältigen und anspruchsvollen Benchmark-Datensatzes für die Detektion von Schweinen in Bildern\n",
      "PY  - 2024\n",
      "T2  - Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)\n",
      "VL  - P-344\n",
      "SP  - 293\n",
      "EP  - 298\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216736405&partnerID=40&md5=21e2ccd590e2070a68fca68ea5a74dd2\n",
      "AD  - Georg-August-Universität Göttingen, Department für Volkswirtschaftslehre, Humboldtallee 3, Göttingen, 37073, Germany\n",
      "AD  - Georg-August-Universität Göttingen, Department für Nutztierwissenschaften, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany\n",
      "AD  - Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen, Burckhardtweg 4, Göttingen, 37077, Germany\n",
      "AD  - Christian-Albrechts-Universität zu Kiel, Institut für Tierzucht und Tierhaltung, Olshausenstrase 40, Kiel, 24098, Germany\n",
      "KW  - Datensatz\n",
      "KW  - Objektdetektion\n",
      "KW  - Schweine\n",
      "A2  - Hoffmann C.\n",
      "A2  - Stein A.\n",
      "A2  - Gallmann E.\n",
      "A2  - Dorr J.\n",
      "A2  - Krupitzer C.\n",
      "A2  - Floto H.\n",
      "PB  - Gesellschaft fur Informatik (GI)\n",
      "SN  - 16175468 (ISSN); 978-388579738-8 (ISBN)\n",
      "LA  - German\n",
      "J2  - Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: Die 44. Jahrestagung der Gesellschaft fur Informatik in der Land-, Forst- und Ernahrungswirtschaft 2024 - 44th Annual Conference of the German Association for Informatics in Agriculture, Forestry, and the Food Sector 2024; Conference date: 27 February 2024 through 28 February 2024; Conference code: 206180\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 55 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Paymal, S.B.\n",
      "AU  - Patil, M.S.\n",
      "TI  - Elderly People's Abnormal Behavior Detection Using HAR and CNN Algorithms\n",
      "PY  - 2024\n",
      "T2  - International Journal of Intelligent Systems and Applications in Engineering\n",
      "VL  - 12\n",
      "IS  - 13s\n",
      "SP  - 646\n",
      "EP  - 653\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186885771&partnerID=40&md5=1f285b526666c4d43688f4c215c89c14\n",
      "AD  - RIT, Islampur, 415409, India\n",
      "AD  - E&TC Department, RIT, Islampur City, 415409, India\n",
      "AB  - Artificial intelligence machine learning systems are developing rapidly and very fast with the period with increasing demand and dependencies on them. New developments have been made for Artificial Intelligence and machine learning has made artificial brains for detection, identification, and decision-making abilities available for computer machines. The paper proposes the recognition of human health prediction by recognizing abnormal actions, or signs given by humans using OPENCV CNN, a tensor flow platform comparing live actions with different action datasets stored. The system detects falls of a person, sleep, heart pain, stomach pain, shoulder pain, dizziness, and different actions related to daily routine such as exercise, reading, writing, playing, makeup, etc recognize actions are sent to the Firebase cloud platform to be monitored by user or user recommended physician. Abnormal action will provide a warning message for help or raise an alarm for help. The system can detect action using surveillance cameras, or Pi camera, or a webcam. © 2024, Auricle Global Society of Education and Research. All rights reserved.\n",
      "KW  - Convolutional Neural network (CNN)\n",
      "KW  - Deep Learning (DL)\n",
      "KW  - firebase\n",
      "KW  - Human Activity Recognition (HAR)\n",
      "KW  - IOT\n",
      "KW  - OpenCV\n",
      "PB  - Auricle Global Society of Education and Research\n",
      "SN  - 21476799 (ISSN)\n",
      "LA  - English\n",
      "J2  - Internat. J. Intel. Syst. Appl. Eng.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 56 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Yuan, L.\n",
      "AU  - Gundavarapu, N.B.\n",
      "AU  - Zhao, L.\n",
      "AU  - Zhou, H.\n",
      "AU  - Cui, Y.\n",
      "AU  - Jiang, L.\n",
      "AU  - Yang, X.\n",
      "AU  - Jia, M.\n",
      "AU  - Weyand, T.\n",
      "AU  - Friedman, L.\n",
      "AU  - Sirotenko, M.\n",
      "AU  - Wang, H.\n",
      "AU  - Schroff, F.\n",
      "AU  - Adam, H.\n",
      "AU  - Yang, M.-H.\n",
      "AU  - Liu, T.\n",
      "AU  - Gong, B.\n",
      "TI  - VideoGLUE: Video General Understanding Evaluation of Foundation Models\n",
      "PY  - 2024\n",
      "T2  - Transactions on Machine Learning Research\n",
      "VL  - 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214719542&partnerID=40&md5=233eb75154a65ba8c848ce972e04fd4e\n",
      "AD  - Google DeepMind, United Kingdom\n",
      "AB  - We evaluate the video understanding capabilities of existing foundation models (FMs) using a carefully designed experiment protocol consisting of three hallmark tasks (action recognition, temporal localization, and spatiotemporal localization), eight datasets well received by the community, and four adaptation methods tailoring an FM for downstream tasks. Furthermore, we jointly profile FMs’ efficacy and efficiency when adapting to general video understanding tasks using cost measurements during both training and inference. Our main findings are as follows. First, task-specialized models significantly outperform the seven FMs studied in this work, in sharp contrast to what FMs have achieved in natural language and image understanding. Second, video-native FMs, whose pretraining data mainly contains the video modality, are generally better than image-native FMs in classifying motion-rich videos, localizing actions in time, and understanding a video of more than one action. Third, the video-native FMs can perform well on video tasks under light adaptations to downstream tasks (e.g., freezing the FM backbones), while image-native FMs win in full end-to-end finetuning. The first two observations reveal the need and tremendous opportunities to conduct research on video-focused FMs, and the last confirms that both tasks and adaptation methods matter when it comes to the evaluation of FMs. Our code is released under https://github.com/tensorflow/models/tree/master/official/projects/videoglue. © 2024, Transactions on Machine Learning Research. All rights reserved.\n",
      "PB  - Transactions on Machine Learning Research\n",
      "SN  - 28358856 (ISSN)\n",
      "LA  - English\n",
      "J2  - Transact. mach. learn. res.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Yuan; Google DeepMind, United Kingdom; email: lzyuan@google.com; B. Gong; Google DeepMind, United Kingdom; email: bgong@google.com\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 57 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, P.\n",
      "AU  - Ji, L.\n",
      "TI  - Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation\n",
      "PY  - 2024\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 235\n",
      "SP  - 31262\n",
      "EP  - 31292\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203802158&partnerID=40&md5=c3d7ee5d807b31c699313f8bddb953c7\n",
      "AD  - School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China\n",
      "AB  - Uncertainty estimation (UE), as an effective means to quantify predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks. Our source code is available at https://github.com/liupei101/MIREL. Copyright 2024 by the author(s)\n",
      "KW  - Contrastive Learning\n",
      "KW  - Federated learning\n",
      "KW  - Risk assessment\n",
      "KW  - Risk perception\n",
      "KW  - Semi-supervised learning\n",
      "KW  - Uncertainty analysis\n",
      "KW  - Decisions makings\n",
      "KW  - Estimation schemes\n",
      "KW  - Fine grained\n",
      "KW  - Fundamental theorems\n",
      "KW  - Labeled data\n",
      "KW  - Multiple-instance learning\n",
      "KW  - Predictive uncertainty\n",
      "KW  - Risks scenarios\n",
      "KW  - Symmetric functions\n",
      "KW  - Uncertainty estimation\n",
      "KW  - Self-supervised learning\n",
      "A2  - Salakhutdinov R.\n",
      "A2  - Kolter Z.\n",
      "A2  - Heller K.\n",
      "A2  - Weller A.\n",
      "A2  - Oliver N.\n",
      "A2  - Scarlett J.\n",
      "A2  - Berkenkamp F.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Ji; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; email: jiluping@uestc.edu.cn; Conference name: 41st International Conference on Machine Learning, ICML 2024; Conference date: 21 July 2024 through 27 July 2024; Conference code: 201670\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 58 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, H.\n",
      "AU  - Lin, Y.\n",
      "AU  - Li, P.\n",
      "AU  - Hu, J.\n",
      "AU  - Hu, X.\n",
      "TI  - Class-Specific Semantic Generation and Reconstruction Learning for Open Set Recognition\n",
      "PY  - 2024\n",
      "T2  - IJCAI International Joint Conference on Artificial Intelligence\n",
      "SP  - 2045\n",
      "EP  - 2053\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204288973&partnerID=40&md5=6d3208c8aa1909bf7591e294ecf4de47\n",
      "AD  - Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, China\n",
      "AD  - School of Computer Science and Information Engineering, Hefei University of Technology, China\n",
      "AD  - School of Computer Science, Minnan Normal University, China\n",
      "AD  - Key Laboratory of Data Science and Intelligence Application, Minnan Normal University, China\n",
      "AD  - School of Computing, National University of Singapore, Singapore\n",
      "AD  - Anhui Province Key Laboratory of Industry Safety and Emergency Technology, China\n",
      "AB  - Open set recognition is a crucial research theme for open-environment machine learning.For this problem, a common solution is to learn compact representations of known classes and identify unknown samples by measuring deviations from these known classes.However, the aforementioned methods (1) lack open training consideration, which is detrimental to the fitting of known classes, and (2) recognize unknown classes on an inadequate basis, which limits the accuracy of recognition.In this study, we propose an open reconstruction learning framework that learns a union boundary region of known classes to characterize unknown space.This facilitates the isolation of known space from unknown space to represent known classes compactly and provides a more reliable recognition basis from the perspective of both known and unknown space.Specifically, an adversarial constraint is used to generate class-specific boundary samples.Then, the known classes and approximate unknown space are fitted with manifolds represented by class-specific auto-encoders.Finally, the auto-encoders output the reconstruction error in terms of known and unknown spaces to recognize samples.Extensive experimental results show that the proposed method outperforms existing advanced methods and achieves new state-of-the-art performance.The code is available at https://github.com/Ashowman98/CSGRL. © 2024 International Joint Conferences on Artificial Intelligence. All rights reserved.\n",
      "KW  - Contrastive Learning\n",
      "KW  - Federated learning\n",
      "KW  - Semantics\n",
      "KW  - Auto encoders\n",
      "KW  - Compact representation\n",
      "KW  - Learn+\n",
      "KW  - Learning frameworks\n",
      "KW  - Machine-learning\n",
      "KW  - Measuring deviation\n",
      "KW  - Open environment\n",
      "KW  - Semantic generations\n",
      "KW  - Specific semantics\n",
      "KW  - Unknown class\n",
      "KW  - Adversarial machine learning\n",
      "A2  - Larson K.\n",
      "PB  - International Joint Conferences on Artificial Intelligence\n",
      "SN  - 10450823 (ISSN); 978-195679204-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - IJCAI Int. Joint Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: J. Hu; School of Computing, National University of Singapore, Singapore; email: jun.hu@nus.edu.sg; Conference name: 33rd International Joint Conference on Artificial Intelligence, IJCAI 2024; Conference date: 3 August 2024 through 9 August 2024; Conference code: 202043\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 59 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhou, W.\n",
      "AU  - Li, Y.\n",
      "AU  - Zhao, J.\n",
      "AU  - Zhao, C.\n",
      "TI  - Relabeling Abnormal Videos via Intra-Video Label Propagation for Weakly Supervised Video Anomaly Detection\n",
      "PY  - 2024\n",
      "T2  - 14th Asian Control Conference, ASCC 2024\n",
      "SP  - 1200\n",
      "EP  - 1205\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205684048&partnerID=40&md5=50a8670e594f959268c18b446eb90537\n",
      "AD  - College of Control Science and Engineering, Zhejiang University, State Key Laboratory of Industrial Control Technology, Hangzhou, China\n",
      "AB  - Weakly supervised video anomaly detection is chal-lenging due to the lack of frame-level labels in abnormal videos. Previous work generally formulates it as a multiple instance learning (MIL) problem, in which abundant snippets especially including hard snippets in abnormal videos are underutilized, causing the training process to be dominated by redundant easy snippets. In this work, we introduce a semi-supervised learning perspective and aim to provide reliable snippet-level labels for abnormal videos to fully leverage them. Specifically, we propose a relabeling strategy by utilizing the Label Propagation Network (LPN) to propagate labels from confident snippets to hard snippets. Considering label propagation tends to be fragile and inaccurate in this case due to incorrect pseudo labels, we develop a feature relation module to enhance the intra-class compactness and inter-class dispersion, as well as a label separation module to enlarge the discrepancy of propagated labels. The overall model is designed to be concise and universal, which can be applied to existing weakly supervised methods to improve performance. Extensive experiments demonstrate that our method achieves state-of-the-art results on three benchmark datasets.  © 2024 Asian Control Association.\n",
      "KW  - intra-video label propagation\n",
      "KW  - pseudo labels\n",
      "KW  - video anomaly detection\n",
      "KW  - weakly supervised learning\n",
      "KW  - Anomaly detection\n",
      "KW  - Semi-supervised learning\n",
      "KW  - Video recording\n",
      "KW  - Anomaly detection\n",
      "KW  - Intra-video label propagation\n",
      "KW  - Label propagation\n",
      "KW  - Learning problem\n",
      "KW  - Multiple-instance learning\n",
      "KW  - Pseudo label\n",
      "KW  - Relabelling\n",
      "KW  - Training process\n",
      "KW  - Video anomaly detection\n",
      "KW  - Weakly supervised learning\n",
      "KW  - Self-supervised learning\n",
      "PB  - Institute of Electrical and Electronics Engineers Inc.\n",
      "SN  - 978-988758159-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Asian Control Conf., ASCC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: C. Zhao; College of Control Science and Engineering, Zhejiang University, State Key Laboratory of Industrial Control Technology, Hangzhou, China; email: chhzhao@zju.edu.cn; Conference name: 14th Asian Control Conference, ASCC 2024; Conference date: 5 July 2024 through 8 July 2024; Conference code: 202772\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 60 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhao, L.\n",
      "AU  - Gundavarapu, N.B.\n",
      "AU  - Yuan, L.\n",
      "AU  - Zhou, H.\n",
      "AU  - Yan, S.\n",
      "AU  - Sun, J.J.\n",
      "AU  - Friedman, L.\n",
      "AU  - Qian, R.\n",
      "AU  - Weyand, T.\n",
      "AU  - Zhao, Y.\n",
      "AU  - Hornung, R.\n",
      "AU  - Schroff, F.\n",
      "AU  - Yang, M.-H.\n",
      "AU  - Ross, D.A.\n",
      "AU  - Wang, H.\n",
      "AU  - Adam, H.\n",
      "AU  - Sirotenko, M.\n",
      "AU  - Liu, T.\n",
      "AU  - Gong, B.\n",
      "TI  - VideoPrism: A Foundational Visual Encoder for Video Understanding\n",
      "PY  - 2024\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 235\n",
      "SP  - 60785\n",
      "EP  - 60811\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203843582&partnerID=40&md5=033f811a5e7a75ab68868f1fc9bbb45c\n",
      "AD  - Google, United States\n",
      "AB  - We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model.We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts).The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos.We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 31 out of 33 video understanding benchmarks. Copyright 2024 by the author(s)\n",
      "KW  - Benchmarking\n",
      "KW  - Image coding\n",
      "KW  - Video analysis\n",
      "KW  - Video recording\n",
      "KW  - Video signal processing\n",
      "KW  - Embeddings\n",
      "KW  - Global-local\n",
      "KW  - High-quality videos\n",
      "KW  - Parallel text\n",
      "KW  - Pre-training\n",
      "KW  - Video captions\n",
      "KW  - Video encoder\n",
      "KW  - Video understanding\n",
      "KW  - Video-clips\n",
      "KW  - Web video\n",
      "KW  - Semantics\n",
      "A2  - Salakhutdinov R.\n",
      "A2  - Kolter Z.\n",
      "A2  - Heller K.\n",
      "A2  - Weller A.\n",
      "A2  - Oliver N.\n",
      "A2  - Scarlett J.\n",
      "A2  - Berkenkamp F.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Zhao; Google, United States; email: longzh@google.com; M. Sirotenko; Google, United States; email: msirotenko@google.com; T. Liu; Google, United States; email: liuti@google.com; B. Gong; Google, United States; email: bgong@google.com; Conference name: 41st International Conference on Machine Learning, ICML 2024; Conference date: 21 July 2024 through 27 July 2024; Conference code: 201670\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 61 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Gao, J.\n",
      "AU  - Yao, X.\n",
      "AU  - Xu, C.\n",
      "TI  - Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation\n",
      "PY  - 2024\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 235\n",
      "SP  - 14902\n",
      "EP  - 14919\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203804387&partnerID=40&md5=07c3cf5f6313a0ba84fd3a7539ebd03b\n",
      "AD  - State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China\n",
      "AD  - School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), China\n",
      "AD  - Peng Cheng Laboratory, ShenZhen, China\n",
      "AB  - The ability to accurately comprehend natural language instructions and navigate to the target location is essential for an embodied agent. Such agents are typically required to execute user instructions in an online manner, leading us to explore the use of unlabeled test samples for effective online model adaptation. However, for online Vision-and-Language Navigation (VLN), due to the intrinsic nature of inter-sample online instruction execution and intra-sample multi-step action decision, frequent updates can result in drastic changes in model parameters, while occasional updates can make the model ill-equipped to handle dynamically changing environments. Therefore, we propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for online VLN by performing joint decomposition-accumulation analysis for both gradients and parameters in a unified framework. Extensive experiments show that our method obtains impressive performance gains on four popular benchmarks. Code is available at https://github.com/Feliciaxyao/ICML2024-FSTTA. Copyright 2024 by the author(s)\n",
      "KW  - Visual languages\n",
      "KW  - Embodied agent\n",
      "KW  - Intrinsic nature\n",
      "KW  - Model Adaptation\n",
      "KW  - Multisteps\n",
      "KW  - Natural languages\n",
      "KW  - On-line modelling\n",
      "KW  - Online instructions\n",
      "KW  - Target location\n",
      "KW  - Test samples\n",
      "KW  - Test time\n",
      "KW  - Benchmarking\n",
      "A2  - Salakhutdinov R.\n",
      "A2  - Kolter Z.\n",
      "A2  - Heller K.\n",
      "A2  - Weller A.\n",
      "A2  - Oliver N.\n",
      "A2  - Scarlett J.\n",
      "A2  - Berkenkamp F.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: J. Gao; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China; email: junyu.gao@nlpr.ia.ac.cn; C. Xu; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China; email: csxu@nlpr.ia.ac.cn; Conference name: 41st International Conference on Machine Learning, ICML 2024; Conference date: 21 July 2024 through 27 July 2024; Conference code: 201670\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 62 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ji, Z.\n",
      "AU  - Lin, C.\n",
      "AU  - Wang, H.\n",
      "AU  - Shen, C.\n",
      "TI  - Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis\n",
      "PY  - 2024\n",
      "T2  - IJCAI International Joint Conference on Artificial Intelligence\n",
      "SP  - 413\n",
      "EP  - 421\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204284900&partnerID=40&md5=1f910e7ab3e4f8d9c7d1f9fbcdfe303c\n",
      "AD  - School of Software Engineering, Xi'an Jiaotong University, China\n",
      "AD  - School of Cyber Science and Engineering, Xi'an Jiaotong University, China\n",
      "AD  - School of Automation Science and Engineering, Xi'an Jiaotong University, China\n",
      "AD  - Department of Computing, The Hong Kong Polytechnic University, Hong Kong\n",
      "AB  - Detecting synthetic from real speech is increasingly crucial due to the risks of misinformation and identity impersonation. While various datasets for synthetic speech analysis have been developed, they often focus on specific areas, limiting their utility for comprehensive research. To fill this gap, we propose the Speech-Forensics dataset by extensively covering authentic, synthetic, and partially forged speech samples that include multiple segments synthesized by different high-quality algorithms. Moreover, we propose a TEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously performing authenticity detection, multiple fake segments localization, and synthesis algorithms recognition, without any complex post-processing. TEST effectively integrates LSTM and Transformer to extract more powerful temporal speech representations and utilizes dense prediction on multi-scale pyramid features to estimate the synthetic spans. Our model achieves an average mAP of 83.55% and an EER of 5.25% at the utterance level. At the segment level, it attains an EER of 1.07% and a 92.19% F1 score. These results highlight the model's robust capability for a comprehensive analysis of synthetic speech, offering a promising avenue for future research and practical applications in this field. © 2024 International Joint Conferences on Artificial Intelligence. All rights reserved.\n",
      "KW  - Authentication\n",
      "KW  - Digital forensics\n",
      "KW  - Speech analysis\n",
      "KW  - Algorithm recognition\n",
      "KW  - Comprehensive research\n",
      "KW  - High quality\n",
      "KW  - Localisation\n",
      "KW  - Localization algorithm\n",
      "KW  - Specific areas\n",
      "KW  - Speech forensics\n",
      "KW  - Synthesis algorithms\n",
      "KW  - Synthesised\n",
      "KW  - Synthetic speech\n",
      "KW  - Spatio-temporal data\n",
      "A2  - Larson K.\n",
      "PB  - International Joint Conferences on Artificial Intelligence\n",
      "SN  - 10450823 (ISSN); 978-195679204-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - IJCAI Int. Joint Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: C. Lin; School of Cyber Science and Engineering, Xi'an Jiaotong University, China; email: linchenhao@xjtu.edu.cn; Conference name: 33rd International Joint Conference on Artificial Intelligence, IJCAI 2024; Conference date: 3 August 2024 through 9 August 2024; Conference code: 202043\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 63 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ignat, O.\n",
      "AU  - Castro, S.\n",
      "AU  - Li, W.\n",
      "AU  - Mihalcea, R.\n",
      "TI  - Learning Human Action Representations from Temporal Context in Lifestyle Vlogs\n",
      "PY  - 2024\n",
      "T2  - TextGraphs at ACL 2024 - Proceedings of TextGraphs-17: Graph-Based Methods for Natural Language Processing, 62nd Annual Meeting of the Association of Computational Linguistics\n",
      "SP  - 1\n",
      "EP  - 18\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204870799&partnerID=40&md5=f3a06e3cf7f44d6ca3807def017dad0c\n",
      "AD  - University of Michigan, Ann Arbor, United States\n",
      "AB  - We address the task of human action representation and show how the approach of generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time. We create and make publicly available the CO-ACT (Action Co-occurrence) dataset, consisting of a large graph of ∼12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. © 2024 Association for Computational Linguistics.\n",
      "KW  - Computational linguistics\n",
      "KW  - Contrastive Learning\n",
      "KW  - Action representations\n",
      "KW  - Co-occurrence\n",
      "KW  - Human actions\n",
      "KW  - Large graphs\n",
      "KW  - Link prediction\n",
      "KW  - Online video\n",
      "KW  - Prediction modelling\n",
      "KW  - Video-clips\n",
      "KW  - Visual information\n",
      "KW  - Word representations\n",
      "KW  - Video analysis\n",
      "A2  - Ustalov D.\n",
      "A2  - Gao Y.\n",
      "A2  - Panchenko A.\n",
      "A2  - Tutubalina E.\n",
      "A2  - Nikishina I.\n",
      "A2  - Ramesh A.\n",
      "A2  - Sakhovskiy A.\n",
      "A2  - Usbeck R.\n",
      "A2  - Penn G.\n",
      "A2  - Valentino M.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 979-889176145-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - TextGraphs ACL - Proc. TextGraphs-17: Graph-Based Methods Nat. Lang. Process., Annu. Meet. Assoc. Comput. Linguist.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 17th Graph-Based Methods for Natural Language Processing, TextGraphs 2024 - co-located with the 62nd Annual Meeting of the Association of Computational Linguistics, ACL 2024; Conference code: 202471\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 64 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th European Conference on Computer Vision, ECCV 2024\n",
      "PY  - 2024\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \n",
      "VL  - 15135 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209933836&partnerID=40&md5=d164662daa04965cb3c8d10985a8ea8e\n",
      "AB  - The proceedings contain 430 papers. The special focus in this conference is on Computer Vision. The topics include: SINDER: Repairing the Singular Defects of DINOv2; SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow; learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation; general and Task-Oriented Video Segmentation; VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement; LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors; controlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai.github.io/ControlNet_Plus_Plus; TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-spoofing; prompting Future Driven Diffusion Model for Hand Motion Prediction; defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics; unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement; RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation; UMBRAE: Unified Multimodal Brain Decoding; NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models; 3D Single-Object Tracking in Point Clouds with High Temporal Variation; adaptive Multi-task Learning for Few-Shot Object Detection; event Trojan: Asynchronous Event-Based Backdoor Attacks; stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization; imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems; dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning; oneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers; loA-Trans: Enhancing Visual Grounding by Location-Aware Transformers; HAC: Hash-Grid Assisted Context for 3D Gaussian Splatting Compression; Energy-Induced Explicit Quantification for Multi-modality MRI Fusion; colorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement; exemplar-Free Continual Representation Learning via Learnable Drift Compensation.\n",
      "A2  - Leonardis A.\n",
      "A2  - Ricci E.\n",
      "A2  - Roth S.\n",
      "A2  - Russakovsky O.\n",
      "A2  - Sattler T.\n",
      "A2  - Varol G.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303172979-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th European Conference on Computer Vision, ECCV 2024; Conference date: 29 September 2024 through 4 October 2024; Conference code: 320649\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 65 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wang, J.\n",
      "AU  - Xu, S.\n",
      "AU  - Zhang, T.\n",
      "TI  - A UNIQUE M-PATTERN FOR MICRO-EXPRESSION SPOTTING IN LONG VIDEOS\n",
      "PY  - 2024\n",
      "T2  - 12th International Conference on Learning Representations, ICLR 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200597711&partnerID=40&md5=daa015b05fd9562f8ca17c434d2b1dff\n",
      "AD  - South China University of Technology & Engineering Research Center, The Ministry of Education on Health Intelligent Perception and Paralleled Digital-Human, China\n",
      "AD  - Pazhou Lab, Guangzhou, China\n",
      "AB  - Micro-expression spotting (MES) is challenging since the small magnitude of micro-expression (ME) makes them susceptible to global movements like head rotation. However, the unique movement pattern and inherent characteristics of ME allow them to be distinguished from other movements. Existing MES methods based on fixed reference frame degrade optical flow accuracy and are overly dependent on facial alignment. In this paper, we propose a skip-k-frame block-wise main directional mean optical flow (MDMO) feature for MES based on unfixed reference frame. By employing skip-k-frame strategy, we substantiate the existence of a distinct and exclusive movement pattern in ME, called M-pattern due to its feature curve resembling the letter 'M'. Based on M-pattern and characteristics of ME, we then provide a novel spotting rules to precisely locate ME intervals. Block-wise MDMO feature is capable of removing global movements without compromising complete ME movements in the early feature extraction stage. Besides, A novel pixelmatch-based facial alignment algorithm with dynamic update of reference frame is proposed to better align facial images and reduce jitter between frames. Experimental results on CAS(ME)2, SAMM-LV and CASME II validate the proposed methods are superior to the state-of-the-art methods. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\n",
      "KW  - Feature curves\n",
      "KW  - Features extraction\n",
      "KW  - Flow accuracy\n",
      "KW  - Flow features\n",
      "KW  - Head rotation\n",
      "KW  - Inherent characteristics\n",
      "KW  - Micro-expressions\n",
      "KW  - Movement pattern\n",
      "KW  - Pattern characteristic\n",
      "KW  - Reference frame\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: T. Zhang; South China University of Technology & Engineering Research Center, The Ministry of Education on Health Intelligent Perception and Paralleled Digital-Human, China; email: tony@scut.edu.cn; Conference name: 12th International Conference on Learning Representations, ICLR 2024; Conference date: 7 May 2024 through 11 May 2024; Conference code: 200372\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 66 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xu, M.\n",
      "AU  - Soldan, M.\n",
      "AU  - Gao, J.\n",
      "AU  - Liu, S.\n",
      "AU  - Pérez-Rúa, J.-M.\n",
      "AU  - Ghanem, B.\n",
      "TI  - BOUNDARY DENOISING FOR VIDEO ACTIVITY LOCALIZATION\n",
      "PY  - 2024\n",
      "T2  - 12th International Conference on Learning Representations, ICLR 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200610970&partnerID=40&md5=536ce30e3be18b9e6d03c192a00f30aa\n",
      "AD  - King Abdullah University of Science and Technology (KAUST), Saudi Arabia\n",
      "AD  - National University of Singapore, Singapore\n",
      "AD  - Meta AI, United States\n",
      "AB  - Video activity localization aims at understanding the semantic content in long, untrimmed videos and retrieving actions of interest. The retrieved action with its start and end locations can be used for highlight generation, temporal action detection, etc. Unfortunately, learning the exact boundary location of activities is highly challenging because temporal activities are continuous in time, and there are often no clear-cut transitions between actions. Moreover, the definition of the start and end of events is subjective, which may confuse the model. To alleviate the boundary ambiguity, we propose to study the video activity localization problem from a denoising perspective. Specifically, we propose an encoder-decoder model named DenoiseLoc. During training, a set of temporal spans is randomly generated from the ground truth with a controlled noise scale. Then, we attempt to reverse this process by boundary denoising, allowing the localizer to predict activities with precise boundaries and resulting in faster convergence speed. Experiments show that DenoiseLoc advances several video activity understanding tasks. For example, we observe a gain of +12.36% average mAP on the QV-Highlights dataset. Moreover, DenoiseLoc achieves state-of-the-art performance on the MAD dataset but with much fewer predictions than others. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\n",
      "KW  - Boundaries ambiguity\n",
      "KW  - Clear cuts\n",
      "KW  - De-noising\n",
      "KW  - Encoder-decoder\n",
      "KW  - Fast convergence speed\n",
      "KW  - Ground truth\n",
      "KW  - Localisation\n",
      "KW  - Localization problems\n",
      "KW  - Semantic content\n",
      "KW  - Video activity\n",
      "KW  - Semantics\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 12th International Conference on Learning Representations, ICLR 2024; Conference date: 7 May 2024 through 11 May 2024; Conference code: 200372\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 67 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chen, M.\n",
      "AU  - Gao, J.\n",
      "AU  - Xu, C.\n",
      "TI  - R-EDL: RELAXING NONESSENTIAL SETTINGS OF EVIDENTIAL DEEP LEARNING\n",
      "PY  - 2024\n",
      "T2  - 12th International Conference on Learning Representations, ICLR 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200535346&partnerID=40&md5=c453104dc03d8247375da18a4393f97e\n",
      "AD  - State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China\n",
      "AD  - School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), China\n",
      "AD  - Peng Cheng Laboratory, ShenZhen, China\n",
      "AB  - A newly-arising uncertainty estimation method named Evidential Deep Learning (EDL), which can obtain reliable predictive uncertainty in a single forward pass, has garnered increasing interest. Guided by the subjective logic theory, EDL obtains Dirichlet concentration parameters from deep neural networks, thus constructing a Dirichlet probability density function (PDF) to model the distribution of class probabilities. Despite its great success, we argue that EDL keeps nonessential settings in both stages of model construction and optimization. In constructing the Dirichlet PDF, a commonly ignored prior weight parameter governs the balance between leveraging the proportion of evidence and its magnitude in deriving predictive scores. In model optimization, a variance-minimized regularization term adopted by traditional EDL encourages the Dirichlet PDF to approach a Dirac delta function, potentially exacerbating overconfidence. Therefore, we propose the R-EDL (Relaxed-EDL) method by relaxing these nonessential settings. Specifically, R-EDL treats the prior weight as an adjustable hyper-parameter instead of a fixed scalar, and directly optimizes the expectation of the Dirichlet PDF provided to deprecate the variance-minimized regularization term. Extensive experiments and SOTA performances demonstrate the effectiveness of our method. Source codes are provided in Appendix E. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\n",
      "KW  - Deep neural networks\n",
      "KW  - Delta functions\n",
      "KW  - Density functional theory\n",
      "KW  - Probability distributions\n",
      "KW  - Class probabilities\n",
      "KW  - Dirichlet\n",
      "KW  - Estimation methods\n",
      "KW  - Logic theory\n",
      "KW  - Model construction\n",
      "KW  - Model optimization\n",
      "KW  - Predictive uncertainty\n",
      "KW  - Regularization terms\n",
      "KW  - Subjective Logic\n",
      "KW  - Uncertainty estimation\n",
      "KW  - Probability density function\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Correspondence Address: C. Xu; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA), China; email: csxu@nlpr.ia.ac.cn; Conference name: 12th International Conference on Learning Representations, ICLR 2024; Conference date: 7 May 2024 through 11 May 2024; Conference code: 200372\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 68 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xiong, Y.\n",
      "AU  - Zhao, L.\n",
      "AU  - Gong, B.\n",
      "AU  - Yang, M.-H.\n",
      "AU  - Schroff, F.\n",
      "AU  - Liu, T.\n",
      "AU  - Hsieh, C.-J.\n",
      "AU  - Yuan, L.\n",
      "TI  - STRUCTURED VIDEO-LANGUAGE MODELING WITH TEMPORAL GROUPING AND SPATIAL GROUNDING\n",
      "PY  - 2024\n",
      "T2  - 12th International Conference on Learning Representations, ICLR 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200604867&partnerID=40&md5=1af39f7313879116176423754dc16f22\n",
      "AD  - Google Research, United States\n",
      "AD  - Google, United States\n",
      "AD  - UCLA, United States\n",
      "AB  - Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal localization and semantic reasoning. A powerful model is expected to be capable of capturing region-object correspondences and recognizing scene changes in a video clip, reflecting spatial and temporal granularity, respectively. To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, S-ViLM, by exploiting the intrinsic structures of these two modalities. It includes two novel designs, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features, simultaneously. Comprehensive evaluations demonstrate that S-ViLM performs favorably against existing approaches in learning more expressive representations. Specifically, S-ViLM surpasses the state-of-the-art methods substantially on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition, and temporal action localization. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\n",
      "KW  - Character recognition\n",
      "KW  - Computational linguistics\n",
      "KW  - Learning systems\n",
      "KW  - Natural language processing systems\n",
      "KW  - Semantics\n",
      "KW  - Video cameras\n",
      "KW  - Down-stream\n",
      "KW  - Fine grained\n",
      "KW  - Language model\n",
      "KW  - Local information\n",
      "KW  - Pre-training\n",
      "KW  - Structured video\n",
      "KW  - Temporal semantics\n",
      "KW  - Training methods\n",
      "KW  - Video captions\n",
      "KW  - Video-clips\n",
      "KW  - Modeling languages\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 12th International Conference on Learning Representations, ICLR 2024; Conference date: 7 May 2024 through 11 May 2024; Conference code: 200372\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 69 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tao, Y.\n",
      "AU  - Lu, C.\n",
      "AU  - Liu, M.\n",
      "AU  - Xu, K.\n",
      "AU  - Liu, T.\n",
      "AU  - Tian, Y.\n",
      "AU  - Du, Y.\n",
      "TI  - A Fast and High-quality Text-to-Speech Method with Compressed Auxiliary Corpus and Limited Target Speaker Corpus\n",
      "PY  - 2024\n",
      "T2  - 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings\n",
      "SP  - 525\n",
      "EP  - 535\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195988096&partnerID=40&md5=45dcd84f772edec4146c40af9842caef\n",
      "AD  - School of Information Science and Technology, Qingdao University of Science and Technology, Qingdao, China\n",
      "AD  - School of Reliability and Systems Engineering, Beihang University, Beijing, China\n",
      "AD  - BYD Automotive Co., Ltd., Xi'an, China\n",
      "AD  - National Engineering Research Center of Digital Home Networking, Qingdao, China\n",
      "AB  - With an auxiliary corpus (non-target speaker corpus) for model pre-training, Text-to-Speech (TTS) methods can generate high-quality speech with a limited target speaker corpus. However, this approach comes with expensive training costs. To overcome the challenge, a high-quality TTS method is proposed, significantly reducing training costs while maintaining the naturalness of synthesized speech. In this paper, we propose an auxiliary corpus compression algorithm that reduces the training cost while the naturalness of the synthesized speech is not significantly degraded. We then use the compressed corpus to pre-train the proposed TTS model CMDTTS, which fuses phoneme and word multi-level prosody modeling components and denoises the generated mel-spectrograms using denoising diffusion probabilistic models (DDPMs). In addition, a fine-tuning step that the conditional generative adversarial network (cGAN) is introduced to embed the target speaker feature and improve speech quality using the target speaker corpus. Experiments are conducted on Chinese and English single speaker's corpora, and the results show that the method effectively balances the model training speed and the synthesized speech quality and outperforms the current models. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.\n",
      "KW  - auxiliary corpus\n",
      "KW  - fine-tuning\n",
      "KW  - reducing training costs\n",
      "KW  - Text-to-Speech\n",
      "KW  - Cost reduction\n",
      "KW  - Auxiliary corpus\n",
      "KW  - Fine tuning\n",
      "KW  - High quality\n",
      "KW  - Pre-training\n",
      "KW  - Reducing training cost\n",
      "KW  - Speech quality\n",
      "KW  - Synthesized speech\n",
      "KW  - Target speaker\n",
      "KW  - Text to speech\n",
      "KW  - Training costs\n",
      "KW  - Generative adversarial networks\n",
      "A2  - Calzolari N.\n",
      "A2  - Kan M.-Y.\n",
      "A2  - Hoste V.\n",
      "A2  - Lenci A.\n",
      "A2  - Sakti S.\n",
      "A2  - Xue N.\n",
      "PB  - European Language Resources Association (ELRA)\n",
      "SN  - 978-249381410-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Jt. Int. Conf. Comput. Linguist., Lang. Resour. Eval., LREC-COLING - Main Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: Y. Tao; School of Information Science and Technology, Qingdao University of Science and Technology, Qingdao, China; email: ye.tao@qust.edu.cn; Conference name: Joint 30th International Conference on Computational Linguistics and 14th International Conference on Language Resources and Evaluation, LREC-COLING 2024; Conference date: 20 May 2024 through 25 May 2024; Conference code: 199620\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 70 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Sharma, R.\n",
      "AU  - Ji, K.\n",
      "AU  - Xu, Z.\n",
      "AU  - Chen, C.\n",
      "TI  - AUC-CL: A BATCHSIZE-ROBUST FRAMEWORK FOR SELF-SUPERVISED CONTRASTIVE REPRESENTATION LEARNING\n",
      "PY  - 2024\n",
      "T2  - 12th International Conference on Learning Representations, ICLR 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185692855&partnerID=40&md5=f8951400b39c3290f345456be0ba7207\n",
      "AD  - University at Buffalo, United States\n",
      "AD  - MBZUAI, United Arab Emirates\n",
      "AB  - Self-supervised learning through contrastive representations is an emergent and promising avenue, aiming at alleviating the availability of labeled data. Recent research in the field also demonstrates its viability for several downstream tasks, henceforth leading to works that implement the contrastive principle through innovative loss functions and methods. However, despite achieving impressive progress, most methods depend on prohibitively large batch sizes and compute requirements for good performance. In this work, we propose the AUC-Contrastive Learning, a new approach to contrastive learning that demonstrates robust and competitive performance in compute-limited regimes. We propose to incorporate the contrastive objective within the AUC-maximization framework, by noting that the AUC metric is maximized upon enhancing the probability of the network's binary prediction difference between positive and negative samples which inspires adequate embedding space arrangements in representation learning. Unlike standard contrastive methods, when performing stochastic optimization, our method maintains unbiased stochastic gradients and thus is more robust to batchsizes as opposed to standard stochastic optimization problems. Remarkably, our method with a batch size of 256, outperforms several state-of-the-art methods that may need much larger batch sizes (e.g., 4096), on ImageNet and other standard datasets. Experiments on transfer learning and few-shot learning tasks also demonstrate the downstream viability of our method. Code is available at AUC-CL. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\n",
      "KW  - Learning systems\n",
      "KW  - Stochastic systems\n",
      "KW  - AUC maximizations\n",
      "KW  - Batch sizes\n",
      "KW  - Competitive performance\n",
      "KW  - Down-stream\n",
      "KW  - Labeled data\n",
      "KW  - Loss functions\n",
      "KW  - New approaches\n",
      "KW  - Performance\n",
      "KW  - Recent researches\n",
      "KW  - Robust performance\n",
      "KW  - Optimization\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 12th International Conference on Learning Representations, ICLR 2024; Conference date: 7 May 2024 through 11 May 2024; Conference code: 200372\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 71 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhao, Q.\n",
      "AU  - Wang, Y.\n",
      "AU  - Xu, J.\n",
      "AU  - He, Y.\n",
      "AU  - Song, Z.\n",
      "AU  - Wang, L.\n",
      "AU  - Qiao, Y.\n",
      "AU  - Zhao, C.\n",
      "TI  - Does Video-Text Pretraining Help Open-Vocabulary Online Action Detection?\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000514871&partnerID=40&md5=c9004adfe21bca8debbcbb753c70fbd0\n",
      "AD  - Tongji University, China\n",
      "AD  - Shanghai AI Laboratory, China\n",
      "AD  - Nanjing University, China\n",
      "AD  - Fudan University, China\n",
      "AB  - Video understanding relies on accurate action detection for temporal analysis. However, existing mainstream methods have limitations in real-world applications due to their offline and closed-set evaluation approaches, as well as their dependence on manual annotations. To address these challenges and enable real-time action understanding in open-world scenarios, we propose OV-OAD, a zero-shot online action detector that leverages vision-language models and learns solely from text supervision. By introducing an object-centered decoder unit into a Transformer-based model, we aggregate frames with similar semantics using video-text correspondence. Extensive experiments on four action detection benchmarks demonstrate that OV-OAD outperforms other advanced zero-shot methods. Specifically, it achieves 37.5% mean average precision on THUMOS'14 and 73.8% calibrated average precision on TVSeries. This research establishes a robust baseline for zero-shot transfer in online action detection, enabling scalable solutions for open-world temporal understanding. The code will be available for download at https://github.com/OpenGVLab/OV-OAD. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: C. Zhao; Tongji University, China; email: zhaocairong@tongji.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 72 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fu, K.\n",
      "AU  - Luo, X.\n",
      "AU  - Qu, L.\n",
      "AU  - Wang, S.\n",
      "AU  - Xiong, Y.\n",
      "AU  - Maglogiannis, I.\n",
      "AU  - Gao, L.\n",
      "AU  - Wang, M.\n",
      "TI  - FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000520236&partnerID=40&md5=9b00d30f41188d4f39e51edd0106a430\n",
      "AD  - Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology, Shandong Academy of Sciences, Jinan, China\n",
      "AD  - Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, China\n",
      "AD  - Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention, China\n",
      "AD  - Fudan University, China\n",
      "AD  - University of Piraeus, Greece\n",
      "AD  - Shandong Provincial Key Laboratory of Computing Power Internet and Service Computing, Shandong Fundamental Research Center for Computer Science, Jinan, China\n",
      "AB  - The expensive fine-grained annotation and data scarcity have become the primary obstacles for the widespread adoption of deep learning-based Whole Slide Images (WSI) classification algorithms in clinical practice. Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation. They lack sufficient mining of available WSIs, severely limiting WSI classification performance. To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST. FAST consists of a dual-level annotation strategy and a dual-branch classification framework. Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches. Then, to fully mining the available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification. In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using the text encoder of visual-language models for patch classification. Finally, we integrate the results from both branches to achieve WSI classification. Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22% annotation costs. All codes and models will be publicly available on https://github.com/fukexue/FAST. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Gao; Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology, Shandong Academy of Sciences, Jinan, China; email: gaolx@sdas.org; M. Wang; Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, China; email: mnwang@fudan.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 73 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Eyzaguirre, C.\n",
      "AU  - Tang, E.\n",
      "AU  - Buch, S.\n",
      "AU  - Gaidon, A.\n",
      "AU  - Wu, J.\n",
      "AU  - Niebles, J.C.\n",
      "TI  - Streaming Detection of Queried Event Start\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000538310&partnerID=40&md5=b80147322ed93e8a90f02adc350415ce\n",
      "AD  - Stanford University, United States\n",
      "AB  - Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding-Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate four vision-language backbones and three adapter architectures in both short-clip and untrimmed video settings. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 74 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Guo, X.\n",
      "AU  - Asnani, V.\n",
      "AU  - Liu, S.\n",
      "AU  - Liu, X.\n",
      "TI  - Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000489579&partnerID=40&md5=f4ec60d29ec02c80a6f5487c1ca16d08\n",
      "AD  - Michigan State University, United States\n",
      "AB  - Model Parsing defines the task of predicting hyperparameters of the generative model (GM), given a GM-generated image as the input. Since a diverse set of hyperparameters is jointly employed by the generative model, and dependencies often exist among them, it is crucial to learn these hyperparameter dependencies for improving the model parsing performance. To explore such important dependencies, we propose a novel model parsing method called Learnable Graph Pooling Network (LGPN), in which we formulate model parsing as a graph node classification problem, using graph nodes and edges to represent hyperparameters and their dependencies, respectively. Furthermore, LGPN incorporates a learnable pooling-unpooling mechanism tailored to model parsing, which adaptively learns hyperparameter dependencies of GMs used to generate the input image. Also, we introduce a Generation Trace Capturing Network (GTC) that can efficiently identify generation traces of input images, enhancing the understanding of generated images' provenances. Empirically, we achieve state-of-the-art performance in model parsing and its extended applications, showing the superiority of the proposed LGPN. The source code is available at link. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 75 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, H.\n",
      "AU  - Li, Y.\n",
      "AU  - Mu, T.-J.\n",
      "AU  - Hu, S.-M.\n",
      "TI  - Recovering Complete Actions for Cross-dataset Skeleton Action Recognition\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000525378&partnerID=40&md5=4f20007ff46ba8cccf7f95dae5fce51a\n",
      "AD  - BNRist, Department of Computer Science and Technology, Tsinghua University, China\n",
      "AB  - Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. In this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: T.-J. Mu; BNRist, Department of Computer Science and Technology, Tsinghua University, China; email: taijiang@tsinghua.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 76 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, Y.\n",
      "AU  - Kim, H.-J.\n",
      "AU  - Lee, S.-W.\n",
      "TI  - Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000543345&partnerID=40&md5=7a9d2639dcc4626ad603b34292c98af1\n",
      "AD  - Dept. of Artificial Intelligence, Korea University, Seoul, South Korea\n",
      "AB  - Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin: 41.2% (+ 11.0%) on THUMOS14 and 32.0% (+ 5.4%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: S.-W. Lee; Dept. of Artificial Intelligence, Korea University, Seoul, South Korea; email: sw.lee@korea.ac.kr; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 77 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xiao, Y.\n",
      "AU  - Song, L.\n",
      "AU  - Huang, S.\n",
      "AU  - Wang, J.\n",
      "AU  - Song, S.\n",
      "AU  - Ge, Y.\n",
      "AU  - Li, X.\n",
      "AU  - Shan, Y.\n",
      "TI  - MambaTree: Tree Topology is All You Need in State Space Model\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000515022&partnerID=40&md5=c1d6075cfc48e0447946d54f1d63128a\n",
      "AD  - Tsinghua Shenzhen International Graduate School, Tsinghua University, China\n",
      "AD  - ARC Lab, Tencent PCG, China\n",
      "AD  - Tencent AI Lab, China\n",
      "AD  - South China Normal University, China\n",
      "AB  - The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the MambaTree network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. MambaTree is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost. Code is available at https://github.com/EasonXiao-888/GrootVL. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Song; ARC Lab, Tencent PCG, China; email: ronnysong@tencent.com; ; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 78 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xie, F.\n",
      "AU  - Zhang, W.\n",
      "AU  - Wang, Z.\n",
      "AU  - Ma, C.\n",
      "TI  - QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000502825&partnerID=40&md5=ad0f4d529348616bbf8490cc37571580\n",
      "AD  - MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China\n",
      "AD  - Huawei Noah's Ark Lab, Canada\n",
      "AB  - Recent advancements in State Space Models, notably Mamba, have demonstrated superior performance over the dominant Transformer models, particularly in reducing the computational complexity from quadratic to linear. Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens. Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities. To address these limitations, we present a new vision Mamba model, coined QuadMamba, that effectively captures local dependencies of varying granularities via quadtree-based image partition and scan. Concretely, our lightweight quadtree-based scan module learns to preserve the 2D locality of spatial regions within learned window quadrants. The module estimates the locality score of each token from their features, before adaptively partitioning tokens into window quadrants. An omnidirectional window shifting scheme is also introduced to capture more intact and informative features across different local regions. To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator. Extensive experiments demonstrate that QuadMamba achieves state-of-the-art performance in various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is in https://github.com/VISION-SJTU/QuadMamba. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: C. Ma; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; email: chaoma@sjtu.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 79 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tang, T.\n",
      "AU  - Deldari, S.\n",
      "AU  - Xue, H.\n",
      "AU  - De Melo, C.\n",
      "AU  - Salim, F.\n",
      "TI  - ViLCo-Bench: VIdeo Language COntinual learning Benchmark\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000520178&partnerID=40&md5=0a87297d14e6fc1d8155faacecbacfbc\n",
      "AD  - School of Computer Science Engineering, University of New South Wales, Australia\n",
      "AD  - DEVCOM Army Research Laboratory, United States\n",
      "AB  - Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model's ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark, ViLCo-Bench, designed to evaluate continual learning models across a range of video-text tasks. The dataset comprises ten-minute-long videos and corresponding language queries collected from publicly available datasets. Additionally, we introduce a novel memory-efficient framework that incorporates self-supervised learning and mimics long-term and short-term memory effects. This framework addresses challenges including memory complexity from long video clips, natural language complexity from open queries, and text-video misalignment. We posit that ViLCo-Bench, with greater complexity compared to existing continual learning benchmarks, would serve as a critical tool for exploring the video-language domain, extending beyond conventional class-incremental tasks, and addressing complex and limited annotation issues. The curated data, evaluations, and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 80 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, M.\n",
      "AU  - Kim, J.\n",
      "AU  - Heo, J.-P.\n",
      "TI  - Activating Self-Attention for Multi-Scene Absolute Pose Regression\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000557311&partnerID=40&md5=0e5b0c95e434425fb831205b1e14ed62\n",
      "AD  - Sungkyunkwan University, South Korea\n",
      "AB  - Multi-scene absolute pose regression addresses the demand for fast and memory-efficient camera pose estimation across various real-world environments. Nowadays, transformer-based model has been devised to regress the camera pose directly in multi-scenes. Despite its potential, transformer encoders are underutilized due to the collapsed self-attention map, having low representation capacity. This work highlights the problem and investigates it from a new perspective: distortion of query-key embedding space. Based on the statistical analysis, we reveal that queries and keys are mapped in completely different spaces while only a few keys are blended into the query region. This leads to the collapse of the self-attention map as all queries are considered similar to those few keys. Therefore, we propose simple but effective solutions to activate self-attention. Concretely, we present an auxiliary loss that aligns queries and keys, preventing the distortion of query-key space and encouraging the model to find global relations by self-attention. In addition, the fixed sinusoidal positional encoding is adopted instead of undertrained learnable one to reflect appropriate positional clues into the inputs of self-attention. As a result, our approach resolves the aforementioned problem effectively, thus outperforming existing methods in both outdoor and indoor scenes. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: J.-P. Heo; Sungkyunkwan University, South Korea; email: jaepilheo@skku.edu; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 81 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Nie, M.\n",
      "AU  - Ding, D.\n",
      "AU  - Wang, C.\n",
      "AU  - Guo, Y.\n",
      "AU  - Han, J.\n",
      "AU  - Xu, H.\n",
      "AU  - Zhang, L.\n",
      "TI  - SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000491349&partnerID=40&md5=5df3d6eb2f78a891941385dc37379f57\n",
      "AD  - School of Data Science, Fudan University, China\n",
      "AD  - Noah's Ark Lab, Huawei, China\n",
      "AB  - Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Zhang; School of Data Science, Fudan University, China; email: lizhangfd@fudan.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 82 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhong, Q.\n",
      "AU  - Ding, G.\n",
      "AU  - Yao, A.\n",
      "TI  - OnlineTAS: An Online Baseline for Temporal Action Segmentation\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000509647&partnerID=40&md5=eb3eaa9a16aa1635092a72f4e1c6aecd\n",
      "AD  - University of Adelaide, Australia\n",
      "AD  - National University of Singapore, Singapore\n",
      "AB  - Temporal context plays a significant role in temporal action segmentation. In an offline setting, the context is typically captured by the segmentation network after observing the entire sequence. However, capturing and using such context information in an online setting remains an under-explored problem. This work presents the an online framework for temporal action segmentation. At the core of the framework is an adaptive memory designed to accommodate dynamic changes in context over time, alongside a feature augmentation module that enhances the frames with the memory. In addition, we propose a post-processing approach to mitigate the severe over-segmentation in the online setting. On three common segmentation benchmarks, our approach achieves state-of-the-art performance. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 83 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fang, X.\n",
      "AU  - Mao, K.\n",
      "AU  - Duan, H.\n",
      "AU  - Zhao, X.\n",
      "AU  - Li, Y.\n",
      "AU  - Lin, D.\n",
      "AU  - Chen, K.\n",
      "TI  - MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000539214&partnerID=40&md5=505f7eed6930561f97091873c1da5fec\n",
      "AD  - Zhejiang University, China\n",
      "AD  - Shanghai AI Laboratory, China\n",
      "AD  - Shanghai Jiao Tong University, China\n",
      "AD  - The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - CPII under InnoHK, Hong Kong\n",
      "AB  - The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 84 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ji, W.\n",
      "AU  - Li, J.\n",
      "AU  - Li, W.\n",
      "AU  - Shen, Y.\n",
      "AU  - Cheng, L.\n",
      "AU  - Jin, H.\n",
      "TI  - Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New UAV-View Benchmark\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000551346&partnerID=40&md5=00c85f3111b15e6f380f51c8809a9e73\n",
      "AD  - University of Alberta, Canada\n",
      "AD  - Yale University, United States\n",
      "AD  - Samsung AI Center-Mountain View, United States\n",
      "AB  - Thanks to the rapid progress in RGB & thermal imaging, also known as multispectral imaging, the task of multispectral video semantic segmentation, or MVSS in short, has recently drawn significant attentions. Noticeably, it offers new opportunities in improving segmentation performance under unfavorable visual conditions such as poor light or overexposure. Unfortunately, there are currently very few datasets available, including for example MVSeg dataset that focuses purely toward eye-level view; and it features the sparse annotation nature due to the intensive demands of labeling process. To address these key challenges of the MVSS task, this paper presents two major contributions: the introduction of MVUAV, a new MVSS benchmark dataset, and the development of a dedicated semi-supervised MVSS baseline - SemiMV. Our MVUAV dataset is captured via Unmanned Aerial Vehicles (UAV), which offers a unique oblique bird's-eye view complementary to the existing MVSS datasets; it also encompasses a broad range of day/night lighting conditions and over 30 semantic categories. In the meantime, to better leverage the sparse annotations and extra unlabeled RGB-Thermal videos, a semi-supervised learning baseline, SemiMV, is proposed to enforce consistency regularization through a dedicated Cross-collaborative Consistency Learning (C3L) module and a denoised temporal aggregation strategy. Comprehensive empirical evaluations on both MVSeg and MVUAV benchmark datasets have showcased the efficacy of our SemiMV baseline. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: J. Li; University of Alberta, Canada; email: jingjin1@ualberta.ca; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 85 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Han, Z.\n",
      "AU  - Gao, C.\n",
      "AU  - Liu, J.\n",
      "AU  - Zhang, J.\n",
      "AU  - Zhang, S.Q.\n",
      "TI  - Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey\n",
      "PY  - 2024\n",
      "T2  - Transactions on Machine Learning Research\n",
      "VL  - 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000099444&partnerID=40&md5=61eb7331da670a50300863fa23325197\n",
      "AD  - Northeastern University, United States\n",
      "AD  - University of California, Riverside, United States\n",
      "AD  - Arizona State University, United States\n",
      "AD  - New York University, United States\n",
      "AB  - Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate PEFT computation costs. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications. © 2024, Transactions on Machine Learning Research. All rights reserved.\n",
      "PB  - Transactions on Machine Learning Research\n",
      "SN  - 28358856 (ISSN)\n",
      "LA  - English\n",
      "J2  - Transact. mach. learn. res.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Correspondence Address: S.Q. Zhang; New York University, United States; email: sai.zhang@nyu.edu\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 86 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Gong, D.\n",
      "AU  - Kwak, S.\n",
      "AU  - Cho, M.\n",
      "TI  - ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000542570&partnerID=40&md5=8f17446f43789a8039c1df6d32097d8e\n",
      "AD  - Pohang University of Science and Technology (POSTECH), South Korea\n",
      "AB  - Temporal action segmentation and long-term action anticipation are two popular vision tasks for the temporal analysis of actions in videos. Despite apparent relevance and potential complementarity, these two problems have been investigated as separate and distinct tasks. In this work, we tackle these two problems, action segmentation and action anticipation, jointly using a unified diffusion model dubbed ActFusion. The key idea to unification is to train the model to effectively handle both visible and invisible parts of the sequence in an integrated manner; the visible part is for temporal segmentation, and the invisible part is for future anticipation. To this end, we introduce a new anticipative masking strategy during training in which a late part of the video frames is masked as invisible, and learnable tokens replace these frames to learn to predict the invisible future. Experimental results demonstrate the bi-directional benefits between action segmentation and anticipation. ActFusion achieves the state-of-the-art performance across the standard benchmarks of 50 Salads, Breakfast, and GTEA, outperforming task-specific models in both of the two tasks with a single unified model through joint learning. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 87 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, G.\n",
      "AU  - Liu, C.\n",
      "AU  - Cui, Y.\n",
      "AU  - Zhao, X.\n",
      "AU  - Ma, K.\n",
      "AU  - Wang, L.\n",
      "TI  - VFIMamba: Video Frame Interpolation with State Space Models\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000530516&partnerID=40&md5=d0fdfcf914becf2e322a44dd62c7b026\n",
      "AD  - State Key Laboratory for Novel Software Technology, Nanjing University, China\n",
      "AD  - Platform and Content Group (PCG), Tencent, China\n",
      "AD  - Shanghai AI Lab, China\n",
      "AB  - Inter-frame modeling is pivotal in generating intermediate frames for video frame interpolation (VFI). Current approaches predominantly rely on convolution or attention-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, Selective State Space Models (S6) have emerged, tailored specifically for long sequence modeling, offering both linear complexity and data-dependent modeling capabilities. In this paper, we propose VFIMamba, a novel frame interpolation method for efficient and dynamic inter-frame modeling by harnessing the S6 model. Our approach introduces the Mixed-SSM Block (MSB), which initially rearranges tokens from adjacent frames in an interleaved fashion and subsequently applies multi-directional S6 modeling. This design facilitates the efficient transmission of information across frames while upholding linear complexity. Furthermore, we introduce a novel curriculum learning strategy that progressively cultivates proficiency in modeling inter-frame dynamics across varying motion magnitudes, fully unleashing the potential of the S6 model. Experimental findings showcase that our method attains state-of-the-art performance across diverse benchmarks, particularly excelling in high-resolution scenarios. In particular, on the X-TEST dataset, VFIMamba demonstrates a noteworthy improvement of 0.80 dB for 4K frames and 0.96 dB for 2K frames. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Wang; State Key Laboratory for Novel Software Technology, Nanjing University, China; email: lmwang@nju.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 88 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chen, H.\n",
      "AU  - Wang, X.\n",
      "AU  - Chen, H.\n",
      "AU  - Zhang, Z.\n",
      "AU  - Feng, W.\n",
      "AU  - Huang, B.\n",
      "AU  - Jia, J.\n",
      "AU  - Zhu, W.\n",
      "TI  - VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000506391&partnerID=40&md5=4ee2d5ad943fbf00a6863f2983675a58\n",
      "AD  - Department of Computer Science and Technology, Tsinghua University, Beijing, China\n",
      "AD  - BNRIST, Tsinghua University, Beijing, China\n",
      "AB  - Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic VidEo-text annotation pipeline to generate captions with RelIable FInE-grained statics and Dynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in https://github.com/hlchen23/VERIFIED. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: X. Wang; Department of Computer Science and Technology, Tsinghua University, Beijing, China; email: xin_wang@tsinghua.edu.cn; J. Jia; Department of Computer Science and Technology, Tsinghua University, Beijing, China; email: jjia@tsinghua.edu.cn; W. Zhu; Department of Computer Science and Technology, Tsinghua University, Beijing, China; email: wwzhu@tsinghua.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 89 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, Y.\n",
      "AU  - Ma, Z.\n",
      "AU  - Qi, Z.\n",
      "AU  - Wu, Y.\n",
      "AU  - Shan, Y.\n",
      "AU  - Chen, C.W.\n",
      "TI  - E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000541242&partnerID=40&md5=afab36690366d75b6dc023d5a1f3de0b\n",
      "AD  - The Hong Kong Polytechnic University, Hong Kong\n",
      "AD  - ARC Lab, Tencent PCG, China\n",
      "AD  - Chinese Academy of Sciences, China\n",
      "AD  - Huawei Noah's Ark Lab, Canada\n",
      "AD  - Tencent AI Lab, China\n",
      "AB  - Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: ; ; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 90 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shin, Y.\n",
      "AU  - Park, J.\n",
      "AU  - Song, H.\n",
      "AU  - Yoon, S.\n",
      "AU  - Lee, B.S.\n",
      "AU  - Lee, J.-G.\n",
      "TI  - Exploiting Representation Curvature for Boundary Detection in Time Series\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000520417&partnerID=40&md5=c06cd433f88d0be71aa02801c4db3adc\n",
      "AD  - KAIST, South Korea\n",
      "AD  - Korea University, South Korea\n",
      "AD  - University of Vermont, United States\n",
      "AB  - Boundaries are the timestamps at which a class in a time series changes. Recently, representation-based boundary detection has gained popularity, but its emphasis on consecutive distance difference backfires, especially when the changes are gradual. In this paper, we propose a boundary detection method, RECURVE, based on a novel change metric, the curvature of a representation trajectory, to accommodate both gradual and abrupt changes. Here, a sequence of representations in the representation space is interpreted as a trajectory, and a curvature at each timestamp can be computed. Using the theory of random walk, we formally show that the mean curvature is lower near boundaries than at other points. Extensive experiments using diverse real-world time-series datasets confirm the superiority of RECURVE over state-of-the-art methods. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: J.-G. Lee; KAIST, South Korea; email: jaegil@kaist.ac.kr; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 91 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhu, Y.\n",
      "AU  - Ji, Y.\n",
      "AU  - Zhao, Z.\n",
      "AU  - Wu, G.\n",
      "AU  - Wang, L.\n",
      "TI  - AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000502508&partnerID=40&md5=08b264efa147aadb6f471042d6dc69f3\n",
      "AD  - State Key Laboratory for Novel Software Technology, Nanjing University, China\n",
      "AD  - Shanghai AI Laboratory, China\n",
      "AB  - Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: L. Wang; State Key Laboratory for Novel Software Technology, Nanjing University, China; email: lmwang@nju.edu.cn; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 92 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Vo, K.\n",
      "AU  - Phan, T.\n",
      "AU  - Yamazaki, K.\n",
      "AU  - Tran, M.\n",
      "AU  - Le, N.\n",
      "TI  - HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000542841&partnerID=40&md5=b0b36c40adf1b524c3116a8d4199bd6d\n",
      "AD  - AICV Lab, University of Arkansas, Fayetteville, United States\n",
      "AB  - Current video-language models (VLMs) rely extensively on instance-level alignment between video and language modalities, which presents two major limitations: (1) visual reasoning disobeys the natural perception that humans do in first-person perspective, leading to a lack of reasoning interpretation; and (2) learning is limited in capturing inherent fine-grained relationships between two modalities. In this paper, we take an inspiration from human perception and explore a compositional approach for egocentric video representation. We introduce HENASY (Hierarchical ENtities ASsemblY), which includes a spatiotemporal token grouping mechanism to explicitly assemble dynamically evolving scene entities through time and model their relationship for video representation. By leveraging compositional structure understanding, HENASY possesses strong interpretability via visual grounding with free-form text queries. We further explore a suite of multi-grained contrastive losses to facilitate entity-centric understandings. This comprises three alignment types: video-narration, noun-entity, verb-entities alignments. Our method demonstrates strong interpretability in both quantitative and qualitative experiments; while maintaining competitive performances on five downstream tasks via zero-shot transfer or as video/text representation, including video/text retrieval, action recognition, multi-choice query, natural language query, and moments query. Project page: https://uark-aicv.github.io/HENASY. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 93 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shen, X.\n",
      "AU  - Du, H.\n",
      "AU  - Sheng, H.\n",
      "AU  - Wang, S.\n",
      "AU  - Chen, H.\n",
      "AU  - Chen, H.\n",
      "AU  - Wu, Z.\n",
      "AU  - Du, X.\n",
      "AU  - Ying, J.\n",
      "AU  - Lu, R.\n",
      "AU  - Xu, Q.\n",
      "AU  - Yu, X.\n",
      "TI  - MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000507050&partnerID=40&md5=e9ccab9e336ea80296dcc3ccdea655c6\n",
      "AD  - The University of Queensland, Australia\n",
      "AB  - Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language signs. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate the first large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) the largest amount of data, (2) the most extensive vocabulary, and (3) the most diverse of multi-modal camera views. Specifically, we record 282K+ sign videos covering 3,215 commonly used Auslan glosses presented by 73 signers in a studio environment. Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at MM-WLAuslan. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 94 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yang, Y.\n",
      "AU  - Zhai, W.\n",
      "AU  - Wang, C.\n",
      "AU  - Yu, C.\n",
      "AU  - Cao, Y.\n",
      "AU  - Zha, Z.-J.\n",
      "TI  - EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000490208&partnerID=40&md5=85a515ff35b4b4729831dec4fbf5f32b\n",
      "AD  - University of Science and Technology of China, China\n",
      "AD  - Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China\n",
      "AB  - Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g., “what” interaction is occurring, capturing “where” the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric view. However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy. From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight. In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos. To achieve this, we present EgoChoir, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact. Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios. Moreover, 3D contact and affordance are annotated for egocentric videos collected from Ego-Exo4D and GIMO to support the task. Extensive experiments on them demonstrate the effectiveness and superiority of EgoChoir. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 95 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chen, M.\n",
      "AU  - Gao, J.\n",
      "AU  - Xu, C.\n",
      "TI  - Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models\n",
      "PY  - 2024\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000503707&partnerID=40&md5=759e9d73704350c587dac80acec4ad27\n",
      "AD  - MAIS, Institute of Automation, CAS, School of Artificial Intelligence, UCAS, China\n",
      "AD  - MAIS, Institute of Automation, CAS, School of Artificial Intelligence, UCAS, Pengcheng Laboratory, China\n",
      "AB  - A straightforward pipeline for zero-shot out-of-distribution (OOD) detection involves selecting potential OOD labels from an extensive semantic pool and then leveraging a pre-trained vision-language model to perform classification on both in-distribution (ID) and OOD labels. In this paper, we theorize that enhancing performance requires expanding the semantic pool, while increasing the expected probability of selected OOD labels being activated by OOD samples, and ensuring low mutual dependence among the activations of these OOD labels. A natural expansion manner is to adopt a larger lexicon; however, the inevitable introduction of numerous synonyms and uncommon words fails to meet the above requirements, indicating that viable expansion manners move beyond merely selecting words from a lexicon. Since OOD detection aims to correctly classify input images into ID/OOD class groups, we can \"make up\" OOD label candidates which are not standard class names but beneficial for the process. Observing that the original semantic pool is comprised of unmodified specific class names, we correspondingly construct a conjugated semantic pool (CSP) consisting of modified superclass names, each serving as a cluster center for samples sharing similar properties across different categories. Consistent with our established theory, expanding OOD label candidates with the CSP satisfies the requirements and outperforms existing works by 7.89% in FPR95. Codes are available in https://github.com/MengyuanChen21/NeurIPS2024-CSP. © 2024 Neural information processing systems foundation. All rights reserved.\n",
      "A2  - Globerson A.\n",
      "A2  - Mackey L.\n",
      "A2  - Belgrave D.\n",
      "A2  - Fan A.\n",
      "A2  - Paquet U.\n",
      "A2  - Tomczak J.\n",
      "A2  - Zhang C.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 38th Conference on Neural Information Processing Systems, NeurIPS 2024; Conference date: 9 December 2024 through 15 December 2024; Conference code: 207061\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 96 without DOI:\n",
      "TY  - CONF\n",
      "TI  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023\n",
      "PY  - 2023\n",
      "T2  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023\n",
      "VL  - 37\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167682106&partnerID=40&md5=1d20a89064265646345d9301f2a1e6db\n",
      "AB  - The proceedings contain 138 papers. The topics discussed include: DC-former: diverse and compact transformer for person re-identification; panoramic video salient object detection with ambisonic audio guidance; adaptive texture filtering for single-domain generalized segmentation; gradient corner pooling for keypoint-based object detection; towards real-time segmentation on the edge; learning single image defocus deblurring with misaligned training pairs; curriculum temperature for knowledge distillation; actionness inconsistency-guided contrastive learning for weakly-supervised temporal action localization; global dilated attention and target focusing network for robust tracking; only a few classes confusing: pixel-wise candidate labels disambiguation for foggy scene understanding; actional atomic-concept learning for demystifying vision-language navigation; self-supervised image denoising using implicit deep denoiser prior; accelerating the training of video super-resolution models; and the devil is in the frequency: geminated gestalt autoencoder for self-supervised visual pre-training.\n",
      "A2  - Williams B.\n",
      "A2  - Chen Y.\n",
      "A2  - Neville J.\n",
      "PB  - AAAI Press\n",
      "SN  - 978-157735880-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc. AAAI Conf. Artif. Intell., AAAI\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 37th AAAI Conference on Artificial Intelligence, AAAI 2023; Conference date: 7 February 2023 through 14 February 2023; Conference code: 190493\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 97 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Cheng, C.\n",
      "AU  - Song, L.\n",
      "AU  - Xue, R.\n",
      "AU  - Wang, H.\n",
      "AU  - Sun, H.\n",
      "AU  - Ge, Y.\n",
      "AU  - Shan, Y.\n",
      "TI  - Meta-Adapter: An Online Few-shot Learner for Vision-Language Model\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205444912&partnerID=40&md5=821e1e440b004067c00a7180bd149525\n",
      "AD  - Xi'an JiaoTong University, China\n",
      "AD  - Tencent AI Lab, China\n",
      "AB  - The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner. With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency. Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6% on eight image classification datasets with higher inference speed. Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks. Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Contrastive Learning\n",
      "KW  - Image segmentation\n",
      "KW  - Visual languages\n",
      "KW  - Fine tuning\n",
      "KW  - Language model\n",
      "KW  - Learning capabilities\n",
      "KW  - Learning methods\n",
      "KW  - Offline\n",
      "KW  - Open world\n",
      "KW  - Overfitting\n",
      "KW  - Pre-training\n",
      "KW  - Training sample\n",
      "KW  - Visual concept\n",
      "KW  - Zero-shot learning\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Correspondence Address: ; ; ; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 98 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hu, M.\n",
      "AU  - Wang, L.\n",
      "AU  - Yan, S.\n",
      "AU  - Ma, D.\n",
      "AU  - Ren, Q.\n",
      "AU  - Xia, P.\n",
      "AU  - Feng, W.\n",
      "AU  - Duan, P.\n",
      "AU  - Ju, L.\n",
      "AU  - Ge, Z.\n",
      "TI  - NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191184128&partnerID=40&md5=46c9f81a67b1b4b67bfe68376eb2b741\n",
      "AD  - AIM Lab, Faculty of IT, Monash University, Australia\n",
      "AD  - Airdoc-Monash Research Airdoc, China\n",
      "AD  - Faculty of Engineering, Monash University, Australia\n",
      "AD  - College of Intelligent Systems Science and Engineering, Harbin Engineering University, China\n",
      "AD  - Nursing College, Shanxi Medical University, China\n",
      "AB  - The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing procedure activity understanding. NurViD consists of over 1.5k videos totaling 144 hours, making it approximately four times longer than the existing largest nursing activity datasets. Notably, it encompasses 51 distinct nursing procedures and 177 action steps, providing a much more comprehensive coverage compared to existing datasets that primarily focus on limited procedures. To evaluate the efficacy of current deep learning methods on nursing activity understanding, we establish three benchmarks on NurViD: procedure recognition on untrimmed videos, procedure and action recognition on trimmed videos, and action detection. Our benchmark and code will be available at https://github.com/minghu0830/NurViD-benchmark. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Compliance control\n",
      "KW  - Deep learning\n",
      "KW  - Large datasets\n",
      "KW  - Learning systems\n",
      "KW  - Quality assurance\n",
      "KW  - Quality control\n",
      "KW  - Video recording\n",
      "KW  - Action step\n",
      "KW  - Automatic recognition system\n",
      "KW  - Compliance monitoring\n",
      "KW  - Labeled dataset\n",
      "KW  - Patient interaction\n",
      "KW  - Quality and safeties\n",
      "KW  - Small scale\n",
      "KW  - Training and education\n",
      "KW  - Video database\n",
      "KW  - Video dataset\n",
      "KW  - Nursing\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 99 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, J.\n",
      "AU  - Kong, M.\n",
      "AU  - Chen, L.\n",
      "AU  - Liang, T.\n",
      "AU  - Zhu, Q.\n",
      "TI  - Temporal RPN Learning for Weakly-Supervised Temporal Action Localization\n",
      "PY  - 2023\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 222\n",
      "SP  - 470\n",
      "EP  - 485\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189641446&partnerID=40&md5=e6e180226c9efffdfa9882af3949090c\n",
      "AD  - Zhejiang University, Hangzhou, 310058, China\n",
      "AD  - College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China\n",
      "AD  - Hikvision Research Institute, Hangzhou, 310051, China\n",
      "AD  - Beijing Information Science and Technology University, Beijing, 100101, China\n",
      "AD  - College of Computer Science, Zhejiang University, Hangzhou, 310058, China\n",
      "AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model's learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: https://github.com/ZJUHJ/TRPN. © 2023 J. Huang, M. Kong, L. Chen, T. Liang & Q. Zhu✉.\n",
      "KW  - Action Localization\n",
      "KW  - Temporal Region Proposal\n",
      "KW  - Weakly-Supervised Learning\n",
      "KW  - Learning systems\n",
      "KW  - Network architecture\n",
      "KW  - Object recognition\n",
      "KW  - Action localization\n",
      "KW  - Detection tasks\n",
      "KW  - Learning performance\n",
      "KW  - Localisation\n",
      "KW  - Localization method\n",
      "KW  - Localization modeling\n",
      "KW  - Model learning\n",
      "KW  - Objects detection\n",
      "KW  - Temporal region proposal\n",
      "KW  - Weakly supervised learning\n",
      "KW  - Object detection\n",
      "A2  - Yanikoglu B.\n",
      "A2  - Buntine W.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: Q. Zhu; College of Computer Science, Zhejiang University, Hangzhou, 310058, China; email: zhuq@zju.edu.cn; Conference name: 15th Asian Conference on Machine Learning, ACML 2023; Conference date: 11 November 2023 through 14 November 2023; Conference code: 198364\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 100 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Dong, Y.\n",
      "AU  - Li, Y.\n",
      "AU  - Zhao, D.\n",
      "AU  - Shen, G.\n",
      "AU  - Zeng, Y.\n",
      "TI  - Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186369296&partnerID=40&md5=6d28f55448b402b8ba26f46d6d0db022\n",
      "AD  - School of Future Technology, University of Chinese Academy of Sciences, China\n",
      "AD  - School of Artificial Intelligence, University of Chinese Academy of Sciences, China\n",
      "AD  - Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences, China\n",
      "AD  - Center for Long-term Artificial Intelligence\n",
      "AB  - The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment. To address the problem, we leverage Dynamic Vision Sensors (DVS) camera to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10, 000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Large datasets\n",
      "KW  - Privacy-preserving techniques\n",
      "KW  - Security systems\n",
      "KW  - Brightness variations\n",
      "KW  - Daily lives\n",
      "KW  - Dynamic vision sensors\n",
      "KW  - Large-scales\n",
      "KW  - Neuromorphic\n",
      "KW  - Privacy invasions\n",
      "KW  - Privacy preserving\n",
      "KW  - Public space\n",
      "KW  - Surveillance cameras\n",
      "KW  - Well being\n",
      "KW  - Cameras\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 7; Correspondence Address: Y. Zeng; School of Future Technology, University of Chinese Academy of Sciences, China; email: yi.zeng@ia.ac.cn; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 101 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Modi, R.\n",
      "AU  - Vineet, V.\n",
      "AU  - Rawat, Y.S.\n",
      "TI  - On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199575729&partnerID=40&md5=066bb84583791c771d981fae15e3a3cc\n",
      "AD  - CRCV, University of Central Florida, United States\n",
      "AD  - Microsoft Research, United States\n",
      "AB  - This paper explores the impact of occlusions in video action detection. We facilitate this study by introducing five new benchmark datasets namely O-UCF and O-JHMDB consisting of synthetically controlled static/dynamic occlusions, OVIS-UCF and OVIS-JHMDB consisting of occlusions with realistic motions and Real-OUCF for occlusions in realistic-world scenarios. We formally confirm an intuitive expectation: existing models suffer a lot as occlusion severity is increased and exhibit different behaviours when occluders are static vs when they are moving. We discover several intriguing phenomenon emerging in neural nets: 1) transformers can naturally outperform CNN models which might have even used occlusion as a form of data augmentation during training 2) incorporating symbolic-components like capsules to such backbones allows them to bind to occluders never even seen during training and 3) Islands of agreement can emerge in realistic images/videos without instance-level supervision, distillation or contrastive-based objectives2(eg. video-textual training). Such emergent properties allow us to derive simple yet effective training recipes which lead to robust occlusion models inductively satisfying the first two stages of the binding mechanism (grouping/segregation). Models leveraging these recipes outperform existing video action-detectors under occlusion by 32.3% on O-UCF, 32.7% on O-JHMDB & 2.6% on Real-OUCF in terms of the vMAP metric. The code for this work has been released at https://github.com/rajatmodi62/OccludedActionBenchmark. © 2023 Advances in Neural Information Processing Systems. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Image coding\n",
      "KW  - Video analysis\n",
      "KW  - Benchmark datasets\n",
      "KW  - Binding mechanisms\n",
      "KW  - CNN models\n",
      "KW  - Data augmentation\n",
      "KW  - Emergent property\n",
      "KW  - Occluder\n",
      "KW  - Occlusion model\n",
      "KW  - Realistic images\n",
      "KW  - Simple++\n",
      "KW  - Static dynamics\n",
      "KW  - Data handling\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: R. Modi; CRCV, University of Central Florida, United States; email: rajatmodi@ucf.edu; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 102 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Nagarajan, T.\n",
      "AU  - Ramakrishnan, S.K.\n",
      "AU  - Desai, R.\n",
      "AU  - Hillis, J.\n",
      "AU  - Grauman, K.\n",
      "TI  - EgoEnv: Human-centric environment representations from egocentric video\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191196522&partnerID=40&md5=c13b1681a0e0e920d8b079eb244aa2f7\n",
      "AD  - University of Texas at Austin, United States\n",
      "AD  - FAIR, Meta, United States\n",
      "AB  - First-person video highlights a camera-wearer's activities in the context of their persistent environment. However, current video understanding approaches reason over visual features from short video clips that are detached from the underlying physical space and capture only what is immediately visible. To facilitate human-centric environment understanding, we present an approach that links egocentric video and the environment by learning representations that are predictive of the camera-wearer's (potentially unseen) local surroundings. We train such models using videos from agents in simulated 3D environments where the environment is fully observable, and test them on human-captured real-world videos from unseen environments. On two human-centric video tasks, we show that models equipped with our environment-aware features consistently outperform their counterparts with traditional clip features. Moreover, despite being trained exclusively on simulated videos, our approach successfully handles real-world videos from HouseTours and Ego4D, and achieves state-of-the-art results on the Ego4D NLQ challenge. Project page: https://vision.cs.utexas.edu/projects/ego-env/. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - 'current\n",
      "KW  - 3-D environments\n",
      "KW  - Environment representations\n",
      "KW  - First person\n",
      "KW  - Human-centric\n",
      "KW  - Real world videos\n",
      "KW  - State of the art\n",
      "KW  - Video understanding\n",
      "KW  - Video-clips\n",
      "KW  - Visual feature\n",
      "KW  - Cameras\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 103 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Bai, Z.\n",
      "AU  - Wang, R.\n",
      "AU  - Chen, X.\n",
      "TI  - Glance and Focus: Memory Prompting for Multi-Event Video Question Answering\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188906546&partnerID=40&md5=611a4dc691576168372a0dbea3e10c42\n",
      "AD  - Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China\n",
      "AD  - University of Chinese Academy of Sciences, Beijing, 100049, China\n",
      "AB  - Video Question Answering (VideoQA) has emerged as a vital tool to evaluate agents' ability to understand human daily behaviors. Despite the recent success of large vision language models in many multi-modal tasks, complex situation reasoning over videos involving multiple human-object interaction events still remains challenging. In contrast, humans can easily tackle it by using a series of episode memories as anchors to quickly locate question-related key moments for reasoning. To mimic this effective reasoning strategy, we propose the Glance-Focus model. One simple way is to apply an action detection model to predict a set of actions as key memories. However, these actions within a closed set vocabulary are hard to generalize to various video domains. Instead of that, we train an Encoder-Decoder to generate a set of dynamic event memories at the glancing stage. Apart from using supervised bipartite matching to obtain the event memories, we further design an unsupervised memory generation method to get rid of dependence on event annotations. Next, at the focusing stage, these event memories act as a bridge to establish the correlation between the questions with high-level event concepts and low-level lengthy video content. Given the question, the model first focuses on the generated key event memory, then focuses on the most relevant moment for reasoning through our designed multi-level cross-attention mechanism. We conduct extensive experiments on four Multi-Event VideoQA benchmarks including STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves state-of-the-art results, surpassing current large models in various challenging reasoning tasks. The code and models are available at https://github.com/ByZ0e/Glance-Focus. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Closed set\n",
      "KW  - Daily behaviors\n",
      "KW  - Detection models\n",
      "KW  - Human-object interaction\n",
      "KW  - Language model\n",
      "KW  - Multi-modal\n",
      "KW  - Question Answering\n",
      "KW  - Related keys\n",
      "KW  - Simple++\n",
      "KW  - Situation reasonings\n",
      "KW  - Behavioral research\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 104 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Song, Y.\n",
      "AU  - Byrne, E.\n",
      "AU  - Nagarajan, T.\n",
      "AU  - Wang, H.\n",
      "AU  - Martin, M.\n",
      "AU  - Torresani, L.\n",
      "TI  - Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186335931&partnerID=40&md5=fac89a26ad5d721cc6781816e1dcd159\n",
      "AD  - Fundamental AI Research (FAIR), Meta, United States\n",
      "AB  - Human activities are goal-oriented and hierarchical, comprising primary goals at the top level, sequences of steps and substeps in the middle, and atomic actions at the lowest level. Recognizing human activities thus requires relating atomic actions and steps to their functional objectives (what the actions contribute to) and modeling their sequential and hierarchical dependencies towards achieving the goals. Current activity recognition research has primarily focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step segments (430 hours) and high-level goal annotations for 2, 807 hours of Ego4D videos. Compared to existing procedural video datasets, it is substantially larger in size, contains hierarchical action labels (goals - steps - substeps), and provides goal-oriented auxiliary information including natural language summary description, step completion status, and step-to-goal relevance information. We take a data-driven approach to build our taxonomy, resulting in dense step annotations that do not suffer from poor label-data alignment issues resulting from a taxonomy defined a priori. Through comprehensive evaluations and analyses, we demonstrate how Ego4D Goal-Step supports exploring various questions in procedural activity understanding, including goal inference, step prediction, hierarchical relation learning, and long-term temporal modeling. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Atoms\n",
      "KW  - 'current\n",
      "KW  - Activity recognition\n",
      "KW  - Atomic actions\n",
      "KW  - Atomic step\n",
      "KW  - Goal-oriented\n",
      "KW  - Hierarchical taxonomy\n",
      "KW  - High-level goals\n",
      "KW  - Human activities\n",
      "KW  - Level sequence\n",
      "KW  - Procedural steps\n",
      "KW  - Taxonomies\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 8; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 105 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yang, A.\n",
      "AU  - Nagrani, A.\n",
      "AU  - Laptev, I.\n",
      "AU  - Sivic, J.\n",
      "AU  - Schmid, C.\n",
      "TI  - VidChapters-7M: Video Chapters at Scale\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205444377&partnerID=40&md5=efe906f1b1591eebc39509100aa78f9e\n",
      "AD  - Inria Paris, DI ENS, CNRS, PSL Research University, France\n",
      "AD  - VGG, University of Oxford, United Kingdom\n",
      "AD  - Czech Institute of Informatics, Robotics and Cybernetics, The Czech Technical University, Prague, Czech Republic\n",
      "AB  - Segmenting long videos into chapters enables users to quickly navigate to the information of their interest. This important topic has been understudied due to the lack of publicly released datasets. To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. We introduce the following three tasks based on this data. First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment. To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title. We benchmark both simple baselines and state-of-the-art video-language models for these three tasks. We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks. Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Data handling\n",
      "KW  - Modeling languages\n",
      "KW  - Video analysis\n",
      "KW  - Down-stream\n",
      "KW  - Ground truth\n",
      "KW  - Language model\n",
      "KW  - Manual annotation\n",
      "KW  - Performance\n",
      "KW  - Pre-training\n",
      "KW  - Simple++\n",
      "KW  - State of the art\n",
      "KW  - Task-based\n",
      "KW  - Video segments\n",
      "KW  - Spatio-temporal data\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 10; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 106 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Weng, Y.\n",
      "AU  - Han, M.\n",
      "AU  - He, H.\n",
      "AU  - Li, M.\n",
      "AU  - Yao, L.\n",
      "AU  - Chang, X.\n",
      "AU  - Zhuang, B.\n",
      "TI  - Mask Propagation for Efficient Video Semantic Segmentation\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190819897&partnerID=40&md5=0da48a74527bc70a293dfef96c4b0499\n",
      "AD  - ZIP Lab, Monash University, Australia\n",
      "AD  - Baidu Inc., China\n",
      "AD  - ReLER, AAII, UTS\n",
      "AD  - Data61, CSIRO, Australia\n",
      "AD  - Mohamed bin Zayed University of AI, United Arab Emirates\n",
      "AB  - Video Semantic Segmentation (VSS) involves assigning a semantic label to each pixel in a video sequence. Prior work in this field has demonstrated promising results by extending image semantic segmentation models to exploit temporal relationships across video frames; however, these approaches often incur significant computational costs. In this paper, we propose an efficient mask propagation framework for VSS, called MPVSS. Our approach first employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions. We then design a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps, each associated with a mask prediction from the key frame. Finally, the mask-flow pairs are warped to serve as the mask predictions for the non-key frames. By reusing predictions from key frames, we circumvent the need to process a large volume of video frames individually with resource-intensive segmentors, alleviating temporal redundancy and significantly reducing computational costs. Extensive experiments on VSPW and Cityscapes demonstrate that our mask propagation framework achieves SOTA accuracy and efficiency trade-offs. For instance, our best model with Swin-L backbone outperforms the SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW dataset. Moreover, our framework reduces up to 4× FLOPs compared to the per-frame Mask2Former baseline with only up to 2% mIoU degradation on the Cityscapes validation set. Code is available at https://github.com/ziplab/MPVSS. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Economic and social effects\n",
      "KW  - Semantic Segmentation\n",
      "KW  - Semantics\n",
      "KW  - Computational costs\n",
      "KW  - Image semantics\n",
      "KW  - Key-frames\n",
      "KW  - Segmentation models\n",
      "KW  - Semantic labels\n",
      "KW  - Semantic segmentation\n",
      "KW  - Temporal relationships\n",
      "KW  - Video frame\n",
      "KW  - Video semantics\n",
      "KW  - Video sequences\n",
      "KW  - Forecasting\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 8; Correspondence Address: B. Zhuang; ZIP Lab, Monash University, Australia; email: bohan.zhuang@gmail.com; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 107 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhao, S.\n",
      "AU  - Xu, H.\n",
      "TI  - NEUCORE: Neural Concept Reasoning for Composed Image Retrieval\n",
      "PY  - 2023\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 243\n",
      "SP  - 291\n",
      "EP  - 301\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196101470&partnerID=40&md5=d4636b905fc3f7388ace6f6692374ec5\n",
      "AD  - Pennsylvania State University, University Park, United States\n",
      "AB  - Composed image retrieval which combines a reference image and a text modifier to identify the desired target image is a challenging task, and requires the model to comprehend both vision and language modalities and their interactions. Existing approaches focus on holistic multi-modal interaction modeling, and ignore the composed and complimentary property between the reference image and text modifier. In order to better utilize the complementarity of multi-modal inputs for effective information fusion and retrieval, we move the multi-modal understanding to fine-granularity at concept-level, and learn the multi-modal concept alignment to identify the visual location in reference or target images corresponding to text modifier. Toward the end, we propose a NEUral COncept REasoning (NEUCORE) model which incorporates multi-modal concept alignment and progressive multi-modal fusion over aligned concepts. Specifically, considering that text modifier may refer to semantic concepts not existing in the reference image and requiring to be added into the target image, we learn the multi-modal concept alignment between the text modifier and the concatenation of reference and target images, under multiple-instance learning framework with image and sentence level weak supervision. Furthermore, based on aligned concepts, to form discriminative fusion features of the input modalities for accurate target image retrieval, we propose a progressive fusion strategy with unified execution architecture instantiated by the attended language semantic concepts. Our proposed approach is evaluated on three datasets and achieves state-of-the-art results. Code is available at https://github.com/VisionLanguageLab/NEUCORE. © 2023 Proceedings of Machine Learning Research.\n",
      "KW  - Alignment\n",
      "KW  - Image fusion\n",
      "KW  - Image retrieval\n",
      "KW  - Machine learning\n",
      "KW  - Concept alignments\n",
      "KW  - Fine granularity\n",
      "KW  - Interaction modeling\n",
      "KW  - Learn+\n",
      "KW  - Multi-modal\n",
      "KW  - Multimodal Interaction\n",
      "KW  - Property\n",
      "KW  - Reference image\n",
      "KW  - Semantic concept\n",
      "KW  - Target images\n",
      "KW  - Semantics\n",
      "A2  - Fumero M.\n",
      "A2  - Rodola E.\n",
      "A2  - Domine C.C.J.\n",
      "A2  - Locatello F.\n",
      "A2  - Dziugaite G.K.\n",
      "A2  - Caron M.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 1st Workshop on Unifying Representations in Neural Models, UniReps 2023; Conference code: 200096\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 108 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shen, X.\n",
      "AU  - Yuan, S.\n",
      "AU  - Sheng, H.\n",
      "AU  - Du, H.\n",
      "AU  - Yu, X.\n",
      "TI  - Auslan-Daily: Australian Sign Language Translation for Daily Communication and News\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191157982&partnerID=40&md5=21f11948833ce6966a3aa92ab6046b10\n",
      "AD  - The University of Queensland, Australia\n",
      "AD  - JD AI, Beijing, China\n",
      "AB  - Sign language translation (SLT) aims to convert a continuous sign language video clip into a spoken language. Considering different geographic regions generally have their own native sign languages, it is valuable to establish corresponding SLT datasets to support related communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale dataset for SLT. To fill this gap, we curate an Australian Sign Language translation dataset, dubbed Auslan-Daily, which is collected from the Auslan educational TV series and Auslan TV programs. The former involves daily communications among multiple signers in the wild, while the latter comprises sign language videos for up-to-date news, weather forecasts, and documentaries. In particular, Auslan-Daily has two main features: (1) the topics are diverse and signed by multiple signers, and (2) the scenes in our dataset are more complex, e.g., captured in various environments, gesture interference during multi-signers' interactions and various camera positions. With a collection of more than 45 hours of high-quality Auslan video materials, we invite Auslan experts to align different fine-grained visual and language pairs, including video ↔ fingerspelling, video ↔ gloss, and video ↔ sentence. As a result, Auslan-Daily contains multi-grained annotations that can be utilized to accomplish various fundamental sign language tasks, such as signer detection, sign spotting, fingerspelling detection, isolated sign language recognition, sign language translation and alignment. Moreover, we benchmark results with state-of-the-art models for each task in Auslan-Daily. Experiments indicate that Auslan-Daily is a highly challenging SLT dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide in a broader context. All datasets and benchmarks are available at Auslan-Daily. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Cameras\n",
      "KW  - Translation (languages)\n",
      "KW  - Visual languages\n",
      "KW  - Weather forecasting\n",
      "KW  - Australia\n",
      "KW  - Camera positions\n",
      "KW  - Geographics\n",
      "KW  - High quality\n",
      "KW  - Language translation\n",
      "KW  - Large-scale datasets\n",
      "KW  - Sign language\n",
      "KW  - Spoken languages\n",
      "KW  - TV programs\n",
      "KW  - Video-clips\n",
      "KW  - Large datasets\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Correspondence Address: X. Shen; The University of Queensland, Australia; email: x.shen3@uqconnect.edu.au; ; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 109 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yang, J.\n",
      "AU  - Song, L.\n",
      "AU  - Liu, S.\n",
      "AU  - Mao, W.\n",
      "AU  - Li, Z.\n",
      "AU  - Li, X.\n",
      "AU  - Sun, H.\n",
      "AU  - Sun, J.\n",
      "AU  - Zheng, N.\n",
      "TI  - DBQ-SSD: DYNAMIC BALL QUERY FOR EFFICIENT 3D OBJECT DETECTION\n",
      "PY  - 2023\n",
      "T2  - 11th International Conference on Learning Representations, ICLR 2023\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192468764&partnerID=40&md5=25ba170fdfb0b5eb4fd18c76cb940e73\n",
      "AD  - Huazhong University of Science and Technology, China\n",
      "AD  - Tencent AI Lab, China\n",
      "AD  - MEGVII Technology, China\n",
      "AD  - Xi'an Jiaotong University, China\n",
      "AB  - Many point-based 3D detectors adopt point-feature sampling strategies to drop some points for efficient inference. These strategies are typically based on fixed and handcrafted rules, making it difficult to handle complicated scenes. Different from them, we propose a Dynamic Ball Query (DBQ) network to adaptively select a subset of input points according to the input features, and assign the feature transform with a suitable receptive field for each selected point. It can be embedded into some state-of-the-art 3D detectors and trained in an end-to-end manner, which significantly reduces the computational cost. Extensive experiments demonstrate that our method can increase the inference speed by 30%-100% on KITTI, Waymo, and ONCE datasets. Specifically, the inference speed of our detector can reach 162 FPS on KITTI scene, and 30 FPS on Waymo and ONCE scenes without performance degradation. Due to skipping the redundant points, some evaluation metrics show significant improvements. © 2023 11th International Conference on Learning Representations, ICLR 2023. All rights reserved.\n",
      "KW  - Feature extraction\n",
      "KW  - Object recognition\n",
      "KW  - 3-D detectors\n",
      "KW  - 3D object\n",
      "KW  - Fixed rules\n",
      "KW  - Handcrafted rules\n",
      "KW  - Input features\n",
      "KW  - Objects detection\n",
      "KW  - Point features\n",
      "KW  - Point-based\n",
      "KW  - Query networks\n",
      "KW  - Sampling strategies\n",
      "KW  - Object detection\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: ; ; Conference name: 11th International Conference on Learning Representations, ICLR 2023; Conference date: 1 May 2023 through 5 May 2023; Conference code: 200817\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 110 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Dai, B.\n",
      "AU  - Li, C.\n",
      "TI  - RankSEG: A Consistent Ranking-based Framework for Segmentation\n",
      "PY  - 2023\n",
      "T2  - Journal of Machine Learning Research\n",
      "VL  - 24\n",
      "SP  - 1\n",
      "EP  - 50\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213865026&partnerID=40&md5=fa0310c1efe3220d156f70a7b5250f42\n",
      "AD  - Department of Statistics, The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - School of Statistics, University of Minnesota, 55455, MN, United States\n",
      "AB  - Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel execution are developed to implement the proposed framework in large-scale and high-dimensional segmentation. We study statistical properties of the proposed framework. We show it is Dice-/IoU-calibrated, and its excess risk bounds and the rate of convergence are also provided. The numerical effectiveness of RankDice/mRankDice is demonstrated in various simulated examples and Fine-annotated CityScapes, Pascal VOC and Kvasir-SEG datasets with state-of-the-art deep learning architectures. Python module and source code are available on GITHUB at https://github.com/statmlben/rankseg. ©2023 Ben Dai and Chunlin Li.\n",
      "KW  - Bayes rule\n",
      "KW  - Dice-calibrated\n",
      "KW  - excess risk bounds\n",
      "KW  - GPU computing\n",
      "KW  - normal approximation\n",
      "KW  - Poisson-binomial distribution\n",
      "KW  - ranking\n",
      "KW  - Segmentation\n",
      "KW  - Deep learning\n",
      "KW  - Image coding\n",
      "KW  - Image segmentation\n",
      "KW  - Natural language processing systems\n",
      "KW  - Normal distribution\n",
      "KW  - Pascal (programming language)\n",
      "KW  - Poisson distribution\n",
      "KW  - Problem oriented languages\n",
      "KW  - Three dimensional computer graphics\n",
      "KW  - Bayes' rule\n",
      "KW  - Dice-calibrated\n",
      "KW  - Excess risk bound\n",
      "KW  - Fundamental field\n",
      "KW  - GPU computing\n",
      "KW  - Normal approximation\n",
      "KW  - Poisson binomial distributions\n",
      "KW  - Ranking\n",
      "KW  - Risk bounds\n",
      "KW  - Segmentation\n",
      "KW  - Calibration\n",
      "PB  - Microtome Publishing\n",
      "SN  - 15324435 (ISSN)\n",
      "LA  - English\n",
      "J2  - J. Mach. Learn. Res.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 111 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ma, C.\n",
      "AU  - Yang, Y.\n",
      "AU  - Ju, C.\n",
      "AU  - Zhang, F.\n",
      "AU  - Zhang, Y.\n",
      "AU  - Wang, Y.\n",
      "TI  - AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191181689&partnerID=40&md5=c00f1599c05b046b2389f795626437f7\n",
      "AD  - Coop. Medianet Innovation Center, Shanghai Jiao Tong University, China\n",
      "AD  - Shanghai AI Laboratory, China\n",
      "AB  - Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names. For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training. However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users. To address these issues, this work proposes a novel attribute decomposition-aggregation framework, AttrSeg, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to complement semantic contexts from multiple perspectives. Two attribute construction strategies are designed: using large language models for common categories, and involving manually labelling for human-invented categories. In the aggregation stage, we group diverse attributes into an integrated global description, to form a discriminative classifier that distinguishes the target object from others. One hierarchical aggregation architecture is further proposed to achieve multi-level aggregations, leveraging the meticulously designed clustering module. The final results are obtained by computing the similarity between aggregated attributes and images embeddings. To evaluate the effectiveness, we annotate three types of datasets with attribute descriptions, and conduct extensive experiments and ablation studies. The results show the superior performance of attribute decomposition-aggregation. We refer readers to the latest arXiv version at https://arxiv.org/abs/2309.00096. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Semantic Segmentation\n",
      "KW  - Attribute construction\n",
      "KW  - Construction strategies\n",
      "KW  - Human cognition\n",
      "KW  - Language model\n",
      "KW  - Low qualities\n",
      "KW  - Multiple perspectives\n",
      "KW  - Object categories\n",
      "KW  - Pre-training\n",
      "KW  - Semantic context\n",
      "KW  - Semantic segmentation\n",
      "KW  - Semantics\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 6; Correspondence Address: Y. Wang; Coop. Medianet Innovation Center, Shanghai Jiao Tong University, China; email: wangyanfeng622@sjtu.edu.cn; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 112 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, J.\n",
      "AU  - Ji, W.\n",
      "AU  - Wang, S.\n",
      "AU  - Li, W.\n",
      "AU  - Cheng, L.\n",
      "TI  - DVSOD: RGB-D Video Salient Object Detection\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190363178&partnerID=40&md5=84aca1c1df7b832ec87d30b8f375001f\n",
      "AD  - University of Alberta, Canada\n",
      "AD  - Samsung Research America AI Center, United States\n",
      "AB  - Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. To bridge this gap, we in this work introduce the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (DVSOD). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, our dataset and benchmark results are publicly accessible at: https://dvsod.github.io/. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Object recognition\n",
      "KW  - Bounding-box\n",
      "KW  - Complex scenes\n",
      "KW  - Data constraints\n",
      "KW  - Detection models\n",
      "KW  - Multi-modal\n",
      "KW  - Potential researches\n",
      "KW  - Publicly accessible\n",
      "KW  - Salient object detection\n",
      "KW  - Temporal Data\n",
      "KW  - Object detection\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 13; Correspondence Address: W. Ji; University of Alberta, Canada; email: wji3@ualberta.ca; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 113 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hedegaard, L.\n",
      "AU  - Bakhtiarnia, A.\n",
      "AU  - Iosifidis, A.\n",
      "TI  - CONTINUAL TRANSFORMERS: REDUNDANCY-FREE ATTENTION FOR ONLINE INFERENCE\n",
      "PY  - 2023\n",
      "T2  - 11th International Conference on Learning Representations, ICLR 2023\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198523533&partnerID=40&md5=d70e1211af92dd4b0c4e2f98ab47b977\n",
      "AD  - Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark\n",
      "AB  - Transformers in their common form are inherently limited to operate on whole token sequences rather than on one token at a time.Consequently, their use during online inference on time-series data entails considerable redundancy due to the overlap in successive token sequences.In this work, we propose novel formulations of the Scaled Dot-Product Attention, which enable Transformers to perform efficient online token-by-token inference on a continual input stream.Importantly, our modifications are purely to the order of computations, while the outputs and learned weights are identical to those of the original Transformer Encoder.We validate our Continual Transformer Encoder with experiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable results: Our Continual one-and two-block architectures reduce the floating point operations per prediction by up to 63× and 2.6×, respectively, while retaining predictive performance. © 2023 11th International Conference on Learning Representations, ICLR 2023. All rights reserved.\n",
      "KW  - Digital arithmetic\n",
      "KW  - Signal encoding\n",
      "KW  - Floating point operations\n",
      "KW  - Input streams\n",
      "KW  - Online inferences\n",
      "KW  - Predictive performance\n",
      "KW  - Time-series data\n",
      "KW  - Token sequences\n",
      "KW  - Redundancy\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 11th International Conference on Learning Representations, ICLR 2023; Conference date: 1 May 2023 through 5 May 2023; Conference code: 200817\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 114 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Bainson, A.B.\n",
      "AU  - Boyarski, A.\n",
      "AU  - Hermanns, J.\n",
      "AU  - Petsinis, P.\n",
      "AU  - Aavad, N.\n",
      "AU  - Larsen, C.D.\n",
      "AU  - Swayne, T.\n",
      "AU  - Mottin, D.\n",
      "AU  - Bronstein, A.M.\n",
      "AU  - Karras, P.\n",
      "TI  - Spectral Subgraph Localization\n",
      "PY  - 2023\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 231\n",
      "SP  - 71\n",
      "EP  - 711\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193477872&partnerID=40&md5=a495ba8db86919897071563e01b4ab6b\n",
      "AD  - Aarhus University, Denmark\n",
      "AD  - Technion – IIT, Israel\n",
      "AD  - Technion, Israel Institute of Technology, Israel\n",
      "AD  - U. of Copenhagen, Denmark\n",
      "AB  - Several graph analysis problems are based on some variant of subgraph isomorphism: Given two graphs, G and Q, does G contain a subgraph isomorphic to Q? As this problem is NP-complete, past work usually avoids addressing it explicitly. In this paper, we propose a method that localizes, i.e., finds the best-match position of, Q in G, by aligning their Laplacian spectra and enhance its stability via bagging strategies; we relegate the finding of an exact node correspondence from Q to G to a subsequent and separate graph alignment task. We demonstrate that our localization strategy outperforms a baseline based on the state-of-the-art method for graph alignment in terms of accuracy on real graphs and scales to hundreds of nodes as no other method does. © 2023 Proceedings of Machine Learning Research. All rights reserved.\n",
      "KW  - Machine learning\n",
      "KW  - Analysis problems\n",
      "KW  - Best match\n",
      "KW  - Graph analysis\n",
      "KW  - Graph G\n",
      "KW  - Laplacian spectrum\n",
      "KW  - Localisation\n",
      "KW  - NP Complete\n",
      "KW  - Subgraph isomorphism\n",
      "KW  - Subgraphs\n",
      "KW  - Two-graphs\n",
      "KW  - Graph theory\n",
      "A2  - Villar S.\n",
      "A2  - Chamberlain B.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2nd Learning on Graphs Conference, LOG 2023; Conference date: 27 November 2023 through 30 November 2023; Conference code: 199421\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 115 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th International Symposium on Visual Computing, ISVC 2023\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 14362\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180637370&partnerID=40&md5=36d351da7dedc4d2ec88b660c2caead3\n",
      "AB  - The proceedings contain 83 papers. The special focus in this conference is on Visual Computing. The topics include: Comparison of Autoencoder Models for Unsupervised Representation Learning of Skeleton Sequences; local and Global Context Reasoning for Spatio-Temporal Action Localization; Zero-Shot Video Moment Retrieval Using BLIP-Based Models; self-supervised Representation Learning for Fine Grained Human Hand Action Recognition in Industrial Assembly Lines; pretext Tasks in Bridge Defect Segmentation Within a ViT-Adapter Framework; a Few-Shot Attention Recurrent Residual U-Net for Crack Segmentation; efficient Resource Provisioning in Critical Infrastructures Based on Multi-Agent Rollout Enabled by Deep Q-Learning; Video-Based Recognition of Aquatic Invasive Species Larvae Using Attention-LSTM Transformer; Latent Space Navigation for Face Privacy: A Case Study on the MNIST Dataset; Deep Learning Based GABA Edited-MRS Signal Reconstruction; domain Generalization for Foreground Segmentation Using Federated Learning; probabilistic Local Equivalence Certification for Robustness Evaluation; challenges of Depth Estimation for Transparent Objects; volumetric Body Composition Through Cross-Domain Consistency Training for Unsupervised Domain Adaptation; Water Animation Using Coupled SPH and Wave Equation; uniTorch - Integrating Neural Rendering into Unity; virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination; swarmCurves: Evolutionary Curve Reconstruction; brain Cortical Surface Registration with Anatomical Atlas Constraints; when System Model Meets Image Prior: An Unsupervised Deep Learning Architecture for Accelerated Magnetic Resonance Imaging; investigating the Impact of Attention on Mammogram Classification; 3D Reconstruction from 2D Cerebral Angiograms as a Volumetric Denoising Problem; an Integrated Shape-Texture Descriptor for Modeling Whole-Organism Phenotypes in Drug Screening; enhancing Image Reconstruction via Phase-Constrained Data in an Iterative Process; i Got Your Emotion: Emotion Preserving Face De-identification Using Injection-Based Generative Adversarial Networks; doppelVer: A Benchmark for Face Verification; two-Stage Face Detection and Anti-spoofing.\n",
      "A2  - Bebis G.\n",
      "A2  - Ghiasi G.\n",
      "A2  - Fang Y.\n",
      "A2  - Sharf A.\n",
      "A2  - Dong Y.\n",
      "A2  - Weaver C.\n",
      "A2  - Leo Z.\n",
      "A2  - LaViola Jr. J.J.\n",
      "A2  - Kohli L.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303147965-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th International Symposium on Visual Computing, ISVC 2023; Conference date: 16 October 2023 through 18 October 2023; Conference code: 305689\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 116 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wang, X.\n",
      "AU  - Yuan, H.\n",
      "AU  - Zhang, S.\n",
      "AU  - Chen, D.\n",
      "AU  - Wang, J.\n",
      "AU  - Zhang, Y.\n",
      "AU  - Shen, Y.\n",
      "AU  - Zhao, D.\n",
      "AU  - Zhou, J.\n",
      "TI  - VideoComposer: Compositional Video Synthesis with Motion Controllability\n",
      "PY  - 2023\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 36\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179162224&partnerID=40&md5=37efe4963bf701da0c471769299b9006\n",
      "AD  - Alibaba Group, China\n",
      "AD  - Ant Group, China\n",
      "AB  - The pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis.However, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency.Based on the paradigm of compositional generation, this work presents VideoComposer that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions.Specifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics.In addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency.Extensive experimental results suggest that VideoComposer is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions.The code and models are publicly available at https://videocomposer.github.io. © 2023 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Condition\n",
      "KW  - Content creation\n",
      "KW  - Cross-frames\n",
      "KW  - Customizable\n",
      "KW  - High standards\n",
      "KW  - Images synthesis\n",
      "KW  - Temporal consistency\n",
      "KW  - Temporal dynamics\n",
      "KW  - Video synthesis\n",
      "KW  - Visual content\n",
      "A2  - Oh A.\n",
      "A2  - Neumann T.\n",
      "A2  - Globerson A.\n",
      "A2  - Saenko K.\n",
      "A2  - Hardt M.\n",
      "A2  - Levine S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171389992-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 41; Conference name: 37th Conference on Neural Information Processing Systems, NeurIPS 2023; Conference date: 10 December 2023 through 16 December 2023; Conference code: 198465\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 117 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 18th International Symposium on Visual Computing, ISVC 2023\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 14361\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180625635&partnerID=40&md5=a7c6b97f6e88a90a4682c1d2254fcab5\n",
      "AB  - The proceedings contain 83 papers. The special focus in this conference is on Visual Computing. The topics include: Comparison of Autoencoder Models for Unsupervised Representation Learning of Skeleton Sequences; local and Global Context Reasoning for Spatio-Temporal Action Localization; Zero-Shot Video Moment Retrieval Using BLIP-Based Models; self-supervised Representation Learning for Fine Grained Human Hand Action Recognition in Industrial Assembly Lines; pretext Tasks in Bridge Defect Segmentation Within a ViT-Adapter Framework; a Few-Shot Attention Recurrent Residual U-Net for Crack Segmentation; efficient Resource Provisioning in Critical Infrastructures Based on Multi-Agent Rollout Enabled by Deep Q-Learning; Video-Based Recognition of Aquatic Invasive Species Larvae Using Attention-LSTM Transformer; Latent Space Navigation for Face Privacy: A Case Study on the MNIST Dataset; Deep Learning Based GABA Edited-MRS Signal Reconstruction; domain Generalization for Foreground Segmentation Using Federated Learning; probabilistic Local Equivalence Certification for Robustness Evaluation; challenges of Depth Estimation for Transparent Objects; volumetric Body Composition Through Cross-Domain Consistency Training for Unsupervised Domain Adaptation; Water Animation Using Coupled SPH and Wave Equation; uniTorch - Integrating Neural Rendering into Unity; virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination; swarmCurves: Evolutionary Curve Reconstruction; brain Cortical Surface Registration with Anatomical Atlas Constraints; when System Model Meets Image Prior: An Unsupervised Deep Learning Architecture for Accelerated Magnetic Resonance Imaging; investigating the Impact of Attention on Mammogram Classification; 3D Reconstruction from 2D Cerebral Angiograms as a Volumetric Denoising Problem; an Integrated Shape-Texture Descriptor for Modeling Whole-Organism Phenotypes in Drug Screening; enhancing Image Reconstruction via Phase-Constrained Data in an Iterative Process; i Got Your Emotion: Emotion Preserving Face De-identification Using Injection-Based Generative Adversarial Networks; doppelVer: A Benchmark for Face Verification; two-Stage Face Detection and Anti-spoofing.\n",
      "A2  - Bebis G.\n",
      "A2  - Ghiasi G.\n",
      "A2  - Fang Y.\n",
      "A2  - Sharf A.\n",
      "A2  - Dong Y.\n",
      "A2  - Weaver C.\n",
      "A2  - Leo Z.\n",
      "A2  - LaViola Jr. J.J.\n",
      "A2  - Kohli L.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303147968-7 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 18th International Symposium on Visual Computing, ISVC 2023; Conference date: 16 October 2023 through 18 October 2023; Conference code: 305689\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 118 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Kim, T.\n",
      "AU  - Kim, J.\n",
      "AU  - Shim, M.\n",
      "AU  - Yun, S.\n",
      "AU  - Kang, M.\n",
      "AU  - Wee, D.\n",
      "AU  - Lee, S.\n",
      "TI  - EXPLORING TEMPORALLY DYNAMIC DATA AUGMENTATION FOR VIDEO RECOGNITION\n",
      "PY  - 2023\n",
      "T2  - 11th International Conference on Learning Representations, ICLR 2023\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182318526&partnerID=40&md5=451899a97ed0c4aaa60d2a303d92e225\n",
      "AD  - NAVER Cloud, AI Tech, United States\n",
      "AD  - LG AI Research\n",
      "AD  - NAVER AI Lab, United States\n",
      "AD  - Yonsei University, South Korea\n",
      "AB  - Data augmentation has recently emerged as an essential component of modern training recipes for visual recognition tasks. However, data augmentation for video recognition has been rarely explored despite its effectiveness. Few existing augmentation recipes for video recognition naively extend the image augmentation methods by applying the same operations to the whole video frames. Our main idea is that the magnitude of augmentation operations for each frame needs to be changed over time to capture the real-world video's temporal variations. These variations should be generated as diverse as possible using fewer additional hyperparameters during training. Through this motivation, we propose a simple yet effective video data augmentation framework, DynaAugment. The magnitude of augmentation operations on each frame is changed by an effective mechanism, Fourier Sampling that parameterizes diverse, smooth, and realistic temporal variations. DynaAugment also includes an extended search space suitable for video for automatic data augmentation methods. DynaAugment experimentally demonstrates that there are additional performance rooms to be improved from static augmentations on diverse video models. Specifically, we show the effectiveness of DynaAugment on various video datasets and tasks: large-scale video recognition (Kinetics-400 and Something-Something-v2), small-scale video recognition (UCF-101 and HMDB-51), fine-grained video recognition (Diving-48 and FineGym), video action segmentation on Breakfast, video action localization on THUMOS'14, and video object detection on MOT17Det. © 2023 11th International Conference on Learning Representations, ICLR 2023. All rights reserved.\n",
      "KW  - Object detection\n",
      "KW  - Augmentation methods\n",
      "KW  - Data augmentation\n",
      "KW  - Dynamic data\n",
      "KW  - Hyper-parameter\n",
      "KW  - Real world videos\n",
      "KW  - Temporal variation\n",
      "KW  - Time-to-capture\n",
      "KW  - Video frame\n",
      "KW  - Video recognition\n",
      "KW  - Visual recognition\n",
      "KW  - Large datasets\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 11th International Conference on Learning Representations, ICLR 2023; Conference date: 1 May 2023 through 5 May 2023; Conference code: 200817\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 119 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shin, Y.\n",
      "AU  - Yoon, S.\n",
      "AU  - Song, H.\n",
      "AU  - Park, D.\n",
      "AU  - Kim, B.\n",
      "AU  - Lee, J.-G.\n",
      "AU  - Lee, B.S.\n",
      "TI  - Context Consistency Regularization for Label Sparsity in Time Series\n",
      "PY  - 2023\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 202\n",
      "SP  - 31579\n",
      "EP  - 31595\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174395052&partnerID=40&md5=caa8df719a642749bbde0665eb84dae9\n",
      "AD  - Graduate School of Data Science, KAIST, South Korea\n",
      "AD  - Department of Computer Science, University of Illinois at Urbana-Champaign, United States\n",
      "AD  - AWS AI Labs, United States\n",
      "AD  - School of Computing, KAIST, South Korea\n",
      "AD  - Department of Computer Science, University of Vermont, United States\n",
      "AB  - Labels are typically sparse in real-world time series due to the high annotation cost. Recently, consistency regularization techniques have been used to generate artificial labels from unlabeled augmented instances. To fully exploit the sequential characteristic of time series in consistency regularization, we propose a novel method of data augmentation called context-attached augmentation, which adds preceding and succeeding instances to a target instance to form its augmented instance. Unlike the existing augmentation techniques that modify a target instance by directly perturbing its attributes, the context-attached augmentation generates instances augmented with varying contexts while maintaining the target instance. Based on our augmentation method, we propose a context consistency regularization framework, which first adds different contexts to a target instance sampled from a given time series and then shares unitary reliability-based cross-window labels across the augmented instances to maintain consistency. We demonstrate that the proposed framework outperforms the existing state-of-the-art consistency regularization frameworks through comprehensive experiments on real-world time-series datasets. © 2023 Proceedings of Machine Learning Research. All rights reserved.\n",
      "KW  - Machine learning\n",
      "KW  - Augmentation methods\n",
      "KW  - Augmentation techniques\n",
      "KW  - Data augmentation\n",
      "KW  - Novel methods\n",
      "KW  - Real-world time series\n",
      "KW  - Regularisation\n",
      "KW  - Regularization framework\n",
      "KW  - Regularization technique\n",
      "KW  - Reliability-based\n",
      "KW  - Times series\n",
      "KW  - Time series\n",
      "A2  - Krause A.\n",
      "A2  - Brunskill E.\n",
      "A2  - Cho K.\n",
      "A2  - Engelhardt B.\n",
      "A2  - Sabato S.\n",
      "A2  - Scarlett J.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Correspondence Address: J.-G. Lee; School of Computing, KAIST, South Korea; email: jaegil@kaist.ac.kr; Conference name: 40th International Conference on Machine Learning, ICML 2023; Conference date: 23 July 2023 through 29 July 2023; Conference code: 191855\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 120 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Deng, D.\n",
      "AU  - Chen, G.\n",
      "AU  - Yu, Y.\n",
      "AU  - Liu, F.\n",
      "AU  - Heng, P.-A.\n",
      "TI  - Uncertainty Estimation by Fisher Information-based Evidential Deep Learning\n",
      "PY  - 2023\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 202\n",
      "SP  - 7596\n",
      "EP  - 7616\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174394314&partnerID=40&md5=bb6930078af00f8efa7d0993f788080b\n",
      "AD  - Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - Institute of Medical Intelligence and XR, The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - Zhejiang Lab, China\n",
      "AB  - Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning (I-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focus on the representation learning of uncertain classes. The generalization ability of our network is further improved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our proposed method consistently outperforms traditional EDL-related algorithms in multiple uncertainty estimation tasks, especially in the more challenging few-shot classification settings. © 2023 Proceedings of Machine Learning Research. All rights reserved.\n",
      "KW  - Deep learning\n",
      "KW  - Learning systems\n",
      "KW  - Data uncertainty\n",
      "KW  - Dirichlet distributions\n",
      "KW  - Fisher information\n",
      "KW  - Key factors\n",
      "KW  - Learning process\n",
      "KW  - Neural-networks\n",
      "KW  - Novel methods\n",
      "KW  - Performance\n",
      "KW  - Uncertainty\n",
      "KW  - Uncertainty estimation\n",
      "KW  - Fisher information matrix\n",
      "A2  - Krause A.\n",
      "A2  - Brunskill E.\n",
      "A2  - Cho K.\n",
      "A2  - Engelhardt B.\n",
      "A2  - Sabato S.\n",
      "A2  - Scarlett J.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 17; Correspondence Address: G. Chen; Zhejiang Lab, China; email: gychen@zhejianglab.com; Conference name: 40th International Conference on Machine Learning, ICML 2023; Conference date: 23 July 2023 through 29 July 2023; Conference code: 191855\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 121 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Communications in Computer and Information Science\n",
      "VL  - 1793 CCIS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161624829&partnerID=40&md5=79fcbb8e7854cc664233b9c02f1ef436\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 18650929 (ISSN); 978-981991644-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Commun. Comput. Info. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 122 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 23nd Scandinavian Conference on Image Analysis, SCIA 2023\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13885 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161372429&partnerID=40&md5=df132daa1f0f1e0f6a432bf2c6e41e5a\n",
      "AB  - The proceedings contain 67 papers. The special focus in this conference is on Image Analysis. The topics include: RELIEF: Joint Low-Light Image Enhancement and Super-Resolution with Transformers; to Quantify an Image Relevance Relative to a Target 3D Object; deep Active Learning for Glioblastoma Quantification; improved Sensitivity of No-Reference Image Visual Quality Metrics to the Presence of Noise; rethinking Matching-Based Few-Shot Action Recognition; accuracy of Parallel Distance Mapping Algorithms When Applied to Sub-Pixel Precision Transform; distortion-Based Transparency Detection Using Deep Learning on a Novel Synthetic Image Dataset; Regenerated Image Texture Features for COVID-19 Detection in Lung Images; depth-Aware Image Compositing Model for Parallax Camera Motion Blur; BrackishMOT: The Brackish Multi-Object Tracking Dataset; affine Moment Invariants of Tensor Fields; Fashion CUT: Unsupervised Domain Adaptation for Visual Pattern Classification in Clothes Using Synthetic Data and Pseudo-labels; Long Range Object-Level Monocular Depth Estimation for UAVs; radarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model; drawing and Analysis of Bounding Boxes for Object Detection with Anchor-Based Models; Raw or Cooked? Object Detection on RAW Images; local Neighborhood Features for 3D Classification; 3D Point Cloud Registration for GNSS-denied Aerial Localization over Forests; cleaner Categories Improve Object Detection and Visual-Textual Grounding; camera Calibration Without Camera Access - A Robust Validation Technique for Extended PnP Methods; CHAD: Charlotte Anomaly Dataset; iDFD: A Dataset Annotated for Depth and Defocus; TBPos: Dataset for Large-Scale Precision Visual Localization; finnWoodlands Dataset; re-identification of Saimaa Ringed Seals from Image Sequences; attention-guided Boundary Refinement on Anchor-free Temporal Action Detection.\n",
      "A2  - Gade R.\n",
      "A2  - Felsberg M.\n",
      "A2  - Kämäräinen J.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303131434-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 23nd Scandinavian Conference on Image Analysis, SCIA 2023; Conference date: 18 April 2023 through 21 April 2023; Conference code: 293949\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 123 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13623 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161262460&partnerID=40&md5=d7b329254de46a503bcd5885decf3668\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303130104-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 124 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lin, W.\n",
      "AU  - Kukleva, A.\n",
      "AU  - Possegger, H.\n",
      "AU  - Kuehne, H.\n",
      "AU  - Bischof, H.\n",
      "TI  - TAEC: Unsupervised action segmentation with temporal-Aware embedding and clustering\n",
      "PY  - 2023\n",
      "T2  - CEUR Workshop Proceedings\n",
      "VL  - 3349\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149368639&partnerID=40&md5=056af0ac11bb75236f3b1c7262740317\n",
      "AD  - Institute of Computer Graphics and Vision, Graz University of Technology, Austria\n",
      "AD  - Christian Doppler Laboratory for Semantic 3D Computer Vision, Austria\n",
      "AD  - Max-Planck-Institute for Informatics, Germany\n",
      "AD  - Goethe University Frankfurt, Germany\n",
      "AB  - Temporal action segmentation in untrimmed videos has gained increased attention recently. However, annotating action classes and frame-wise boundaries is extremely time consuming and cost intensive, especially on large-scale datasets. To address this issue, we propose an unsupervised approach for learning action classes from untrimmed video sequences. In particular, we propose a temporal embedding network that combines relative time prediction, feature reconstruction, and sequence-To-sequence learning, to preserve the spatial layout and sequential nature of the video features. A two-step clustering pipeline on these embedded feature representations then allows us to enforce temporal consistency within, as well as across videos. Based on the identified clusters, we decode the video into coherent temporal segments that correspond to semantically meaningful action classes. Our evaluation on three challenging datasets shows the impact of each component and, furthermore, demonstrates our state-of-The-Art unsupervised action segmentation results.  © 2023 Copyright for this paper by its authors.\n",
      "KW  - Action segmentation\n",
      "KW  - Unsupervised clustering\n",
      "KW  - Unsupervised learning\n",
      "KW  - Image segmentation\n",
      "KW  - Machine learning\n",
      "KW  - Network embeddings\n",
      "KW  - Action segmentation\n",
      "KW  - Clusterings\n",
      "KW  - Cost-intensive\n",
      "KW  - Embedding network\n",
      "KW  - Embeddings\n",
      "KW  - Large-scale datasets\n",
      "KW  - Learning actions\n",
      "KW  - Unsupervised approaches\n",
      "KW  - Unsupervised clustering\n",
      "KW  - Video sequences\n",
      "KW  - Large dataset\n",
      "A2  - Sablatnig R.\n",
      "A2  - Kleber F.\n",
      "PB  - CEUR-WS\n",
      "SN  - 16130073 (ISSN)\n",
      "LA  - English\n",
      "J2  - CEUR Workshop Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: W. Lin; Institute of Computer Graphics and Vision, Graz University of Technology, Austria; email: wei.lin@icg.tugraz.at; Conference name: 26th Computer Vision Winter Workshop, CVWW 2023; Conference date: 15 February 2023 through 17 February 2023; Conference code: 186840\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 125 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 23nd Scandinavian Conference on Image Analysis, SCIA 2023\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13886 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161370878&partnerID=40&md5=62b4709483fdc0f3cf00493bfb402727\n",
      "AB  - The proceedings contain 67 papers. The special focus in this conference is on Image Analysis. The topics include: RELIEF: Joint Low-Light Image Enhancement and Super-Resolution with Transformers; to Quantify an Image Relevance Relative to a Target 3D Object; deep Active Learning for Glioblastoma Quantification; improved Sensitivity of No-Reference Image Visual Quality Metrics to the Presence of Noise; rethinking Matching-Based Few-Shot Action Recognition; accuracy of Parallel Distance Mapping Algorithms When Applied to Sub-Pixel Precision Transform; distortion-Based Transparency Detection Using Deep Learning on a Novel Synthetic Image Dataset; Regenerated Image Texture Features for COVID-19 Detection in Lung Images; depth-Aware Image Compositing Model for Parallax Camera Motion Blur; BrackishMOT: The Brackish Multi-Object Tracking Dataset; affine Moment Invariants of Tensor Fields; Fashion CUT: Unsupervised Domain Adaptation for Visual Pattern Classification in Clothes Using Synthetic Data and Pseudo-labels; Long Range Object-Level Monocular Depth Estimation for UAVs; radarFormer: Lightweight and Accurate Real-Time Radar Object Detection Model; drawing and Analysis of Bounding Boxes for Object Detection with Anchor-Based Models; Raw or Cooked? Object Detection on RAW Images; local Neighborhood Features for 3D Classification; 3D Point Cloud Registration for GNSS-denied Aerial Localization over Forests; cleaner Categories Improve Object Detection and Visual-Textual Grounding; camera Calibration Without Camera Access - A Robust Validation Technique for Extended PnP Methods; CHAD: Charlotte Anomaly Dataset; iDFD: A Dataset Annotated for Depth and Defocus; TBPos: Dataset for Large-Scale Precision Visual Localization; finnWoodlands Dataset; re-identification of Saimaa Ringed Seals from Image Sequences; attention-guided Boundary Refinement on Anchor-free Temporal Action Detection.\n",
      "A2  - Gade R.\n",
      "A2  - Felsberg M.\n",
      "A2  - Kämäräinen J.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303131437-7 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 23nd Scandinavian Conference on Image Analysis, SCIA 2023; Conference date: 18 April 2023 through 21 April 2023; Conference code: 293949\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 126 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13624 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161686890&partnerID=40&md5=7bfc240709f53988746512802e7fb6fe\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303130107-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 127 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Communications in Computer and Information Science\n",
      "VL  - 1792 CCIS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161682691&partnerID=40&md5=b89399ad44d3ff147581e17df5ef089c\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 18650929 (ISSN); 978-981991641-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Commun. Comput. Info. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 128 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Communications in Computer and Information Science\n",
      "VL  - 1791 CCIS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161646389&partnerID=40&md5=fe5798125ac858908b355d6addc3e5a1\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 18650929 (ISSN); 978-981991638-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Commun. Comput. Info. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 129 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Gao, K.\n",
      "AU  - Chen, L.\n",
      "AU  - Zhang, H.\n",
      "AU  - Xiao, J.\n",
      "AU  - Sun, Q.\n",
      "TI  - COMPOSITIONAL PROMPT TUNING WITH MOTION CUES FOR OPEN-VOCABULARY VIDEO RELATION DETECTION\n",
      "PY  - 2023\n",
      "T2  - 11th International Conference on Learning Representations, ICLR 2023\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151071765&partnerID=40&md5=e9e1b101b871214ce8abf9923b193979\n",
      "AD  - Zhejiang University, China\n",
      "AD  - The Hong Kong University of Science and Technology, Hong Kong\n",
      "AD  - Nanyang Technological University, Singapore\n",
      "AD  - Singapore Management University, Singapore\n",
      "AB  - Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary predictions trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatio-temporal motion patterns of the subject-object compositions. Without bells and whistles, our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompts. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD. © 2023 11th International Conference on Learning Representations, ICLR 2023. All rights reserved.\n",
      "KW  - Object detection\n",
      "KW  - Semantics\n",
      "KW  - Time and motion study\n",
      "KW  - Language model\n",
      "KW  - Large-scales\n",
      "KW  - Motion cues\n",
      "KW  - Motion pattern\n",
      "KW  - Object classification\n",
      "KW  - Objects detection\n",
      "KW  - Semantic roles\n",
      "KW  - Spatio-temporal\n",
      "KW  - Technical challenges\n",
      "KW  - Video data\n",
      "KW  - Benchmarking\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 19; Correspondence Address: L. Chen; The Hong Kong University of Science and Technology, Hong Kong; email: zjuchenlong@gmail.com; Conference name: 11th International Conference on Learning Representations, ICLR 2023; Conference date: 1 May 2023 through 5 May 2023; Conference code: 200817\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 130 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Communications in Computer and Information Science\n",
      "VL  - 1794 CCIS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161637425&partnerID=40&md5=ac4b1b1ed7aa54586865c9da2ad68c01\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 18650929 (ISSN); 978-981991647-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Commun. Comput. Info. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 131 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Rodriguez-Opazo, C.\n",
      "AU  - Marrese-Taylor, E.\n",
      "AU  - Fernando, B.\n",
      "AU  - Takamura, H.\n",
      "AU  - Wu, Q.\n",
      "TI  - Memory-efficient Temporal Moment Localization in Long Videos\n",
      "PY  - 2023\n",
      "T2  - EACL 2023 - 17th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference\n",
      "SP  - 1901\n",
      "EP  - 1916\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159860593&partnerID=40&md5=1f7814300e83d019da23d0e7730d89e9\n",
      "AD  - Australian Institute for Machine Learning, Australia\n",
      "AD  - National Institute of Advanced Industrial Science and Technology, Japan\n",
      "AD  - The University of Tokyo, Japan\n",
      "AD  - A*STAR, Singapore\n",
      "AB  - Temporal Moment Localization is a challenging multimodal task which aims to identify the start and end timestamps of a moment of interest in an input untrimmed video, given a query in natural language. Solving this task correctly requires understanding the temporal relationships in the entire input video, but processing such long inputs and reasoning about them is memory and computationally expensive. In light of this issue, we propose Stochastic Bucket-wise Feature Sampling (SBFS), a stochastic sampling module that allows methods to process long videos at a constant memory footprint. We further combine SBFS with a new consistency loss to propose LOCFORMER, a Transformer-based model that can process videos as long as 18 minutes. We test our proposals on relevant benchmark datasets, showing that not only can LOCFORMER achieve excellent results, but also that our sampling is more effective than competing counterparts. Concretely, SBFS consistently improves the performance of prior work, by up to 3.13% in the mean temporal IoU, leading to a new state-of-the-art performance on Charades-STA and YouCookII, while also obtaining up to 12.8x speed-up at testing time and reducing memory requirements by up to 5x. © 2023 Association for Computational Linguistics.\n",
      "KW  - Computational linguistics\n",
      "KW  - Input videos\n",
      "KW  - Localisation\n",
      "KW  - Memory efficient\n",
      "KW  - Multi-modal\n",
      "KW  - Natural languages\n",
      "KW  - Stochastic sampling\n",
      "KW  - Stochastics\n",
      "KW  - Temporal moments\n",
      "KW  - Temporal relationships\n",
      "KW  - Time-stamp\n",
      "KW  - Stochastic systems\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 978-195942944-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - EACL - Conf. Eur. Chapter Assoc. Comput. Linguist., Proc. Conf.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023; Conference date: 2 May 2023 through 6 May 2023; Conference code: 188424\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 132 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 29th International Conference on Neural Information Processing, ICONIP 2022\n",
      "PY  - 2023\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13625 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161686575&partnerID=40&md5=5361036d6a060d3eed63d807429e4653\n",
      "AB  - The proceedings contain 359 papers. The special focus in this conference is on Neural Information Processing. The topics include: Local-Global Semantic Fusion Single-shot Classification Method; self-Reinforcing Feedback Domain Adaptation Channel; general Algorithm for Learning from Grouped Uncoupled Data and Pairwise Comparison Data; Additional Learning for Joint Probability Distribution Matching in BiGAN; multi-view Self-attention for Regression Domain Adaptation with Feature Selection; EigenGRF: Layer-Wise Eigen-Learning for Controllable Generative Radiance Fields; partial Label Learning with Gradually Induced Error-Correction Output Codes; HMC-PSO: A Hamiltonian Monte Carlo and Particle Swarm Optimization-Based Optimizer; heterogeneous Graph Representation for Knowledge Tracing; patch Mix Augmentation with Dual Encoders for Meta-Learning; intuitionistic Fuzzy Universum Support Vector Machine; support Vector Machine Based Models with Sparse Auto-encoder Based Features for Classification Problem; Selectively Increasing the Diversity of GAN-Generated Samples; cooperation and Competition: Flocking with Evolutionary Multi-Agent Reinforcement Learning; differentiable Causal Discovery Under Heteroscedastic Noise; IDPL: Intra-subdomain Adaptation Adversarial Learning Segmentation Method Based on Dynamic Pseudo Labels; adaptive Scaling for U-Net in Time Series Classification; permutation Elementary Cellular Automata: Analysis and Application of Simple Examples; SSPR: A Skyline-Based Semantic Place Retrieval Method; Double Regularization-Based RVFL and edRVFL Networks for Sparse-Dataset Classification; tacit Commitments Emergence in Multi-agent Reinforcement Learning; adaptive Tabu Dropout for Regularization of Deep Neural Networks; class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization; nearest Neighbor Classifier with Margin Penalty for Active Learning; factual Error Correction in Summarization with Retriever-Reader Pipeline; context-Adapted Multi-policy Ensemble Method for Generalization in Reinforcement Learning; self-attention Based Multi-scale Graph Convolutional Networks; synesthesia Transformer with Contrastive Multimodal Learning; context-Based Point Generation Network for Point Cloud Completion; temporal Neighborhood Change Centrality for Important Node Identification in Temporal Networks; DOM2R-Graph: A Web Attribute Extraction Architecture with Relation-Aware Heterogeneous Graph Transformer; saccade Direction Information Channel; shared-Attribute Multi-Graph Clustering with Global Self-Attention.\n",
      "A2  - Tanveer M.\n",
      "A2  - Agarwal S.\n",
      "A2  - Ozawa S.\n",
      "A2  - Ekbal A.\n",
      "A2  - Jatowt A.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303130110-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th International Conference on Neural Information Processing, ICONIP 2022; Conference date: 22 November 2022 through 26 November 2022; Conference code: 293409\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 133 without DOI:\n",
      "TY  - CHAP\n",
      "AU  - Jana, P.\n",
      "AU  - Mohanta, P.P.\n",
      "TI  - Recent trends in 2D object detection and applications in video event recognition\n",
      "PY  - 2022\n",
      "T2  - Advancement of Deep Learning and Its Applications in Object Detection and Recognition\n",
      "SP  - 173\n",
      "EP  - 195\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130484695&partnerID=40&md5=f676feb351f4213fc42d421c11dcc7cc\n",
      "AD  - Dept. of Computer Science and Engg., Indian Institute of Technology, India\n",
      "AD  - Electronics and Communication Sc. Unit, Indian Statistical Institute, India\n",
      "AB  - Object detection serves as a significant step in improving performance of complex downstream computer vision tasks. It has been extensively studied for many years now and current state-of-the-art 2D object detection techniques proffer superlative results even in complex images. In this chapter, we discuss the geometry-based pioneering works in object detection, followed by the recent breakthroughs that employ deep learning. Some of these use a monolithic architecture that takes a RGB image as input and passes it to a feed-forward ConvNet or vision Transformer. These methods, thereby predict class-probability and bounding-box coordinates, all in a single unified pipeline. Two-stage architectures on the other hand, first generate region proposals and then feed it to a CNN to extract features and predict object category and bounding-box. We also elaborate upon the applications of object detection in video event recognition, to achieve better fine-grained video classification performance. Further, we highlight recent datasets for 2D object detection both in images and videos, and present a comparative performance summary of various state-of-the-art object detection techniques. © 2022 River Publishers. All rights reserved.\n",
      "KW  - Activity classification\n",
      "KW  - Deep learning\n",
      "KW  - Localization and classification\n",
      "KW  - Object detection\n",
      "KW  - Video event recognition\n",
      "PB  - River Publishers\n",
      "SN  - 978-877022700-1 (ISBN); 978-877022702-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. of Deep Learn. and Its Appl. in Object Detect. and Recognit.\n",
      "M3  - Book chapter\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: P. Jana; Dept. of Computer Science and Engg., Indian Institute of Technology, India; email: pjana@ieee.org\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 134 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Souri, Y.\n",
      "AU  - Farha, Y.A.\n",
      "AU  - Bahrami, E.\n",
      "AU  - Francesca, G.\n",
      "AU  - Gall, J.\n",
      "TI  - Robust Action Segmentation from Timestamp Supervision\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171635712&partnerID=40&md5=c370769d9b2ba118d95b727dbf2eeb9d\n",
      "AD  - Computer Vision Group, University of Bonn, Bonn, Germany\n",
      "AD  - Microsoft, Redmond, United States\n",
      "AD  - Birzeit University, West Bank, Birzeit, Palestine\n",
      "AD  - Toyota Motor Europe, Brussels, Belgium\n",
      "AB  - Action segmentation is the task of predicting an action label for each frame of an untrimmed video. As obtaining annotations to train an approach for action segmentation in a fully supervised way is expensive, various approaches have been proposed to train action segmentation models using different forms of weak supervision, e.g., action transcripts, action sets, or more recently timestamps. Timestamp supervision is a promising type of weak supervision as obtaining one timestamp per action is less expensive than annotating all frames, but it provides more information than other forms of weak supervision. However, previous works assume that every action instance is annotated with a timestamp, which is a restrictive assumption since it assumes that annotators do not miss any action. In this work, we relax this restrictive assumption and take missing annotations for some action instances into account. We show that our approach is more robust to missing annotations compared to other approaches and various baselines. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Action segmentation\n",
      "KW  - Action sets\n",
      "KW  - Segmentation models\n",
      "KW  - Time-stamp\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 135 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fang, Z.\n",
      "AU  - Li, Y.\n",
      "AU  - Lu, J.\n",
      "AU  - Dong, J.\n",
      "AU  - Han, B.\n",
      "AU  - Liu, F.\n",
      "TI  - Is Out-of-Distribution Detection Learnable?\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163186180&partnerID=40&md5=606d9864d4b867dd1aa30eae8f64c53b\n",
      "AD  - Australian Artificial Intelligence Institute, University of Technology Sydney, Australia\n",
      "AD  - Department of Computer Sciences, University of Wisconsin-Madison, United States\n",
      "AD  - State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, China\n",
      "AD  - ETH Zurich, Switzerland\n",
      "AD  - Department of Computer Science, Hong Kong Baptist University, Hong Kong\n",
      "AD  - School of Mathematics and Statistics, University of Melbourne, Australia\n",
      "AB  - Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Condition\n",
      "KW  - Detection algorithm\n",
      "KW  - Generalisation\n",
      "KW  - Generalization ability\n",
      "KW  - Learnability\n",
      "KW  - Learning Theory\n",
      "KW  - Probably approximately correct learning\n",
      "KW  - Setting outs\n",
      "KW  - Test data\n",
      "KW  - Training data\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 71; Correspondence Address: J. Lu; Australian Artificial Intelligence Institute, University of Technology Sydney, Australia; email: jie.lu@uts.edu.au; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 136 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Niu, S.\n",
      "AU  - Wu, J.\n",
      "AU  - Zhang, Y.\n",
      "AU  - Chen, Y.\n",
      "AU  - Zheng, S.\n",
      "AU  - Zhao, P.\n",
      "AU  - Tan, M.\n",
      "TI  - Efficient Test-Time Model Adaptation without Forgetting\n",
      "PY  - 2022\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 162\n",
      "SP  - 16888\n",
      "EP  - 16905\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163068470&partnerID=40&md5=197c3c8e9c65560e65d70e85005bc821\n",
      "AD  - School of Software Engineering, South China University of Technology, China\n",
      "AD  - Pazhou Laboratory, China\n",
      "AD  - Tencent AI Lab, China\n",
      "AD  - National University of Singapore, Singapore\n",
      "AD  - Key Laboratory of Big Data and Intelligent Robot, Ministry of Education, China\n",
      "AB  - Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and testing data by adapting a given model w.r.t. any testing sample. This task is particularly important for deep models when the test environment changes frequently. Although some recent attempts have been made to handle this task, we still face two practical challenges: 1) existing methods have to perform backward computation for each test sample, resulting in unbearable prediction cost to many applications; 2) while existing TTA solutions can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as catastrophic forgetting). In this paper, we point out that not all the test samples contribute equally to model adaptation, and high-entropy ones may lead to noisy gradients that could disrupt the model. Motivated by this, we propose an active sample selection criterion to identify reliable and non-redundant samples, on which the model is updated to minimize the entropy loss for test-time adaptation. Furthermore, to alleviate the forgetting issue, we introduce a Fisher regularizer to constrain important model parameters from drastic changes, where the Fisher importance is estimated from test samples with generated pseudo labels. Extensive experiments on CIFAR-10-C, ImageNet-C, and ImageNet-R verify the effectiveness of our proposed method. Code is available at https://github.com/mr-eggplant/EATA. Copyright © 2022 by the author(s)\n",
      "KW  - Artificial intelligence\n",
      "KW  - Model Adaptation\n",
      "KW  - Potential distributions\n",
      "KW  - Test Environment\n",
      "KW  - Test samples\n",
      "KW  - Test time\n",
      "KW  - Testing data\n",
      "KW  - Testing samples\n",
      "KW  - Time modeling\n",
      "KW  - Training and testing\n",
      "KW  - Training data\n",
      "KW  - Entropy\n",
      "A2  - Chaudhuri K.\n",
      "A2  - Jegelka S.\n",
      "A2  - Song L.\n",
      "A2  - Szepesvari C.\n",
      "A2  - Niu G.\n",
      "A2  - Sabato S.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 173; Correspondence Address: M. Tan; School of Software Engineering, South China University of Technology, China; email: mingkuitan@scut.edu.cn; Conference name: 39th International Conference on Machine Learning, ICML 2022; Conference date: 17 July 2022 through 23 July 2022; Conference code: 189002\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 137 without DOI:\n",
      "TY  - CHAP\n",
      "TI  - References\n",
      "PY  - 2022\n",
      "T2  - Graph Neural Networks: Foundations, Frontiers, and Applications\n",
      "SP  - 595\n",
      "EP  - 689\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162685367&partnerID=40&md5=f933f793c6715d4650799eb86387e936\n",
      "PB  - Springer Nature\n",
      "SN  - 978-981166054-2 (ISBN); 978-981166053-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Graph Neural Networks: Foundations, Frontiers, and Applications\n",
      "M3  - Book chapter\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 138 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, S.\n",
      "AU  - Zhang, F.\n",
      "AU  - Yang, K.\n",
      "AU  - Liu, L.\n",
      "AU  - Liu, S.\n",
      "AU  - Hou, J.\n",
      "AU  - Yi, S.\n",
      "TI  - Probing Visual-Audio Representation for Video Highlight Detection via Hard-Pairs Guided Contrastive Learning\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174739099&partnerID=40&md5=1358e635520ee26a3c8662f0abe0dd90\n",
      "AD  - Sensetime Research\n",
      "AD  - The Hong Kong Polytechnic University, Hong Kong\n",
      "AB  - Video highlight detection (VHD) is a crucial yet challenging problem which aims to identify the interesting moments in untrimmed videos. The key to this task lies in effective video representations that jointly pursue two goals, i.e., 1) cross-modal representation learning and 2) fine-grained feature discrimination. To issue 1), the dominant VHD models adopt cross-attention based transformer to learn audio-visual information and inter-modality alignment. They always assume that multi-modal signals are synchronized which may not hold in practice due to spurious noise and appearance shift in untrimmed videos. To relieve this problem, we propose a cross-modality co-occurrence encoding by considering not only single visual/audio but asynchronous cross-modal correlations. We also explore the additional global contextual information abstracted from local region to further promote the inter-modality learning. To issue 2), to enlarge the discriminative power of feature embedding, we propose a hard-pairs guided contrastive learning (HPCL) scheme to reflect intrinsic semantic representation. A hard-pairs sampling strategy is employed in HPCL to mine the hard segment samples for improving feature discrimination and providing significant gradient information. Extensive experiments conducted on two benchmarks demonstrate the effectiveness and superiority of our proposed methods compared to other state-of-the-art methods. © 2022. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Audio representation\n",
      "KW  - Audio-visual information\n",
      "KW  - Cross-modal representations\n",
      "KW  - Detection models\n",
      "KW  - Feature discrimination\n",
      "KW  - Fine grained\n",
      "KW  - Highlights detection\n",
      "KW  - Intermodality\n",
      "KW  - Learn+\n",
      "KW  - Video representations\n",
      "KW  - Semantics\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Correspondence Address: F. Zhang; Sensetime Research; email: zhangfeng4@sensetime.com; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 139 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shin, Y.\n",
      "AU  - Yoon, S.\n",
      "AU  - Kim, S.\n",
      "AU  - Song, H.\n",
      "AU  - Lee, J.-G.\n",
      "AU  - Lee, B.S.\n",
      "TI  - COHERENCE-BASED LABEL PROPAGATION OVER TIME SERIES FOR ACCELERATED ACTIVE LEARNING\n",
      "PY  - 2022\n",
      "T2  - ICLR 2022 - 10th International Conference on Learning Representations\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150390203&partnerID=40&md5=5af958b68d1027a7d393175b122bdc31\n",
      "AD  - KAIST, South Korea\n",
      "AD  - UIUC, United States\n",
      "AD  - Institute for Basic Science\n",
      "AD  - NAVER AI Lab\n",
      "AD  - University of Vermont, United States\n",
      "AB  - Time-series data are ubiquitous these days, but lack of the labels in time-series data is regarded as a hurdle for its broad applicability. Meanwhile, active learning has been successfully adopted to reduce the labeling efforts in various tasks. Thus, this paper addresses an important issue, time-series active learning. Inspired by the temporal coherence in time-series data, where consecutive data points tend to have the same label, our label propagation framework, called TCLP, automatically assigns a queried label to the data points within an accurately estimated time-series segment, thereby significantly boosting the impact of an individual query. Compared with traditional time-series active learning, TCLP is shown to improve the classification accuracy by up to 7.1 times when only 0.8% of data points in the entire time series are queried for their labels. © 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Active Learning\n",
      "KW  - Classification accuracy\n",
      "KW  - Datapoints\n",
      "KW  - Label propagation\n",
      "KW  - Labelings\n",
      "KW  - Temporal coherence\n",
      "KW  - Time-series data\n",
      "KW  - Times series\n",
      "KW  - Time series\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - ICLR - Int. Conf. Learn. Represent.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Correspondence Address: J.-G. Lee; KAIST, South Korea; email: jaegil@kaist.ac.kr; Conference name: 10th International Conference on Learning Representations, ICLR 2022; Conference date: 25 April 2022 through 29 April 2022; Conference code: 186704\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 140 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Qian, R.\n",
      "AU  - Li, Y.\n",
      "AU  - Yuan, L.\n",
      "AU  - Gong, B.\n",
      "AU  - Liu, T.\n",
      "AU  - Brown, M.\n",
      "AU  - Belongie, S.\n",
      "AU  - Yang, M.-H.\n",
      "AU  - Adam, H.\n",
      "AU  - Cui, Y.\n",
      "TI  - On Temporal Granularity in Self-Supervised Video Representation Learning\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174731278&partnerID=40&md5=cc73b9050864ed15ece5720c82606a09\n",
      "AD  - Google Research\n",
      "AD  - Cornell University, United States\n",
      "AD  - University of Copenhagen, Denmark\n",
      "AB  - This work presents an empirical exploration of temporal granularity in self-supervised video representation learning. While state-of-the-art methods commonly enforce the learned features to be temporally-persistent across the whole video, we argue that this objective may not be suitable for all video tasks. To reveal the impact of temporal granularity, we propose a simple unified framework to learn features from same unlabeled videos with varying granularities from temporally fine-grained to persistent, by only adjusting one coefficient. We conduct a comprehensive empirical study covering a variety of classic and emerging video benchmarks and find video-level understanding tasks prefer temporally persistent features while temporal understanding inside one video favors fine-grained features. The flexibility of our framework gives rise to competitive or state-of-the-art performance, even outperforming supervised pre-training in a few cases. Code will be available at https://github.com/tensorflow/models/tree/master/official/. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Empirical studies\n",
      "KW  - Fine grained\n",
      "KW  - Learn+\n",
      "KW  - Persistent feature\n",
      "KW  - Simple++\n",
      "KW  - State-of-the-art methods\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Temporal granularity\n",
      "KW  - Unified framework\n",
      "KW  - Video representations\n",
      "KW  - Video recording\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 141 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fish, E.\n",
      "AU  - Weinbren, J.\n",
      "AU  - Gilbert, A.\n",
      "TI  - Two-Stream Transformer Architecture for Long Form Video Understanding\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163577841&partnerID=40&md5=6db53ef71d043871924026998b15437b\n",
      "AD  - The University of Surrey, Guildford, United Kingdom\n",
      "AB  - Pure vision transformer architectures are highly effective for short video classification and action recognition tasks. However, due to the quadratic complexity of self attention and lack of inductive bias, transformers are resource intensive and suffer from data inefficiencies. Long form video understanding tasks amplify data and memory efficiency problems in transformers making current approaches unfeasible to implement on data or memory restricted domains. This paper introduces an efficient Spatio-Temporal Attention Network (STAN) which uses a two-stream transformer architecture to model dependencies between static image features and temporal contextual features. Our proposed approach can classify videos up to two minutes in length on a single GPU, is data efficient, and achieves SOTA performance on several long video understanding tasks. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Network architecture\n",
      "KW  - 'current\n",
      "KW  - Action recognition\n",
      "KW  - Inductive bias\n",
      "KW  - Memory-efficiency problem\n",
      "KW  - Quadratic complexity\n",
      "KW  - Restricted-domain\n",
      "KW  - Spatio-temporal\n",
      "KW  - Two-stream\n",
      "KW  - Video classification\n",
      "KW  - Video understanding\n",
      "KW  - Memory architecture\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 142 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, D.\n",
      "AU  - Hu, W.\n",
      "TI  - Learning to Focus on the Foreground for Temporal Sentence Grounding\n",
      "PY  - 2022\n",
      "T2  - Proceedings - International Conference on Computational Linguistics, COLING\n",
      "VL  - 29\n",
      "IS  - 1\n",
      "SP  - 5532\n",
      "EP  - 5541\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150001849&partnerID=40&md5=2d157b7039011f9c70428a485969581f\n",
      "AD  - Wangxuan Institute of Computer Technology, Peking University, China\n",
      "AB  - Temporal sentence grounding (TSG) is crucial and fundamental for video understanding. Previous works typically model the target activity referred to the sentence query in a video by extracting the appearance information from each whole frame. However, these methods fail to distinguish visually similar background noise and capture subtle details of small objects. Although a few recent works additionally adopt a detection model to filter out the background contents and capture local appearances of foreground objects, they rely on the quality of the detection model and suffer from the time-consuming detection process. To this end, we propose a novel detection-free framework for TSG—Grounding with Learnable Foreground (GLF), which efficiently learns to locate the foreground regions related to the query in consecutive frames for better modelling the target activity. Specifically, we first split each video frame into multiple patch candidates of equal size, and reformulate the foreground detection problem as a patch localization task. Then, we develop a self-supervised coarse-to-fine paradigm to learn to locate the most query-relevant patch in each frame and aggregate them among the video for final grounding. Further, we employ a multi-scale patch reasoning strategy to capture more fine-grained foreground information. Extensive experiments on three challenging datasets (Charades-STA, TACoS, ActivityNet) show that the proposed GLF outperforms state-of-the-art methods. © 2022 Proceedings - International Conference on Computational Linguistics, COLING. All rights reserved.\n",
      "KW  - Background noise\n",
      "KW  - Detection models\n",
      "KW  - Detection process\n",
      "KW  - Foreground objects\n",
      "KW  - Foreground regions\n",
      "KW  - Learn+\n",
      "KW  - Small objects\n",
      "KW  - Target activity\n",
      "KW  - Video frame\n",
      "KW  - Video understanding\n",
      "A2  - Calzolari N.\n",
      "A2  - Huang C.-R.\n",
      "A2  - Kim H.\n",
      "A2  - Pustejovsky J.\n",
      "A2  - Wanner L.\n",
      "A2  - Choi K.-S.\n",
      "A2  - Ryu P.-M.\n",
      "A2  - Chen H.-H.\n",
      "A2  - Donatelli L.\n",
      "A2  - Ji H.\n",
      "A2  - Kurohashi S.\n",
      "A2  - Paggio P.\n",
      "A2  - Paggio P.\n",
      "A2  - Xue N.\n",
      "A2  - Kim S.\n",
      "A2  - Hahm Y.\n",
      "A2  - He Z.\n",
      "A2  - Lee T.K.\n",
      "A2  - Santus E.\n",
      "A2  - Bond F.\n",
      "A2  - Na S.-H.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 29512093 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Main Conf. Int. Conf. Comput. Linguist., COLING\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 6; Correspondence Address: W. Hu; Wangxuan Institute of Computer Technology, Peking University, China; email: forhuwei@pku.edu.cn; Conference name: 29th International Conference on Computational Linguistics, COLING 2022; Conference date: 12 October 2022 through 17 October 2022; Conference code: 186893\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 143 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, J.\n",
      "AU  - Bao, Y.\n",
      "AU  - Yin, W.\n",
      "AU  - Wang, H.\n",
      "AU  - Gao, Y.\n",
      "AU  - Sonke, J.-J.\n",
      "AU  - Gavves, E.\n",
      "TI  - Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170402562&partnerID=40&md5=d68b24e72f295fba06a593ccf6085779\n",
      "AD  - University of Amsterdam, Amsterdam, Netherlands\n",
      "AD  - Nanjing University, Nanjing, China\n",
      "AD  - The Netherlands Cancer Institute, Amsterdam, Netherlands\n",
      "AB  - Few-shot semantic segmentation (FSS) aims to achieve novel objects segmentation with only a few annotated samples and has made great progress recently. Most of the existing FSS models focus on the feature matching between support and query to tackle FSS. However, the appearance variations between objects from the same category could be extremely large, leading to unreliable feature matching and query mask prediction. To this end, we propose a Support-induced Graph Convolutional Network (SiGCN) to explicitly excavate latent context structure in query images. Specifically, we propose a Support-induced Graph Reasoning (SiGR) module to capture salient query object parts at different semantic levels with a Support-induced GCN. Furthermore, an instance association (IA) module is designed to capture high-order instance context from both support and query instances. By integrating the proposed two modules, SiGCN can learn rich query context representation, and thus being more robust to appearance variations. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our SiGCN achieves state-of-the-art performance. © 2022. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Convolution\n",
      "KW  - Semantic Web\n",
      "KW  - Semantics\n",
      "KW  - Convolutional networks\n",
      "KW  - Features matching\n",
      "KW  - High-order\n",
      "KW  - Higher-order\n",
      "KW  - Objects segmentation\n",
      "KW  - Query images\n",
      "KW  - Query object\n",
      "KW  - Segmentation models\n",
      "KW  - Semantic levels\n",
      "KW  - Semantic segmentation\n",
      "KW  - Semantic Segmentation\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 144 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Luo, Z.\n",
      "AU  - Durante, Z.\n",
      "AU  - Li, L.\n",
      "AU  - Xie, W.\n",
      "AU  - Liu, R.\n",
      "AU  - Jin, E.\n",
      "AU  - Huang, Z.\n",
      "AU  - Li, L.Y.\n",
      "AU  - Wu, J.\n",
      "AU  - Niebles, J.C.\n",
      "AU  - Adeli, E.\n",
      "AU  - Fei-Fei, L.\n",
      "TI  - MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163175482&partnerID=40&md5=d12ea9588114e34682006c363f17c92a\n",
      "AD  - Stanford University, United States\n",
      "AB  - Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, subactivity, and atomic action level. We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on activity parsing and few-shot video classification, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Character recognition\n",
      "KW  - Graphic methods\n",
      "KW  - Modeling languages\n",
      "KW  - Syntactics\n",
      "KW  - Activity recognition\n",
      "KW  - Fine grained\n",
      "KW  - Generalisation\n",
      "KW  - Graph languages\n",
      "KW  - Human activities\n",
      "KW  - Language model\n",
      "KW  - Large models\n",
      "KW  - Multi actors\n",
      "KW  - Multiobject\n",
      "KW  - Video understanding\n",
      "KW  - Large dataset\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 10; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 145 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Pan, J.\n",
      "AU  - Lin, Z.\n",
      "AU  - Zhu, X.\n",
      "AU  - Shao, J.\n",
      "AU  - Li, H.\n",
      "TI  - ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149795447&partnerID=40&md5=48665ddda132e091ac69bbbb90a564dd\n",
      "AD  - Multimedia Laboratory, The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - Surrey Institute for People-Centred Artificial Intelligence, CVSSP, University of Surrey, United Kingdom\n",
      "AD  - Centre for Perceptual and Interactive Intelligence Limited, Hong Kong\n",
      "AB  - Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small (∼8%) per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Adaptation strategies\n",
      "KW  - Down-stream\n",
      "KW  - Fine tuning\n",
      "KW  - Model size\n",
      "KW  - Model training\n",
      "KW  - Performance\n",
      "KW  - Spatio-temporal\n",
      "KW  - Task adaptation\n",
      "KW  - Transfer learning\n",
      "KW  - Video transfer\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 129; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 146 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tan, J.\n",
      "AU  - Zhao, X.\n",
      "AU  - Shi, X.\n",
      "AU  - Kang, B.\n",
      "AU  - Wang, L.\n",
      "TI  - PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163166776&partnerID=40&md5=0b00df2e45d4f1af2908506e6550e184\n",
      "AD  - State Key Laboratory for Novel Software Technology, Nanjing University, China\n",
      "AD  - Platform and Content Group (PCG), Tencent\n",
      "AD  - Shanghai AI Lab, China\n",
      "AB  - Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Decoding process\n",
      "KW  - Detection framework\n",
      "KW  - Different class\n",
      "KW  - Fine grained\n",
      "KW  - Flexible mechanisms\n",
      "KW  - Localisation\n",
      "KW  - Multi-labels\n",
      "KW  - Multilevels\n",
      "KW  - Point-based representations\n",
      "KW  - Query points\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 16; Correspondence Address: L. Wang; State Key Laboratory for Novel Software Technology, Nanjing University, China; email: lmwang@nju.edu.cn; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 147 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Sun, Y.\n",
      "AU  - Xue, H.\n",
      "AU  - Song, R.\n",
      "AU  - Liu, B.\n",
      "AU  - Yang, H.\n",
      "AU  - Fu, J.\n",
      "TI  - Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163214245&partnerID=40&md5=368617f6d596a0dcfa5fa5d49200b5db\n",
      "AD  - Renmin University of China, Beijing, China\n",
      "AD  - University of Science and Technology of China, Hefei, China\n",
      "AD  - Microsoft Research, Beijing, China\n",
      "AB  - Large-scale video-language pre-training has shown significant improvement in video-language understanding tasks. Previous studies of video-language pre-training mainly focus on short-form videos (i.e., within 30 seconds) and sentences, leaving long-form video-language pre-training rarely explored. Directly learning representation from long-form videos and language may benefit many long-form video-language understanding tasks. However, it is challenging due to the difficulty of modeling long-range relationships and the heavy computational burden caused by more frames. In this paper, we introduce a Long-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a large-scale long-form video and paragraph dataset constructed from an existing public dataset. To effectively capture the rich temporal dynamics and to better align video and language in an efficient end-to-end manner, we introduce two novel designs in our LF-VILA model. We first propose a Multimodal Temporal Contrastive (MTC) loss to learn the temporal relation across different modalities by encouraging fine-grained alignment between long-form videos and paragraphs. Second, we propose a Hierarchical Temporal Window Attention (HTWA) mechanism to effectively capture long-range dependency while reducing computational cost in Transformer. We fine-tune the pre-trained LF-VILA model on seven downstream long-form video-language understanding tasks of paragraph-to-video retrieval and long-form video question-answering, and achieve new state-of-the-art performances. Specifically, our model achieves 16.1% relative improvement on ActivityNet paragraph-to-video retrieval task and 2.4% on How2QA task, respectively. We release our code, dataset, and pre-trained models at https://github.com/microsoft/XPretrain. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Computational burden\n",
      "KW  - End to end\n",
      "KW  - Language understanding\n",
      "KW  - Large-scales\n",
      "KW  - Multi-modal\n",
      "KW  - Pre-training\n",
      "KW  - Public dataset\n",
      "KW  - Temporal dynamics\n",
      "KW  - Training model\n",
      "KW  - Video retrieval\n",
      "KW  - Large dataset\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 42; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 148 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ülger, O.\n",
      "AU  - Wiederer, J.\n",
      "AU  - Ghafoorian, M.\n",
      "AU  - Belagiannis, V.\n",
      "AU  - Mettes, P.\n",
      "TI  - Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174687585&partnerID=40&md5=972cd9b27c0456b164ab97f275670d4a\n",
      "AD  - University of Amsterdam Amsterdam, Netherlands\n",
      "AD  - Mercedes-Benz Stuttgart, Germany\n",
      "AD  - Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany\n",
      "AB  - Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods. Code is available at https://github.com/ozzyou/MTD-GNN. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Forecasting\n",
      "KW  - Graph theory\n",
      "KW  - Object detection\n",
      "KW  - Dynamic graph\n",
      "KW  - Dynamic videos\n",
      "KW  - Graph networks\n",
      "KW  - Graph neural networks\n",
      "KW  - Learn+\n",
      "KW  - Multi tasks\n",
      "KW  - Node graph\n",
      "KW  - Spatio-temporal\n",
      "KW  - Spatio-temporal graphs\n",
      "KW  - Types of relations\n",
      "KW  - Graph neural networks\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 149 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Rana, A.J.\n",
      "AU  - Rawat, Y.S.\n",
      "TI  - Are all Frames Equal? Active Sparse Labeling for Video Action Detection\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163199388&partnerID=40&md5=bcd69d7f499bee8ea85bee86115193d5\n",
      "AD  - Center for Research in Computer Vision (CRCV), University of Central Florida, United States\n",
      "AB  - Video action detection requires annotations at every frame, which drastically increases the labeling cost. In this work, we focus on efficient labeling of videos for action detection to minimize this cost. We propose active sparse labeling (ASL), a novel active learning strategy for video action detection. Sparse labeling will reduce the annotation cost but poses two main challenges; 1) how to estimate the utility of annotating a single frame for action detection as detection is performed at video level?, and 2) how these sparse labels can be used for action detection which require annotations on all the frames? This work attempts to address these challenges within a simple active learning framework. For the first challenge, we propose a novel frame-level scoring mechanism aimed at selecting most informative frames in a video. Next, we introduce a novel loss formulation which enables training of action detection model with these sparsely selected frames. We evaluate the proposed approach on two different action detection benchmark datasets, UCF-101-24 and J-HMDB-21, and observed that active sparse labeling can be very effective in saving annotation costs. We demonstrate that the proposed approach performs better than random selection, outperforming all other baselines, with performance comparable to supervised approach using merely 10% annotations. Project details available at https://sites.google.com/view/activesparselabeling/home. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Learning systems\n",
      "KW  - Active Learning\n",
      "KW  - Active learning strategies\n",
      "KW  - Benchmark datasets\n",
      "KW  - Detection models\n",
      "KW  - Labelings\n",
      "KW  - Learning frameworks\n",
      "KW  - Performance\n",
      "KW  - Random selection\n",
      "KW  - Simple++\n",
      "KW  - Single frames\n",
      "KW  - Cost benefit analysis\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 150 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fang, J.\n",
      "AU  - Xu, M.\n",
      "AU  - Chen, H.\n",
      "AU  - Shuai, B.\n",
      "AU  - Tu, Z.\n",
      "AU  - Tighe, J.\n",
      "TI  - An In-depth Study of Stochastic Backpropagation\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163142681&partnerID=40&md5=0d2c3f55fe07777beadd8bfd89294eb7\n",
      "AD  - AWS AI Labs\n",
      "AB  - In this paper, we provide an in-depth study of Stochastic Backpropagation (SBP) when training deep neural networks for standard image classification and object detection tasks. During backward propagation, SBP calculates gradients by using only a subset of feature maps to save GPU memory and computational cost. We interpret SBP as an efficient way to implement stochastic gradient decent by performing backpropagation dropout, which leads to significant memory saving and training run-time reduction, with a minimal impact on the overall model accuracy. We offer best practices to apply SBP for training image recognition models, which can be adopted in learning a wide range of deep neural networks. Experiments on image classification and object detection show that SBP can save up to 40% of GPU memory with less than 1% accuracy degradation. Code is available at: https://github.com/amazon-research/stochastic-backpropagation. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Deep neural networks\n",
      "KW  - Image classification\n",
      "KW  - Image recognition\n",
      "KW  - Object detection\n",
      "KW  - Object recognition\n",
      "KW  - Stochastic models\n",
      "KW  - Stochastic systems\n",
      "KW  - Backward propagation\n",
      "KW  - Detection tasks\n",
      "KW  - Feature map\n",
      "KW  - Image objects\n",
      "KW  - Images classification\n",
      "KW  - In-depth study\n",
      "KW  - Memory cost\n",
      "KW  - Objects detection\n",
      "KW  - Standard images\n",
      "KW  - Stochastics\n",
      "KW  - Backpropagation\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: M. Xu; AWS AI Labs; email: xumingze@amazon.com; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 151 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, S.\n",
      "AU  - Zhang, F.\n",
      "AU  - Zhao, R.-W.\n",
      "AU  - Yang, K.\n",
      "AU  - Liu, L.\n",
      "AU  - Feng, R.\n",
      "AU  - Hou, J.\n",
      "TI  - Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation\n",
      "PY  - 2022\n",
      "T2  - BMVC 2022 - 33rd British Machine Vision Conference Proceedings\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170377408&partnerID=40&md5=de2d96c504e2733d7b89bed6230a0e5c\n",
      "AD  - Sensetime Research, China\n",
      "AD  - Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China\n",
      "AD  - Academy for Engineering and Technology, Fudan University, Shanghai, China\n",
      "AD  - The Hong Kong Polytechnic University, Hong Kong\n",
      "AB  - It has been found that temporal action proposal generation, which aims to discover the temporal action instances within the range of the start and end frames in the untrimmed videos, can largely benefit from proper temporal and semantic context exploitation. The latest efforts were dedicated to considering the temporal context and similarity-based semantic context through self-attention modules. However, they still suffer from cluttered background information and limited contextual feature learning. In this paper, we propose a novel Pyramid Region-based Slot Attention (PRSlot) modules to address these issues. Instead of using the similarity computation, our PRSlot module directly learns the local relations in an encoder-decoder manner and generates the representation of a local region enhanced based on the attention over input features called slot. Specifically, upon the input snippet-level features, PRSlot module takes the target snippet as query, its surrounding region as key and then generates slot representations for each query-key slot by aggregating the local snippet context with a parallel pyramid strategy. Based on PRSlot modules, we present a novel Pyramid Region-based Slot Attention Network termed PRSA-Net to learn a unified visual representation with rich temporal and semantic context for better proposal generation. Extensive experiments are conducted on two widely adopted THUMOS14 and ActivityNet-1.3 benchmarks. In particular, we improve the AR@100 from the previous best 50.67% to 56.12% for proposal generation and raise the mAP under 0.5 tIoU from 51.9% to 58.7% for action detection on THUMOS14. Code is available at https://github.com/handhand123/PRSA-Net. © 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Background information\n",
      "KW  - Background limiteds\n",
      "KW  - Cluttered backgrounds\n",
      "KW  - Contextual feature\n",
      "KW  - Encoder-decoder\n",
      "KW  - Feature learning\n",
      "KW  - Learn+\n",
      "KW  - Region-based\n",
      "KW  - Semantic context\n",
      "KW  - Similarity computation\n",
      "KW  - Semantics\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - BMVC - Br. Mach. Vis. Conf. Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Correspondence Address: R. Feng; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; email: fengrui@fudan.edu.cn; Conference name: 33rd British Machine Vision Conference Proceedings, BMVC 2022; Conference date: 21 November 2022 through 24 November 2022; Conference code: 192560\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 152 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Piao, Y.\n",
      "AU  - Lu, C.\n",
      "AU  - Zhang, M.\n",
      "AU  - Lu, H.\n",
      "TI  - Semi-Supervised Video Salient Object Detection Based on Uncertainty-Guided Pseudo Labels\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163192220&partnerID=40&md5=436726c1f21cb60ce4f356e2d9a5cbab\n",
      "AD  - Dalian University of Technology, China\n",
      "AD  - Pengcheng Lab, Shenzhen, China\n",
      "AB  - Semi-Supervised Video Salient Object Detection (SS-VSOD) is challenging because of the lack of temporal information caused by sparse annotations in video sequences. Most works address this problem by generating pseudo labels for unlabeled data. However, error-prone pseudo labels negatively affect the VOSD model. Therefore, a deeper insight into pseudo labels should be developed. In this work, we aim to explore 1) how to utilize the incorrect predictions in pseudo labels to guide the network to generate more robust pseudo labels and 2) how to further screen out the noise that still exists in the improved pseudo labels. To this end, we propose an Uncertainty-Guided Pseudo Label Generator (UGPLG), which makes full use of inter-frame information to ensure the temporal consistency of the pseudo-labels and improves the robustness of the pseudo labels by strengthening the learning of difficult scenarios. Furthermore, we also introduce adversarial learning to address the noise problems in pseudo labels, guaranteeing the positive guidance of pseudo labels during model training. Experimental results demonstrate that our methods outperform existing semi-supervised method and partial fully-supervised methods across five public benchmarks of DAVIS, FBMS, MCL, ViSal, and SegTrack-V2. Code and dataset are available at https://github.com/Lanezzz/UGPL. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Object recognition\n",
      "KW  - Supervised learning\n",
      "KW  - Adversarial learning\n",
      "KW  - Error prones\n",
      "KW  - Inter-frame information\n",
      "KW  - Salient object detection\n",
      "KW  - Semi-supervised\n",
      "KW  - Temporal consistency\n",
      "KW  - Temporal information\n",
      "KW  - Uncertainty\n",
      "KW  - Unlabeled data\n",
      "KW  - Video sequences\n",
      "KW  - Object detection\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 10; Correspondence Address: M. Zhang; Dalian University of Technology, China; email: miaozhang@dlut.edu.cn; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 153 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhu, W.\n",
      "AU  - Pang, B.\n",
      "AU  - Thapliyal, A.V.\n",
      "AU  - Wang, W.Y.\n",
      "AU  - Soricut, R.\n",
      "TI  - End-to-end Dense Video Captioning as Sequence Generation\n",
      "PY  - 2022\n",
      "T2  - Proceedings - International Conference on Computational Linguistics, COLING\n",
      "VL  - 29\n",
      "IS  - 1\n",
      "SP  - 5651\n",
      "EP  - 5665\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164563453&partnerID=40&md5=091c32e8632f1b51ed394c32a30d65be\n",
      "AD  - UC Santa Barbara, United States\n",
      "AD  - Google Research\n",
      "AB  - Dense video captioning aims to identify the events of interest in an input video, and generate descriptive captions for each event. Previous approaches usually follow a two-stage generative process, which first proposes a segment for each event, then renders a caption for each identified segment. Recent advances in large-scale sequence generation pretraining have seen great success in unifying task formulation for a great variety of tasks, but so far, more complex tasks such as dense video captioning are not able to fully utilize this powerful paradigm. In this work, we show how to model the two subtasks of dense video captioning jointly as one sequence generation task, and simultaneously predict the events and the corresponding descriptions. Experiments on YouCook2 and ViTT show encouraging results and indicate the feasibility of training complex tasks such as end-to-end dense video captioning integrated into large-scale pretrained models. © 2022 Proceedings - International Conference on Computational Linguistics, COLING. All rights reserved.\n",
      "KW  - Complex task\n",
      "KW  - End to end\n",
      "KW  - Generative process\n",
      "KW  - Input videos\n",
      "KW  - Large-scale sequences\n",
      "KW  - Large-scales\n",
      "KW  - Pre-training\n",
      "KW  - Sequence generation\n",
      "KW  - Subtask\n",
      "A2  - Calzolari N.\n",
      "A2  - Huang C.-R.\n",
      "A2  - Kim H.\n",
      "A2  - Pustejovsky J.\n",
      "A2  - Wanner L.\n",
      "A2  - Choi K.-S.\n",
      "A2  - Ryu P.-M.\n",
      "A2  - Chen H.-H.\n",
      "A2  - Donatelli L.\n",
      "A2  - Ji H.\n",
      "A2  - Kurohashi S.\n",
      "A2  - Paggio P.\n",
      "A2  - Paggio P.\n",
      "A2  - Xue N.\n",
      "A2  - Kim S.\n",
      "A2  - Hahm Y.\n",
      "A2  - He Z.\n",
      "A2  - Lee T.K.\n",
      "A2  - Santus E.\n",
      "A2  - Bond F.\n",
      "A2  - Na S.-H.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 29512093 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Main Conf. Int. Conf. Comput. Linguist., COLING\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 16; Conference name: 29th International Conference on Computational Linguistics, COLING 2022; Conference date: 12 October 2022 through 17 October 2022; Conference code: 186893\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 154 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xu, Z.\n",
      "AU  - Rawat, Y.S.\n",
      "AU  - Wong, Y.\n",
      "AU  - Kankanhalli, M.S.\n",
      "AU  - Shah, M.\n",
      "TI  - Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153769612&partnerID=40&md5=a7037ecff5a88e61cc489c0aba868080\n",
      "AD  - School of Computing, National University of Singapore, Singapore\n",
      "AD  - Center for Research in Computer Vision, University of Central Florida, United States\n",
      "AB  - We propose Differentiable Temporal Logic (DTL), a model-agnostic framework that introduces temporal constraints to deep networks. DTL treats the outputs of a network as a truth assignment of a temporal logic formula, and computes a temporal logic loss reflecting the consistency between the output and the constraints. We propose a comprehensive set of constraints, which are implicit in data annotations, and incorporate them with deep networks via DTL. We evaluate the effectiveness of DTL on the temporal action segmentation task and observe improved performance and reduced logical errors in the output of different task models. Furthermore, we provide an extensive analysis to visualize the desirable effects of DTL. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Computer circuits\n",
      "KW  - Action segmentation\n",
      "KW  - Data annotation\n",
      "KW  - Logical errors\n",
      "KW  - Performance\n",
      "KW  - Task modelling\n",
      "KW  - Temporal constraints\n",
      "KW  - Temporal logic formula\n",
      "KW  - Truth assignment\n",
      "KW  - Temporal logic\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 20; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 155 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lin, K.Q.\n",
      "AU  - Wang, A.J.\n",
      "AU  - Soldan, M.\n",
      "AU  - Wray, M.\n",
      "AU  - Yan, R.\n",
      "AU  - Xu, E.Z.\n",
      "AU  - Gao, D.\n",
      "AU  - Tu, R.\n",
      "AU  - Zhao, W.\n",
      "AU  - Kong, W.\n",
      "AU  - Cai, C.\n",
      "AU  - Wang, H.\n",
      "AU  - Damen, D.\n",
      "AU  - Ghanem, B.\n",
      "AU  - Liu, W.\n",
      "AU  - Shou, M.Z.\n",
      "TI  - Egocentric Video-Language Pretraining\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143080693&partnerID=40&md5=031bdcd8a076eefaa74c04e725960013\n",
      "AD  - Show Lab, National University of Singapore, Singapore\n",
      "AD  - University of Bristol, United Kingdom\n",
      "AD  - King Abdullah University of Science and Technology, Saudi Arabia\n",
      "AD  - Tencent Data Platform\n",
      "AB  - Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Character recognition\n",
      "KW  - Large dataset\n",
      "KW  - Natural language processing systems\n",
      "KW  - Text processing\n",
      "KW  - Daily activity\n",
      "KW  - Design decisions\n",
      "KW  - Down-stream\n",
      "KW  - Large-scales\n",
      "KW  - Learn+\n",
      "KW  - Negative samples\n",
      "KW  - Performance\n",
      "KW  - Performing work\n",
      "KW  - Pre-training\n",
      "KW  - Text retrieval\n",
      "KW  - Classification (of information)\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 68; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 156 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shang, J.\n",
      "AU  - Das, S.\n",
      "AU  - Ryoo, M.S.\n",
      "TI  - Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145882761&partnerID=40&md5=2674ea24914fbf57ed3ed83386ef5506\n",
      "AD  - Department of Computer Science, Stony Brook University, United States\n",
      "AD  - Department of Computer Science, University of North Carolina, Charlotte, United States\n",
      "AB  - Humans are remarkably flexible in understanding viewpoint changes due to visual cortex supporting the perception of 3D structure. In contrast, most of the computer vision models that learn visual representation from a pool of 2D images often fail to generalize over novel camera viewpoints. Recently, the vision architectures have shifted towards convolution-free architectures, visual Transformers, which operate on tokens derived from image patches. However, these Transformers do not perform explicit operations to learn viewpoint-agnostic representation for visual understanding. To this end, we propose a 3D Token Representation Layer (3DTRL) that estimates the 3D positional information of the visual tokens and leverages it for learning viewpoint-agnostic representations. The key elements of 3DTRL include a pseudo-depth estimator and a learned camera matrix to impose geometric transformations on the tokens, trained in an unsupervised fashion. These enable 3DTRL to recover the 3D positional information of the tokens from 2D patches. In practice, 3DTRL is easily plugged-in into a Transformer. Our experiments demonstrate the effectiveness of 3DTRL in many vision tasks including image classification, multi-view video alignment, and action recognition. The models with 3DTRL outperform their backbone Transformers in all the tasks with minimal added computation. Our code is available at https://github.com/elicassion/3DTRL. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Linear transformations\n",
      "KW  - 2D images\n",
      "KW  - 3D spaces\n",
      "KW  - 3D Structure\n",
      "KW  - Image patches\n",
      "KW  - Key elements\n",
      "KW  - Learn+\n",
      "KW  - Positional information\n",
      "KW  - Vision model\n",
      "KW  - Visual cortexes\n",
      "KW  - Visual representations\n",
      "KW  - Cameras\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 157 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tu, D.\n",
      "AU  - Sun, W.\n",
      "AU  - Min, X.\n",
      "AU  - Zhai, G.\n",
      "AU  - Shen, W.\n",
      "TI  - Video-based Human-Object Interaction Detection from Tubelet Tokens\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141697159&partnerID=40&md5=6cc56fe1eb7c5d617336162b3a79ee6b\n",
      "AD  - Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China\n",
      "AD  - MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China\n",
      "AB  - We present a novel vision Transformer, named TUTOR, which is able to learn tubelet tokens, served as highly-abstracted spatiotemporal representations, for video-based human-object interaction (V-HOI) detection. The tubelet tokens structurize videos by agglomerating and linking semantically-related patch tokens along spatial and temporal domains, which enjoy two benefits: 1) Compactness: each tubelet token is learned by a selective attention mechanism to reduce redundant spatial dependencies from others; 2) Expressiveness: each tubelet token is enabled to align with a semantic instance, i.e., an object or a human, across frames, thanks to agglomeration and linking. The effectiveness and efficiency of TUTOR are verified by extensive experiments. Results show our method outperforms existing works by large margins, with a relative mAP gain of 16.14% on VidHOI and a 2 points gain on CAD-120 as well as a 4× speedup. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Computer aided design\n",
      "KW  - Object detection\n",
      "KW  - Object recognition\n",
      "KW  - Effectiveness and efficiencies\n",
      "KW  - Human-object interaction\n",
      "KW  - Interaction detection\n",
      "KW  - Is-enabled\n",
      "KW  - Large margins\n",
      "KW  - Learn+\n",
      "KW  - Selective attention mechanism\n",
      "KW  - Spatial dependencies\n",
      "KW  - Spatial domains\n",
      "KW  - Temporal domain\n",
      "KW  - Semantics\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Correspondence Address: G. Zhai; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China; email: zhaiguangtao@sjtu.edu.cn; W. Shen; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; email: wei.shen@sjtu.edu.cn; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 158 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, J.\n",
      "AU  - Hu, Y.\n",
      "AU  - Wang, Y.\n",
      "AU  - Qin, Y.\n",
      "TI  - Self-Attention Refinement for Actionness Temporal Action Localization\n",
      "PY  - 2022\n",
      "T2  - CEUR Workshop Proceedings\n",
      "VL  - 3344\n",
      "SP  - 194\n",
      "EP  - 199\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148588863&partnerID=40&md5=179dfcdbbbdfe78059339450ff198372\n",
      "AD  - Beijing University of Posts and Telecommunications, Beijing, China\n",
      "AD  - Institute Of Urban Safety And Environmental Science, Beijing Academy Of Science And Technology, Beijing, China\n",
      "AB  - In this paper, In this paper, for the problem of temporal action localisation, we propose a way to obtain the start and end times and types of actions based on actionness by aggregating action instance segmentation on a sequence of temporal features. In addition we believe that the context of actions is not only reflected in the results of convolution, but also the characteristics of inter class similarity and intra class consistency are essential. For this reason, we designed temporal self-attention mechanism (TSA) and temporal pyramid pooling module (TPP). Our results show that the single-stage model can achieve considerable accuracy after proper feature fusion. © 2022 Copyright for this paper by its authors.\n",
      "KW  - actionness\n",
      "KW  - TPP\n",
      "KW  - TSA\n",
      "KW  - Actionness\n",
      "KW  - Attention mechanisms\n",
      "KW  - Class similarities\n",
      "KW  - Inter class\n",
      "KW  - Intra class\n",
      "KW  - Localisation\n",
      "KW  - Similarity class\n",
      "KW  - Temporal features\n",
      "KW  - Temporal pyramid pooling module\n",
      "KW  - Temporal self-attention mechanism\n",
      "A2  - Yuan X.\n",
      "A2  - Yang Q.\n",
      "PB  - CEUR-WS\n",
      "SN  - 16130073 (ISSN)\n",
      "LA  - English\n",
      "J2  - CEUR Workshop Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: J. Li; Beijing University of Posts and Telecommunications, Beijing, China; email: ljl19971013@163.com; Conference name: 3rd International Conference on Computer Engineering and Intelligent Control, ICCEIC 2022; Conference date: 25 November 2022 through 27 November 2022; Conference code: 186710\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 159 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, Z.\n",
      "AU  - Ding, M.\n",
      "AU  - Ding, Y.\n",
      "AU  - Ma, G.\n",
      "TI  - Research on Special Vehicle Detection and Passenger Elevator Docking Behavior Recognition in Intelligent Monitoring\n",
      "PY  - 2022\n",
      "T2  - ISCTT 2021 - 6th International Conference on Information Science, Computer Technology and Transportation\n",
      "SP  - 524\n",
      "EP  - 528\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137075920&partnerID=40&md5=6f7eb1f2da79023dfd8f5effdbc0cc2e\n",
      "AD  - College of Civil Aviation, Nanjing University of Aeronautics and Astronautics, Nanjing, China\n",
      "AB  - Aiming at the problem of time node entry in the airport collaborative decision-making (airport collaborative decision-making, A-CDM) system, a method for temporal Action localization based on the results of target detection is proposed. First, the YOLOv3 target detection algorithm was selected. Since the mainstream dataset does not include airport special ground work vehicles, the data set is recreated and migration learning is performed to detect and identify airport ground targets. The bounding box and category obtained by the target detection can further obtain the on-site target object Speed and pixel coordinates, through mobile learning, near detection and recognition of airport flying targets. Features such as the interaction relationship and speed of events occurring at each moment can make judgments on airport behavior and realize real-time monitoring of the real-time screen to ensure its working process. Now we have realized the temporal action localization of the passenger elevator car docking behavior. © VDE VERLAG GMBH · Berlin · Offenbach.\n",
      "KW  - Airport security\n",
      "KW  - Behavioral research\n",
      "KW  - Decision making\n",
      "KW  - Elevators\n",
      "KW  - Behaviour recognition\n",
      "KW  - Collaborative decision making\n",
      "KW  - Decision-making systems\n",
      "KW  - Intelligent monitoring\n",
      "KW  - Localisation\n",
      "KW  - Passenger elevators\n",
      "KW  - Target detection algorithm\n",
      "KW  - Targets detection\n",
      "KW  - Time nodes\n",
      "KW  - Vehicles detection\n",
      "KW  - Airports\n",
      "A2  - Zhang T.\n",
      "PB  - VDE VERLAG GMBH\n",
      "SN  - 978-380075728-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - ISCTT - Int. Conf. Inf. Sci., Comput. Technol. Transp.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: M. Ding; College of Civil Aviation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; email: nuaa_dm@nuaa.edu.cn; Conference name: 2021 6th International Conference on Information Science, Computer Technology and Transportation, ISCTT 2021; Conference date: 26 November 2021 through 28 November 2021; Conference code: 181661\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 160 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Barbu, T.\n",
      "TI  - Parametric and geometric PDE-based models for automatic image segmentation\n",
      "PY  - 2022\n",
      "T2  - Applied Sciences\n",
      "VL  - 24\n",
      "SP  - 1\n",
      "EP  - 11\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141150417&partnerID=40&md5=2ac54937f60948094727e380cddf18bb\n",
      "AB  - This research paper represents a survey describing some high-level mathematical models for digital image segmentation. They are divided into parametric and geometric partial differential equation (PDE)-based segmentation models. So, various parametric Active Contour models, which are based on PDE variational schemes, are surveyed first. The geometric, or geodesic, Active Contour models are presented next. They represent variational image segmentation solutions that combine successfully some parametric Active Contours to level-set functions in order to achieve an improved image partitioning. Our own contributions in this image analysis field are also discussed here. © 2022 Balkan Society of Geometers, Geometry Balkan Press. All Rights Reserved.\n",
      "KW  - automatic image segmentation\n",
      "KW  - energy minimization\n",
      "KW  - geometric active contour\n",
      "KW  - level sets\n",
      "KW  - parametric active contour\n",
      "KW  - PDE variational models\n",
      "PB  - Balkan Society of Geometers\n",
      "SN  - 14545101 (ISSN)\n",
      "LA  - English\n",
      "J2  - Appl. Sci.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 161 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Feichtenhofer, C.\n",
      "AU  - Fan, H.\n",
      "AU  - Li, Y.\n",
      "AU  - He, K.\n",
      "TI  - Masked Autoencoders As Spatiotemporal Learners\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140195650&partnerID=40&md5=e9e81121c63136cadb33681f189345e2\n",
      "AD  - Meta AI, FAIR, United States\n",
      "AB  - This paper studies a conceptually simple extension of Masked Autoencoders (MAE) [31] to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images [31]), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4× in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers [18]. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT [15], MAE [31], etc.) can be a unified methodology for representation learning with minimal domain knowledge. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Auto encoders\n",
      "KW  - Clock time\n",
      "KW  - Embeddings\n",
      "KW  - Inductive bias\n",
      "KW  - Information redundancies\n",
      "KW  - Learn+\n",
      "KW  - Learning from videos\n",
      "KW  - Random masking\n",
      "KW  - Simple++\n",
      "KW  - Spacetime\n",
      "KW  - Learning systems\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 228; Correspondence Address: ; ; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 162 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Baldanza, A.\n",
      "AU  - Aujol, J.-F.\n",
      "AU  - Traonmilin, Y.\n",
      "AU  - Alary, F.\n",
      "TI  - Piecewise linear prediction model for action tracking in sports\n",
      "PY  - 2022\n",
      "T2  - European Signal Processing Conference\n",
      "VL  - 2022-August\n",
      "SP  - 518\n",
      "EP  - 522\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141011461&partnerID=40&md5=dccee570ae382959b20cdd110d5df2c4\n",
      "AD  - Rematch\n",
      "AD  - Univ. Bordeaux, Bordeaux INP, CNRS, IMB, UMR 5251, Talence, F-33400, France\n",
      "AB  - Recent tracking methods in professional team sports reach very high accuracy by tracking the ball and players. However, it remains difficult for these methods to perform accurate real-time tracking in amateur acquisition conditions where the vertical position or orientation of the camera is not controlled and cameras use heterogeneous sensors. This article presents a method for tracking interesting content in an amateur sport game by analyzing player displacements. Defining optical flow of the foreground in the image as the player motions, we propose a piecewise linear supervised learning model for predicting the camera global motion needed to follow the action. © 2022 European Signal Processing Conference, EUSIPCO. All rights reserved.\n",
      "KW  - Cameras\n",
      "KW  - Piecewise linear techniques\n",
      "KW  - Signal processing\n",
      "KW  - Action tracking\n",
      "KW  - Condition\n",
      "KW  - High-accuracy\n",
      "KW  - Linear prediction models\n",
      "KW  - Piecewise linear\n",
      "KW  - Piecewise-linear\n",
      "KW  - Professional team\n",
      "KW  - Real time tracking\n",
      "KW  - Team sports\n",
      "KW  - Tracking method\n",
      "KW  - Sports\n",
      "PB  - European Signal Processing Conference, EUSIPCO\n",
      "SN  - 22195491 (ISSN); 978-908279709-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - European Signal Proces. Conf.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: A. Baldanza; Rematch; email: a.baldanza@rematch.fr; Conference name: 30th European Signal Processing Conference, EUSIPCO 2022; Conference date: 29 August 2022 through 2 September 2022; Conference code: 183617\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 163 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chen, P.\n",
      "AU  - Ji, D.\n",
      "AU  - Lin, K.\n",
      "AU  - Zeng, R.\n",
      "AU  - Li, T.H.\n",
      "AU  - Tan, M.\n",
      "AU  - Gan, C.\n",
      "TI  - Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation\n",
      "PY  - 2022\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 35\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141222309&partnerID=40&md5=1f63b55bbb31252b8e5e8d8322c32fcd\n",
      "AD  - South China University of Technology, China\n",
      "AD  - Pazhou Laboratory, China\n",
      "AD  - MIT-IBM Watson AI Lab, United States\n",
      "AD  - UMass Amherst, United States\n",
      "AD  - Shenzhen University, China\n",
      "AD  - Information Technology R&D Innovation Center of Peking University, China\n",
      "AD  - Key Laboratory of Big Data and Intelligent Robot, Ministry of Education, China\n",
      "AB  - We address a practical yet challenging problem of training robot agents to navigate in an environment following a path described by some language instructions. The instructions often contain descriptions of objects in the environment. To achieve accurate and efficient navigation, it is critical to build a map that accurately represents both spatial location and the semantic information of the environment objects. However, enabling a robot to build a map that well represents the environment is extremely challenging as the environment often involves diverse objects with various attributes. In this paper, we propose a multi-granularity map, which contains both object fine-grained details (e.g., color, texture) and semantic classes, to represent objects more comprehensively. Moreover, we propose a weakly-supervised auxiliary task, which requires the agent to localize instruction-relevant objects on the map. Through this task, the agent not only learns to localize the instruction-relevant objects for navigation but also is encouraged to learn a better map representation that reveals object information. We then feed the learned map and instruction to a waypoint predictor to determine the next navigation goal. Experimental results show our method outperforms the state-of-the-art by 4.0% and 4.6% w.r.t. success rate both in seen and unseen environments, respectively on VLN-CE dataset. Code is available at https://github.com/PeihaoChen/WS-MGMap. © 2022 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Robots\n",
      "KW  - Semantics\n",
      "KW  - Textures\n",
      "KW  - Color semantic\n",
      "KW  - Color textures\n",
      "KW  - Diverse objects\n",
      "KW  - Fine grained\n",
      "KW  - Learn+\n",
      "KW  - Learning for vision\n",
      "KW  - Map learning\n",
      "KW  - Multi-granularity\n",
      "KW  - Semantics Information\n",
      "KW  - Spatial location\n",
      "KW  - Navigation\n",
      "A2  - Koyejo S.\n",
      "A2  - Mohamed S.\n",
      "A2  - Agarwal A.\n",
      "A2  - Belgrave D.\n",
      "A2  - Cho K.\n",
      "A2  - Oh A.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171387108-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 40; Correspondence Address: M. Tan; South China University of Technology, China; email: mingkuitan@scut.edu.cn; Conference name: 36th Conference on Neural Information Processing Systems, NeurIPS 2022; Conference date: 28 November 2022 through 9 December 2022; Conference code: 189185\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 164 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, Z.\n",
      "AU  - Zhang, S.\n",
      "AU  - Pan, L.\n",
      "AU  - Qing, Z.\n",
      "AU  - Tang, M.\n",
      "AU  - Liu, Z.\n",
      "AU  - Ang, M.H.\n",
      "TI  - TADA! TEMPORALLY-ADAPTIVE CONVOLUTIONS FOR VIDEO UNDERSTANDING\n",
      "PY  - 2022\n",
      "T2  - ICLR 2022 - 10th International Conference on Learning Representations\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126970809&partnerID=40&md5=95cc5b2851798c2f115454cbfa1de021\n",
      "AD  - Advanced Robotics Centre, National University of Singapore, Singapore\n",
      "AD  - DAMO Academy, Alibaba Group, China\n",
      "AD  - S-Lab, Nanyang Technological University, Singapore\n",
      "AB  - Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin. © 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Calibration\n",
      "KW  - Adaptive weights\n",
      "KW  - Model complexes\n",
      "KW  - Modeling abilities\n",
      "KW  - Spatial convolution\n",
      "KW  - Spatio-temporal\n",
      "KW  - Temporal dimensions\n",
      "KW  - Temporal dynamics\n",
      "KW  - Temporal models\n",
      "KW  - Video modeling\n",
      "KW  - Video understanding\n",
      "KW  - Convolution\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - ICLR - Int. Conf. Learn. Represent.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 20; Correspondence Address: S. Zhang; DAMO Academy, Alibaba Group, China; email: qingzhiwu.qzw@alibaba-inc.com; M.H. Ang; Advanced Robotics Centre, National University of Singapore, Singapore; email: mpeangh@nus.edu.sg; Conference name: 10th International Conference on Learning Representations, ICLR 2022; Conference date: 25 April 2022 through 29 April 2022; Conference code: 186704\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 165 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Huu, P.N.\n",
      "AU  - Tien, D.N.\n",
      "AU  - Manh, K.N.\n",
      "TI  - Action recognition application using artificial intelligence for smart social surveillance system\n",
      "PY  - 2022\n",
      "T2  - Journal of Information Hiding and Multimedia Signal Processing\n",
      "VL  - 13\n",
      "IS  - 1\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126703897&partnerID=40&md5=f66682e58f4ecf4bc644700934865af5\n",
      "AD  - School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hanoi, Viet Nam\n",
      "AB  - Computer vision is an important area of artificial intelligence that aims to help it gain the ability to activate similar to humans. In the past, we often classify fruit by hand. Today, it is performed by the development of image-processing technology. When the quantity of fruits is huge, we need machine learning for classifying them. There-fore, we propose the fruit classification using the Tensorflow and Keras model (high-level framework of Tensorflow) in the paper. This is a simple problem of computer vision since it solves the basis problems such as object detection or face recognition. In the paper, we focus on modifying the network architecture of the Tensorflow model. As a result, the accuracy of the proposed model achieves 99% with only five epochs. © The Authors.\n",
      "KW  - Action recognition\n",
      "KW  - Deep learning; human tracking; COVID19\n",
      "KW  - Smart surveillance\n",
      "PB  - Taiwan Ubiquitous Information CO LTD\n",
      "SN  - 20734212 (ISSN)\n",
      "LA  - English\n",
      "J2  - J. Inf. Hiding Multimedia Signal Proces.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Correspondence Address: P.N. Huu; School of Electrical and Electronic Engineering, Hanoi University of Science and Technology, Hanoi, Viet Nam; email: phat.nguyenhuu@hust.edu.vn\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 166 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, J.\n",
      "AU  - Jain, M.\n",
      "AU  - Park, H.\n",
      "AU  - Yun, S.\n",
      "TI  - CROSS-ATTENTIONAL AUDIO-VISUAL FUSION FOR WEAKLY-SUPERVISED ACTION LOCALIZATION\n",
      "PY  - 2021\n",
      "T2  - ICLR 2021 - 9th International Conference on Learning Representations\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138455341&partnerID=40&md5=a41a6d35510e10faaf026edfeba0cf57\n",
      "AD  - Qualcomm AI Research\n",
      "AB  - Temporally localizing actions in videos is one of the key components for video understanding. Learning from weakly-labeled data is seen as a potential solution towards avoiding expensive frame-level annotations. Different from other works which only depend on visual-modality, we propose to learn richer audiovisual representation for weakly-supervised action localization. First, we propose a multi-stage cross-attention mechanism to collaboratively fuse audio and visual features, which preserves the intra-modal characteristics. Second, to model both foreground and background frames, we construct an open-max classifier which treats the background class as an open-set. Third, for precise action localization, we design consistency losses to enforce temporal continuity for the action-class prediction, and also help with foreground-prediction reliability. Extensive experiments on two publicly available video-datasets (AVE and ActivityNet1.2) show that the proposed method effectively fuses audio and visual modalities, and achieves the state-of-the-art results for weakly-supervised action localization. © 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.\n",
      "KW  - Attention mechanisms\n",
      "KW  - Audio features\n",
      "KW  - Audio-visual fusion\n",
      "KW  - Labeled data\n",
      "KW  - Learn+\n",
      "KW  - Localisation\n",
      "KW  - Multi-stages\n",
      "KW  - Video understanding\n",
      "KW  - Visual feature\n",
      "KW  - Visual modalities\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - ICLR - Int. Conf. Learn. Represent.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 33; Conference name: 9th International Conference on Learning Representations, ICLR 2021; Conference date: 3 May 2021 through 7 May 2021; Conference code: 186703\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 167 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hwang, S.\n",
      "AU  - Lim, H.\n",
      "AU  - Myung, H.\n",
      "TI  - Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176097222&partnerID=40&md5=8cb3abec2e4ddecd42dd2237762c44ed\n",
      "AD  - School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea\n",
      "AB  - Training a Convolutional Neural Network (CNN) to be robust against rotation has mostly been done with data augmentation. In this paper, another progressive vision of research direction is highlighted to encourage less dependence on data augmentation by achieving structural rotational invariance of a network. The deep equivariance-bridged SO(2) invariant network is proposed to echo such vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the graph representation of an image to acquire rotationally equivariant representation, as GCN is more suitable for constructing deeper network than spectral graph convolution-based approaches. Then, invariant representation is eventually obtained with Global Average Pooling (GAP), a permutation-invariant operation suitable for aggregating high-dimensional representations, over the equivariant set of vertices retrieved from SWN-GCN. Our method achieves the state-of-the-art image classification performance on rotated MNIST and CIFAR-10 images, where the models are trained with a non-augmented dataset only. Quantitative validations over invariance of the representations also demonstrate strong invariance of deep representations of SWN-GCN over rotations. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Classification (of information)\n",
      "KW  - Computer vision\n",
      "KW  - Convolutional neural networks\n",
      "KW  - AS graph\n",
      "KW  - Convolutional networks\n",
      "KW  - Convolutional neural network\n",
      "KW  - Data augmentation\n",
      "KW  - Equivariance\n",
      "KW  - Graph representation\n",
      "KW  - Invariant representation\n",
      "KW  - Nearest-neighbour\n",
      "KW  - Neighbor graph\n",
      "KW  - Rotational invariances\n",
      "KW  - Convolution\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 168 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Radevski, G.\n",
      "AU  - Moens, M.-F.\n",
      "AU  - Tuytelaars, T.\n",
      "TI  - Revisiting spatio-temporal layouts for compositional action recognition\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176091082&partnerID=40&md5=acc98d8346952d2b7228e869588404df\n",
      "AD  - PSI-ESAT, KU Leuven, Leuven, Belgium\n",
      "AD  - LIIR-CS Department, KU Leuven, Leuven, Belgium\n",
      "AB  - Recognizing human actions is fundamentally a spatio-temporal reasoning problem, and should be, at least to some extent, invariant to the appearance of the human and the objects involved. Motivated by this hypothesis, in this work, we take an object-centric approach to action recognition. Multiple works have studied this setting before, yet it remains unclear (i) how well a carefully crafted, spatio-temporal layout-based method can recognize human actions, and (ii) how, and when, to fuse the information from layout- and appearance-based models. The main focus of this paper is compositional/few-shot action recognition, where we advocate the usage of multi-head attention (proven to be effective for spatial reasoning) over spatio-temporal layouts, i.e., configurations of object bounding boxes. We evaluate different schemes to inject video appearance information to the system, and benchmark our approach on background cluttered action recognition. On the Something-Else and Action Genome datasets, we demonstrate (i) how to extend multi-head attention for spatio-temporal layout-based action recognition, (ii) how to improve the performance of appearance-based models by fusion with layout-based models, (iii) that even on non-compositional background-cluttered video datasets, a fusion between layout- and appearance-based models improves the performance. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Action recognition\n",
      "KW  - Appearance-based modeling\n",
      "KW  - Bounding-box\n",
      "KW  - Human actions\n",
      "KW  - Performance\n",
      "KW  - Reasoning problems\n",
      "KW  - Spatial reasoning\n",
      "KW  - Spatio-temporal\n",
      "KW  - Spatio-temporal reasoning\n",
      "KW  - Video dataset\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 12; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 169 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wang, D.\n",
      "AU  - Karout, S.\n",
      "TI  - Fine-grained Multi-Modal Self-Supervised Learning\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171891534&partnerID=40&md5=41e4b8196e4e8d9e4a9d9e8b31cf4704\n",
      "AD  - Department of Computer Science and Technology, University of Cambridge, Cambridge, UK Huawei R&D UK, 302 Cambridge Science Park, Cambridge, UK\n",
      "AB  - Multi-Modal Self-Supervised Learning from videos has been shown to improve model's performance on various downstream tasks. However, such Self-Supervised pretraining requires large batch sizes and a large amount of computation resources due to the noise present in the uncurated data. This is partly due to the fact that the prevalent training scheme is trained on coarse-grained setting, in which vectors representing the whole video clips or natural language sentences are used for computing similarity. Such scheme makes training noisy as part of the video clips can be totally not correlated with the other-modality input such as text description. In this paper, we propose a fine-grained multimodal self-supervised training scheme that computes the similarity between embeddings at finer-scale (such as individual feature map embeddings and embeddings of phrases), and uses attention mechanisms to reduce noisy pairs' weighting in the loss function. We show that with the proposed pre-training scheme, we can train smaller models, with smaller batch-size and much less computational resources to achieve downstream tasks performances comparable to State-Of-The-Art, for tasks including action recognition and text-image retrievals. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Character recognition\n",
      "KW  - Computer vision\n",
      "KW  - Supervised learning\n",
      "KW  - Video cameras\n",
      "KW  - Batch sizes\n",
      "KW  - Down-stream\n",
      "KW  - Embeddings\n",
      "KW  - Fine grained\n",
      "KW  - Learning from videos\n",
      "KW  - Modeling performance\n",
      "KW  - Multi-modal\n",
      "KW  - Pre-training\n",
      "KW  - Training schemes\n",
      "KW  - Video-clips\n",
      "KW  - Embeddings\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Correspondence Address: D. Wang; email: wd263@cam.ac.uk; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 170 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wei, H.\n",
      "AU  - Rudzicz, F.\n",
      "AU  - Fleet, D.\n",
      "AU  - Grantcharov, T.\n",
      "AU  - Taati, B.\n",
      "TI  - Intraoperative Adverse Event Detection in Laparoscopic Surgery: Stabilized Multi-Stage Temporal Convolutional Network with Focal-Uncertainty Loss\n",
      "PY  - 2021\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 149\n",
      "SP  - 283\n",
      "EP  - 307\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128003821&partnerID=40&md5=6c76c3447d47387c884c2322d4f0b8f4\n",
      "AD  - University of Toronto, International Centre for Surgical Safety, Surgical Safety Technologies, Canada\n",
      "AD  - University of Toronto, Vector Institute, International Centre for Surgical Safety, Surgical Safety Technologies, Canada\n",
      "AD  - University of Toronto, Vector Institute, Canada\n",
      "AD  - University of Toronto, International Centre for Surgical Safety, Surgical Safety Technologies, University Health Network, Canada\n",
      "AD  - University of Toronto, Toronto Rehabilitation Institute, University Health Network, Canada\n",
      "AB  - Intraoperative adverse events (iAEs) increase rates of postoperative mortality and morbidity. Identifying iAEs is important to quality assurance and postoperative care, but requires expertise, is time consuming, and expensive. Automated or partially-automated techniques are, therefore, desirable. Previous work showed that conventional image processing has not worked well with real-world laparoscopic videos. We present a novel modular deep learning system that can partially automate the process of iAE screening using videos of laparoscopic procedures. The system consists of a stabilizer to reduce camera motion, a spatiotemporal feature extractor, and a multi-stage temporal convolutional neural network to detect adverse events. We apply a novel focal-uncertainty smoothing loss to handle class imbalance and to address multi-task uncertainty. The system is evaluated using 5-fold cross-validation on a large (228 hours) dataset of laparoscopic videos, and we perform ablation studies to investigate the effects of stabilization and focal-uncertainty loss. Our system achieves an AUROC of 0.952, an average precision (AP) of 0.626 in thermal injury detection, and an AUROC of 0.823 and an AP of 0.336 in bleeding detection. Our novel modular deep learning system outperforms conventional deep learning baselines. The model can be used as a screening tool to search for high risk events and to provide feedback for operation quality improvements and postoperative care. Source code available on GitHub: https://github.com/ICSSresearch/IAE-video. © 2021 H. Wei, F. Rudzicz, D. Fleet, T. Grantcharov & B. Taati.\n",
      "KW  - Convolution\n",
      "KW  - Convolutional neural networks\n",
      "KW  - Deep learning\n",
      "KW  - Image processing\n",
      "KW  - Large dataset\n",
      "KW  - Quality assurance\n",
      "KW  - Adverse events\n",
      "KW  - Automated techniques\n",
      "KW  - Convolutional networks\n",
      "KW  - Events detection\n",
      "KW  - Intra-operative\n",
      "KW  - Laparoscopic surgery\n",
      "KW  - Modulars\n",
      "KW  - Multi-stages\n",
      "KW  - Postoperative care\n",
      "KW  - Uncertainty\n",
      "KW  - Learning systems\n",
      "A2  - Jung K.\n",
      "A2  - Yeung S.\n",
      "A2  - Sendak M.\n",
      "A2  - Sjoding M.\n",
      "A2  - Ranganath R.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 8; Conference name: 6th Machine Learning for Healthcare Conference, MLHC 2021; Conference date: 6 August 2021 through 7 August 2021; Conference code: 189347\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 171 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Kazakos, E.\n",
      "AU  - Huh, J.\n",
      "AU  - Nagrani, A.\n",
      "AU  - Zisserman, A.\n",
      "AU  - Damen, D.\n",
      "TI  - With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150962280&partnerID=40&md5=3c8d90f2a5212b322afd629d3540646e\n",
      "AD  - Dept. of Computer Science, University of Bristol, Bristol, United Kingdom\n",
      "AD  - Visual Geometry Group, University of Oxford, Oxford, United Kingdom\n",
      "AB  - In egocentric videos, actions occur in quick succession. We capitalise on the action's temporal context and propose a method that learns to attend to surrounding actions in order to improve recognition performance. To incorporate the temporal context, we propose a transformer-based multimodal model that ingests video and audio as input modalities, with an explicit language model providing action sequence context to enhance the predictions. We test our approach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art performance. Our ablations showcase the advantage of utilising temporal context as well as incorporating audio input modality and language model to rescore predictions. Code and models at: https://github.com/ekazakos/MTCN. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computational linguistics\n",
      "KW  - Action recognition\n",
      "KW  - Action sequences\n",
      "KW  - Audio input\n",
      "KW  - Input modalities\n",
      "KW  - Language model\n",
      "KW  - Learn+\n",
      "KW  - Multi-modal\n",
      "KW  - Multimodal models\n",
      "KW  - Performance\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 18; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 172 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Khan, S.\n",
      "AU  - Cuzzolin, F.\n",
      "TI  - Spatiotemporal Deformable Scene Graphs for Complex Activity Detection\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164890552&partnerID=40&md5=c539e429c840e9037e01fa427d44f330\n",
      "AD  - Visual Artificial Intelligence Laboratory, Oxford Brookes University, Oxford, United Kingdom\n",
      "AB  - Long-term complex activity recognition and localisation can be crucial for decision making in autonomous systems such as smart cars and surgical robots. Here we address the problem via a novel deformable, spatiotemporal scene graph approach, consisting of three main building blocks: (i) action tube detection, (ii) the modelling of the deformable geometry of parts, and (iii) a graph convolutional network. Firstly, action tubes are detected in a series of snippets. Next, a new 3D deformable RoI pooling layer is designed for learning the flexible, deformable geometry of the constituent action tubes. Finally, a scene graph is constructed by considering all parts as nodes and connecting them based on different semantics such as order of appearance, sharing the same action label and feature similarity. We also contribute fresh temporal complex activity annotation for the recently released ROAD autonomous driving and SARAS-ESAD surgical action datasets and show the adaptability of our framework to different domains. Our method is shown to significantly outperform graph-based competitors on both augmented datasets. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Complex networks\n",
      "KW  - Computer vision\n",
      "KW  - Decision making\n",
      "KW  - Graphic methods\n",
      "KW  - Surgery\n",
      "KW  - Activity detection\n",
      "KW  - Activity recognition\n",
      "KW  - Building blockes\n",
      "KW  - Complex activity\n",
      "KW  - Convolutional networks\n",
      "KW  - Decisions makings\n",
      "KW  - Deformable geometry\n",
      "KW  - Localisation\n",
      "KW  - Scene-graphs\n",
      "KW  - Smart car\n",
      "KW  - Semantics\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 173 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yi, F.\n",
      "AU  - Wen, H.\n",
      "AU  - Jiang, T.\n",
      "TI  - ASFormer: Transformer for Action Segmentation\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166474941&partnerID=40&md5=46808d737b9357c7922e1e85834aa53e\n",
      "AD  - NELVT Department of Computer Science, Peking University, China\n",
      "AB  - Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for the action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate the effectiveness of our methods. Code is available at https://github.com/ChinaYi/ASFormer. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Decoding\n",
      "KW  - Action segmentation\n",
      "KW  - Daily activity\n",
      "KW  - Decoder architecture\n",
      "KW  - Inductive bias\n",
      "KW  - Input sequence\n",
      "KW  - Sequential data\n",
      "KW  - Small training\n",
      "KW  - Temporal models\n",
      "KW  - Temporal relation\n",
      "KW  - Training sets\n",
      "KW  - Forecasting\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 78; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 174 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhou, Y.\n",
      "AU  - Li, B.\n",
      "AU  - Wang, Z.\n",
      "AU  - Li, H.\n",
      "TI  - Video Action Recognition with Neural Architecture Search\n",
      "PY  - 2021\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 157\n",
      "SP  - 1675\n",
      "EP  - 1690\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129913107&partnerID=40&md5=d4dfe7d270dfae22b0d50e2813182d01\n",
      "AD  - Dalian University of Technology, Liaoning Province, Dalian, 116024, China\n",
      "AD  - Baidu Research, Sunnyvale, 94089, CA, United States\n",
      "AB  - Recently, deep convolutional neural networks have been widely used in the field of video action recognition. Current approaches tend to concentrate on the structure design for different backbone networks, but what kind of network structures can process video both effectively and quickly still remains to be solved despite the encouraging progress. With the help of neural architecture search (NAS), we search for three hyperparameters in the video processing network, which are the number of frames, the number of layers per residual stage and the channel number for all layers. We relax the entire search space into a continuous search space, and search for a set of network architectures that balance accuracy and computational efficiency by considering accuracy as the primary optimization goal and computational complexity as the secondary optimization goal. We conduct experiments on UCF101 and Kinetics400 datasets, validating new state-of-the-art results of the proposed NAS based scheme for video action recognition. © 2021 Y. Zhou, B. Li*, Z. Wang* & H. Li.\n",
      "KW  - Neural Architecture Search\n",
      "KW  - Video Action Recognition\n",
      "KW  - Convolutional neural networks\n",
      "KW  - Deep neural networks\n",
      "KW  - Network architecture\n",
      "KW  - Video signal processing\n",
      "KW  - 'current\n",
      "KW  - Action recognition\n",
      "KW  - Back-bone network\n",
      "KW  - Convolutional neural network\n",
      "KW  - Neural architecture search\n",
      "KW  - Neural architectures\n",
      "KW  - Optimization goals\n",
      "KW  - Search spaces\n",
      "KW  - Structure design\n",
      "KW  - Video action recognition\n",
      "KW  - Computational efficiency\n",
      "A2  - Balasubramanian V.N.\n",
      "A2  - Tsang I.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: B. Li; Baidu Research, Sunnyvale, 94089, United States; email: baopuli@baidu.com; Z. Wang; Dalian University of Technology, Dalian, Liaoning Province, 116024, China; email: zhwang@dlut.edu.cn; Conference name: 13th Asian Conference on Machine Learning, ACML 2021; Conference date: 17 November 2021 through 19 November 2021; Conference code: 188696\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 175 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Vo, K.\n",
      "AU  - Joo, H.\n",
      "AU  - Yamazaki, K.\n",
      "AU  - Truong, S.\n",
      "AU  - Kitani, K.\n",
      "AU  - Tran, M.-T.\n",
      "AU  - Le, N.\n",
      "TI  - AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139900225&partnerID=40&md5=97a2485e849356aa29ed37d219a90087\n",
      "AD  - AICV Lab, University of Arkansas, Fayetteville, AR, United States\n",
      "AD  - University of Maryland, College Park, MD, United States\n",
      "AD  - Carnegie Mellon University, Pittsburgh, PA, United States\n",
      "AD  - University of Science, VNU-HCM, Ho Chi Minh City, Viet Nam\n",
      "AD  - Vietnam National University, Ho Chi Minh City, Viet Nam\n",
      "AB  - Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. An action only starts when the main actor in the video begins to interact with the environment, while it ends when the main actor stops the interaction. Despite the great progress in temporal action proposal generation, most existing works ignore the aforementioned fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of a human by proposing Actor Environment Interaction (AEI) network to improve the video representation for temporal action proposals generation. AEI contains two modules, i.e., perception-based visual representation (PVR) and boundary-matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed adaptive attention mechanism. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary-matching architectures (i.e., CNN-based and GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly outperforms the state-of-the-art methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection. Source code is available at. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Classification (of information)\n",
      "KW  - Computer vision\n",
      "KW  - Black boxes\n",
      "KW  - Boundary matching\n",
      "KW  - Human environment\n",
      "KW  - Human relations\n",
      "KW  - Interaction networks\n",
      "KW  - Model learning\n",
      "KW  - Perception-based\n",
      "KW  - Surrounding environment\n",
      "KW  - Video representations\n",
      "KW  - Visual representations\n",
      "KW  - Video recording\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 12; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 176 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Mo, S.\n",
      "AU  - Xia, J.\n",
      "AU  - Tan, X.\n",
      "AU  - Raj, B.\n",
      "TI  - Point3D: tracking actions as moving points with 3D CNNs\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147146555&partnerID=40&md5=2095237f6608b4aa0087b35dececbd3f\n",
      "AD  - Carnegie Mellon University, Pittsburgh, 15213, PA, United States\n",
      "AD  - University of Pittsburgh, Pittsburgh, 15260, PA, United States\n",
      "AB  - Spatio-temporal action recognition has been a challenging task that involves detecting where and when actions occur. Current state-of-the-art action detectors are mostly anchor-based, requiring sensitive anchor designs and huge computations due to calculating large numbers of anchor boxes. Motivated by nascent anchor-free approaches, we propose Point3D, a flexible and computationally efficient network with high precision for spatio-temporal action recognition. Our Point3D consists of a Point Head for action localization and a 3D Head for action classification. Firstly, Point Head is used to track center points and knot key points of humans to localize the bounding box of an action. These location features are then piped into a time-wise attention to learn long-range dependencies across frames. The 3D Head is later deployed for the final action classification. Our Point3D achieves state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in terms of frame-mAP and video-mAP. Comprehensive ablation studies also demonstrate the effectiveness of each module proposed in our Point3D. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - 'current\n",
      "KW  - 3D head\n",
      "KW  - Action classifications\n",
      "KW  - Action recognition\n",
      "KW  - Anchor-box\n",
      "KW  - Anchor-free\n",
      "KW  - Computationally efficient\n",
      "KW  - High-precision\n",
      "KW  - Spatio-temporal\n",
      "KW  - State of the art\n",
      "KW  - Benchmarking\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Correspondence Address: B. Raj; Carnegie Mellon University, Pittsburgh, 15213, United States; email: bhiksha@cs.cmu.edu; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 177 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Dai, R.\n",
      "AU  - Das, S.\n",
      "AU  - Brémond, F.\n",
      "TI  - CTRN: Class Temporal Relational Network for Action Detection\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128267878&partnerID=40&md5=b0ab852c1421a71323151e54472c5421\n",
      "AD  - Inria, Université Côte d'Azur, France\n",
      "AD  - Stony Brook University, United States\n",
      "AB  - Action detection is an essential and challenging task, especially for densely labelled datasets of untrimmed videos. There are many real-world challenges in those datasets, such as composite action, co-occurring action, and high temporal variation of instance duration. For handling these challenges, we propose to explore both the class and temporal relations of detected actions. In this work, we introduce an end-to-end network: Class-Temporal Relational Network (CTRN). It contains three key components: (1) The Representation Transform Module filters the class-specific features from the mixed representations to build a graph structured data. (2) The Class-Temporal Module models the class and temporal relations in a sequential manner. (3) G-classifier leverages the privileged knowledge of the snippet-wise co-occurring action pairs to further improve the co-occurring action detection. We evaluate CTRN on three challenging densely labelled datasets and achieve state-of-the-art performance, reflecting the effectiveness and robustness of our method. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Class specific features\n",
      "KW  - Composite action\n",
      "KW  - End-to-end network\n",
      "KW  - Graph structured data\n",
      "KW  - Labeled dataset\n",
      "KW  - Module filters\n",
      "KW  - Real-world\n",
      "KW  - Relational network\n",
      "KW  - Temporal relation\n",
      "KW  - Temporal variation\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 7; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 178 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Piergiovanni, A.J.\n",
      "AU  - Angelova, A.\n",
      "AU  - Ryoo, M.S.\n",
      "AU  - Essa, I.\n",
      "TI  - Unsupervised Discovery of Actions in Instructional Videos\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176104688&partnerID=40&md5=5989232e823bb9a76124b4d8ead08a40\n",
      "AD  - Google Research, United States\n",
      "AD  - Robotics at Google, United States\n",
      "AB  - In this paper we address the problem of automatically discovering atomic actions from instructional videos. Instructional videos contain complex activities and are a rich source of information for intelligent agents, such as, autonomous robots or virtual assistants, which can, for example, automatically 'read' the steps from an instructional video and execute them. However, videos are rarely annotated with atomic activities, their boundaries or duration. We present an unsupervised approach to learn atomic actions of structured human tasks from a variety of instructional videos. We propose a sequential stochastic autoregressive model for temporal segmentation of videos, which learns to represent and discover the sequential relationship between different actions of the task, and provides automatic and unsupervised self-labeling. We evaluate on the breakfast, 50-salads and narrated instructional videos datasets. Code will be open sourced. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Atoms\n",
      "KW  - Computer vision\n",
      "KW  - Intelligent agents\n",
      "KW  - Intelligent robots\n",
      "KW  - Stochastic models\n",
      "KW  - Atomic actions\n",
      "KW  - Atomic activities\n",
      "KW  - Complex activity\n",
      "KW  - Human tasks\n",
      "KW  - Instructional videos\n",
      "KW  - Learn+\n",
      "KW  - Sources of informations\n",
      "KW  - Stochastics\n",
      "KW  - Unsupervised approaches\n",
      "KW  - Virtual assistants\n",
      "KW  - Stochastic systems\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 179 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Jain, M.\n",
      "AU  - Yahia, H.B.\n",
      "AU  - Ghodrati, A.\n",
      "AU  - Porikli, F.\n",
      "AU  - Habibian, A.\n",
      "TI  - Conditional Model Selection for Efficient Video Understanding\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176136015&partnerID=40&md5=5aaa639bff6a41afcb0f0b62cd351ac8\n",
      "AD  - Qualcomm AI Research, Qualcomm Technologies Netherlands B.V.\n",
      "AD  - Qualcomm AI Research, Qualcomm Technologies, Inc.\n",
      "AB  - Video action classification and temporal localization are two key components of video understanding where we witnessed significant progress leveraging neural network architectures. Recently, the research focus in this area shifted towards computationally efficient solutions to support real-world applications. Existing methods mainly aim to pick salient frames or video clips with fixed architectures. As an alternative, here, we propose to learn policies to select the most efficient neural model conditioned on the given input video. Specifically, we train a novel model-selector offline with model-affinity annotations that consolidate recognition quality and efficiency. Further, we incorporate the disparity between appearance and motion to estimate action background priors that enable efficient action localization without temporal annotations. To the best of our knowledge, this is the first attempt at computationally efficient action localization. We report classification results on two video benchmarks, Kinetics and multi-label HVU, and show that our method achieves state-of-the-art results while allowing a trade-off between accuracy and efficiency. For localization, we present evaluations on Thumos'14 and MultiThumos, where our approach improves or maintains the state-of-the-art performance while using only a fraction of the computation. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Economic and social effects\n",
      "KW  - Network architecture\n",
      "KW  - Neural networks\n",
      "KW  - Action classifications\n",
      "KW  - Computationally efficient\n",
      "KW  - Conditional models\n",
      "KW  - Localisation\n",
      "KW  - Model Selection\n",
      "KW  - Neural network architecture\n",
      "KW  - Real-world\n",
      "KW  - Research focus\n",
      "KW  - Temporal localization\n",
      "KW  - Video understanding\n",
      "KW  - Computational efficiency\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: H.B. Yahia; Qualcomm AI Research, Qualcomm Technologies Netherlands B.V.; email: hyahia@qti.qualcomm.com; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 180 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Patrick, M.\n",
      "AU  - Campbell, D.\n",
      "AU  - Asano, Y.\n",
      "AU  - Misra, I.\n",
      "AU  - Metze, F.\n",
      "AU  - Feichtenhofer, C.\n",
      "AU  - Vedaldi, A.\n",
      "AU  - Henriques, J.F.\n",
      "TI  - Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 15\n",
      "SP  - 12493\n",
      "EP  - 12506\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131824886&partnerID=40&md5=4564a4ce30c243ac9476d0e2c738dccf\n",
      "AD  - Facebook AI, United States\n",
      "AD  - University of Oxford, United Kingdom\n",
      "AB  - In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame t may be entirely unrelated to what is found at that location in frame t + k. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers-trajectory attention-that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos. While these ideas are useful in a range of settings, we apply them to the specific task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something-Something V2, and Epic-Kitchens datasets. Code and models are available at: https://github.com/facebookresearch/Motionformer. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Action recognition\n",
      "KW  - Ball trajectories\n",
      "KW  - Dynamic scenes\n",
      "KW  - High resolution\n",
      "KW  - Input size\n",
      "KW  - Motion path\n",
      "KW  - Quadratic dependence\n",
      "KW  - Spatial dimension\n",
      "KW  - Specific tasks\n",
      "KW  - Time dimension\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 193; Correspondence Address: M. Patrick; Facebook AI, United States; email: mandelapatrick@fb.com; D. Campbell; University of Oxford, United Kingdom; email: dylan@robots.ox.ac.uk; Y. Asano; University of Oxford, United Kingdom; email: yuki@robots.ox.ac.uk; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 181 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, W.\n",
      "AU  - Ki, M.\n",
      "AU  - Mun, C.\n",
      "AU  - Kho, S.\n",
      "AU  - Byun, H.\n",
      "TI  - Foreground Mining via Contrastive Guidance for Weakly Supervised Object Localization\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176111544&partnerID=40&md5=203ee0f8ca807126c9cc6cc884d09d71\n",
      "AD  - Department of Artificial Intelligence, Yonsei University, Seoul, South Korea\n",
      "AD  - AI Imaging Tech. Team LG Uplus, Seoul, South Korea\n",
      "AD  - Department of Computer Science, Yonsei University, Seoul, South Korea\n",
      "AB  - Weakly supervised object localization (WSOL) locates the target object within an image using only image-level labels. Recent methods try to extend the feature activation to cover entire object regions by dropping the most discriminative parts. However, they either overextend the activation into the background or are still limited to covering the most discriminative parts. In this paper, we propose a novel WSOL framework that localizes the entire object to the right extent via contrastive learning. Our framework contains three key components: 1) scheduled region drop, 2) contrastive guidance, and 3) pairwise non-local block. The scheduled region drop progressively erases the most discriminative parts of the original feature at a region-level. The erased feature facilitates the network to discover less discriminative regions in the original feature. Then, our contrastive guidance encourages the foregrounds of the original and erased features to be closer while pushing away from each background. In this manner, the network earns the capacity to differentiate the foregrounds from backgrounds, spreading out the activation within object regions. Last but not least, we utilize the pairwise non-local block, which provides an enhanced attention map to strengthen the spatial correlations between each pixel. In conclusion, our method achieves the state-of-the-art performance on CUB-200-2011 and ImageNet benchmarks regarding Top-1 Loc, GT-Loc and MaxBoxAccV2. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Benchmarking\n",
      "KW  - Computer vision\n",
      "KW  - Drops\n",
      "KW  - Object recognition\n",
      "KW  - Nonlocal\n",
      "KW  - Object localization\n",
      "KW  - Object region\n",
      "KW  - Spatial correlations\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Target object\n",
      "KW  - Chemical activation\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 182 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xu, M.\n",
      "AU  - Pérez-Rúa, J.-M.\n",
      "AU  - Zhu, X.\n",
      "AU  - Ghanem, B.\n",
      "AU  - Martinez, B.\n",
      "TI  - Low-Fidelity Video Encoder Optimization for Temporal Action Localization\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 12\n",
      "SP  - 9923\n",
      "EP  - 9935\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128537707&partnerID=40&md5=320a6cb7e26d013bbef839166d0cf24a\n",
      "AD  - Samsung AI Centre, Cambridge, United Kingdom\n",
      "AD  - King Abdullah University of Science and Technology, Saudi Arabia\n",
      "AB  - Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline, first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder – trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical flow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-fi/lofi_action_localization . © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Classification (of information)\n",
      "KW  - Large dataset\n",
      "KW  - Learning systems\n",
      "KW  - Signal encoding\n",
      "KW  - Action classifications\n",
      "KW  - Classification datasets\n",
      "KW  - Encoder optimization\n",
      "KW  - Joint optimization\n",
      "KW  - Localisation\n",
      "KW  - Localization method\n",
      "KW  - Low fidelities\n",
      "KW  - Target domain\n",
      "KW  - Transfer learning\n",
      "KW  - Video encoder\n",
      "KW  - Budget control\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 21; Correspondence Address: M. Xu; Samsung AI Centre, Cambridge, United Kingdom; email: mengmeng.xu@kaust.edu.sa; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 183 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Nag, S.\n",
      "AU  - Zhu, X.\n",
      "AU  - Xiang, T.\n",
      "TI  - Few-Shot Temporal Action Localization with Query Adaptive Transformer\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176135997&partnerID=40&md5=64435e47e3c574898f1888870cea32bd\n",
      "AD  - Centre for Vision Speech and Signal Processing (CVSSP), University of Surrey, United Kingdom\n",
      "AD  - iFlyTek-Surrey Joint Research Centre on Artificial Intelligence, United Kingdom\n",
      "AB  - Existing temporal action localization (TAL) works rely on a large number of training videos with exhaustive segment-level annotation, preventing them from scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this setting is not only unnatural - actions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further, a novel FS-TAL model is proposed which maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both the new class and each video of that class simultaneously. This is achieved by introducing a query adaptive Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our method can outperform all the state-of-the-art alternatives significantly in both single-domain and cross-domain scenarios. The source code can be found in https://github.com/sauradip/fewshotQAT. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Action segmentation\n",
      "KW  - Contextual cue\n",
      "KW  - Cross-domain\n",
      "KW  - Knowledge transfer\n",
      "KW  - Localisation\n",
      "KW  - Scalings\n",
      "KW  - Single domains\n",
      "KW  - State of the art\n",
      "KW  - Training class\n",
      "KW  - Video segments\n",
      "KW  - Knowledge management\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 184 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lin, Y.-B.\n",
      "AU  - Tseng, H.-Y.\n",
      "AU  - Lee, H.-Y.\n",
      "AU  - Lin, Y.-Y.\n",
      "AU  - Yang, M.-H.\n",
      "TI  - Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 14\n",
      "SP  - 11449\n",
      "EP  - 11461\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128578322&partnerID=40&md5=01b4c6e156ea4f53a72ea769843ed812\n",
      "AD  - National Yang Ming Chiao Tung University, Taiwan\n",
      "AD  - UNC Chapel Hill, United States\n",
      "AD  - UC Merced, United States\n",
      "AD  - Snap Research\n",
      "AD  - Google Research, United States\n",
      "AD  - Yonsei University, South Korea\n",
      "AB  - The audio-visual video parsing task aims to temporally parse a video into audio or visual event categories. However, it is labor-intensive to temporally annotate audio and visual events and thus hampers the learning of a parsing model. To this end, we propose to explore additional cross-video and cross-modality supervisory signals to facilitate weakly-supervised audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, our method explores event co-occurrence across audio, visual, and audio-visual streams. We leverage the explored cross-modality co-occurrence to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Quantitative and qualitative results demonstrate that the proposed method performs favorably against existing methods on weakly-supervised audio-visual video parsing. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Audio-visual\n",
      "KW  - Co-occurrence\n",
      "KW  - Cross modality\n",
      "KW  - Event semantics\n",
      "KW  - Labour-intensive\n",
      "KW  - Video parsing\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 57; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 185 without DOI:\n",
      "TY  - CONF\n",
      "TI  - MMSys 2021 - Proceedings of the 2021 Multimedia Systems Conference\n",
      "PY  - 2021\n",
      "T2  - MMSys 2021 - Proceedings of the 2021 Multimedia Systems Conference\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111446024&partnerID=40&md5=a7320f5de55cfc64c3b8c96bc0f6c7c1\n",
      "AB  - The proceedings contain 18 papers. The topics discussed include: motion segmentation and tracking for integrating event cameras; enabling hyperspectral imaging in diverse illumination conditions for indoor applications; Livelyzer: analyzing the first-mile ingest performance of live video streaming; playing chunk-transferred DASH segments at low latency with QLive; VRComm: an end-to-end web system for real-time photo-realistic social VR communication; towards cloud-edge collaborative online video analytics with fine-grained serverless pipelines; DataPlanner: data-budget driven approach to resource-efficient ABR streaming; user-assisted video reflection removal; LiveROI: region of interest analysis for viewport prediction in live mobile virtual reality streaming; and EScALation: a framework for efficient and scalable spatio-temporal action localization.\n",
      "PB  - Association for Computing Machinery, Inc\n",
      "SN  - 978-145038434-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - MMSys - Proc. Multimed. Syst. Conf.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 12th ACM Multimedia Systems Conference, MMSys 2021; Conference date: 28 September 2021 through 1 October 2021; Conference code: 170346\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 186 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Luo, Z.\n",
      "AU  - Xie, W.\n",
      "AU  - Kapoor, S.\n",
      "AU  - Liang, Y.\n",
      "AU  - Cooper, M.\n",
      "AU  - Niebles, J.C.\n",
      "AU  - Adeli, E.\n",
      "AU  - Fei-Fei, L.\n",
      "TI  - MOMA: Multi-Object Multi-Actor Activity Parsing\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 22\n",
      "SP  - 17939\n",
      "EP  - 17955\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128335214&partnerID=40&md5=95a40196054e365a544d724fede25379\n",
      "AD  - Stanford University, United States\n",
      "AB  - Complex activities often involve multiple humans utilizing different objects to complete actions (e.g., in healthcare settings, physicians, nurses, and patients interact with each other and various medical devices). Recognizing activities poses a challenge that requires a detailed understanding of actors' roles, objects' affordances, and their associated relationships. Furthermore, these purposeful activities comprise multiple achievable steps, including sub-activities and atomic actions, which jointly define a hierarchy of action parts. This paper introduces Activity Parsing as the overarching task of temporal segmentation and classification of activities, sub-activities, atomic actions, along with an instance-level understanding of actors, objects, and their relationships in videos. Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a new representation of spatial-temporal graphs containing hyperedges (i.e., edges with higher-order relationships). In addition, we introduce Multi-Object Multi-Actor (MOMA), the first benchmark and dataset dedicated to activity parsing. Lastly, to parse a video, we propose the HyperGraph Activity Parsing (HGAP) network, which outperforms several baselines, including those based on regular graphs and raw video data. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Action parts\n",
      "KW  - Affordances\n",
      "KW  - Atomic actions\n",
      "KW  - Complex activity\n",
      "KW  - Hyper graph\n",
      "KW  - Medical Devices\n",
      "KW  - Multi actors\n",
      "KW  - Multiobject\n",
      "KW  - Temporal classification\n",
      "KW  - Temporal segmentations\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 16; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 187 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, H.\n",
      "AU  - Chen, F.\n",
      "AU  - Yao, A.\n",
      "TI  - Weakly-Supervised Dense Action Anticipation\n",
      "PY  - 2021\n",
      "T2  - 32nd British Machine Vision Conference, BMVC 2021\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141162820&partnerID=40&md5=2bb7d86decb4b2010a2e3d2d82399f7c\n",
      "AD  - Department of Mathematics, National University of Singapore, Singapore\n",
      "AD  - Department of Computer Science, National University of Singapore, Singapore\n",
      "AB  - Dense anticipation aims to forecast future actions and their durations for long horizons. Existing approaches rely on fully-labelled data, i.e. sequences labelled with all future actions and their durations. We present a (semi-) weakly supervised method using only a small number of fully-labelled sequences and predominantly sequences in which only the (one) upcoming action is labelled. To this end, we propose a framework that generates pseudo-labels for future actions and their durations and adaptively refines them through a refinement module. Given only the upcoming action label as input, these pseudo-labels guide action/duration prediction for the future. We further design an attention mechanism to predict context-aware durations. Experiments on the Breakfast and 50Salads benchmarks verify our method's effectiveness; we are competitive even when compared to fully supervised state-of-the-art models. We will make our code available at: https://github.com/zhanghaotong1/WSLVideoDenseAnticipation. © 2021. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Action anticipations\n",
      "KW  - ART model\n",
      "KW  - Attention mechanisms\n",
      "KW  - Context-Aware\n",
      "KW  - Duration predictions\n",
      "KW  - Labeled data\n",
      "KW  - State of the art\n",
      "KW  - Supervised methods\n",
      "KW  - Forecasting\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 32nd British Machine Vision Conference, BMVC 2021; Conference date: 22 November 2021 through 25 November 2021; Conference code: 193122\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 188 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Xu, M.\n",
      "AU  - Xiong, Y.\n",
      "AU  - Chen, H.\n",
      "AU  - Li, X.\n",
      "AU  - Xia, W.\n",
      "AU  - Tu, Z.\n",
      "AU  - Soatto, S.\n",
      "TI  - Long Short-Term Transformer for Online Action Detection\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2\n",
      "SP  - 1086\n",
      "EP  - 1099\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122472416&partnerID=40&md5=fad2f1a45badc74a4011fbe6946bcefb\n",
      "AD  - Amazon, AWS AI, United States\n",
      "AB  - We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Empirical analysis\n",
      "KW  - Fine-scale\n",
      "KW  - Historical information\n",
      "KW  - Memory mechanism\n",
      "KW  - Model algorithms\n",
      "KW  - Sequence data\n",
      "KW  - Short term memory\n",
      "KW  - Short time windows\n",
      "KW  - Temporal models\n",
      "KW  - Temporal windows\n",
      "KW  - Heuristic methods\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 97; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 189 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Liu, M.\n",
      "AU  - Gao, X.\n",
      "AU  - Ge, F.\n",
      "AU  - Liu, H.\n",
      "AU  - Li, W.\n",
      "TI  - Learning Background Suppression Model for Weakly-supervised Temporal Action Localization\n",
      "PY  - 2021\n",
      "T2  - IAENG International Journal of Computer Science\n",
      "VL  - 48\n",
      "IS  - 4\n",
      "C7  - IJCS_48_4_22\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122452108&partnerID=40&md5=e2ce7644ce56d69ec0651fef0499305b\n",
      "AD  - College of Computer Science and Technology, Huaibei Normal University, Huaibei, 235000, China\n",
      "AB  - Weakly-supervised temporal action localization aims to identify all action instances and their correspondingcategories in the untrimmed videos. Since it involves only videolevellabels during training, resulting in this problem beingmore challenging. Existing attention-based action localizationmethods use the attention module to identify action segmentsand assign them to the appropriate action categories. However,such methods inevitably suffer from many backgroundsegments that are similar to the target actions, being recognizedas actions. To address this issue, we propose a newweakly-supervised temporal action localization network usingbackground suppression (BS-WTAL). The network defines thefiltering module, which can suppress the activation of thebackground regions, classification module, which identifies theactivity categories, and generative attention module, whichis learned to model a segment-wise representation. This enablesBS-WTAL to accurately distinguish actions from thebackground. Furthermore, we conduct ablation studies fromdifferent perspectives. Extensive experiments are performed ontwo datasets: THUMOS14 and ActivityNet1.2. Our approachexhibits better performance on these two datasets and achievesperformance comparable to the state-of-the-art fully-supervisedmethods © 2021. IAENG International Journal of Computer Science. All Rights Reserved.\n",
      "KW  - Background suppression\n",
      "KW  - Filtering module\n",
      "KW  - Temporal action localization\n",
      "KW  - Weak supervision\n",
      "KW  - Background suppression\n",
      "KW  - Filtering module\n",
      "KW  - Localisation\n",
      "KW  - Performance\n",
      "KW  - Regions Classification\n",
      "KW  - State of the art\n",
      "KW  - Temporal action localization\n",
      "KW  - Weak supervision\n",
      "KW  - Computers\n",
      "PB  - International Association of Engineers\n",
      "SN  - 1819656X (ISSN)\n",
      "LA  - English\n",
      "J2  - IAENG Int. J. Comput. Sci.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: X. Gao; College of Computer Science and Technology, Huaibei Normal University, Huaibei, 235000, China; email: xjgao75@163.com\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 190 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tan, R.\n",
      "AU  - Plummer, B.A.\n",
      "AU  - Saenko, K.\n",
      "AU  - Jin, H.\n",
      "AU  - Russell, B.\n",
      "TI  - Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 18\n",
      "SP  - 14476\n",
      "EP  - 14487\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123340942&partnerID=40&md5=a7bacb513e5fab34caac64c1f0479204\n",
      "AD  - Boston University, United States\n",
      "AD  - MIT-IBM Watson AI Lab., IBM Research, United States\n",
      "AD  - Adobe Research, United States\n",
      "AB  - We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations. To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities’ representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset. We show that our approach outperforms alternative baselines, including shallow co-attention and full cross-modal attention. We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective and, when combined with a word-to-region loss, achieves state of the art on recall-at-one and pointing hand accuracies. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Cross-modal\n",
      "KW  - Instructional videos\n",
      "KW  - Large corpora\n",
      "KW  - Learn+\n",
      "KW  - Localised\n",
      "KW  - Natural languages\n",
      "KW  - Optimisations\n",
      "KW  - Self-training\n",
      "KW  - Stackings\n",
      "KW  - Video dataset\n",
      "KW  - Visual languages\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 18; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 191 without DOI:\n",
      "TY  - CONF\n",
      "TI  - Proceedings - 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021\n",
      "PY  - 2021\n",
      "T2  - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123189480&partnerID=40&md5=eff17e3f6edfeb7c819f56887d0c4bc6\n",
      "AB  - The proceedings contain 1658 papers. The topics discussed include: single-stage instance shadow detection with bidirectional relation learning; learning Delaunay surface elements for mesh reconstruction; fusing the old with the new: learning relative camera pose with geometry-guided uncertainty; uncertainty guided collaborative training for weakly supervised temporal action detection; privacy-preserving collaborative learning with automatic transformation search; rethinking and improving the robustness of image style transfer; style-aware normalized loss for improving arbitrary style transfer; faster meta update strategy for noise-robust deep learning; a hyperbolic-to-hyperbolic graph convolutional network; training networks in null space of feature covariance for continual learning; and exponential moving average normalization for self-supervised and semi-supervised learning.\n",
      "PB  - IEEE Computer Society\n",
      "SN  - 10636919 (ISSN); 978-166544509-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021; Conference date: 19 June 2021 through 25 June 2021; Conference code: 174911; CODEN: PIVRE\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 192 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 11th International Conference on Image and Graphics, ICIG 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 12889 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117149193&partnerID=40&md5=91fd8f84c28f2aa040af7b1abe1d20b2\n",
      "AB  - The proceedings contain 198 papers. The special focus in this conference is on Image and Graphics. The topics include: Moving Object Detection Based on Self-adaptive Contour Extraction; small Infrared Aerial Target Detection Using Spatial and Temporal Cues; boundary Information Aggregation and Adaptive Keypoint Combination Enhanced Object Detection; accurate Oriented Instance Segmentation in Aerial Images; six-Channel Image Representation for Cross-Domain Object Detection; skeleton-Aware Network for Aircraft Landmark Detection; efficient Spectral Pyramid and Spectral-Spatial Feature Interactive Hyperspectral Image Classification; semi-supervised Cloud Edge Collaborative Power Transmission Line Insulator Anomaly Detection Framework; Illumination-Enhanced Crowd Counting Based on IC-Net in Low Lighting Conditions; HQ-Trans: A High-Quality Screening Based Image Translation Framework for Unsupervised Cross-Domain Pedestrian Detection; An Open-Source Library of 2D-GMM-HMM Based on Kaldi Toolkit and Its Application to Handwritten Chinese Character Recognition; automatic Leaf Diseases Detection System Based on Multi-stage Recognition; aerial Image Object Detection Based on Superpixel-Related Patch; AROA: Attention Refinement One-Stage Anchor-Free Detector for Objects in Remote Sensing Imagery; human-Object Interaction Detection Based on Multi-scale Attention Fusion; progressive Fusion Network for Safety Protection Detection; A Densely Connected Neural Network Based on SSD for Multiscale SAR Ship Detection; multi-level Features Selection Network Based on Multi-attention for Salient Object Detection; learning Disentangled Representation for Fine-Grained Visual Categorization; timeception Single Shot Action Detector: A Single-Stage Method for Temporal Action Detection; FER-YOLO: Detection and Classification Based on Facial Expressions; LLNet: A Lightweight Lane Line Detection Network; two-Stage Polishing Network for Camouflaged Object Detection; towards More Powerful Multi-column Convolutional Network for Crowd Counting; scene Text Transfer for Cross-Language.\n",
      "A2  - Peng Y.\n",
      "A2  - Hu S.-M.\n",
      "A2  - Gabbouj M.\n",
      "A2  - Zhou K.\n",
      "A2  - Elad M.\n",
      "A2  - Xu K.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303087357-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 11th International Conference on Image and Graphics, ICIG 2021; Conference date: 6 August 2021 through 8 August 2021; Conference code: 266359\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 193 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Song, L.\n",
      "AU  - Zhang, S.\n",
      "AU  - Liu, S.\n",
      "AU  - Li, Z.\n",
      "AU  - He, X.\n",
      "AU  - Sun, H.\n",
      "AU  - Sun, J.\n",
      "AU  - Zheng, N.\n",
      "TI  - Dynamic Grained Encoder for Vision Transformers\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 7\n",
      "SP  - 5770\n",
      "EP  - 5783\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121145838&partnerID=40&md5=454caa5916b083be39645602cf0801ef\n",
      "AD  - College of Artificial Intelligence, Xi'an Jiaotong University, China\n",
      "AD  - ShanghaiTech University, China\n",
      "AD  - Megvii Inc. (Face++)\n",
      "AD  - University of Chinese Academy of Sciences, China\n",
      "AD  - Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, China\n",
      "AB  - Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40%-60% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Modeling languages\n",
      "KW  - Object detection\n",
      "KW  - Computational costs\n",
      "KW  - De facto standard\n",
      "KW  - Fine grained\n",
      "KW  - Higher efficiency\n",
      "KW  - Language model\n",
      "KW  - Natural images\n",
      "KW  - Performance\n",
      "KW  - Spatial redundancy\n",
      "KW  - Spatial regions\n",
      "KW  - State of the art\n",
      "KW  - Signal encoding\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 22; Correspondence Address: H. Sun; College of Artificial Intelligence, Xi'an Jiaotong University, China; email: hsun@mail.xjtu.edu.cn; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 194 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13020 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118229320&partnerID=40&md5=f80cd8db09a2ab829ec56961c8eba85e\n",
      "AB  - The proceedings contain 201 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Relation-Guided Actor Attention for Group Activity Recognition; MVAD-Net: Learning View-Aware and Domain-Invariant Representation for Baggage Re-identification; joint Attention Mechanism for Unsupervised Video Object Segmentation; foreground Feature Selection and Alignment for Adaptive Object Detection; exploring Category-Shared and Category-Specific Features for Fine-Grained Image Classification; deep Mixture of Adversarial Autoencoders Clustering Network; SA-InterNet: Scale-Aware Interaction Network for Joint Crowd Counting and Localization; conditioners for Adaptive Regression Tracking; attention Template Update Model for Siamese Tracker; 3D Multi-object Detection and Tracking with Sparse Stationary LiDAR; insight on Attention Modules for Skeleton-Based Action Recognition; AO-AutoTrack: Anti-occlusion Real-Time UAV Tracking Based on Spatio-temporal Context; two-Stage Recognition Algorithm for Untrimmed Converter Steelmaking Flame Video; scale-Aware Multi-branch Decoder for Salient Object Detection; densely End Face Detection Network for Counting Bundled Steel Bars Based on YoloV5; POT: A Dataset of Panoramic Object Tracking; DP-YOLOv5: Computer Vision-Based Risk Behavior Detection in Power Grids; distillation-Based Multi-exit Fully Convolutional Network for Visual Tracking; handwriting Trajectory Reconstruction Using Spatial-Temporal Encoder-Decoder Network; scene Semantic Guidance for Object Detection; CRNet: Centroid Radiation Network for Temporal Action Localization; training Person Re-identification Networks with Transferred Images; ACFIM: Adaptively Cyclic Feature Information-Interaction Model for Object Detection; research of Robust Video Object Tracking Algorithm Based on Jetson Nano Embedded Platform; classification-IoU Joint Label Assignment for End-to-End Object Detection; joint Learning Appearance and Motion Models for Visual Tracking; reFlowNet: Revisiting Coarse-to-fine Learning of Optical Flow; control Variates for Similarity Search.\n",
      "A2  - Ma H.\n",
      "A2  - Wang L.\n",
      "A2  - Zhang C.\n",
      "A2  - Wu F.\n",
      "A2  - Tan T.\n",
      "A2  - Wang Y.\n",
      "A2  - Lai J.\n",
      "A2  - Zhao Y.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303088006-4 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021; Conference date: 29 October 2021 through 1 November 2021; Conference code: 267239\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 195 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lei, J.\n",
      "AU  - Berg, T.L.\n",
      "AU  - Bansal, M.\n",
      "TI  - QVHIGHLIGHTS: Detecting Moments and Highlights in Videos via Natural Language Queries\n",
      "PY  - 2021\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 15\n",
      "SP  - 11846\n",
      "EP  - 11858\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120539417&partnerID=40&md5=72c3999e6e2a257b3ac8d50d6add9974\n",
      "AD  - Department of Computer Science, University of North Carolina, Chapel Hill, United States\n",
      "AB  - Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHIGHLIGHTS) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr. © 2021 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Encoder-decoder\n",
      "KW  - Freeforms\n",
      "KW  - Natural language queries\n",
      "KW  - Natural languages\n",
      "KW  - News video\n",
      "KW  - Political activity\n",
      "KW  - Prediction problem\n",
      "KW  - Social activities\n",
      "KW  - User query\n",
      "KW  - YouTube\n",
      "A2  - Ranzato M.\n",
      "A2  - Beygelzimer A.\n",
      "A2  - Dauphin Y.\n",
      "A2  - Liang P.S.\n",
      "A2  - Wortman Vaughan J.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN); 978-171384539-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 140; Conference name: 35th Conference on Neural Information Processing Systems, NeurIPS 2021; Conference date: 6 December 2021 through 14 December 2021; Conference code: 179642\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 196 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13022 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118958730&partnerID=40&md5=fd6be1fd5f5407684b875b9748dadb58\n",
      "AB  - The proceedings contain 201 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Relation-Guided Actor Attention for Group Activity Recognition; MVAD-Net: Learning View-Aware and Domain-Invariant Representation for Baggage Re-identification; joint Attention Mechanism for Unsupervised Video Object Segmentation; foreground Feature Selection and Alignment for Adaptive Object Detection; exploring Category-Shared and Category-Specific Features for Fine-Grained Image Classification; deep Mixture of Adversarial Autoencoders Clustering Network; SA-InterNet: Scale-Aware Interaction Network for Joint Crowd Counting and Localization; conditioners for Adaptive Regression Tracking; attention Template Update Model for Siamese Tracker; 3D Multi-object Detection and Tracking with Sparse Stationary LiDAR; insight on Attention Modules for Skeleton-Based Action Recognition; AO-AutoTrack: Anti-occlusion Real-Time UAV Tracking Based on Spatio-temporal Context; two-Stage Recognition Algorithm for Untrimmed Converter Steelmaking Flame Video; scale-Aware Multi-branch Decoder for Salient Object Detection; densely End Face Detection Network for Counting Bundled Steel Bars Based on YoloV5; POT: A Dataset of Panoramic Object Tracking; DP-YOLOv5: Computer Vision-Based Risk Behavior Detection in Power Grids; distillation-Based Multi-exit Fully Convolutional Network for Visual Tracking; handwriting Trajectory Reconstruction Using Spatial-Temporal Encoder-Decoder Network; scene Semantic Guidance for Object Detection; CRNet: Centroid Radiation Network for Temporal Action Localization; training Person Re-identification Networks with Transferred Images; ACFIM: Adaptively Cyclic Feature Information-Interaction Model for Object Detection; research of Robust Video Object Tracking Algorithm Based on Jetson Nano Embedded Platform; classification-IoU Joint Label Assignment for End-to-End Object Detection; joint Learning Appearance and Motion Models for Visual Tracking; reFlowNet: Revisiting Coarse-to-fine Learning of Optical Flow; control Variates for Similarity Search.\n",
      "A2  - Ma H.\n",
      "A2  - Wang L.\n",
      "A2  - Zhang C.\n",
      "A2  - Wu F.\n",
      "A2  - Tan T.\n",
      "A2  - Wang Y.\n",
      "A2  - Lai J.\n",
      "A2  - Zhao Y.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303088012-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021; Conference date: 29 October 2021 through 1 November 2021; Conference code: 267239\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 197 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Cohen, I.\n",
      "AU  - Gal, A.\n",
      "TI  - Uncertain process data with probabilistic knowl- edge: Problem characterization and challenges\n",
      "PY  - 2021\n",
      "T2  - CEUR Workshop Proceedings\n",
      "VL  - 2938\n",
      "SP  - 51\n",
      "EP  - 56\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114661636&partnerID=40&md5=042bc008dfc33a5ac6925a7a3cfc855b\n",
      "AD  - Bar-Ilan University, Ramat-Gan, Israel\n",
      "AD  - The Technion, Israel Institute of Technology, Haifa, Israel\n",
      "AB  - Motivated by the abundance of uncertain event data from mul- tiple sources including physical devices and sensors, this paper presents the task of relating a stochastic process observation to a process model that can be rendered from a dataset. In contrast to previous research that suggested to transform a stochastically known event log into a less informative uncertain log with upper and lower bounds on activity frequencies, we consider the challenge of accommodating the probabilistic knowledge into conformance checking techniques. Based on a taxonomy that captures the spectrum of conformance checking cases under stochastic process observations, we present three types of challenging cases. The first includes conformance checking of a stochastically known log with respect to a given process model. The second case extends the first to classify a stochastically known log into one of several process models. The third case extends the two previous ones into settings in which process models are only stochastically known. The suggested problem captures the increasingly growing number of applications in which sensors provide probabilistic process information. © 2021 CEUR-WS. All rights reserved.\n",
      "KW  - Conformance checking\n",
      "KW  - Process classification\n",
      "KW  - Sensors\n",
      "KW  - Stochastically known traces\n",
      "KW  - Enterprise resource management\n",
      "KW  - Random processes\n",
      "KW  - Stochastic models\n",
      "KW  - Stochastic systems\n",
      "KW  - Conformance checking\n",
      "KW  - Physical devices\n",
      "KW  - Probabilistic knowledge\n",
      "KW  - Probabilistic process\n",
      "KW  - Problem characterization\n",
      "KW  - Process Modeling\n",
      "KW  - Uncertain process\n",
      "KW  - Upper and lower bounds\n",
      "KW  - Uncertainty analysis\n",
      "A2  - Beerepoot I.\n",
      "A2  - Di Ciccio C.\n",
      "A2  - Marrella A.\n",
      "A2  - Reijers H.A.\n",
      "A2  - Rinderle-Ma S.\n",
      "A2  - Weber B.\n",
      "PB  - CEUR-WS\n",
      "SN  - 16130073 (ISSN)\n",
      "LA  - English\n",
      "J2  - CEUR Workshop Proc.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 7; Conference name: 2021 International Workshop on BPM Problems to Solve Before We Die, PROBLEMS 2021; Conference date: 6 September 2021 through 10 September 2021; Conference code: 171521\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 198 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 11th International Conference on Image and Graphics, ICIG 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 12890 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117102611&partnerID=40&md5=cde194621a5e62d69a148fa2b7a1900b\n",
      "AB  - The proceedings contain 198 papers. The special focus in this conference is on Image and Graphics. The topics include: Moving Object Detection Based on Self-adaptive Contour Extraction; small Infrared Aerial Target Detection Using Spatial and Temporal Cues; boundary Information Aggregation and Adaptive Keypoint Combination Enhanced Object Detection; accurate Oriented Instance Segmentation in Aerial Images; six-Channel Image Representation for Cross-Domain Object Detection; skeleton-Aware Network for Aircraft Landmark Detection; efficient Spectral Pyramid and Spectral-Spatial Feature Interactive Hyperspectral Image Classification; semi-supervised Cloud Edge Collaborative Power Transmission Line Insulator Anomaly Detection Framework; Illumination-Enhanced Crowd Counting Based on IC-Net in Low Lighting Conditions; HQ-Trans: A High-Quality Screening Based Image Translation Framework for Unsupervised Cross-Domain Pedestrian Detection; An Open-Source Library of 2D-GMM-HMM Based on Kaldi Toolkit and Its Application to Handwritten Chinese Character Recognition; automatic Leaf Diseases Detection System Based on Multi-stage Recognition; aerial Image Object Detection Based on Superpixel-Related Patch; AROA: Attention Refinement One-Stage Anchor-Free Detector for Objects in Remote Sensing Imagery; human-Object Interaction Detection Based on Multi-scale Attention Fusion; progressive Fusion Network for Safety Protection Detection; A Densely Connected Neural Network Based on SSD for Multiscale SAR Ship Detection; multi-level Features Selection Network Based on Multi-attention for Salient Object Detection; learning Disentangled Representation for Fine-Grained Visual Categorization; timeception Single Shot Action Detector: A Single-Stage Method for Temporal Action Detection; FER-YOLO: Detection and Classification Based on Facial Expressions; LLNet: A Lightweight Lane Line Detection Network; two-Stage Polishing Network for Camouflaged Object Detection; towards More Powerful Multi-column Convolutional Network for Crowd Counting; scene Text Transfer for Cross-Language.\n",
      "A2  - Peng Y.\n",
      "A2  - Hu S.-M.\n",
      "A2  - Gabbouj M.\n",
      "A2  - Zhou K.\n",
      "A2  - Elad M.\n",
      "A2  - Xu K.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303087360-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 11th International Conference on Image and Graphics, ICIG 2021; Conference date: 6 August 2021 through 8 August 2021; Conference code: 266359\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 199 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13019 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118172497&partnerID=40&md5=6a504df7d4990721c12094d2b024216b\n",
      "AB  - The proceedings contain 201 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Relation-Guided Actor Attention for Group Activity Recognition; MVAD-Net: Learning View-Aware and Domain-Invariant Representation for Baggage Re-identification; joint Attention Mechanism for Unsupervised Video Object Segmentation; foreground Feature Selection and Alignment for Adaptive Object Detection; exploring Category-Shared and Category-Specific Features for Fine-Grained Image Classification; deep Mixture of Adversarial Autoencoders Clustering Network; SA-InterNet: Scale-Aware Interaction Network for Joint Crowd Counting and Localization; conditioners for Adaptive Regression Tracking; attention Template Update Model for Siamese Tracker; 3D Multi-object Detection and Tracking with Sparse Stationary LiDAR; insight on Attention Modules for Skeleton-Based Action Recognition; AO-AutoTrack: Anti-occlusion Real-Time UAV Tracking Based on Spatio-temporal Context; two-Stage Recognition Algorithm for Untrimmed Converter Steelmaking Flame Video; scale-Aware Multi-branch Decoder for Salient Object Detection; densely End Face Detection Network for Counting Bundled Steel Bars Based on YoloV5; POT: A Dataset of Panoramic Object Tracking; DP-YOLOv5: Computer Vision-Based Risk Behavior Detection in Power Grids; distillation-Based Multi-exit Fully Convolutional Network for Visual Tracking; handwriting Trajectory Reconstruction Using Spatial-Temporal Encoder-Decoder Network; scene Semantic Guidance for Object Detection; CRNet: Centroid Radiation Network for Temporal Action Localization; training Person Re-identification Networks with Transferred Images; ACFIM: Adaptively Cyclic Feature Information-Interaction Model for Object Detection; research of Robust Video Object Tracking Algorithm Based on Jetson Nano Embedded Platform; classification-IoU Joint Label Assignment for End-to-End Object Detection; joint Learning Appearance and Motion Models for Visual Tracking; reFlowNet: Revisiting Coarse-to-fine Learning of Optical Flow; control Variates for Similarity Search.\n",
      "A2  - Ma H.\n",
      "A2  - Wang L.\n",
      "A2  - Zhang C.\n",
      "A2  - Wu F.\n",
      "A2  - Tan T.\n",
      "A2  - Wang Y.\n",
      "A2  - Lai J.\n",
      "A2  - Zhao Y.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303088003-3 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021; Conference date: 29 October 2021 through 1 November 2021; Conference code: 267239\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 200 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 11th International Conference on Image and Graphics, ICIG 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 12888 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116918474&partnerID=40&md5=ffe355daf0f26654108c7f33fc62b43c\n",
      "AB  - The proceedings contain 198 papers. The special focus in this conference is on Image and Graphics. The topics include: Moving Object Detection Based on Self-adaptive Contour Extraction; small Infrared Aerial Target Detection Using Spatial and Temporal Cues; boundary Information Aggregation and Adaptive Keypoint Combination Enhanced Object Detection; accurate Oriented Instance Segmentation in Aerial Images; six-Channel Image Representation for Cross-Domain Object Detection; skeleton-Aware Network for Aircraft Landmark Detection; efficient Spectral Pyramid and Spectral-Spatial Feature Interactive Hyperspectral Image Classification; semi-supervised Cloud Edge Collaborative Power Transmission Line Insulator Anomaly Detection Framework; Illumination-Enhanced Crowd Counting Based on IC-Net in Low Lighting Conditions; HQ-Trans: A High-Quality Screening Based Image Translation Framework for Unsupervised Cross-Domain Pedestrian Detection; An Open-Source Library of 2D-GMM-HMM Based on Kaldi Toolkit and Its Application to Handwritten Chinese Character Recognition; automatic Leaf Diseases Detection System Based on Multi-stage Recognition; aerial Image Object Detection Based on Superpixel-Related Patch; AROA: Attention Refinement One-Stage Anchor-Free Detector for Objects in Remote Sensing Imagery; human-Object Interaction Detection Based on Multi-scale Attention Fusion; progressive Fusion Network for Safety Protection Detection; A Densely Connected Neural Network Based on SSD for Multiscale SAR Ship Detection; multi-level Features Selection Network Based on Multi-attention for Salient Object Detection; learning Disentangled Representation for Fine-Grained Visual Categorization; timeception Single Shot Action Detector: A Single-Stage Method for Temporal Action Detection; FER-YOLO: Detection and Classification Based on Facial Expressions; LLNet: A Lightweight Lane Line Detection Network; two-Stage Polishing Network for Camouflaged Object Detection; towards More Powerful Multi-column Convolutional Network for Crowd Counting; scene Text Transfer for Cross-Language.\n",
      "A2  - Peng Y.\n",
      "A2  - Hu S.-M.\n",
      "A2  - Gabbouj M.\n",
      "A2  - Zhou K.\n",
      "A2  - Elad M.\n",
      "A2  - Xu K.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303087354-7 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 11th International Conference on Image and Graphics, ICIG 2021; Conference date: 6 August 2021 through 8 August 2021; Conference code: 266359\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 201 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 13021 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118225260&partnerID=40&md5=f27776b65d15eafdd1f5657792a53d73\n",
      "AB  - The proceedings contain 201 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Relation-Guided Actor Attention for Group Activity Recognition; MVAD-Net: Learning View-Aware and Domain-Invariant Representation for Baggage Re-identification; joint Attention Mechanism for Unsupervised Video Object Segmentation; foreground Feature Selection and Alignment for Adaptive Object Detection; exploring Category-Shared and Category-Specific Features for Fine-Grained Image Classification; deep Mixture of Adversarial Autoencoders Clustering Network; SA-InterNet: Scale-Aware Interaction Network for Joint Crowd Counting and Localization; conditioners for Adaptive Regression Tracking; attention Template Update Model for Siamese Tracker; 3D Multi-object Detection and Tracking with Sparse Stationary LiDAR; insight on Attention Modules for Skeleton-Based Action Recognition; AO-AutoTrack: Anti-occlusion Real-Time UAV Tracking Based on Spatio-temporal Context; two-Stage Recognition Algorithm for Untrimmed Converter Steelmaking Flame Video; scale-Aware Multi-branch Decoder for Salient Object Detection; densely End Face Detection Network for Counting Bundled Steel Bars Based on YoloV5; POT: A Dataset of Panoramic Object Tracking; DP-YOLOv5: Computer Vision-Based Risk Behavior Detection in Power Grids; distillation-Based Multi-exit Fully Convolutional Network for Visual Tracking; handwriting Trajectory Reconstruction Using Spatial-Temporal Encoder-Decoder Network; scene Semantic Guidance for Object Detection; CRNet: Centroid Radiation Network for Temporal Action Localization; training Person Re-identification Networks with Transferred Images; ACFIM: Adaptively Cyclic Feature Information-Interaction Model for Object Detection; research of Robust Video Object Tracking Algorithm Based on Jetson Nano Embedded Platform; classification-IoU Joint Label Assignment for End-to-End Object Detection; joint Learning Appearance and Motion Models for Visual Tracking; reFlowNet: Revisiting Coarse-to-fine Learning of Optical Flow; control Variates for Similarity Search.\n",
      "A2  - Ma H.\n",
      "A2  - Wang L.\n",
      "A2  - Zhang C.\n",
      "A2  - Wu F.\n",
      "A2  - Tan T.\n",
      "A2  - Wang Y.\n",
      "A2  - Lai J.\n",
      "A2  - Zhao Y.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303088009-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021; Conference date: 29 October 2021 through 1 November 2021; Conference code: 267239\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 202 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 15th Asian Conference on Computer Vision, ACCV 2020\n",
      "PY  - 2021\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 12628 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104484389&partnerID=40&md5=fc604bf4ddbf8e0f599582d24da4d51e\n",
      "AB  - The proceedings contain 13 papers. The special focus in this conference is on Computer Vision. The topics include: Iterative Self-distillation for Precise Facial Landmark Localization; multiview Similarity Learning for Robust Visual Clustering; real-Time Spatio-Temporal Action Localization via Learning Motion Representation; parallel-Connected Residual Channel Attention Network for Remote Sensing Image Super-Resolution; unsupervised Multispectral and Hyperspectral Image Fusion with Deep Spatial and Spectral Priors; G-GCSN: Global Graph Convolution Shrinkage Network for Emotion Perception from Gait; Cell Detection and Segmentation in Microscopy Images with Improved Mask R-CNN; BdSL36: A Dataset for Bangladeshi Sign Letters Recognition; 3D Semantic Segmentation for Large-Scale Scene Understanding; a Weakly Supervised Convolutional Network for Change Segmentation and Classification; visible and Thermal Camera-Based Jaywalking Estimation Using a Hierarchical Deep Learning Framework.\n",
      "A2  - Sato I.\n",
      "A2  - Han B.\n",
      "PB  - Springer Science and Business Media Deutschland GmbH\n",
      "SN  - 03029743 (ISSN); 978-303069755-6 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 15th Asian Conference on Computer Vision, ACCV 2020; Conference date: 30 November 2020 through 4 December 2020; Conference code: 255909\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 203 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Gao, J.\n",
      "AU  - Shi, Z.\n",
      "AU  - Wang, G.\n",
      "AU  - Li, J.\n",
      "AU  - Yuan, Y.\n",
      "AU  - Ge, S.\n",
      "AU  - Zhou, X.\n",
      "TI  - Accurate temporal action proposal generation with relation-aware pyramid network\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 10810\n",
      "EP  - 10817\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106413120&partnerID=40&md5=e54b073e5b98ef51406bbb28ccf7a8c9\n",
      "AD  - Cooperative Medianet Innovation Center, Shanghai Jiao Tong University\n",
      "AD  - CloudWalk Technology Co., Ltd, China\n",
      "AD  - Institute of Information Engineering, Chinese Academy of Sciences\n",
      "AB  - Accurate temporal action proposals play an important role in detecting actions from untrimmed videos. The existing approaches have difficulties in capturing global contextual information and simultaneously localizing actions with different durations. To this end, we propose a Relation-aware pyramid Network (RapNet) to generate highly accurate temporal action proposals. In RapNet, a novel relation-aware module is introduced to exploit bi-directional long-range relations between local features for context distilling. This embedded module enhances the RapNet in terms of its multi-granularity temporal proposal generation ability, given predefined anchor boxes. We further introduce a two-stage adjustment scheme to refine the proposal boundaries and measure their confidence in containing an action with snippet-level actionness. Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks demonstrate our RapNet generates superior accurate proposals over the existing state-of-the-art methods. © AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.\n",
      "KW  - Bi-directional\n",
      "KW  - Contextual information\n",
      "KW  - Embedded module\n",
      "KW  - Highly accurate\n",
      "KW  - Local feature\n",
      "KW  - Multi-granularity\n",
      "KW  - Pyramid network\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 70; Correspondence Address: S. Ge; Institute of Information Engineering, Chinese Academy of Sciences; email: geshiming@iie.ac.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 204 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, J.\n",
      "AU  - Liu, X.\n",
      "AU  - Zong, Z.\n",
      "AU  - Zhao, W.\n",
      "AU  - Zhang, M.\n",
      "AU  - Song, J.\n",
      "TI  - Graph attention based proposal 3D convnets for action detection\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 4626\n",
      "EP  - 4633\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106034200&partnerID=40&md5=10060ccebcd7d795d3471207da12cbc2\n",
      "AD  - State Key Lab of Software Development Environment, Beihang University, Beijing, China\n",
      "AD  - Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China\n",
      "AD  - Innovation Center, University of Electronic Science and Technology of China, Chengdu, China\n",
      "AB  - The recent advances in 3D Convolutional Neural Networks (3D CNNs) have shown promising performance for untrimmed video action detection, employing the popular detection framework that heavily relies on the temporal action proposal generations as the input of the action detector and localization regressor. In practice the proposals usually contain strong intra and inter relations among them, mainly stemming from the temporal and spatial variations in the video actions. However, most of existing 3D CNNs ignore the relations and thus suffer from the redundant proposals degenerating the detection performance and efficiency. To address this problem, we propose graph attention based proposal 3D ConvNets (AGCN-P-3DCNNs) for video action detection. Specifically, our proposed graph attention is composed of intra attention based GCN and inter attention based GCN. We use intra attention to learn the intra long-range dependencies inside each action proposal and update node matrix of Intra Attention based GCN, and use inter attention to learn the inter dependencies between different action proposals as adjacency matrix of Inter Attention based GCN. Afterwards, we fuse intra and inter attention to model intra long-range dependencies and inter dependencies simultaneously. Another contribution is that we propose a simple and effective framewise classifier, which enhances the feature presentation capabilities of backbone model. Experiments on two proposal 3D ConvNets based models (P-C3D and P-ResNet) and two popular action detection benchmarks (THUMOS 2014, ActivityNet v1.3) demonstrate the state-of-the-art performance achieved by our method. Particularly, P-C3D embedded with our module achieves average mAP 3.7% improvement on THUMOS 2014 dataset compared to original model. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Adjacency matrices\n",
      "KW  - Detection framework\n",
      "KW  - Detection performance\n",
      "KW  - Inter-dependencies\n",
      "KW  - Long-range dependencies\n",
      "KW  - Original model\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Temporal and spatial variation\n",
      "KW  - Convolutional neural networks\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 51; Correspondence Address: X. Liu; State Key Lab of Software Development Environment, Beihang University, Beijing, China; email: xlliu@nlsde.buaa.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 205 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Jiang, J.\n",
      "AU  - Chen, Z.\n",
      "AU  - Lin, H.\n",
      "AU  - Zhao, X.\n",
      "AU  - Gao, Y.\n",
      "TI  - Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11101\n",
      "EP  - 11108\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106445348&partnerID=40&md5=3cd435b5e3bf4fc6d86e9ae33c0ef590\n",
      "AD  - BNRist, KLISS, School of Software, Tsinghua University, China\n",
      "AB  - Understanding questions and finding clues for answers are the key for video question answering. Compared with image question answering, video question answering (Video QA) requires to find the clues accurately on both spatial and temporal dimension simultaneously, and thus is more challenging. However, the relationship between spatio-temporal information and question still has not been well utilized in most existing methods for Video QA. To tackle this problem, we propose a Question-Guided Spatio-Temporal Contextual Attention Network (QueST) method. In QueST, we divide the semantic features generated from question into two separate parts: the spatial part and the temporal part, respectively guiding the process of constructing the contextual attention on spatial and temporal dimension. Under the guidance of the corresponding contextual attention, visual features can be better exploited on both spatial and temporal dimensions. To evaluate the effectiveness of the proposed method, experiments are conducted on TGIF-QA dataset, MSRVTTQA dataset and MSVD-QA dataset. Experimental results and comparisons with the state-of-the-art methods have shown that our method can achieve superior performance.  Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Semantics\n",
      "KW  - Divide and conquer\n",
      "KW  - Question Answering\n",
      "KW  - Semantic features\n",
      "KW  - Spatio temporal\n",
      "KW  - Spatiotemporal information\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Temporal dimensions\n",
      "KW  - Visual feature\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 104; Correspondence Address: X. Zhao; BNRist, KLISS, School of Software, Tsinghua University, China; email: zxb@tsinghua.edu.cn; Y. Gao; BNRist, KLISS, School of Software, Tsinghua University, China; email: gaoyue@tsinghua.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 206 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, S.\n",
      "AU  - Peng, H.\n",
      "AU  - Fu, J.\n",
      "AU  - Luo, J.\n",
      "TI  - Learning 2D temporal adjacent networks for moment localization with natural language\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 12870\n",
      "EP  - 12877\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106651792&partnerID=40&md5=d5f0c6ad02a0a88151266a295260cc45\n",
      "AD  - University of Rochester, United States\n",
      "AD  - Microsoft Research\n",
      "AB  - We address the problem of retrieving a specific moment from an untrimmed video by a query sentence. This is a challenging problem because a target moment may take place in relations to other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they consider temporal moments individually and neglect the temporal dependencies. In this paper, we model the temporal relations between video moments by a two-dimensional map, where one dimension indicates the starting time of a moment and the other indicates the end time. This 2D temporal map can cover diverse video moments with different lengths, while representing their adjacent relations. Based on the 2D map, we propose a Temporal Adjacent Network (2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal relation, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed 2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our 2D-TAN outperforms the state-of-the-art. Copyright 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Adjacent relation\n",
      "KW  - Discriminative features\n",
      "KW  - Natural languages\n",
      "KW  - Referring expressions\n",
      "KW  - State of the art\n",
      "KW  - Temporal moments\n",
      "KW  - Temporal relation\n",
      "KW  - Two-dimensional map\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 401; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 207 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, D.\n",
      "AU  - Xu, C.\n",
      "AU  - Yu, X.\n",
      "AU  - Zhang, K.\n",
      "AU  - Swift, B.\n",
      "AU  - Suominen, H.\n",
      "AU  - Li, H.\n",
      "TI  - TSPNet: Hierarchical feature learning via temporal semantic pyramid for sign language translation\n",
      "PY  - 2020\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2020-December\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108237089&partnerID=40&md5=1cbea06e9259c6bab9e52b50d2134b8d\n",
      "AD  - The Australian National University (ANU), Australia\n",
      "AD  - Australian Centre for Robotic Vision (ACRV), Australia\n",
      "AD  - Data61, CSIRO\n",
      "AD  - University of Technology Sydney (UTS), Australia\n",
      "AD  - Tencent AI Lab.\n",
      "AD  - University of Turku, Finland\n",
      "AB  - Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of sign videos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet. © 2020 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Computer aided language translation\n",
      "KW  - Image segmentation\n",
      "KW  - Machine learning\n",
      "KW  - Continuous sequences\n",
      "KW  - Discriminative features\n",
      "KW  - Hierarchical features\n",
      "KW  - Semantic ambiguities\n",
      "KW  - Semantic consistency\n",
      "KW  - Semantic structures\n",
      "KW  - Temporal granularity\n",
      "KW  - Temporal information\n",
      "KW  - Semantics\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 89; Correspondence Address: D. Li; The Australian National University (ANU), Australia; email: Dongxu.Li@anu.edu.au; ; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 208 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Guo, Y.\n",
      "AU  - Chen, Y.\n",
      "AU  - Zheng, Y.\n",
      "AU  - Zhao, P.\n",
      "AU  - Chen, J.\n",
      "AU  - Huang, J.\n",
      "AU  - Tan, M.\n",
      "TI  - Breaking the curse of space explosion: Towards effcient NAS with curriculum search\n",
      "PY  - 2020\n",
      "T2  - 37th International Conference on Machine Learning, ICML 2020\n",
      "VL  - PartF168147-5\n",
      "SP  - 3780\n",
      "EP  - 3789\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105243162&partnerID=40&md5=e3e10e93ba95a65399340ece7b697e55\n",
      "AD  - School of Software Engineering, South China University of Technology, China\n",
      "AD  - Pazhou Laboratory, China\n",
      "AD  - Weixin Group, Tencent, China\n",
      "AD  - Tencent AI Lab, Tencent, China\n",
      "AD  - Guangdong Key Laboratory of Big Data Analysis and Processing, China\n",
      "AB  - Neural architecture search (NAS) has become an important approach to automatically fnd effective architectures. To cover all possible good architectures, we need to search in an extremely large search space with billions of candidate architectures. More critically, given a large search space, we may face a very challenging issue of space explosion. However, due to the limitation of computational resources, we can only sample a very small proportion of the architectures, which provides insuffcient information for the training. As a result, existing methods may often produce suboptimal architectures. To alleviate this issue, we propose a curriculum search method that starts from a small search space and gradually incorporates the learned knowledge to guide the search in a large space. With the proposed search strategy, our Curriculum Neural Architecture Search (CNAS) method signifcantly improves the search effciency and fnds better architectures than existing NAS methods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method. Copyright 2020 by the author(s).\n",
      "KW  - Curricula\n",
      "KW  - Machine learning\n",
      "KW  - Computational resources\n",
      "KW  - Large spaces\n",
      "KW  - Neural architectures\n",
      "KW  - Search method\n",
      "KW  - Search spaces\n",
      "KW  - Search strategies\n",
      "KW  - Space explosion\n",
      "KW  - Architecture\n",
      "A2  - Daume H.\n",
      "A2  - Singh A.\n",
      "PB  - International Machine Learning Society (IMLS)\n",
      "SN  - 978-171382112-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - Int. Conf. Machin. Learn., ICML\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 15; Correspondence Address: J. Chen; School of Software Engineering, South China University of Technology, China; email: ellachen@scut.edu.cn; M. Tan; School of Software Engineering, South China University of Technology, China; email: mingkuitan@scut.edu.cn; Conference name: 37th International Conference on Machine Learning, ICML 2020; Conference date: 13 July 2020 through 18 July 2020; Conference code: 168147\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 209 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, D.\n",
      "AU  - Chen, P.\n",
      "AU  - Zeng, R.\n",
      "AU  - Du, Q.\n",
      "AU  - Tan, M.\n",
      "AU  - Gan, C.\n",
      "TI  - Location-aware graph convolutional networks for video question answering\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11021\n",
      "EP  - 11028\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106416027&partnerID=40&md5=d86fe33284b3f87d62ab0dabc35d1154\n",
      "AD  - South China University of Technology\n",
      "AD  - Peng Cheng Laboratory, Shenzhen, China\n",
      "AD  - MIT-IBM Watson AI Lab\n",
      "AB  - We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning. In this work, we propose to represent the contents in the video as a locationaware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action. As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering. Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets.  Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Convolution\n",
      "KW  - Convolutional neural networks\n",
      "KW  - Natural language processing systems\n",
      "KW  - Action recognition\n",
      "KW  - Attention mechanisms\n",
      "KW  - Convolutional networks\n",
      "KW  - Graph construction\n",
      "KW  - Location information\n",
      "KW  - Object interactions\n",
      "KW  - Question Answering\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Location\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 162; Correspondence Address: M. Tan; South China University of Technology; email: mingkuitan@scut.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 210 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, D.\n",
      "AU  - Qu, X.\n",
      "AU  - Dong, J.\n",
      "AU  - Zhou, P.\n",
      "TI  - Reasoning Step-by-Step: Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network\n",
      "PY  - 2020\n",
      "T2  - COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference\n",
      "SP  - 1841\n",
      "EP  - 1851\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104178044&partnerID=40&md5=cb54da0f75354122104304793f731440\n",
      "AD  - Huazhong University of Science and Technology, China\n",
      "AD  - Huawei Cloud\n",
      "AD  - Zhejiang Gongshang University, China\n",
      "AB  - Temporal sentence localization in videos aims to ground the best matched segment in an untrimmed video according to a given sentence query. Previous works in this field mainly rely on single-step attentional frameworks to align the temporal boundaries by a soft selection. Although they focus on the visual content relevant to the query, these attention strategies are insufficient to model complex video contents and restrict the higher-level reasoning demand for temporal relation. In this paper, we propose a novel deep rectification-modulation network (RMN), transforming this task into a multi-step reasoning process by repeating rectification and modulation. In each rectification-modulation layer, unlike existing methods directly conducting the cross-modal interaction, we first devise a rectification module to correct implicit attention misalignment which focuses on wrong position during the interaction process. Then, a modulation module is developed to model the frame-to-frame relation with the help of specific sentence information for better correlating and composing the video contents over time. With multiple such layers cascaded in depth, our RMN progressively refines video and query interactions, thus enabling a further precise localization. Experimental evaluations on three public datasets show that the proposed method achieves state-of-the-art performance. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.\n",
      "KW  - Computational linguistics\n",
      "KW  - Cross-modal interaction\n",
      "KW  - High-level reasoning\n",
      "KW  - Localisation\n",
      "KW  - Model complexes\n",
      "KW  - Multisteps\n",
      "KW  - Reasoning process\n",
      "KW  - Single-step\n",
      "KW  - Temporal relation\n",
      "KW  - Video contents\n",
      "KW  - Visual content\n",
      "KW  - Video recording\n",
      "A2  - Scott D.\n",
      "A2  - Bel N.\n",
      "A2  - Zong C.\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 978-195214827-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - COLING - Int. Conf. Comput. Linguist., Proc. Conf.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 27; Correspondence Address: P. Zhou; Huazhong University of Science and Technology, China; email: panzhou@hust.edu.cn; Conference name: 28th International Conference on Computational Linguistics, COLING 2020; Conference date: 8 December 2020 through 13 December 2020; Conference code: 186886\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 211 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Rahman, M.A.\n",
      "AU  - Laganière, R.\n",
      "TI  - Mid-level Fusion for End-to-End Temporal Activity Detection in Untrimmed Video\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145233418&partnerID=40&md5=6b073223de2951c8cf0591652d53c233\n",
      "AD  - School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada\n",
      "AB  - In this paper, we address the problem of human activity detection in temporally untrimmed long video sequences, where the goal is to classify and temporally localize each activity instance in the input video. Inspired by the recent success of the single-stage object detection methods, we propose an end-to-end trainable framework capable of learning task-specific spatio-temporal features of a video sequence for direct classification and localization of the activities. We, further, systematically investigate how and where to fuse multi-stream feature representations of a video and propose a new fusion strategy for temporal activity detection. Together with the proposed fusion strategy, the novel architecture sets new state-of-the-art on the highly challenging THUMOS'14 benchmark - up from 44.2% to 53.9% mAP (an absolute 9.7% improvement). © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Classification (of information)\n",
      "KW  - Computer vision\n",
      "KW  - Feature extraction\n",
      "KW  - Object detection\n",
      "KW  - Activity detection\n",
      "KW  - Activity instances\n",
      "KW  - End to end\n",
      "KW  - Fusion strategies\n",
      "KW  - Human-activity detection\n",
      "KW  - Input videos\n",
      "KW  - Level fusion\n",
      "KW  - Object detection method\n",
      "KW  - Single stage\n",
      "KW  - Video sequences\n",
      "KW  - Video recording\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 6; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 212 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Fried, D.\n",
      "AU  - Alayrac, J.-B.\n",
      "AU  - Blunsom, P.\n",
      "AU  - Dyer, C.\n",
      "AU  - Clark, S.\n",
      "AU  - Nematzadeh, A.\n",
      "TI  - Learning to segment actions from observation and narration\n",
      "PY  - 2020\n",
      "T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics\n",
      "SP  - 2569\n",
      "EP  - 2588\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117954437&partnerID=40&md5=3391badbc1e3decc6572e5fbb41d4a72\n",
      "AD  - DeepMind, London, United Kingdom\n",
      "AD  - Computer Science Division, UC Berkeley\n",
      "AB  - We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality. © 2020 Association for Computational Linguistics\n",
      "KW  - Action segmentation\n",
      "KW  - Instructional videos\n",
      "KW  - Segmentation quality\n",
      "KW  - Task structure\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 0736587X (ISSN); 978-195214825-5 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc. Annu. Meet. Assoc. Comput Linguist.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 17; Conference name: 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020; Conference date: 5 July 2020 through 10 July 2020; Conference code: 172533\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 213 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Alwassel, H.\n",
      "AU  - Mahajan, D.\n",
      "AU  - Korbar, B.\n",
      "AU  - Torresani, L.\n",
      "AU  - Ghanem, B.\n",
      "AU  - Tran, D.\n",
      "TI  - Self-supervised learning by cross-modal audio-video clustering\n",
      "PY  - 2020\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2020-December\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108435654&partnerID=40&md5=3f440a8cd74547482a34efac8ffe1115\n",
      "AD  - King Abdullah University of Science and Technology (KAUST), Saudi Arabia\n",
      "AD  - Facebook AI\n",
      "AB  - Visual and audio modalities are highly correlated, yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose Cross-Modal Deep Clustering (XDC), a novel self-supervised method that leverages unsupervised clustering in one modality (e.g., audio) as a supervisory signal for the other modality (e.g., video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data significantly outperforms the same model pretrained with full-supervision on ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture. © 2020 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Semantics\n",
      "KW  - Supervised learning\n",
      "KW  - Action recognition\n",
      "KW  - Audio representation\n",
      "KW  - Highly-correlated\n",
      "KW  - Intrinsic differences\n",
      "KW  - Strong correlation\n",
      "KW  - Supervised learning methods\n",
      "KW  - Supervised methods\n",
      "KW  - Unsupervised clustering\n",
      "KW  - Learning systems\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 241; Correspondence Address: H. Alwassel; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; email: humam.alwassel@kaust.edu.sa; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 214 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, Q.\n",
      "AU  - Wang, Z.\n",
      "TI  - Progressive boundary refinement network for temporal action detection\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11612\n",
      "EP  - 11619\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106429474&partnerID=40&md5=e4948fad6b367d278a2bb841e74d94dc\n",
      "AD  - Department of Automation, University of Science and Technology of China\n",
      "AB  - Temporal action detection is a challenging task due to vagueness of action boundaries. To tackle this issue, we propose an end-to-end progressive boundary refinement network (PBRNet) in this paper. PBRNet belongs to the family of one-stage detectors and is equipped with three cascaded detection modules for localizing action boundary more and more precisely. Specifically, PBRNet mainly consists of coarse pyramidal detection, refined pyramidal detection, and fine-grained detection. The first two modules build two feature pyramids to perform the anchor-based detection, and the third one explores the frame-level features to refine the boundaries of each action instance. In the fined-grained detection module, three frame-level classification branches are proposed to augment the frame-level features and update the confidence scores of action instances. Evidently, PBRNet integrates the anchorbased and frame-level methods. We experimentally evaluate the proposed PBRNet and comprehensively investigate the effect of the main components. The results show PBRNet achieves the state-of-the-art detection performances on two popular benchmarks: THUMOS'14 and ActivityNet, and meanwhile possesses a high inference speed. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Boundary refinements\n",
      "KW  - Confidence score\n",
      "KW  - Detection modules\n",
      "KW  - Detection performance\n",
      "KW  - Feature pyramid\n",
      "KW  - Fine grained\n",
      "KW  - Level method\n",
      "KW  - State of the art\n",
      "KW  - Feature extraction\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 117; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 215 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Iashin, V.\n",
      "AU  - Rahtu, E.\n",
      "TI  - A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139642202&partnerID=40&md5=83aa6083bd029a5632568ac2f7d19edd\n",
      "AD  - Computing Sciences, Tampere University, Tampere, Finland\n",
      "AB  - Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track. Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt. © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Audio track\n",
      "KW  - Audio-visual\n",
      "KW  - Feature extractor\n",
      "KW  - Performance\n",
      "KW  - Simple++\n",
      "KW  - Visual cues\n",
      "KW  - Visual feature\n",
      "KW  - Visual modalities\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 64; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 216 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, L.\n",
      "AU  - Huang, Y.\n",
      "AU  - Ouyang, W.\n",
      "AU  - Wang, L.\n",
      "TI  - Relational prototypical network for weakly supervised temporal action localization\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11053\n",
      "EP  - 11060\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106430850&partnerID=40&md5=494e7777beed9e48159c52201d2de1fe\n",
      "AD  - Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR)\n",
      "AD  - Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA)\n",
      "AD  - University of Chinese Academy of Sciences (UCAS)\n",
      "AD  - University of Sydney, SenseTime Computer Vision Research Group, Australia\n",
      "AB  - In this paper, we propose a weakly supervised temporal action localization method on untrimmed videos based on prototypical networks. We observe two challenges posed by weakly supervision, namely action-background separation and action relation construction. Unlike the previous method, we propose to achieve action-background separation only by the original videos. To achieve this, a clustering loss is adopted to separate actions from backgrounds and learn intra-compact features, which helps in detecting complete action instances. Besides, a similarity weighting module is devised to further separate actions from backgrounds. To effectively identify actions, we propose to construct relations among actions for prototype learning. A GCN-based prototype embedding module is introduced to generate relational prototypes. Experiments on THUMOS14 and ActivityNet1.2 datasets show that our method outperforms the state-of-the-art methods.  Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Separation\n",
      "KW  - Background separation\n",
      "KW  - Compact Features\n",
      "KW  - Localization method\n",
      "KW  - Prototype learning\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 68; Correspondence Address: L. Wang; Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR); email: wangliang@nlpr.ia.ac.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 217 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Reddy, G.P.\n",
      "AU  - Kalaiselvi Geetha, M.\n",
      "TI  - Review of improving healthcare services through human activity recognition\n",
      "PY  - 2020\n",
      "T2  - International Journal of Scientific and Technology Research\n",
      "VL  - 9\n",
      "IS  - 4\n",
      "SP  - 2022\n",
      "EP  - 2024\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083837964&partnerID=40&md5=7d61c3689d451b9a1e5f829c11e156b1\n",
      "AB  - Healthcare industry is the one which renders valuable services to humans. With technological innovations, the quality of healthcare services is increased substantially. With the advancements in image processing and deep learning techniques, new avenues in healthcare domain are being witnessed. Human actions with respect to healthcare services exhibit different viewpoints that can help in understanding the status of health of patients under observation and elders under constant care. Traditional approaches for monitoring human actions are prone to errors and time consuming which lead to deteriorated health conditions and delay in seeking healthcare services. With an automated process driven by technology, humans who are in need can be monitored and observed remotely. This sort of research comes under Human Action Recognition (HAR). It is essential to capture spatio temporal features in order to capture live information pertaining to people under observation. Machine learning with its advancements like deep learning is part of Artificial Intelligence (AI) which provides actionable knowledge by processing human actions in real time. This approach has plenty of advantages including saving time, effort, life and money. Due to its indispensable need, in this paper, we study the state of the art and provide useful insights and research gaps if any in order to improve the research further in the area of HAR. © IJSTR 2020.\n",
      "KW  - AI\n",
      "KW  - Healthcare\n",
      "KW  - Human activity recognition\n",
      "KW  - Image processing\n",
      "KW  - Machine learning\n",
      "PB  - International Journal of Scientific and Technology Research\n",
      "SN  - 22778616 (ISSN)\n",
      "LA  - English\n",
      "J2  - Int. J. Sci. Technol. Res.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 218 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chen, Y.\n",
      "AU  - Chen, M.\n",
      "AU  - Wu, R.\n",
      "AU  - Zhu, J.\n",
      "AU  - Zhu, Z.\n",
      "AU  - Gu, Q.\n",
      "TI  - Refinement of Boundary Regression Using Uncertainty in Temporal Action Localization\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112053330&partnerID=40&md5=e6cd4619908b27a0af0b3182e158a3fe\n",
      "AD  - Center of Precision Sensing and Control, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n",
      "AD  - School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n",
      "AD  - Horizon Robotics, Beijing, China\n",
      "AB  - Boundary localization is a key component of most temporal action localization frameworks for untrimmed video. Deep-learning methods have brought remarkable progress in this field due to large-scale annotated datasets (e.g., THUMOS14 and ActivityNet). However, natural ambiguity exists for labeling an accurate action boundaries with such datasets. In this paper, we propose a method to model this uncertainty. Specifically, we construct a Gaussian model for predicting the uncertainty variance of the boundary. The captured variance is further used to select more reliable proposals and to refine proposal boundaries by variance voting during post-processing. For most existing one- and two-stage frameworks, more accurate boundaries and reliable proposals can be obtained without additional computation. For the one-stage decoupled single-shot temporal action detection (Decouple-SSAD) [11] framework, we first apply the adaptive pyramid feature fusion method to fuse its features of different scales and optimize its structure. Then, we introduce the uncertainty based method and improve state-of-the-art mAP@0.5 value from 37.9% to 41.6% on THUMOS14. Moreover, for the two-stage proposalproposal interaction through a graph convolutional network (P-GCN) [33], with such uncertainty method, we also gain significant improvements on both THUMOS14 and ActivityNet v1.3 datasets. Code and more details will be available at https://github.com/shadowclouds/Uty. © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Deep learning\n",
      "KW  - Learning systems\n",
      "KW  - Adaptive pyramid\n",
      "KW  - Annotated datasets\n",
      "KW  - Gaussian modeling\n",
      "KW  - Labelings\n",
      "KW  - Large-scales\n",
      "KW  - Learning methods\n",
      "KW  - Localisation\n",
      "KW  - Post-processing\n",
      "KW  - Single-shot\n",
      "KW  - Uncertainty\n",
      "KW  - Large dataset\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 6; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 219 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, Y.-H.\n",
      "AU  - Hsu, K.-J.\n",
      "AU  - Jeng, S.-K.\n",
      "AU  - Lin, Y.-Y.\n",
      "TI  - Weakly-supervised video re-localization with multiscale attention model\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11077\n",
      "EP  - 11084\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106420780&partnerID=40&md5=426267e089432e46134002f9c667b948\n",
      "AD  - Academia Sinica\n",
      "AD  - National Taiwan University\n",
      "AD  - Qualcomm\n",
      "AD  - National Chiao Tung University\n",
      "AB  - Video re-localization aims to localize a sub-sequence, called target segment, in an untrimmed reference video that is similar to a given query video. In this work, we propose an attention-based model to accomplish this task in a weakly supervised setting. Namely, we derive our CNN-based model without using the annotated locations of the target segments in reference videos. Our model contains three modules. First, it employs a pre-trained C3D network for feature extraction. Second, we design an attention mechanism to extract multiscale temporal features, which are then used to estimate the similarity between the query video and a reference video.Third, a localization layer detects where the target segment is in the reference video by determining whether each frame in the reference video is consistent with the query video. The resultant CNN model is derived based on the proposed coattention loss which discriminatively separates the target segment from the reference video. This loss maximizes the similarity between the query video and the target segment while minimizing the similarity between the target segment and the rest of the reference video. Our model can be modified to fully supervised re-localization. Our method is evaluated on a public dataset and achieves the state-of-the-art performance under both weakly supervised and fully supervised settings.  Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Feature extraction\n",
      "KW  - Attention mechanisms\n",
      "KW  - Attention model\n",
      "KW  - CNN models\n",
      "KW  - Public dataset\n",
      "KW  - Query video\n",
      "KW  - Re-localization\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Temporal features\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 10; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 220 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Agarwal, N.\n",
      "AU  - Chen, Y.-T.\n",
      "AU  - Dariush, B.\n",
      "AU  - Yang, M.-H.\n",
      "TI  - Unsupervised Domain Adaptation for Spatio-Temporal Action Localization\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118714468&partnerID=40&md5=fbd82ac88680cc927fd8a908a0106e02\n",
      "AD  - University of California, Merced, United States\n",
      "AD  - Honda Research Institute, United States\n",
      "AD  - Google Research, United States\n",
      "AB  - Spatio-temporal action localization is an important problem in computer vision that involves detecting where and when activities occur, and therefore requires modeling of both spatial and temporal features. This problem is typically formulated in the context of supervised learning, where the learned classifiers operate on the premise that both training and test data are sampled from the same underlying distribution. However, this assumption does not hold when there is a significant domain shift, leading to poor generalization performance on the test data. To address this, we focus on the hard and novel task of generalizing training models to test samples without access to any labels from the latter for spatio-temporal action localization by proposing an end-to-end unsupervised domain adaptation algorithm. We extend the state-of-the-art object detection framework to localize and classify actions. In order to minimize the domain shift, three domain adaptation modules at image level (temporal and spatial) and instance level (temporal) are designed and integrated. We design a new experimental setup and evaluate the proposed method and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark datasets. We show that significant performance gain can be achieved when spatial and temporal features are adapted separately, or jointly for the most effective results. © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Object detection\n",
      "KW  - Adaptation module\n",
      "KW  - Domain adaptation\n",
      "KW  - Generalization performance\n",
      "KW  - Localisation\n",
      "KW  - Spatial features\n",
      "KW  - Spatio-temporal\n",
      "KW  - Temporal features\n",
      "KW  - Test data\n",
      "KW  - Training data\n",
      "KW  - Underlying distribution\n",
      "KW  - Computer vision\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 221 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Suin, M.\n",
      "AU  - Rajagopalan, A.N.\n",
      "TI  - An efficient framework for dense video captioning\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 12039\n",
      "EP  - 12046\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103275223&partnerID=40&md5=656e8713b2643ee08b095a2b222c54ff\n",
      "AD  - Indian Institute of Technology Madras\n",
      "AB  - Dense video captioning is an extremely challenging task since an accurate and faithful description of events in a video requires a holistic knowledge of the video contents as well as contextual reasoning of individual events. Most existing approaches handle this problem by first proposing event boundaries from a video and then captioning on a subset of the proposals. Generation of dense temporal annotations and corresponding captions from long videos can be dramatically source consuming. In this paper, we focus on the task of generating a dense description of temporally untrimmed videos and aim to significantly reduce the computational cost by processing fewer frames while maintaining accuracy. Existing video captioning methods sample frames with a predefined frequency over the entire video or use all the frames. Instead, we propose a deep reinforcement-based approach which enables an agent to describe multiple events in a video by watching a portion of the frames. The agent needs to watch more frames when it is processing an informative part of the video, and skip frames when there is redundancy. The agent is trained using actor-critic algorithm, where the actor determines the frames to be watched from a video and the critic assesses the optimality of the decisions taken by the actor. Such an efficient frame selection simplifies the event proposal task considerably. This has the added effect of reducing the occurrence of unwanted proposals. The encoded state representation of the frame selection agent is further utilized for guiding event proposal and caption generation tasks. We also leverage the idea of knowledge distillation to improve the accuracy. We conduct extensive evaluations on ActivityNet captions dataset to validate our method. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Distillation\n",
      "KW  - Knowledge management\n",
      "KW  - Actor-critic algorithm\n",
      "KW  - Computational costs\n",
      "KW  - Contextual reasoning\n",
      "KW  - Event boundary\n",
      "KW  - Frame selection\n",
      "KW  - Multiple events\n",
      "KW  - State representation\n",
      "KW  - Video contents\n",
      "KW  - Video signal processing\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 37; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 222 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Kalogeiton, V.\n",
      "AU  - Zisserman, A.\n",
      "TI  - Constrained Video Face Clustering using 1NN Relations\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173980865&partnerID=40&md5=8ed286817e21bf5e8e143f30389782e1\n",
      "AD  - Visual Geometry Group, University of Oxford, United Kingdom\n",
      "AB  - In this work, we introduce the Constrained first nearest neighbour Clustering (C1C) method for video face clustering. Using the premise that the first nearest neighbour (1NN) of an instance is sufficient to discover large chains and groupings, C1C builds upon the hierarchical clustering method FINCH by imposing must-link and cannot-link constraints acquired in a self-supervised manner. We show that adding these constraints leads to performance improvements with low computational cost. C1C is easily scalable and does not require any training. Additionally, we introduce a new Friends dataset for evaluating the performance of face clustering algorithms. Given that most video datasets for face clustering are saturated or emphasize only the main characters, the Friends dataset is larger, contains identities for several main and secondary characters, and tackles more challenging cases as it labels also the 'back of the head'. We evaluate C1C on the Big Bang Theory, Buffy, and Sherlock datasets for video face clustering, and show that it achieves the new state of the art whilst setting the baseline on Friends. © 2020. The copyright of this document resides with its authors.\n",
      "KW  - Cluster analysis\n",
      "KW  - Computation theory\n",
      "KW  - Computer vision\n",
      "KW  - Big-Bang theory\n",
      "KW  - Cannot links\n",
      "KW  - Computational costs\n",
      "KW  - Face clustering\n",
      "KW  - Hierarchical clustering methods\n",
      "KW  - Must links\n",
      "KW  - Nearest neighbor clustering\n",
      "KW  - Nearest-neighbour\n",
      "KW  - Performance\n",
      "KW  - Video dataset\n",
      "KW  - Clustering algorithms\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 10; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 223 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Liu, Z.\n",
      "AU  - Luo, D.\n",
      "AU  - Wang, Y.\n",
      "AU  - Wang, L.\n",
      "AU  - Tai, Y.\n",
      "AU  - Wang, C.\n",
      "AU  - Li, J.\n",
      "AU  - Huang, F.\n",
      "AU  - Lu, T.\n",
      "TI  - TEINet: Towards an efficient architecture for video recognition\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11669\n",
      "EP  - 11676\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106447593&partnerID=40&md5=f36eef52866a8d13051bc36a492e3fee\n",
      "AD  - State Key Lab for Novel Software Technology, Nanjing University, China\n",
      "AD  - Youtu Lab, Tencent\n",
      "AB  - Efficiency is an important issue in designing video architectures for action recognition. 3D CNNs have witnessed remarkable progress in action recognition from videos. However, compared with their 2D counterparts, 3D convolutions often introduce a large amount of parameters and cause high computational cost. To relieve this problem, we propose an efficient temporal module, termed as Temporal Enhancementand- Interaction (TEI Module), which could be plugged into the existing 2D CNNs (denoted by TEINet). The TEI module presents a different paradigm to learn temporal features by decoupling the modeling of channel correlation and temporal interaction. First, it contains a Motion Enhanced Module (MEM) which is to enhance the motion-related features while suppress irrelevant information (e.g., background). Then, it introduces a Temporal Interaction Module (TIM) which supplements the temporal contextual information in a channel-wise manner. This two-stage modeling scheme is not only able to capture temporal structure flexibly and effectively, but also efficient for model inference. We conduct extensive experiments to verify the effectiveness of TEINet on several benchmarks (e.g., Something-Something V1&V2, Kinetics, UCF101 and HMDB51). Our proposed TEINet can achieve a good recognition accuracy on these datasets but still preserve a high efficiency. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Efficiency\n",
      "KW  - Channel correlation\n",
      "KW  - Computational costs\n",
      "KW  - Contextual information\n",
      "KW  - Efficient architecture\n",
      "KW  - Interaction modules\n",
      "KW  - Recognition accuracy\n",
      "KW  - Temporal structures\n",
      "KW  - Video architecture\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 224; Correspondence Address: L. Wang; State Key Lab for Novel Software Technology, Nanjing University, China; email: lmwang@nju.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 224 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhu, L.\n",
      "AU  - Tran, D.\n",
      "AU  - Sevilla-Lara, L.\n",
      "AU  - Yang, Y.\n",
      "AU  - Feiszli, M.\n",
      "AU  - Wang, H.\n",
      "TI  - FASTER recurrent networks for efficient video classification\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 13098\n",
      "EP  - 13105\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106647004&partnerID=40&md5=0485bbdf0ad11a29457ba8e334254df4\n",
      "AD  - Facebook AI\n",
      "AD  - ReLER, University of Technology Sydney, Australia\n",
      "AD  - University of Edinburgh, United Kingdom\n",
      "AB  - Typical video classification methods often divide a video into short clips, do inference on each clip independently, then aggregate the clip-level predictions to generate the video-level results. However, processing visually similar clips independently ignores the temporal structure of the video sequence, and increases the computational cost at inference time. In this paper, we propose a novel framework named FASTER, i.e., Feature Aggregation for Spatio-TEmporal Redundancy. FASTER aims to leverage the redundancy between neighboring clips and reduce the computational cost by learning to aggregate the predictions from models of different complexities. The FASTER framework can integrate high quality representations from expensive models to capture subtle motion information and lightweight representations from cheap models to cover scene changes in the video. A new recurrent network (i.e., FAST-GRU) is designed to aggregate the mixture of different representations. Compared with existing approaches, FASTER can reduce the FLOPs by over 10× while maintaining the state-of-the-art accuracy across popular datasets, such as Kinetics, UCF-101 and HMDB-51. © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Aggregates\n",
      "KW  - Classification (of information)\n",
      "KW  - Predictive analytics\n",
      "KW  - Redundancy\n",
      "KW  - Computational costs\n",
      "KW  - Expensive models\n",
      "KW  - Feature aggregation\n",
      "KW  - Light-weight representation\n",
      "KW  - Recurrent networks\n",
      "KW  - State of the art\n",
      "KW  - Temporal structures\n",
      "KW  - Video classification\n",
      "KW  - Recurrent neural networks\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 48; Correspondence Address: L. Sevilla-Lara; Facebook AI; email: lsevilla@ed.ac.uk; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 225 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Song, L.\n",
      "AU  - Li, Y.\n",
      "AU  - Jiang, Z.\n",
      "AU  - Li, Z.\n",
      "AU  - Sun, H.\n",
      "AU  - Sun, J.\n",
      "AU  - Zheng, N.\n",
      "TI  - Fine-grained dynamic head for object detection\n",
      "PY  - 2020\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2020-December\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104146432&partnerID=40&md5=ec888c60ba84d08fcbec456d19927056\n",
      "AD  - College of Artificial Intelligence, Xi’an Jiaotong University, China\n",
      "AD  - The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - Institute of Automation, Chinese Academy of Sciences, China\n",
      "AD  - Megvii Inc., Face++\n",
      "AB  - The Feature Pyramid Network (FPN) presents a remarkable approach to alleviate the scale variance in object representation by performing instance-level assignments. Nevertheless, this strategy ignores the distinct characteristics of different sub-regions in an instance. To this end, we propose a fine-grained dynamic head to conditionally select a pixel-level combination of FPN features from different scales for each instance, which further releases the ability of multi-scale feature representation. Moreover, we design a spatial gate with the new activation function to reduce computational complexity dramatically through spatially sparse convolutions. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method on several state-of-the-art detection benchmarks. Code is available at https://github.com/StevenGrove/DynamicHead. © 2020 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Effectiveness and efficiencies\n",
      "KW  - Feature pyramid\n",
      "KW  - Fine grained\n",
      "KW  - Multi-scale features\n",
      "KW  - New activation functions\n",
      "KW  - Object representations\n",
      "KW  - Pixel level\n",
      "KW  - State of the art\n",
      "KW  - Object detection\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 33; Correspondence Address: H. Sun; College of Artificial Intelligence, Xi’an Jiaotong University, China; email: hsun@mail.xjtu.edu.cn; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 226 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hahn, M.\n",
      "AU  - Kadav, A.\n",
      "AU  - Rehg, J.M.\n",
      "AU  - Graf, H.P.\n",
      "TI  - Tripping through time: Efficient Localization of Activities in Videos\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175847437&partnerID=40&md5=0367bfb667b26bf547cfaf13f9ca7a1b\n",
      "AD  - College of Computing, Georgia Institute of Technology, Atlanta, GA, United States\n",
      "AD  - Machine Learning Department, NEC Labs America, Princeton, NJ, United States\n",
      "AB  - Localizing moments in untrimmed videos via language queries is a new and interesting task that requires the ability to accurately ground language into video. Previous works have approached this task by processing the entire video, often more than once, to localize relevant activities. In the real world applications of this approach, such as video surveillance, efficiency is a key system requirement. In this paper, we present TripNet, an end-to-end system that uses a gated attention architecture to model fine-grained textual and visual representations in order to align text and video content. Furthermore, TripNet uses reinforcement learning to efficiently localize relevant activity clips in long videos, by learning how to intelligently skip around the video. It extracts visual features for few frames to perform activity classification. In our evaluation over Charades-STA [14], ActivityNet Captions [26] and the TACoS dataset [36], we find that TripNet achieves high accuracy and saves processing time by only looking at 32-41% of the entire video. © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Security systems\n",
      "KW  - End-to-end systems\n",
      "KW  - Fine grained\n",
      "KW  - KeY systems\n",
      "KW  - Localisation\n",
      "KW  - Real-world\n",
      "KW  - System requirements\n",
      "KW  - Textual representation\n",
      "KW  - Time-efficient\n",
      "KW  - Video surveillance\n",
      "KW  - Visual representations\n",
      "KW  - Reinforcement learning\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 28; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 227 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shi, B.\n",
      "AU  - Ji, L.\n",
      "AU  - Liang, Y.\n",
      "AU  - Duan, N.\n",
      "AU  - Chen, P.\n",
      "AU  - Niu, Z.\n",
      "AU  - Zhou, M.\n",
      "TI  - Dense procedure captioning in narrated instructional videos\n",
      "PY  - 2020\n",
      "T2  - ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference\n",
      "SP  - 6382\n",
      "EP  - 6391\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084092597&partnerID=40&md5=0e51003ae427e92c7fb70b0ada9c625f\n",
      "AD  - Beijing Institute of Technology, Beijing, China\n",
      "AD  - Institute of Computing Technology, Chinese Academy of Science, Beijing, China\n",
      "AD  - Microsoft Research Asia, Beijing, China\n",
      "AD  - Microsoft Research and AI Group, Beijing, China\n",
      "AB  - Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of stepwise clips with description. Previous works on video dense captioning learn video segments and generate captions without considering transcripts. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task. © 2019 Association for Computational Linguistics\n",
      "KW  - Computational linguistics\n",
      "KW  - Semantics\n",
      "KW  - Cross modality\n",
      "KW  - Instructional videos\n",
      "KW  - Procedure extraction\n",
      "KW  - Real world web\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Textual information\n",
      "KW  - Video representations\n",
      "KW  - Video segments\n",
      "KW  - Video recording\n",
      "PB  - Association for Computational Linguistics (ACL)\n",
      "SN  - 978-195073748-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 40; Correspondence Address: Z. Niu; Beijing Institute of Technology, Beijing, China; email: zniu@bit.edu.cn; Conference name: 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019; Conference date: 28 July 2019 through 2 August 2019; Conference code: 159206\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 228 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Li, Y.\n",
      "AU  - Lin, W.\n",
      "AU  - Wang, T.\n",
      "AU  - See, J.\n",
      "AU  - Qian, R.\n",
      "AU  - Xu, N.\n",
      "AU  - Wang, L.\n",
      "AU  - Xu, S.\n",
      "TI  - Finding action tubes with a sparse-to-dense framework\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 11466\n",
      "EP  - 11473\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100369474&partnerID=40&md5=55a402d729f39aacc40dee477c3fa2aa\n",
      "AD  - School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China\n",
      "AD  - Shanghai Institute for Advanced Communication and Data Science, Shanghai University, China\n",
      "AD  - Faculty of Computing and Informatics, Multimedia University, Malaysia\n",
      "AD  - Adobe Research, United States\n",
      "AD  - State Key Laboratory for Novel Software Technology, Nanjing University, China\n",
      "AB  - The task of spatial-temporal action detection has attracted increasing attention among researchers. Existing dominant methods solve this problem by relying on short-term information and dense serial-wise detection on each individual frames or clips. Despite their effectiveness, these methods showed inadequate use of long-term information and are prone to inefficiency. In this paper, we propose for the first time, an efficient framework that generates action tube proposals from video streams with a single forward pass in a sparse-to-dense manner. There are two key characteristics in this framework: (1) Both long-term and short-term sampled information are explicitly utilized in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS) is designed to effectively approximate the tube output while keeping the system tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and UCFSports benchmark datasets, achieving promising results that are competitive to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our framework about 7.6 times more efficient than the nearest competitor. Copyright 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Benchmark datasets\n",
      "KW  - Dynamic features\n",
      "KW  - Key characteristics\n",
      "KW  - Short term\n",
      "KW  - Spatial temporals\n",
      "KW  - Spatiotemporal networks\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 18; Correspondence Address: W. Lin; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China; email: wylin@sjtu.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 229 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Song, L.\n",
      "AU  - Li, Y.\n",
      "AU  - Jiang, Z.\n",
      "AU  - Li, Z.\n",
      "AU  - Zhang, X.\n",
      "AU  - Sun, H.\n",
      "AU  - Sun, J.\n",
      "AU  - Zheng, N.\n",
      "TI  - Rethinking learnable tree filter for generic feature transform\n",
      "PY  - 2020\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2020-December\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099797327&partnerID=40&md5=cef5d44c94f05dacdff4d89d9f7400d7\n",
      "AD  - College of Artificial Intelligence, Xi’an Jiaotong University, China\n",
      "AD  - The Chinese University of Hong Kong, Hong Kong\n",
      "AD  - Institute of Automation, Chinese Academy of Sciences, China\n",
      "AD  - Megvii Inc., Face++\n",
      "AB  - The Learnable Tree Filter presents a remarkable approach to model structure-preserving relations for semantic segmentation. Nevertheless, the intrinsic geometric constraint forces it to focus on the regions with close spatial distance, hindering the effective long-range interactions. To relax the geometric constraint, we give the analysis by reformulating it as a Markov Random Field and introduce a learnable unary term. Besides, we propose a learnable spanning tree algorithm to replace the original non-differentiable one, which further improves the flexibility and robustness. With the above improvements, our method can better capture long-range dependencies and preserve structural details with linear complexity, which is extended to several vision tasks for more generic feature transform. Extensive experiments on object detection/instance segmentation demonstrate the consistent improvements over the original version. For semantic segmentation, we achieve leading performance (82.1% mIoU) on the Cityscapes benchmark without bells-and-whistles. Code is available at https://github.com/StevenGrove/LearnableTreeFilterV2. © 2020 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Benchmarking\n",
      "KW  - Image segmentation\n",
      "KW  - Markov processes\n",
      "KW  - Object detection\n",
      "KW  - Semantics\n",
      "KW  - Signaling\n",
      "KW  - Trees (mathematics)\n",
      "KW  - Geometric constraint\n",
      "KW  - Long range interactions\n",
      "KW  - Long-range dependencies\n",
      "KW  - Markov Random Fields\n",
      "KW  - Semantic segmentation\n",
      "KW  - Spanning tree algorithms\n",
      "KW  - Structural details\n",
      "KW  - Structure-preserving\n",
      "KW  - Mathematical transformations\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 16; Correspondence Address: H. Sun; College of Artificial Intelligence, Xi’an Jiaotong University, China; email: hsun@mail.xjtu.edu.cn; Conference name: 34th Conference on Neural Information Processing Systems, NeurIPS 2020; Conference date: 6 December 2020 through 12 December 2020; Conference code: 169463\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 230 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tan, H.\n",
      "AU  - Wang, L.\n",
      "AU  - Zhang, Q.\n",
      "AU  - Gao, Z.\n",
      "AU  - Zheng, N.\n",
      "AU  - Hua, G.\n",
      "TI  - Object affordances graph network for action recognition\n",
      "PY  - 2020\n",
      "T2  - 30th British Machine Vision Conference 2019, BMVC 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087327745&partnerID=40&md5=666cdc696f73b562ca5b143e37e0f6b7\n",
      "AD  - Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, China\n",
      "AD  - HERE Technologies, United States\n",
      "AD  - Alibaba Group, China\n",
      "AD  - Wormpex AI Research, United States\n",
      "AB  - Human actions often involve interactions with objects, and such action possibilities of objects were termed “affordances” in human-computer interaction (HCI) literature. To facilitate action recognition with object affordances, we propose the Object Affordances Graph (OAG), which cast human-object interaction cues into video representations via an iterative refinement procedure. With the spatio-temporal co-occurrences between human and objects captured, the Object Affordances Graph Network (OAGN) is subsequently proposed. To provide a fair evaluation of the role that object affordances could play on human action recognition, we have assembled a new dataset with additional annotated object bounding-boxes to account for human-object interactions. Multiple experiments on this proposed Object-Charades dataset verify the value of including object affordances in human action recognition, specifically via the proposed OAGN, which outperforms existing state-of-the-art affordance-less action recognition methods. © 2019. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Iterative methods\n",
      "KW  - Action recognition\n",
      "KW  - Human computer interaction (HCI)\n",
      "KW  - Human-action recognition\n",
      "KW  - Human-object interaction\n",
      "KW  - Iterative refinement\n",
      "KW  - Spatio temporal\n",
      "KW  - State of the art\n",
      "KW  - Video representations\n",
      "KW  - Human computer interaction\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf. , BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Correspondence Address: L. Wang; email: lewang@xjtu.edu.cn; Conference name: 30th British Machine Vision Conference, BMVC 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 160373\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 231 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, M.\n",
      "AU  - Mudassar, B.\n",
      "AU  - Na, T.\n",
      "AU  - Mukhopadhyay, S.\n",
      "TI  - A spatiotemporal pre-processing network for activity recognition under rain\n",
      "PY  - 2020\n",
      "T2  - 30th British Machine Vision Conference 2019, BMVC 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087337948&partnerID=40&md5=30ccdddcacd817466bc45a73fbedc0c3\n",
      "AD  - Georgia Institute of Technology, Atlanta, 30332, GA, United States\n",
      "AB  - This paper presents a deep neural network (DNN) based fully spatiotemporal rain removal network, MoPE-Spatiotemporal, to enhance accuracy of activity recognition in rainy videos. The proposed network utilizes spatiotemporal information of an image sequence to detect rain streaks and recover the non-rainy image. We also present rain alert network that detects the rain fall and informs the reduction of recognition confidence under rain. Experimental results show that heavy rain can highly degrade activity recognition accuracy. MoPE-Spatiotemporal removes heavy rain better than state-of-the-art methods, and significantly improves (0.15) activity recognition accuracy in rainy videos with minimal impact on recognition accuracy in clean videos. © 2019. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Deep neural networks\n",
      "KW  - Activity recognition\n",
      "KW  - Heavy rains\n",
      "KW  - Image sequence\n",
      "KW  - Pre-processing\n",
      "KW  - Rain removals\n",
      "KW  - Recognition accuracy\n",
      "KW  - Spatiotemporal information\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Rain\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf. , BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 30th British Machine Vision Conference, BMVC 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 160373\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 232 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, X.-Y.\n",
      "AU  - Shi, H.\n",
      "AU  - Li, C.\n",
      "AU  - Li, P.\n",
      "TI  - Multi-instance multi-label action recognition and localization based on spatio-temporal pre-trimming for untrimmed videos\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 12886\n",
      "EP  - 12893\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092673141&partnerID=40&md5=d6ef730166a8670a5ce7850cd61ec8b3\n",
      "AD  - Institute of Information Engineering, Chinese Academy of Sciences, China\n",
      "AD  - School of Cyber Security, University of Chinese Academy of Sciences, China\n",
      "AD  - School of Computer Science and Technology, Beijing Institute of Technology, China\n",
      "AD  - China University of Petroleum (East China, China\n",
      "AB  - Weakly supervised action recognition and localization for untrimmed videos is a challenging problem with extensive applications. The overwhelming irrelevant background contents in untrimmed videos severely hamper effective identification of actions of interest. In this paper, we propose a novel multi-instance multi-label modeling network based on spatio-temporal pre-trimming to recognize actions and locate corresponding frames in untrimmed videos. Motivated by the fact that person is the key factor in a human action, we spatially and temporally segment each untrimmed video into person-centric clips with pose estimation and tracking techniques. Given the bag-of-instances structure associated with video-level labels, action recognition is naturally formulated as a multi-instance multi-label learning problem. The network is optimized iteratively with selective coarse-to-fine pre-trimming based on instance-label activation. After convergence, temporal localization is further achieved with local-global temporal class activation map. Extensive experiments are conducted on two benchmark datasets, i.e. THUMOS14 and ActivityNet1.3, and experimental results clearly corroborate the efficacy of our method when compared with the state-of-the-arts. Copyright 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Arts computing\n",
      "KW  - Chemical activation\n",
      "KW  - Iterative methods\n",
      "KW  - Trimming\n",
      "KW  - Action recognition\n",
      "KW  - Benchmark datasets\n",
      "KW  - Model networks\n",
      "KW  - Multi-instance multi-label learning\n",
      "KW  - Pose estimation\n",
      "KW  - Spatio temporal\n",
      "KW  - State of the art\n",
      "KW  - Temporal localization\n",
      "KW  - Artificial intelligence\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 37; Correspondence Address: C. Li; School of Computer Science and Technology, Beijing Institute of Technology, China; email: changsheng_li_507@hotmail.com; P. Li; China University of Petroleum (East China, China; email: lipeng@upc.edu.cn; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 233 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Bae, K.\n",
      "AU  - Yun, K.\n",
      "AU  - Kim, H.\n",
      "AU  - Lee, Y.\n",
      "AU  - Park, J.\n",
      "TI  - Anti-Litter Surveillance based on Person Understanding via Multi-Task Learning\n",
      "PY  - 2020\n",
      "T2  - 31st British Machine Vision Conference, BMVC 2020\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102223792&partnerID=40&md5=682b4417e95fae3ed2b243cb1ed11203\n",
      "AD  - Artificial Intelligence Research Laboratory, Electronics and Telecommunications Research Institute (ETRI), Daejeon, South Korea\n",
      "AB  - In this paper, we propose a new framework for an anti-litter visual surveillance system to prevent garbage dumping as a real-world application. There have been many efforts to deploy an action recognition based visual surveillance system. However, many conventional methods were overfitted for only specific scenes due to hand-crafted rules and lack of real-world data. To overcome this problem, we propose a novel algorithm that handles the diverse scene properties of the real-world surveillance. In addition to collecting data from the real-world, we train the effective model to understand the person through multiple datasets such as human poses, human coarse action (e.g., upright, bent), and fine action (e.g., pushing a cart) via multi-task learning. As a result, our approach eliminates the need for scene-by-scene tuning and provides robustness to behavior understanding performance in a visual surveillance system. In addition, we propose a new object detection network that is optimized for detecting carryable objects and a person. The proposed detection network reduces the computational cost by specifying potential suspects only to the person who carries an object. Our method outperforms the state-of-the-art methods in detecting the garbage dumping action on real-world surveillance video dataset. © 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computer vision\n",
      "KW  - Learning systems\n",
      "KW  - Security systems\n",
      "KW  - Action recognition\n",
      "KW  - Conventional methods\n",
      "KW  - Detection networks\n",
      "KW  - Handcrafted rules\n",
      "KW  - Multiple data sets\n",
      "KW  - Multitask learning\n",
      "KW  - Novel algorithm\n",
      "KW  - Property\n",
      "KW  - Real-world\n",
      "KW  - Visual surveillance systems\n",
      "KW  - Object detection\n",
      "PB  - British Machine Vision Association, BMVA\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Conference name: 31st British Machine Vision Conference, BMVC 2020; Conference date: 7 September 2020 through 10 September 2020; Conference code: 193121\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 234 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wang, N.\n",
      "AU  - Zhou, W.\n",
      "AU  - Qi, G.\n",
      "AU  - Li, H.\n",
      "TI  - POST: POlicy-based switch tracking\n",
      "PY  - 2020\n",
      "T2  - AAAI 2020 - 34th AAAI Conference on Artificial Intelligence\n",
      "SP  - 12184\n",
      "EP  - 12191\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100364465&partnerID=40&md5=5d3e65da53156c806cc917747dfda7aa\n",
      "AD  - CAS Key Laboratory of GIPAS, University of Science and Technology of China, China\n",
      "AD  - Futurewei Technologies\n",
      "AB  - In visual object tracking, by reasonably fusing multiple experts, ensemble framework typically achieves superior performance compared to the individual experts. However, the necessity of parallelly running all the experts in most existing ensemble frameworks heavily limits their efficiency. In this paper, we propose POST, a POlicy-based Switch Tracker for robust and efficient visual tracking. The proposed POST tracker consists of multiple weak but complementary experts (trackers) and adaptively assigns one suitable expert for tracking in each frame. By formulating this expert switch in consecutive frames as a decision-making problem, we learn an agent via reinforcement learning to directly decide which expert to handle the current frame without running others. In this way, the proposed POST tracker maintains the performance merit of multiple diverse models while favorably ensuring the tracking efficiency. Extensive ablation studies and experimental comparisons against state-of-the-art trackers on 5 prevalent benchmarks verify the effectiveness of the proposed method. Copyright 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Decision making\n",
      "KW  - Efficiency\n",
      "KW  - Reinforcement learning\n",
      "KW  - Current frame\n",
      "KW  - Decision-making problem\n",
      "KW  - Experimental comparison\n",
      "KW  - Policy-based\n",
      "KW  - State of the art\n",
      "KW  - Visual object tracking\n",
      "KW  - Visual Tracking\n",
      "KW  - Object tracking\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735835-0 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI - AAAI Conf. Artif. Intell.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 14; Conference name: 34th AAAI Conference on Artificial Intelligence, AAAI 2020; Conference date: 7 February 2020 through 12 February 2020; Conference code: 166426\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 235 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hou, R.\n",
      "AU  - Chen, C.\n",
      "AU  - Sukthankar, R.\n",
      "AU  - Shah, M.\n",
      "TI  - An efficient 3D CNN for action/object segmentation in video\n",
      "PY  - 2020\n",
      "T2  - 30th British Machine Vision Conference 2019, BMVC 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087329393&partnerID=40&md5=cd6bf02b3362e4dfc2c9882fcd345f77\n",
      "AD  - Niantic Labs Sunnyvale, CA, United States\n",
      "AD  - University of North Carolina at Charlotte, Charlotte, NC, United States\n",
      "AD  - Google Research, Mountain View, CA, United States\n",
      "AD  - University of Central Florida, Orlando, FL, United States\n",
      "AB  - Convolutional Neural Network (CNN) based image segmentation has made great progress in recent years. However, video object segmentation remains a challenging task due to its high computational complexity. Most of the previous methods employ a two-stream CNN framework to handle spatial and motion features separately. In this paper, we propose an end-to-end encoder-decoder style 3D CNN to aggregate spatial and temporal information simultaneously for video object segmentation. To efficiently process video, we propose 3D separable convolution for the pyramid pooling module and decoder, which dramatically reduces the number of operations while maintaining the performance. Moreover, we also extend our framework to video action segmentation by adding an extra classifier to predict the action label for actors in videos. Extensive experiments on several video datasets demonstrate the superior performance of the proposed approach for action and object segmentation compared to the state-of-the-art. © 2019. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Convolution\n",
      "KW  - Convolutional neural networks\n",
      "KW  - Decoding\n",
      "KW  - Motion compensation\n",
      "KW  - Action segmentation\n",
      "KW  - Encoder-decoder\n",
      "KW  - Motion features\n",
      "KW  - Object segmentation\n",
      "KW  - State of the art\n",
      "KW  - Temporal information\n",
      "KW  - Video datasets\n",
      "KW  - Video-object segmentation\n",
      "KW  - Image segmentation\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf. , BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 5; Conference name: 30th British Machine Vision Conference, BMVC 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 160373\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 236 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lee, J.\n",
      "AU  - Lee, J.\n",
      "AU  - Lee, S.\n",
      "AU  - Yoon, S.\n",
      "TI  - Mutual suppression network for video prediction using disentangled features\n",
      "PY  - 2020\n",
      "T2  - 30th British Machine Vision Conference 2019, BMVC 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087338710&partnerID=40&md5=cc22435f4644c1cbaf869fcefed7cc38\n",
      "AD  - Department of Electrical and Computer Engineering, Seoul National University, South Korea\n",
      "AD  - ASRI, INMC, ISRC, Institute of Engineering Research, Seoul National University, South Korea\n",
      "AB  - Video prediction has been considered a difficult problem because the video contains not only high-dimensional spatial information but also complex temporal information. Video prediction can be performed by finding features in recent frames, and using them to generate approximations to upcoming frames. We approach this problem by disentangling spatial and temporal features in videos. We introduce a mutual suppression network (MSnet) which are trained in an adversarial manner and then produces spatial features which are free of motion information, and motion features with no spatial information. MSnet then uses motion-guided connection within an encoder-decoder-based architecture to transform spatial features from a previous frame to the time of an upcoming frame. We show how MSnet can be used for video prediction using disentangled representations. We also carry out experiments to assess the effectiveness of our method to disentangle features. MSnet obtains better results than other recent video prediction methods even though it has simpler encoders. © 2019. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Signal encoding\n",
      "KW  - High-dimensional\n",
      "KW  - Motion features\n",
      "KW  - Motion information\n",
      "KW  - Spatial features\n",
      "KW  - Spatial informations\n",
      "KW  - Temporal features\n",
      "KW  - Temporal information\n",
      "KW  - Video prediction\n",
      "KW  - Forecasting\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf. , BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Correspondence Address: S. Yoon; Department of Electrical and Computer Engineering, Seoul National University, South Korea; email: sryoon@snu.ac.kr; Conference name: 30th British Machine Vision Conference, BMVC 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 160373\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 237 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yu, Z.\n",
      "AU  - Li, X.\n",
      "AU  - Zhao, G.\n",
      "TI  - Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks\n",
      "PY  - 2020\n",
      "T2  - 30th British Machine Vision Conference 2019, BMVC 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087338959&partnerID=40&md5=26d7a14ec48f085fede54428b74e56fd\n",
      "AD  - Center for Machine Vision and Signal Analysis, University of Oulu, FI-90014, Finland\n",
      "AD  - School of Information and Technology, Northwest University, 710069, China\n",
      "AB  - Recent studies demonstrated that the average heart rate (HR) can be measured from facial videos based on non-contact remote photoplethysmography (rPPG). However for many medical applications (e.g., atrial fibrillation (AF) detection) knowing only the average HR is not sufficient, and measuring precise rPPG signals from face for heart rate variability (HRV) analysis is needed. Here we propose an rPPG measurement method, which is the first work to use deep spatio-temporal networks for reconstructing precise rPPG signals from raw facial videos. With the constraint of trend-consistency with ground truth pulse curves, our method is able to recover rPPG signals with accurate pulse peaks. Comprehensive experiments are conducted on two benchmark datasets, and results demonstrate that our method can achieve superior performance on both HR and HRV levels comparing to the state-of-the-art methods. We also achieve promising results of using reconstructed rPPG signals for AF detection and emotion recognition. © 2019. The copyright of this document resides with its authors.\n",
      "KW  - Benchmarking\n",
      "KW  - Computer vision\n",
      "KW  - Heart\n",
      "KW  - Medical applications\n",
      "KW  - Atrial fibrillation\n",
      "KW  - Benchmark datasets\n",
      "KW  - Emotion recognition\n",
      "KW  - Heart rate variability\n",
      "KW  - Measurement methods\n",
      "KW  - Photoplethysmograph\n",
      "KW  - Signal measurement\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Signal reconstruction\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf. , BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 75; Correspondence Address: G. Zhao; Center for Machine Vision and Signal Analysis, University of Oulu, FI-90014, Finland; email: Guoying.Zhao@oulu.fi; Conference name: 30th British Machine Vision Conference, BMVC 2019; Conference date: 9 September 2019 through 12 September 2019; Conference code: 160373\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 238 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lu, Y.\n",
      "AU  - Jiang, P.\n",
      "AU  - Nishide, S.\n",
      "AU  - Kang, X.\n",
      "AU  - Ren, F.\n",
      "TI  - Traffic sign recognition based on up-sampling convolution\n",
      "PY  - 2020\n",
      "T2  - Proceedings of 2019 the 9th International Workshop on Computer Science and Engineering, WCSE 2019\n",
      "SP  - 136\n",
      "EP  - 142\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081101925&partnerID=40&md5=f1f48d034244ba3f3c699c10ddfb1424\n",
      "AD  - School of Electrical Engineering, Nantong University, No.9 Seyuan Road, Chongchuan District, Nantong, Jiangsu, China\n",
      "AD  - Faculty of Engineering, Tokushima University, 2-1 Minami Josanjima, Tokushima, 770-8506, Japan\n",
      "AB  - This paper presented a method makes traffic sign recognition faster and more accurate. Traditional faster detectors are limited by their accuracy and are not sensitive to small objects, in the area of self-driving, it has some inconspicuous but important object of concern, such as traffic sign. We noticed that most traffic signs in dataset is small and easily to confuse with complex backgrounds. In this situation, after a series of convolutional layers, some of these traffic signs can't be detected or classified correctly, and the problem of neglect happens a lot. In order to settle this problem and optimize the result, we simplified the SSD structure and introduced an up-sampling structure to make the geometric details of small objects distinctly. This method significantly improved the result of recognition, we got 97.6% mAP on The German Traffic Sign Benchmark with 96 ×96 input and SSD300 has 79.7% mAP on same dataset. © WCSE 2019. All rights reserved.\n",
      "KW  - Small objects\n",
      "KW  - Traffic sign recognition\n",
      "KW  - Up-sampling\n",
      "KW  - Convolution\n",
      "KW  - Pattern recognition\n",
      "KW  - Signal sampling\n",
      "KW  - Complex background\n",
      "KW  - Geometric details\n",
      "KW  - Important object\n",
      "KW  - Self drivings\n",
      "KW  - Small objects\n",
      "KW  - Traffic sign recognition\n",
      "KW  - Up sampling\n",
      "KW  - Traffic signs\n",
      "PB  - International Workshop on Computer Science and Engineering (WCSE)\n",
      "SN  - 978-981141684-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Proc. Int. Workshop Comput. Sc. Eng., WCSE\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Correspondence Address: F. Ren; Faculty of Engineering, Tokushima University, Tokushima, 2-1 Minami Josanjima, 770-8506, Japan; email: ren@is.tokushima-u.ac.jp; Conference name: 2019 9th International Workshop on Computer Science and Engineering, WCSE 2019; Conference date: 15 June 2019 through 17 June 2019; Conference code: 157471\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 239 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Alvanitopoulos, P.\n",
      "AU  - Diplaris, S.\n",
      "AU  - De Gelder, B.\n",
      "AU  - Shvets, A.\n",
      "AU  - Benayoun, M.\n",
      "AU  - Koulali, P.\n",
      "AU  - Moghnieh, A.\n",
      "AU  - Shekhawat, Y.\n",
      "AU  - Stentoumis, C.\n",
      "AU  - Hosmer, T.\n",
      "AU  - Anadol, R.\n",
      "AU  - Borreguero, M.\n",
      "AU  - Martin, A.\n",
      "AU  - Sciama, P.\n",
      "AU  - Avgerinakis, K.\n",
      "AU  - Petrantonakis, P.\n",
      "AU  - Briassouli, A.\n",
      "AU  - Mille, S.\n",
      "AU  - Tellios, A.\n",
      "AU  - Fraguada, L.\n",
      "AU  - Sprengel, H.\n",
      "AU  - Kalisperakis, I.\n",
      "AU  - Cabanas, N.\n",
      "AU  - Nikolopoulos, S.\n",
      "AU  - Skouras, S.\n",
      "AU  - Vogler, V.\n",
      "AU  - Zavraka, D.\n",
      "AU  - Piesk, J.\n",
      "AU  - Grammatikopoulos, L.\n",
      "AU  - Wanner, L.\n",
      "AU  - Klein, T.\n",
      "AU  - Vrochidis, S.\n",
      "AU  - Kompatsiaris, I.\n",
      "TI  - MindSpaces: Art-driven Adaptive Outdoors and Indoors Design\n",
      "PY  - 2019\n",
      "T2  - Digital Presentation and Preservation of Cultural and Scientific Heritage\n",
      "VL  - 9\n",
      "SP  - 391\n",
      "EP  - 402\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084240673&partnerID=40&md5=6374804e60c2d1183b7825b98918414c\n",
      "AD  - Centre for Research and Technology Hellas, CERTH-ITI, Greece\n",
      "AD  - Universiteit Maastricht (MU), Netherlands\n",
      "AD  - Universitat Pompeu Fabra (UPF), Spain\n",
      "AD  - Aristotle University of Thessaloniki (AUTH), Greece\n",
      "AD  - McNeel, McNeel Europe Sl, Spain\n",
      "AD  - Nurogames (NURO), Germany\n",
      "AD  - UP2METRIC (U2M), Greece\n",
      "AD  - Zaha Hadid Architects (ZHA), Iraq\n",
      "AD  - AnalogNative\n",
      "AD  - Ajuntament de l' Hospitalet de Llobregrat, Spain\n",
      "AD  - Espronceda, Spain\n",
      "AD  - E-Seniors\n",
      "AD  - City University of Hong Kong (CityUHK), Hong Kong\n",
      "AB  - MindSpaces provides solutions for creating functionally and emotionally appealing architectural designs in urban spaces. Social media services, physiological sensing devices and video cameras provide data from sensing environments. State-of-the-Art technology including VR, 3D design tools, emotion extraction, visual behaviour analysis, and textual analysis will be incorporated in MindSpaces platform for analysing data and adapting the design of spaces. © 2019 Digital Presentation and Preservation of Cultural and Scientific Heritage. All rights reserved.\n",
      "KW  - 3d reconstruction\n",
      "KW  - Emotion extraction\n",
      "KW  - Style transfer\n",
      "KW  - Text analysis\n",
      "KW  - Virtual reality (vr)\n",
      "KW  - Visual behavior analysis\n",
      "A2  - Paneva-Marinova D.\n",
      "A2  - Pavlov R.\n",
      "A2  - Stanchev P.\n",
      "A2  - Luchev D.\n",
      "PB  - Bulgarian Academy of Sciences, Institute of Mathematics and Informatics\n",
      "SN  - 13144006 (ISSN)\n",
      "LA  - English\n",
      "J2  - Digit. Present. Preserv. Cult. Sci. Herit.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 9th International Conference on Digital Presentation and Preservation of Cultural and Scientific Heritage, DiPP 2019; Conference date: 26 September 2019 through 28 September 2019; Conference code: 159420\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 240 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Naga Raja, T.\n",
      "AU  - Venkata Ramana, V.V.\n",
      "AU  - Damodharam, A.\n",
      "TI  - Video summarization using adaptive thresholding by machine learning for distributed cloud storage\n",
      "PY  - 2019\n",
      "T2  - International Journal of Engineering and Advanced Technology\n",
      "VL  - 8\n",
      "IS  - 5\n",
      "SP  - 741\n",
      "EP  - 748\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069926529&partnerID=40&md5=857110a07a1bea9cd7d72df977cb602a\n",
      "AD  - JNTUH, Hyderabad, India\n",
      "AD  - NIC, Vijayawada, India\n",
      "AD  - Govt of Andhra Pradesh Amaravathi, India\n",
      "AB  - The growth in the video content-based communication and information management have motivated the applications dealing with the video contents to be placed on cloud-based data-storages, which are associated with datacentres. The applications ranging from social media to distance education and surveillance to business communication or any applications related to the e-governess are considering the video data as one of the most preferred mode of communication. The video data is enriched with audio and visual contents, which makes analysis or expressing information high convenient. Computerized video is an electronic portrayal of moving visual pictures as encoded advanced information. This is as opposed to simple video, which speaks to moving visual pictures with a simple sign. The advanced video contains a progression of computerized pictures showed in quick progression. The number of applications, as mentioned, dealing with video data is increasing and as a result a large amount of video content is generated every day. Hence, the complexity of retrieving the information from the video contents are also increasing. The bottleneck of the video retrieval process is for a lower sized segment of the video content can be retrieved in low time complexity and if the information to be preserved to a higher extend, then the retrieval time complexity increases. Thus, a good number of parallel researchers have introduced various methods for video content summarization and retrieval using summarization with efficient searching methods, but most of the parallel research outcomes are criticized for either higher time complexity or for higher information loss. This problem can be ideally solved by finding the highly accurate ratio of key information video frames from the total video content. Henceforth, this work, presents a novel machine learning method for identifying the key frames, not only based on the information available in the frame, also validating the key frames with the thresholds of the objects or changes in the frame. The work is again enhanced by considering the adaptive thresholding method for distributed and collaborative video information. The measures taken in the proposed algorithm produces a 98% accuracy for video information representation and nearly 99% reduction in the video frames, which results into nearly 99% reduction in the time complexity. © BEIESP.\n",
      "KW  - Adaptive threshold\n",
      "KW  - Complexity reduction\n",
      "KW  - Key frame\n",
      "KW  - Segment noise reduction\n",
      "KW  - Video content retrieval\n",
      "PB  - Blue Eyes Intelligence Engineering and Sciences Publication\n",
      "SN  - 22498958 (ISSN)\n",
      "LA  - English\n",
      "J2  - Int. J. Eng. Adv. Technol.\n",
      "M3  - Article\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 241 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Saran, A.\n",
      "AU  - Short, E.S.\n",
      "AU  - Thomaz, A.\n",
      "AU  - Niekum, S.\n",
      "TI  - Understanding Teacher Gaze Patterns for Robot Learning\n",
      "PY  - 2019\n",
      "T2  - Proceedings of Machine Learning Research\n",
      "VL  - 100\n",
      "SP  - 1247\n",
      "EP  - 1258\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160831677&partnerID=40&md5=443b3037d9b397e8d0aa8fc2f57534c8\n",
      "AD  - Computer Science Department, University of Texas, Austin, United States\n",
      "AD  - Electrical and Computer Engineering, University of Texas, Austin, United States\n",
      "AB  - Human gaze is known to be a strong indicator of underlying human intentions and goals during manipulation tasks. This work studies gaze patterns of human teachers demonstrating tasks to robots and proposes ways in which such patterns can be used to enhance robot learning. Using both kinesthetic teaching and video demonstrations, we identify novel intention-revealing gaze behaviors during teaching. These prove to be informative in a variety of problems ranging from reference frame inference to segmentation of multi-step tasks. Based on our findings, we propose two proof-of-concept algorithms which show that gaze data can enhance subtask classification for a multi-step task up to 6% and reward inference and policy learning for a single-step task up to 67%. Our findings provide a foundation for a model of natural human gaze in robot learning from demonstration settings and present open problems for utilizing human gaze to enhance robot learning. © 2019 CoRL. All Rights Reserved.\n",
      "KW  - Eye gaze\n",
      "KW  - Kinesthetic Teaching\n",
      "KW  - Learning from demonstrations\n",
      "KW  - Learning from observations\n",
      "KW  - Demonstrations\n",
      "KW  - Learning systems\n",
      "KW  - Robot learning\n",
      "KW  - Robots\n",
      "KW  - Eye-gaze\n",
      "KW  - Human intentions\n",
      "KW  - Human teachers\n",
      "KW  - Kinesthetic teachings\n",
      "KW  - Learning from demonstration\n",
      "KW  - Learning from observation\n",
      "KW  - Manipulation task\n",
      "KW  - Multisteps\n",
      "KW  - Teachers'\n",
      "KW  - Work study\n",
      "KW  - Inference engines\n",
      "A2  - Kaelbling L.P.\n",
      "A2  - Kragic D.\n",
      "A2  - Sugiura K.\n",
      "PB  - ML Research Press\n",
      "SN  - 26403498 (ISSN)\n",
      "LA  - English\n",
      "J2  - Proc. Mach. Learn. Res.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 15; Conference name: 3rd Conference on Robot Learning, CoRL 2019; Conference date: 30 October 2019 through 1 November 2019; Conference code: 188844\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 242 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, D.\n",
      "AU  - Dai, X.\n",
      "AU  - Wang, X.\n",
      "AU  - Wang, Y.-F.\n",
      "TI  - S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Network\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084014348&partnerID=40&md5=4190cc5643c92b37c24856544de571ac\n",
      "AD  - Computer Vision Lab, Dept. of Computer Science, UC Santa Barbara, United States\n",
      "AD  - Dept. of Computer Science, University of Maryland, United States\n",
      "AB  - In this paper, we present a novel Single Shot multi-Span Detector for temporal activity detection in long, untrimmed videos using a simple end-to-end fully three-dimensional convolutional (Conv3D) network. Our architecture, named S3D, encodes the entire video stream and discretizes the output space of temporal activity spans into a set of default spans over different temporal locations and scales. At prediction time, S3D predicts scores for the presence of activity categories in each default span and produces temporal adjustments relative to the span location to predict the precise activity duration. Unlike many state-of-the-art systems that require a separate proposal and classification stage, our S3D is intrinsically simple and dedicatedly designed for single-shot, end-to-end temporal activity detection. When evaluating on THUMOS'14 detection benchmark, S3D achieves state-of-the-art performance and is very efficient and can operate at 1271 FPS. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Benchmarking\n",
      "KW  - Convolution\n",
      "KW  - Activity detection\n",
      "KW  - Activity duration\n",
      "KW  - Convolutional networks\n",
      "KW  - Multi-spans\n",
      "KW  - Prediction time\n",
      "KW  - Single shots\n",
      "KW  - State-of-the-art performance\n",
      "KW  - State-of-the-art system\n",
      "KW  - Computer vision\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 13; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 243 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Choi, J.\n",
      "AU  - Gao, C.\n",
      "AU  - Messou, J.C.E.\n",
      "AU  - Huang, J.-B.\n",
      "TI  - Why Can't I dance in the mall? learning to mitigate scene bias in action recognition\n",
      "PY  - 2019\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 32\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090129762&partnerID=40&md5=dac62aba8e1b9435a5adbcf3dba3b0ab\n",
      "AD  - Virginia Tech, United States\n",
      "AB  - Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing. © 2019 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Action classifications\n",
      "KW  - Action recognition\n",
      "KW  - Baseline models\n",
      "KW  - Human activities\n",
      "KW  - Spatio temporal\n",
      "KW  - Temporal localization\n",
      "KW  - Video datasets\n",
      "KW  - Video representations\n",
      "KW  - Basketball\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 130; Conference name: 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019; Conference date: 8 December 2019 through 14 December 2019; Conference code: 161263\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 244 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Behl, H.S.\n",
      "AU  - Sapienza, M.\n",
      "AU  - Singh, G.\n",
      "AU  - Saha, S.\n",
      "AU  - Cuzzolin, F.\n",
      "AU  - Torr, P.\n",
      "TI  - Incremental tube construction for human action detection\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011877&partnerID=40&md5=ec2d4050b4e11f706b0f81ceabc6a72b\n",
      "AD  - Department of Engineering Science, University of Oxford, Oxford, United Kingdom\n",
      "AD  - Think Tank Team, Samsung Research America, Mountain View, CA, United States\n",
      "AD  - Dept. of Computing and Communication Technologies, Oxford Brookes University, Oxford, United Kingdom\n",
      "AB  - Current state-of-the-art action detection systems are tailored for offline batch-processing applications. However, for online applications like human-robot interaction, current systems fall short. In this work, we introduce a real-time and online joint-labelling and association algorithm for action detection that can incrementally construct space-time action tubes on the most challenging untrimmed action videos in which different action categories occur concurrently. In contrast to previous methods, we solve the linking, action labelling and temporal localization problems jointly in a single pass. We demonstrate superior online association accuracy and speed (1.8ms per frame) as compared to the current state-of-the-art offline and online systems. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Batch data processing\n",
      "KW  - Computer vision\n",
      "KW  - Online systems\n",
      "KW  - Association algorithms\n",
      "KW  - Detection system\n",
      "KW  - Human action detections\n",
      "KW  - On-line applications\n",
      "KW  - Processing applications\n",
      "KW  - Single pass\n",
      "KW  - State of the art\n",
      "KW  - Temporal localization\n",
      "KW  - Human robot interaction\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 245 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 14th Asian Conference on Computer Vision, ACCV 2018\n",
      "PY  - 2019\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 11362 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067342835&partnerID=40&md5=7ae144f784b4f74754abff09ff5e57d9\n",
      "AB  - The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Robust Video Background Identification by Dominant Rigid Motion Estimation; SMC: Single-Stage Multi-location Convolutional Network for Temporal Action Detection; SAFE: Scale Aware Feature Encoder for Scene Text Recognition; neural Abstract Style Transfer for Chinese Traditional Painting; detector-in-Detector: Multi-level Analysis for Human-Parts; aligning Salient Objects to Queries: A Multi-modal and Multi-object Image Retrieval Framework; fast Light Field Disparity Estimation via a Parallel Filtered Cost Volume Approach; perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization; self-Referenced Deep Learning; SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-identification; hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking; learning Free-Form Deformations for 3D Object Reconstruction; CalliGAN: Unpaired Mutli-chirography Chinese Calligraphy Image Translation; end-to-End Detection and Re-identification Integrated Net for Person Search; A CNN-Based Depth Estimation Approach with Multi-scale Sub-pixel Convolutions and a Smoothness Constraint; ranking Loss: A Novel Metric Learning Method for Person Re-identification; Unsupervised Transformation Network Based on GANs for Target-Domain Oriented Multi-domain Image Translation; a Trainable Multiplication Layer for Auto-correlation and Co-occurrence Extraction; cross-View Action Recognition Using View-Invariant Pose Feature Learned from Synthetic Data with Domain Adaptation; ImaGAN: Unsupervised Training of Conditional Joint CycleGAN for Transferring Style with Core Structures in Content Preserved; GhostVLAD for Set-Based Face Recognition; stochastic Normalizations as Bayesian Learning; contextual Object Detection with a Few Relevant Neighbors; album to Family Tree: A Graph Based Method for Family Relationship Recognition; robust Multimodal Image Registration Using Deep Recurrent Reinforcement Learning.\n",
      "A2  - Jawahar C.V.\n",
      "A2  - Schindler K.\n",
      "A2  - Mori G.\n",
      "A2  - Li H.\n",
      "PB  - Springer Verlag\n",
      "SN  - 03029743 (ISSN); 978-303020889-9 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 14th Asian Conference on Computer Vision, ACCV 2018; Conference date: 2 December 2018 through 6 December 2018; Conference code: 226489\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 246 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Sun, Y.\n",
      "AU  - Chen, X.\n",
      "AU  - Li, C.\n",
      "AU  - Sawada, K.\n",
      "AU  - Hosono, T.\n",
      "AU  - Zhu, J.\n",
      "AU  - Xie, C.\n",
      "AU  - Huang, S.\n",
      "AU  - Wang, L.\n",
      "AU  - Hu, K.\n",
      "AU  - Zhou, Q.\n",
      "AU  - Gao, C.\n",
      "AU  - Shimamura, J.\n",
      "AU  - Sagata, A.\n",
      "TI  - Ntt_cqupt@trecVid2019 actev：activities in extended video\n",
      "PY  - 2019\n",
      "T2  - 2019 TREC Video Retrieval Evaluation, TRECVID 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204446674&partnerID=40&md5=9de1e226a1468aa2684e08fd18e45720\n",
      "AD  - NTT Media Intelligent Laboratories, Japan\n",
      "AD  - School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, China\n",
      "AD  - National Police Academy, Japan\n",
      "AB  - In this notebook paper, we present our activity detection system, which aims to temporally localize activities in surveillance videos. Our pipeline composed of five modules, object detection, activity proposal generation, feature extraction, classification and post-processing. We input RGB and optical flow into this pipeline separately and obtain frame level predictions by late fusion. The final detections are generated by greedily merging these predictions and filtering invalid results. Copyright © TRECVID 2019.All rights reserved.\n",
      "KW  - Content based retrieval\n",
      "KW  - Feature extraction\n",
      "KW  - Object detection\n",
      "KW  - Security systems\n",
      "KW  - Activity detection\n",
      "KW  - Detection system\n",
      "KW  - Extraction processing\n",
      "KW  - Features extraction\n",
      "KW  - Late fusion\n",
      "KW  - Objects detection\n",
      "KW  - Post-processing\n",
      "KW  - Surveillance video\n",
      "KW  - Pipelines\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Delgado A.\n",
      "A2  - Zhang J.\n",
      "A2  - Godard E.\n",
      "A2  - Diduch L.L.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 2019 TREC Video Retrieval Evaluation, TRECVID 2019; Conference date: 12 November 2019 through 13 November 2019; Conference code: 159652\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 247 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Antunes, J.\n",
      "AU  - Bernardino, A.\n",
      "AU  - Smailagic, A.\n",
      "AU  - Siewiorek, D.\n",
      "TI  - AHA-3D: A labelled dataset for senior fitness exercise recognition and segmentation from 3D skeletal data\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084017695&partnerID=40&md5=d755f74f488271e80b3ecc30953e96f9\n",
      "AD  - Instituto Superior Tecnico Lisboa, Portugal\n",
      "AD  - Carnegie Mellon University, Pittsburgh, United States\n",
      "AB  - Automated assessment of fitness exercises has important applications in computer and robot-based exercise coaches to deploy at home, gymnasiums or care centers. In this work, we introduce AHA-3D, a labeled dataset of sequences of 3D skeletal data depicting standard fitness tests on young and elderly subjects, for the purpose of automatic fitness exercises assessment. To the best of our knowledge, AHA-3D is the first publicly available dataset featuring multi-generational, male and female subjects, with frame-level labels, allowing for action segmentation as well as the estimation of metrics like risk of fall, and autonomy to perform daily tasks. We present two baseline methods for recognition and one for segmentation. For recognition, we trained models on the positions of the joints achieving 88.2%  0.077 accuracy, and on joint positions and velocities, achieving 91%  0.082 accuracy. Using the Kolmogorov-Smirnov test we determined the model trained on velocities was superior. The segmentation baseline achieved an accuracy of 88.29% in detecting actions at frame level. Our results show promising recognition and detection performance suggesting AHA-3D's potential use in practical applications like exercise performance and correction, elderly fitness level estimation and risk of falling for elders. © 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Computational complexity\n",
      "KW  - Computer vision\n",
      "KW  - Risk perception\n",
      "KW  - Statistical tests\n",
      "KW  - Action segmentation\n",
      "KW  - Automated assessment\n",
      "KW  - Baseline methods\n",
      "KW  - Daily tasks\n",
      "KW  - Detection performance\n",
      "KW  - Fitness tests\n",
      "KW  - Kolmogorov-Smirnov test\n",
      "KW  - Labeled dataset\n",
      "KW  - Health\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 248 without DOI:\n",
      "TY  - JOUR\n",
      "TI  - 2nd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2019\n",
      "PY  - 2019\n",
      "T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "VL  - 11857 LNCS\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086149065&partnerID=40&md5=581e31f3b1bfc24b5f3fd2bf2d43d288\n",
      "AB  - The proceedings contain 68 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: High-order graph convolutional network for skeleton-based human action recognition; multi-scale spatial-temporal attention for action recognition; reading digital numbers of water meter with deep learning based object detector; exploiting category-level semantic relationships for fine-grained image recognition; on the multi-scale real-time object detection using resnet; learning attention regularization correlation filter for visual tracking; target tracking via two-branch spatio-temporal regularized correlation filter network; a real-time rock-paper-scissor hand gesture recognition system based on flownet and event camera; cross-category cross-semantic regularization for fine-grained image recognition; the multi-task fully convolutional siamese network with correlation filter layer for real-time visual tracking; table detection using boundary refining via corner locating; dictionary learning and confidence map estimation-based tracker for robot-assisted therapy system; Power line corridor LiDAR point cloud segmentation using convolutional neural network; face liveness detection based on client identity using siamese network; learning weighted video segments for temporal action localization; REAPS: Towards better recognition of fine-grained images by region attending and part sequencing; weakly-supervised action recognition and localization via knowledge transfer; visual tracking with levy flight grasshopper optimization algorithm; exploring context information for accurate and fast object detection; a novel method for thermal image based electrical-equipment detection; state detection of electrical equipment based on infrared thermal imaging technology; attention based convolutional recurrent neural network for environmental sound classification; salient object detection via light-weight multi-path refinement networks; visual object tracking via an improved lightweight siamese network.\n",
      "A2  - Lin Z.\n",
      "A2  - Wang L.\n",
      "A2  - Tan T.\n",
      "A2  - Yang J.\n",
      "A2  - Shi G.\n",
      "A2  - Zheng N.\n",
      "A2  - Chen X.\n",
      "A2  - Zhang Y.\n",
      "PB  - Springer\n",
      "SN  - 03029743 (ISSN); 978-303031653-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Comput. Sci.\n",
      "M3  - Conference review\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2nd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2019; Conference date: 8 November 2019 through 11 November 2019; Conference code: 234779\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 249 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Schroeter, J.\n",
      "AU  - Sidorov, K.\n",
      "AU  - Marshall, D.\n",
      "TI  - Weakly-supervised temporal localization via occurrence count learning\n",
      "PY  - 2019\n",
      "T2  - 36th International Conference on Machine Learning, ICML 2019\n",
      "VL  - 2019-June\n",
      "SP  - 9939\n",
      "EP  - 9950\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078283004&partnerID=40&md5=e5b4f2997cd04e8b39a0aa8b977b554f\n",
      "AD  - Cardiff University, United Kingdom\n",
      "AB  - We propose a novel model for temporal detection and localization which allows the training of deep neural networks using only counts of event occurrences as training labels. This powerful weakly-supervised framework alleviates the burden of the imprecise and time-consuming process of annotating event locations in temporal data. Unlike existing methods, in which localization is explicitly achieved by design, our model learns localization implicitly as a byproduct of learning to count instances. This unique feature is a direct consequence of the model's theoretical properties. We validate the effectiveness of our approach in a number of experiments (drum hit and piano onset detection in audio, digit detection in images) and demonstrate performance comparable to that of fully-supervised state-of-the-art methods, despite much weaker training requirements. © 2019 International Machine Learning Society (IMLS).\n",
      "KW  - Deep neural networks\n",
      "KW  - Machine learning\n",
      "KW  - Event location\n",
      "KW  - Onset detection\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Temporal Data\n",
      "KW  - Temporal detection\n",
      "KW  - Temporal localization\n",
      "KW  - Training requirement\n",
      "KW  - Unique features\n",
      "KW  - Supervised learning\n",
      "PB  - International Machine Learning Society (IMLS)\n",
      "SN  - 978-151088698-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Int. Conf. Mach. Learn., ICML\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 9; Correspondence Address: J. Schroeter; Cardiff University, United Kingdom; email: SchroeterJl@cardiff.ac.uk; Conference name: 36th International Conference on Machine Learning, ICML 2019; Conference date: 9 June 2019 through 15 June 2019; Conference code: 156104\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 250 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, L.\n",
      "AU  - Khusainov, R.\n",
      "AU  - Chiverton, J.P.\n",
      "TI  - Practical action recognition with manifold regularized sparse representations\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084012591&partnerID=40&md5=15aeadd0c08b7f3ed850f25c9031b0aa\n",
      "AD  - School of Computing, University of Portsmouth, Portsmouth, United Kingdom\n",
      "AD  - School of Energy and Electronic Engineering, University of Portsmouth, Portsmouth, United Kingdom\n",
      "AB  - With the explosion of long term health conditions, monitoring human daily activities in home environment is one of the important issues in healthcare. Human action recognition in videos is one of the main topics in this context. Conventional representations are not very effective for encoding dense features extracted from videos. In this work, we propose a novel manifold regularized sparse representation (MRSR) method to encode dense features for human action recognition in assisted living. The new method can effectively incorporate a manifold regularization term to explore the geometric structure of the improved dense trajectories, which are very effective for learning action representations. By introducing a locality constraint, our method ensures each interest point is represented by its local closest words. Moreover, our method has an analytical solution and low computational complexity. Experimental results on different realistic databases show the effectiveness of the proposed algorithm for practical action recognition in assisted living. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Assisted living\n",
      "KW  - Encoding (symbols)\n",
      "KW  - Action recognition\n",
      "KW  - Geometric structure\n",
      "KW  - Health condition\n",
      "KW  - Human-action recognition\n",
      "KW  - Learning actions\n",
      "KW  - Low computational complexity\n",
      "KW  - Manifold regularizations\n",
      "KW  - Sparse representation\n",
      "KW  - Computer vision\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 251 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Mahasseni, B.\n",
      "AU  - Yang, X.\n",
      "AU  - Molchanov, P.\n",
      "AU  - Kautz, J.\n",
      "TI  - Budget-aware activity detection with a recurrent policy network\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011487&partnerID=40&md5=8dd48e54c1eec8086ce6611fd23b9a2f\n",
      "AD  - Oregon State University, United States\n",
      "AD  - NVIDIA Research, United States\n",
      "AD  - NVIDIA Research, United States\n",
      "AB  - In this paper, we address the challenging problem of efficient temporal activity detection in untrimmed long videos. While most recent work has focused and advanced the detection accuracy, the inference time can take seconds to minutes in processing each single video, which is too slow to be useful in real-world settings. This motivates the proposed budget-aware framework, which learns to perform activity detection by intelligently selecting a small subset of frames according to a specified time budget. We formulate this problem as a Markov decision process, and adopt a recurrent network to model the frame selection policy. We derive a recurrent policy gradient based approach to approximate the gradient of the non-decomposable and non-differentiable objective defined in our problem. In the extensive experiments, we achieve competitive detection accuracy, and more importantly, our approach is able to substantially reduce computation time and detect multiple activities with only 0.35s for each untrimmed long video. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Computer vision\n",
      "KW  - Learning algorithms\n",
      "KW  - Markov processes\n",
      "KW  - Recurrent neural networks\n",
      "KW  - Reinforcement learning\n",
      "KW  - Activity detection\n",
      "KW  - Computation time\n",
      "KW  - Detection accuracy\n",
      "KW  - Frame selection\n",
      "KW  - Markov Decision Processes\n",
      "KW  - Non-differentiable objectives\n",
      "KW  - Real world setting\n",
      "KW  - Recurrent networks\n",
      "KW  - Budget control\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 252 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yuan, Y.\n",
      "AU  - Ma, L.\n",
      "AU  - Wang, J.\n",
      "AU  - Liu, W.\n",
      "AU  - Zhu, W.\n",
      "TI  - Semantic conditioned dynamic modulation for temporal sentence grounding in videos\n",
      "PY  - 2019\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 32\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079603756&partnerID=40&md5=3cc1e1b9df3b71ea5b85c16ee2be5d59\n",
      "AD  - Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, China\n",
      "AD  - Tencent AI Lab\n",
      "AD  - Tsinghua University, China\n",
      "AB  - Temporal sentence grounding in videos aims to detect and localize one target video segment, which semantically corresponds to a given sentence. Existing methods mainly tackle this task via matching and aligning semantics between a sentence and candidate video segments, while neglect the fact that the sentence information plays an important role in temporally correlating and composing the described contents in videos. In this paper, we propose a novel semantic conditioned dynamic modulation (SCDM) mechanism, which relies on the sentence semantics to modulate the temporal convolution operations for better correlating and composing the sentence-related video contents over time. More importantly, the proposed SCDM performs dynamically with respect to the diverse video contents so as to establish a more precise matching relationship between sentence and video, thereby improving the temporal grounding accuracy. Extensive experiments on three public datasets demonstrate that our proposed model outperforms the state-of-the-arts with clear margins, illustrating the ability of SCDM to better associate and localize relevant video contents for temporal sentence grounding. Our code for this paper is available at https://github.com/yytzsy/SCDM. © 2019 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Image segmentation\n",
      "KW  - Modulation\n",
      "KW  - Semantics\n",
      "KW  - Video recording\n",
      "KW  - Dynamic modulation\n",
      "KW  - State of the art\n",
      "KW  - Video contents\n",
      "KW  - Video segments\n",
      "KW  - Arts computing\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 169; Conference name: 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019; Conference date: 8 December 2019 through 14 December 2019; Conference code: 161263\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 253 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Cao, J.\n",
      "AU  - Mo, L.\n",
      "AU  - Zhang, Y.\n",
      "AU  - Jia, K.\n",
      "AU  - Shen, C.\n",
      "AU  - Tan, M.\n",
      "TI  - Multi-marginal wasserstein GAN\n",
      "PY  - 2019\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 32\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089213517&partnerID=40&md5=489db29c693b84e679a93cedd847f696\n",
      "AD  - South China University of Technology, Peng Cheng Laboratory, China\n",
      "AD  - University of Adelaide, Australia\n",
      "AB  - Multiple marginal matching problem aims at learning mappings to match a source domain to multiple target domains and it has attracted great attention in many applications, such as multi-domain image translation. However, addressing this problem has two critical challenges: (i) Measuring the multi-marginal distance among different domains is very intractable; (ii) It is very difficult to exploit cross-domain correlations to match the target domain distributions. In this paper, we propose a novel Multi-marginal Wasserstein GAN (MWGAN) to minimize Wasserstein distance among domains. Specifically, with the help of multi-marginal optimal transport theory, we develop a new adversarial objective function with inner- and inter-domain constraints to exploit cross-domain correlations. Moreover, we theoretically analyze the generalization performance of MWGAN, and empirically evaluate it on the balanced and imbalanced translation tasks. Extensive experiments on toy and real-world datasets demonstrate the effectiveness of MWGAN. © 2019 Neural information processing systems foundation. All rights reserved.\n",
      "KW  - Critical challenges\n",
      "KW  - Different domains\n",
      "KW  - Generalization performance\n",
      "KW  - Image translation\n",
      "KW  - Objective functions\n",
      "KW  - Optimal transport\n",
      "KW  - Real-world datasets\n",
      "KW  - Wasserstein distance\n",
      "KW  - Statistical mechanics\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 65; Conference name: 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019; Conference date: 8 December 2019 through 14 December 2019; Conference code: 161263\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 254 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Gao, M.\n",
      "AU  - Davis, L.S.\n",
      "AU  - Socher, R.\n",
      "AU  - Xiong, C.\n",
      "TI  - WSLLN: Weakly supervised natural language localization networks\n",
      "PY  - 2019\n",
      "T2  - EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference\n",
      "SP  - 1481\n",
      "EP  - 1487\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084289962&partnerID=40&md5=5ac36ecd1b53552dbc90bb0c6e296954\n",
      "AD  - University of Maryland, United States\n",
      "AD  - Salesforce Research\n",
      "AB  - We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance. © 2019 Association for Computational Linguistics\n",
      "KW  - End to end\n",
      "KW  - High costs\n",
      "KW  - Matching problems\n",
      "KW  - Natural languages\n",
      "KW  - Segment selection\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Natural language processing systems\n",
      "PB  - Association for Computational Linguistics\n",
      "SN  - 978-195073790-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang. Process., Proc. Conf.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 48; Conference name: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019; Conference date: 3 November 2019 through 7 November 2019; Conference code: 159367\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 255 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Heidarivincheh, F.\n",
      "AU  - Mirmehdi, M.\n",
      "AU  - Damen, D.\n",
      "TI  - Action completion: A temporal model for moment detection\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084016604&partnerID=40&md5=4be09f245ce48c7d0333d0ba9e66fd81\n",
      "AD  - Department of Computer Science, University of Bristol, Bristol, United Kingdom\n",
      "AB  - We introduce completion moment detection for actions - the problem of locating the moment of completion, when the action's goal is confidently considered achieved. The paper proposes a joint classification-regression recurrent model that predicts completion from a given frame, and then integrates frame-level contributions to detect sequence-level completion moment. We introduce a recurrent voting node that predicts the frame's relative position of the completion moment by either classification or regression. The method is also capable of detecting incompletion. For example, the method is capable of detecting a missed ball-catch, as well as the moment at which the ball is safely caught. We test the method on 16 actions from three public datasets, covering sports as well as daily actions. Results show that when combining contributions from frames prior to the completion moment as well as frames post completion, the completion moment is detected within one second in 89% of all tested sequences. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Classification regression\n",
      "KW  - Daily actions\n",
      "KW  - Recurrent models\n",
      "KW  - Relative positions\n",
      "KW  - Temporal modeling\n",
      "KW  - Computer vision\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 4; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 256 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Piergiovanni, A.J.\n",
      "AU  - Ryoo, M.S.\n",
      "TI  - Temporal Gaussian mixture layer for videos\n",
      "PY  - 2019\n",
      "T2  - 36th International Conference on Machine Learning, ICML 2019\n",
      "VL  - 2019-June\n",
      "SP  - 9008\n",
      "EP  - 9023\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078321468&partnerID=40&md5=3b4165dfeffd2284cc536d12dab3bfe1\n",
      "AD  - Department of Computer Science, Indiana University, United States\n",
      "AB  - We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that arc fully diffcrentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts1. © 2019 International Machine Learning Society (IMLS).\n",
      "KW  - Arts computing\n",
      "KW  - Machine learning\n",
      "KW  - Activity detection\n",
      "KW  - Gaussian mixtures\n",
      "KW  - Gaussians\n",
      "KW  - Multiple data sets\n",
      "KW  - State of the art\n",
      "KW  - Temporal information\n",
      "KW  - Video model\n",
      "KW  - Convolution\n",
      "PB  - International Machine Learning Society (IMLS)\n",
      "SN  - 978-151088698-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - Int. Conf. Mach. Learn., ICML\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 32; Correspondence Address: A.J. Piergiovanni; Department of Computer Science, Indiana University, United States; email: ajpiergi@indiana.edu; Conference name: 36th International Conference on Machine Learning, ICML 2019; Conference date: 9 June 2019 through 15 June 2019; Conference code: 156104\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 257 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Hou, Z.\n",
      "AU  - Pan, Y.-W.\n",
      "AU  - Yao, T.\n",
      "AU  - Ngo, C.-W.\n",
      "TI  - VireoJD-MM @ TRECVID 2019: Activities in extended video (ACTEV)\n",
      "PY  - 2019\n",
      "T2  - 2019 TREC Video Retrieval Evaluation, TRECVID 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204470078&partnerID=40&md5=0ae02722f7f00ceab049570b6b7a6cb0\n",
      "AD  - Video Retrieval Group (VIREO), City University of Hong Kong, Hong Kong\n",
      "AD  - JD AI Research\n",
      "AB  - In this paper, we describe the system developed for Activities in Extended Video(ActEV) task at TRECVid 2019 [1] and the achieved results. Activities in Extended Video(ActEV): The goal of Activities in Extended Video is to spatially and temporally localize the action instances in a surveillance setting. We have participated in previous ActEV prize challenge. Since the only difference between the two challenges is evaluation metric, we maintain previous pipeline [2] for this challenge. The pipeline has three stages: object detection, tubelet generation and temporal action localization. This time we extend the system for two aspects separately: better object detection and advanced two-stream action classification. We submit 2 runs, which are summarised below. - VireoJD-MM Pipeline1: This run achieves Partial AUDC=0.6012 using advanced two-stream action classification. It has been recognized in many papers [3, 4] that two-stream structure increases action recognition performance. In our prize challenge model, we only use RGB frames as input. For the submission this time, we extend the action classification stage into an advanced two-stream action classification module. - VireoJD-MM SecondarySystem: This run achieves Partial AUDC=0.6936 using better object detection model. The CMU team released the groundtruth of object bounding box provided by Kitware as well as their object detection and tracking code1 based on VIRAT dataset. They build a system to detect and track small objects in outdoor scenes for surveillance videos. For the submission this time, we replace our object detection and tracking code with their code and keep the remaining stages of tubelet generation and temporal action localization. Copyright © TRECVID 2019.All rights reserved.\n",
      "KW  - Codes (symbols)\n",
      "KW  - Content based retrieval\n",
      "KW  - Image segmentation\n",
      "KW  - Object recognition\n",
      "KW  - Pipelines\n",
      "KW  - Security systems\n",
      "KW  - Action classifications\n",
      "KW  - Action recognition\n",
      "KW  - Evaluation metrics\n",
      "KW  - Is evaluations\n",
      "KW  - Localisation\n",
      "KW  - Object detection and tracking\n",
      "KW  - Objects detection\n",
      "KW  - Stream structure\n",
      "KW  - TRECVID\n",
      "KW  - Two-stream\n",
      "KW  - Object detection\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Delgado A.\n",
      "A2  - Zhang J.\n",
      "A2  - Godard E.\n",
      "A2  - Diduch L.L.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 2019 TREC Video Retrieval Evaluation, TRECVID 2019; Conference date: 12 November 2019 through 13 November 2019; Conference code: 159652\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 258 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yuan, Y.\n",
      "AU  - Lyu, Y.\n",
      "AU  - Shen, X.\n",
      "AU  - Tsang, I.W.\n",
      "AU  - Yeung, D.-Y.\n",
      "TI  - Marginalized average attentional network for weakly-supervised learning\n",
      "PY  - 2019\n",
      "T2  - 7th International Conference on Learning Representations, ICLR 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083951995&partnerID=40&md5=2056687538d67f7d080c3d526ed3c93c\n",
      "AD  - Hong Kong University of Science and Technology, Hong Kong\n",
      "AD  - Alibaba Group, China\n",
      "AD  - University of Technology Sydney, Australia\n",
      "AD  - Ecole des Ponts ParisTech, France\n",
      "AB  - In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from O(2T) to O(T2). Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.\n",
      "KW  - Large dataset\n",
      "KW  - Machine learning\n",
      "KW  - End to end\n",
      "KW  - Fast algorithms\n",
      "KW  - Integral action\n",
      "KW  - Salient regions\n",
      "KW  - Video datasets\n",
      "KW  - Weakly supervised learning\n",
      "KW  - Probability\n",
      "PB  - International Conference on Learning Representations, ICLR\n",
      "LA  - English\n",
      "J2  - Int. Conf. Learn. Represent., ICLR\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 86; Conference name: 7th International Conference on Learning Representations, ICLR 2019; Conference date: 6 May 2019 through 9 May 2019; Conference code: 149936\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 259 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhang, Y.\n",
      "AU  - Tang, S.\n",
      "AU  - Sun, H.\n",
      "AU  - Neumann, H.\n",
      "TI  - Human motion parsing by hierarchical dynamic clustering\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011581&partnerID=40&md5=bd659bd34a357e47703693c28256112a\n",
      "AD  - Institute of Neural Information Processing, Ulm University, Ulm, Germany\n",
      "AD  - Department of Perceiving Systems, Max Planck Institute for Intelligent Systems, Tubingen, Germany\n",
      "AD  - School of Informatics, University of Edinburgh, Edinburgh, United Kingdom\n",
      "AB  - Parsing continuous human motion into meaningful segments plays an essential role in various applications. In this work, we propose a hierarchical dynamic clustering framework to derive action clusters from a sequence of local features in an unsupervised bottom-up manner. We systematically investigate the modules in this framework and particularly propose diverse temporal pooling schemes, in order to realize accurate temporal action localization. We demonstrate our method on two motion parsing tasks: temporal action segmentation and abnormal behavior detection. The experimental results indicate that the proposed framework is significantly more effective than the other related state-of-the-art methods on several datasets. © 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\n",
      "KW  - Abnormal behavior detections\n",
      "KW  - Action segmentation\n",
      "KW  - Bottom-up manner\n",
      "KW  - Dynamic clustering\n",
      "KW  - Human motions\n",
      "KW  - Local feature\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Temporal pooling\n",
      "KW  - Computer vision\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 260 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Lin, S.-Y.\n",
      "AU  - Lin, Y.-Y.\n",
      "TI  - Action recognition with the augmented MoCap data using neural data translation\n",
      "PY  - 2019\n",
      "T2  - British Machine Vision Conference 2018, BMVC 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084014711&partnerID=40&md5=59dc6279667b5303ef4b499427f17c26\n",
      "AD  - Tencent America, Palo Alto, CA, United States\n",
      "AD  - Academic Sinica, Taipei, Taiwan\n",
      "AB  - This study aims at generating reliable augmented training data to learn a robust deep model for action recognition. The prior knowledge inferred from few training data is not sufficient to well represent the real data distribution, which makes action recognition quite challenging. Inspired by the recent advances in neural machine translation, we propose a neural data translation (NDT) to tackle the aforementioned issue by directly learning the mapping between paired data of the same action class in an end-to-end fashion. The proposed NDT is a sequence-to-sequence generative model. It can be trained with few paired training data, and generates an abundant set of augmented actions with diverse appearance. Specifically, we adopt stochastic pair selection to compile a set of paired training data. Each pair consists of two actions of the same class. One action serves as the input to NDT, while the other acts as the desired output. By learning the mapping between data of the same class, NDT implicitly encodes the intra-class variations so that it can synthesize high-quality actions for augmentation. We evaluated our method on two public datasets, including the Florence3D-action and UCI HAR datasets. The promising results demonstrate that the actions generated by our method effectively improve the performance of action recognition with few examples. © 2018. The copyright of this document resides with its authors.\n",
      "KW  - Mapping\n",
      "KW  - Stochastic systems\n",
      "KW  - Action recognition\n",
      "KW  - Data distribution\n",
      "KW  - Generative model\n",
      "KW  - High quality\n",
      "KW  - Intra-class variation\n",
      "KW  - Machine translations\n",
      "KW  - Prior knowledge\n",
      "KW  - Training data\n",
      "KW  - Computer vision\n",
      "PB  - BMVA Press\n",
      "LA  - English\n",
      "J2  - Br. Mach. Vis. Conf., BMVC\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 29th British Machine Vision Conference, BMVC 2018; Conference date: 3 September 2018 through 6 September 2018; Conference code: 151127\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 261 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Rana, A.J.\n",
      "AU  - Tirupattur, P.\n",
      "AU  - Rizve, M.N.\n",
      "AU  - Duarte, K.\n",
      "AU  - Demir, U.\n",
      "AU  - Rawat, Y.\n",
      "AU  - Shah, M.\n",
      "TI  - An online system for real-time activity detection in untrimmed surveillance videos\n",
      "PY  - 2019\n",
      "T2  - 2019 TREC Video Retrieval Evaluation, TRECVID 2019\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114866304&partnerID=40&md5=1481f7b5dbe4d0b7de405af3f0fd2fc5\n",
      "AB  - Activity detection in surveillance videos is a challenging problem due to multiple factors such as large field of view, presence of multiple activities, varying scales and viewpoints, and its untrimmed nature. The requirement of processing the surveillance videos in real-time makes this more challenging. In this work, we propose a real-time online system to perform activity detection on untrimmed surveillance videos. The proposed system consists of three stages: first we detect tubelets with activities, then classify them, and finally merge them to generate spatio-temporal activity detections. We propose a localization network which takes a video clip as input and makes use of feature pyramid, multi-layer loss, and atrous convolutions to address the issue of multiple scales and detect small activities in terms of tubelets. The online processing of videos at a clip level drastically reduces the computation time in detecting activities. The detected tubelets are assigned activity class scores and merged together using our proposed Tubelet-Merge Action-Split (TMAS) algorithm to form action tubes. The TMAS algorithm efficiently connects the tubelets in an online fashion to generate spatio-temporal detections which are robust against varying length activities. We perform our experiments on the DIVA (Deep Intermodal Video Analytics) dataset and demonstrate the effectiveness of the proposed approach in terms of speed (-100 fps) and performance with state-of-the-art results. The code and models will be made publicly available. Copyright © TRECVID 2019.All rights reserved.\n",
      "KW  - Mergers and acquisitions\n",
      "KW  - Security systems\n",
      "KW  - Tubes (components)\n",
      "KW  - Video signal processing\n",
      "KW  - Activity detection\n",
      "KW  - Large field of views\n",
      "KW  - Localisation\n",
      "KW  - Merge actions\n",
      "KW  - Multiple factors\n",
      "KW  - Real- time\n",
      "KW  - Spatio-temporal activities\n",
      "KW  - Split algorithms\n",
      "KW  - Surveillance video\n",
      "KW  - Video-clips\n",
      "KW  - Monitoring\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Delgado A.\n",
      "A2  - Zhang J.\n",
      "A2  - Godard E.\n",
      "A2  - Diduch L.L.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3; Conference name: 2019 TREC Video Retrieval Evaluation, TRECVID 2019; Conference date: 12 November 2019 through 13 November 2019; Conference code: 159652\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 262 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Ranjan, R.\n",
      "AU  - Gleason, J.\n",
      "AU  - Schwarcz, S.\n",
      "AU  - Castillo, C.D.\n",
      "AU  - Chen, J.-C.\n",
      "AU  - Chellappa, R.\n",
      "TI  - Spatio-temporal action detection in untrimmed videos\n",
      "PY  - 2018\n",
      "T2  - 2018 TREC Video Retrieval Evaluation, TRECVID 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204478813&partnerID=40&md5=46ce0453caec8cfb3d625a18c5f19263\n",
      "AD  - University of Maryland College Park, United States\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Joy D.\n",
      "A2  - Delgado A.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "A2  - Magalhaes J.\n",
      "A2  - Semedo D.\n",
      "A2  - Blasi S.G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 2018 TREC Video Retrieval Evaluation, TRECVID 2018; Conference date: 13 November 2018 through 15 November 2018; Conference code: 159651\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 263 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Beigi, M.\n",
      "AU  - Brown, L.M.\n",
      "AU  - Fan, Q.\n",
      "AU  - Henning, J.\n",
      "AU  - Lin, C.-C.\n",
      "AU  - Shi, H.\n",
      "AU  - Shu, C.-F.\n",
      "AU  - Feris, R.\n",
      "TI  - Object-centric spatio-temporal activity detection and recognition\n",
      "PY  - 2018\n",
      "T2  - 2018 TREC Video Retrieval Evaluation, TRECVID 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204446337&partnerID=40&md5=93cb5c0ee41292406dbb8c26f33697b5\n",
      "AD  - IBM, T.J. Watson Research Center, Yorktown Heights, 10598, NY, United States\n",
      "AB  - We only used the data provided by NIST. Below are the Run IDs, submission names, AD and AOD scores followed by a brief description of the entry. Run-ID (AD/AOD) Submission AD AOD Note 283/284 IBM_E2E 0.8396338981 0.8433932966 S1: Baseline entries of IBM system for eval-1a 330/331 IBM_E2E_ALL (TRN3-TRN3 filtered) 0.7716112433 0.7928942585 S2: Baseline entries of IBM system for eval-1a, for all 19 activities. 351/352 IBM_E2E_ALL (unfiltered) 0.7706395834 0.7932181452 S2, but with unfiltered results 355/356 IBM_E2E_ALL (filtered & merged) 0.7762367494 0.7958497241 S2 with a merging algorithm 371/- IBM_E2E_ALL (TRN16) 0.7953969555 - S3: TRN 16 instead of TRN 3. 397/398 IBM_E2E_ALL_New (unfiltered) 0.7589621185 0.7785331614 S4: S2 + new turning algorithm 436/438 IBM_E2E_ALL_New (unfiltered + Ensemble) 0.7189424498 0.7629197006 S5: S2 + newly trained activity classifiers 453/440 IBM_E2E_ALL_New (unfiltered + Ensemble + LR-turns) 0.7087156227 0.7520320714 S6: Our winning entries, S4 + S5. A significant difference between the final runs was achieved from a post merging process to combine proposals. Other improvements were achieved in subsequent runs but it is difficult to analyze the specific contributions of components. For our system, the validation set provided a good measure of generalization to the test set. Copyright © TRECVID 2018.All rights reserved.\n",
      "KW  - Classification (of information)\n",
      "KW  - Content based retrieval\n",
      "KW  - Object detection\n",
      "KW  - Activity detection\n",
      "KW  - Activity recognition\n",
      "KW  - Generalisation\n",
      "KW  - Merging algorithms\n",
      "KW  - Merging process\n",
      "KW  - Spatio-temporal activities\n",
      "KW  - Test sets\n",
      "KW  - Validation sets\n",
      "KW  - Merging\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Joy D.\n",
      "A2  - Delgado A.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "A2  - Magalhaes J.\n",
      "A2  - Semedo D.\n",
      "A2  - Blasi S.G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2018 TREC Video Retrieval Evaluation, TRECVID 2018; Conference date: 13 November 2018 through 15 November 2018; Conference code: 159651\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 264 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Wei, Z.\n",
      "AU  - Wang, B.\n",
      "AU  - Hoai, M.\n",
      "AU  - Zhang, J.\n",
      "AU  - Lin, Z.\n",
      "AU  - Shen, X.\n",
      "AU  - Měch, R.\n",
      "AU  - Samaras, D.\n",
      "TI  - Sequence-to-segments networks for segment detection\n",
      "PY  - 2018\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2018-December\n",
      "SP  - 3507\n",
      "EP  - 3516\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064828931&partnerID=40&md5=06b899e90506f0a904364d0f54b9692c\n",
      "AD  - Stony Brook University, United States\n",
      "AD  - Adobe Research, United States\n",
      "AD  - ByteDance AI Lab, China\n",
      "AB  - Detecting segments of interest from an input sequence is a challenging problem which often requires not only good knowledge of individual target segments, but also contextual understanding of the entire input sequence and the relationships between the target segments. To address this problem, we propose the Sequence-to-Segments Network (S2N), a novel end-to-end sequential encoder-decoder architecture. S2N first encodes the input into a sequence of hidden states that progressively capture both local and holistic information. It then employs a novel decoding architecture, called Segment Detection Unit (SDU), that integrates the decoder state and encoder hidden states to detect segments sequentially. During training, we formulate the assignment of predicted segments to ground truth as the bipartite matching problem and use the Earth Mover's Distance to calculate the localization errors. Experiments on temporal action proposal and video summarization show that S2N achieves state-of-the-art performance on both tasks. © 2018 Curran Associates Inc..All rights reserved.\n",
      "KW  - Decoding\n",
      "KW  - Signal encoding\n",
      "KW  - Bipartite matching problems\n",
      "KW  - Contextual understanding\n",
      "KW  - Earth Mover's distance\n",
      "KW  - Encoder-decoder architecture\n",
      "KW  - Input sequence\n",
      "KW  - Localization errors\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Video summarization\n",
      "KW  - Network architecture\n",
      "A2  - Bengio S.\n",
      "A2  - Cesa-Bianchi N.\n",
      "A2  - Garnett R.\n",
      "A2  - Grauman K.\n",
      "A2  - Wallach H.\n",
      "A2  - Larochelle H.\n",
      "A2  - Grauman K.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 11; Conference name: 32nd Conference on Neural Information Processing Systems, NeurIPS 2018; Conference date: 2 December 2018 through 8 December 2018; Conference code: 147412\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 265 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Duarte, K.\n",
      "AU  - Rawat, Y.S.\n",
      "AU  - Shah, M.\n",
      "TI  - VideocapsuleNet: A simplified network for action detection\n",
      "PY  - 2018\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2018-December\n",
      "SP  - 7610\n",
      "EP  - 7619\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064830987&partnerID=40&md5=8a4e88af43fa3e83267540bc26df4863\n",
      "AD  - Center for Research in Computer Vision, University of Central Florida, Orlando, 32816, FL, United States\n",
      "AB  - The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue and make the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves state-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive ∼20% improvement on UCF-101 and ∼15% improvement on J-HMDB in terms of v-mAP scores. © 2018 Curran Associates Inc.All rights reserved.\n",
      "KW  - Convolution\n",
      "KW  - Deep neural networks\n",
      "KW  - Network routing\n",
      "KW  - Neural networks\n",
      "KW  - Pixels\n",
      "KW  - Action classifications\n",
      "KW  - Action representations\n",
      "KW  - Action segmentation\n",
      "KW  - Convolutional neural network\n",
      "KW  - Detection approach\n",
      "KW  - Human action classifications\n",
      "KW  - State-of-the-art performance\n",
      "KW  - Voting algorithm\n",
      "KW  - Network layers\n",
      "A2  - Wallach H.\n",
      "A2  - Bengio S.\n",
      "A2  - Larochelle H.\n",
      "A2  - Cesa-Bianchi N.\n",
      "A2  - Grauman K.\n",
      "A2  - Garnett R.\n",
      "A2  - Grauman K.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 120; Conference name: 32nd Conference on Neural Information Processing Systems, NeurIPS 2018; Conference date: 2 December 2018 through 8 December 2018; Conference code: 147412\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 266 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Cao, X.\n",
      "AU  - Wang, Z.\n",
      "AU  - Li, P.\n",
      "AU  - Xiang, X.\n",
      "AU  - Li, X.\n",
      "AU  - Li, Y.\n",
      "AU  - Xu, S.\n",
      "AU  - Zhou, L.\n",
      "AU  - Zhao, Z.\n",
      "AU  - Zhao, Y.\n",
      "AU  - Su, F.\n",
      "TI  - BUPt-MCPRL at TRECVID 2018: AcTEV and INS\n",
      "PY  - 2018\n",
      "T2  - 2018 TREC Video Retrieval Evaluation, TRECVID 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077817893&partnerID=40&md5=4e3fbe9572d8d9f7ba13b18820005acd\n",
      "AD  - Multimedia Communication, Pattern Recognition Lab, Beijing Key Laboratory of Network System and Network Culture, Beijing Beijing University of Posts and Telecommunications, Beijing, China\n",
      "AB  - In this paper, we describe BUPT-MCPRL systems and evaluation results for TRECVID 2018 [15]. We join two tasks: activities in extended video and instance search. Activities in Extended Video (ActEV): we mainly improve our method in following three runs: p_baseline_1: the baseline of our method. In this submission, we combine the prior knowledge and design different rules to extract proposals. Then we detect events by applying trajectory analysis, image classification and action classification solutions for different type of events separately. p_baseline_17: use TRN replace TSN for Open_Trunk and Closing_Trunk event detection. p_baseline_19: fuse the target object detection results for interaction event detection. Instance Search (INS): We submit three runs for automatic search and one run for interactive search: F_E_BUPT_MCPRL_1: merge two rank list from location retrieval after person retrieval and person retrieval after location retrieval. F_E_BUPT_MCPRL_2: fine tune the rank score of F_E_BUPT_MCPRL_1 based on random forest classification extra score. F_E_BUPT_MCPRL_3: fine tune the rank score of F_E_BUPT_MCPRL_1 based on random forest classification extra score. I_E_BUPT_MCPRL_4: optimize rank list interactively based on F_E_BUPT_MCPRL_1. Copyright © TRECVID 2018.All rights reserved.\n",
      "KW  - Content based retrieval\n",
      "KW  - Image classification\n",
      "KW  - Object detection\n",
      "KW  - Action classifications\n",
      "KW  - Evaluation results\n",
      "KW  - Events detection\n",
      "KW  - Images classification\n",
      "KW  - Prior-knowledge\n",
      "KW  - Random forest classification\n",
      "KW  - Rank lists\n",
      "KW  - Rank scores\n",
      "KW  - Trajectory analysis\n",
      "KW  - TRECVID\n",
      "KW  - Decision trees\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Joy D.\n",
      "A2  - Delgado A.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "A2  - Magalhaes J.\n",
      "A2  - Semedo D.\n",
      "A2  - Blasi S.G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 1; Conference name: 2018 TREC Video Retrieval Evaluation, TRECVID 2018; Conference date: 13 November 2018 through 15 November 2018; Conference code: 159651\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 267 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Chéron, G.\n",
      "AU  - Alayrac, J.-B.\n",
      "AU  - Laptev, I.\n",
      "AU  - Schmid, C.\n",
      "TI  - A flexible model for training action localization with varying levels of supervision\n",
      "PY  - 2018\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 2018-December\n",
      "SP  - 942\n",
      "EP  - 953\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064813576&partnerID=40&md5=7c809a3d829819863dfbded8527b1e7f\n",
      "AD  - Inria, École normale supérieure, CNRS, PSL, Research University, Paris, 75005, France\n",
      "AD  - University Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, 38000, France\n",
      "AB  - Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos. © 2018 Curran Associates Inc..All rights reserved.\n",
      "KW  - Bounding box\n",
      "KW  - Class labels\n",
      "KW  - Competitive performance\n",
      "KW  - Discriminative clustering\n",
      "KW  - Flexible model\n",
      "KW  - Joint learning\n",
      "KW  - Manual annotation\n",
      "KW  - Spatio temporal\n",
      "A2  - Larochelle H.\n",
      "A2  - Wallach H.\n",
      "A2  - Grauman K.\n",
      "A2  - Grauman K.\n",
      "A2  - Garnett R.\n",
      "A2  - Cesa-Bianchi N.\n",
      "A2  - Bengio S.\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 30; Conference name: 32nd Conference on Neural Information Processing Systems, NeurIPS 2018; Conference date: 2 December 2018 through 8 December 2018; Conference code: 147412\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 268 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Aakur, S.\n",
      "AU  - Sawyer, D.\n",
      "AU  - Balazia, M.\n",
      "AU  - Sarkar, S.\n",
      "TI  - An examination of proposal-based approaches to fine-grained activity detection in untrimmed surveillance videos\n",
      "PY  - 2018\n",
      "T2  - 2018 TREC Video Retrieval Evaluation, TRECVID 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204499553&partnerID=40&md5=b08d27716a7d82037c5f997d362dc721\n",
      "AD  - Department of Computer Science and Engineering, University of South Florida, Tampa, United States\n",
      "AB  - Spatiotemporal localization of activities in untrimmed surveillance videos is a hard task, especially when given the occurrence of simultaneous activities across different temporal and spatial scales. We tackle this problem using a cascaded region proposal and detection (CRPAD) framework implementing frame-level simultaneous activity detection, followed by tracking. We propose the use of a frame-level spatial detection model based on advances in object detection and a temporal linking algorithm that models the temporal dynamics of the detected activities. We also evaluate a proposal-based approach to the multi-activity, multi-label problem through cascaded modules of detection, tracking and recognition. A combination of handcrafted rules and deep learning methods show encouraging results to the activity detection problem. We show results on the VIRAT dataset through our participation at the recent 2018 TRECVID ActEV Challenge. Copyright © TRECVID 2018.All rights reserved.\n",
      "KW  - Deep learning\n",
      "KW  - Security systems\n",
      "KW  - Activity detection\n",
      "KW  - Detection framework\n",
      "KW  - Detection models\n",
      "KW  - Fine grained\n",
      "KW  - Hard task\n",
      "KW  - Localisation\n",
      "KW  - Model-based OPC\n",
      "KW  - Spatial detection\n",
      "KW  - Surveillance video\n",
      "KW  - Temporal and spatial scale\n",
      "KW  - Object detection\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Joy D.\n",
      "A2  - Delgado A.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "A2  - Magalhaes J.\n",
      "A2  - Semedo D.\n",
      "A2  - Blasi S.G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2018 TREC Video Retrieval Evaluation, TRECVID 2018; Conference date: 13 November 2018 through 15 November 2018; Conference code: 159651\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 269 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Inoue, N.\n",
      "AU  - Shiraishi, C.\n",
      "AU  - Drozd, A.\n",
      "AU  - Shinoda, K.\n",
      "AU  - Lee, S.-W.\n",
      "AU  - Kot, A.C.\n",
      "TI  - VANT at TRECVID 2018\n",
      "PY  - 2018\n",
      "T2  - 2018 TREC Video Retrieval Evaluation, TRECVID 2018\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204455367&partnerID=40&md5=0e1e64ee9b10981fccab2fd5baea881f\n",
      "AD  - Tokyo Institute of Technology, Japan\n",
      "AD  - National Institute of Advanced Industrial Science and Technology\n",
      "AD  - Nanyang Technology University, Singapore\n",
      "A2  - Awad G.\n",
      "A2  - Butt A.A.\n",
      "A2  - Curtis K.\n",
      "A2  - Lee Y.\n",
      "A2  - Fiscus J.G.\n",
      "A2  - Godil A.\n",
      "A2  - Joy D.\n",
      "A2  - Delgado A.\n",
      "A2  - Smeaton A.F.\n",
      "A2  - Graham Y.\n",
      "A2  - Kraaij W.\n",
      "A2  - Quenot G.\n",
      "A2  - Magalhaes J.\n",
      "A2  - Semedo D.\n",
      "A2  - Blasi S.G.\n",
      "PB  - National Institute of Standards and Technology (NIST)\n",
      "LA  - English\n",
      "J2  - TREC Video Retr. Eval., TRECVID\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 0; Conference name: 2018 TREC Video Retrieval Evaluation, TRECVID 2018; Conference date: 13 November 2018 through 15 November 2018; Conference code: 159651\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 270 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yan, S.\n",
      "AU  - Xiong, Y.\n",
      "AU  - Lin, D.\n",
      "TI  - Spatial temporal graph convolutional networks for skeleton-based action recognition\n",
      "PY  - 2018\n",
      "T2  - 32nd AAAI Conference on Artificial Intelligence, AAAI 2018\n",
      "SP  - 7444\n",
      "EP  - 7452\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056606832&partnerID=40&md5=5c32eee55a6c7e29859e7e5aeb1d8a5e\n",
      "AD  - Department of Information Engineering, Chinese University of Hong Kong, Hong Kong\n",
      "AB  - Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Convolution\n",
      "KW  - Action recognition\n",
      "KW  - Conventional approach\n",
      "KW  - Convolutional networks\n",
      "KW  - Generalization capability\n",
      "KW  - Human-action recognition\n",
      "KW  - Modeling of dynamics\n",
      "KW  - Spatial and temporal patterns\n",
      "KW  - Spatial temporals\n",
      "KW  - Musculoskeletal system\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735800-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI Conf. Artif. Intell., AAAI\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 3527; Conference name: 32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 2 February 2018 through 7 February 2018; Conference code: 143510\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 271 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Yang, K.\n",
      "AU  - Qiao, P.\n",
      "AU  - Li, D.\n",
      "AU  - Lv, S.\n",
      "AU  - Dou, Y.\n",
      "TI  - Exploring temporal preservation networks for precise temporal action localization\n",
      "PY  - 2018\n",
      "T2  - 32nd AAAI Conference on Artificial Intelligence, AAAI 2018\n",
      "SP  - 7477\n",
      "EP  - 7484\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054865850&partnerID=40&md5=b6d20f8e932a75bad72d3118a1f98eba\n",
      "AD  - National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, China\n",
      "AB  - Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments predetermined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization with minimal loss of time information. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement in both per-frame action prediction and segment-level temporal action localization. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Convolution\n",
      "KW  - Fine granularity\n",
      "KW  - Sliding Window\n",
      "KW  - Spatial resolution\n",
      "KW  - Temporal information\n",
      "KW  - Temporal localization\n",
      "KW  - Temporal resolution\n",
      "KW  - Time information\n",
      "KW  - Video segments\n",
      "KW  - Forecasting\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735800-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI Conf. Artif. Intell., AAAI\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 56; Conference name: 32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 2 February 2018 through 7 February 2018; Conference code: 143510\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 272 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Zhou, L.\n",
      "AU  - Xu, C.\n",
      "AU  - Corso, J.J.\n",
      "TI  - Towards automatic learning of procedures from web instructional videos\n",
      "PY  - 2018\n",
      "T2  - 32nd AAAI Conference on Artificial Intelligence, AAAI 2018\n",
      "SP  - 7590\n",
      "EP  - 7598\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059843845&partnerID=40&md5=aba1b47a1bc848c27abd7b4933cbedb3\n",
      "AD  - Robotics Institute, University of Michigan, United States\n",
      "AD  - Department of CS, University of Rochester, United States\n",
      "AD  - Department of EECS, University of Michigan, United States\n",
      "AB  - The potential for agents, whether embodied or software, to learn by observing other agents performing procedures involving objects and actions is rich. Current research on automatic procedure learning heavily relies on action labels or video subtitles, even during the evaluation phase, which makes them infeasible in real-world scenarios. This leads to our question: can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g., instructional videos from YouTube) with only visual evidence? To answer this question, we introduce the problem of procedure segmentation-to segment a video procedure into category-independent procedure segments. Given that no large-scale dataset is available for this problem, we collect a large-scale procedure segmentation dataset with procedure segments temporally localized and described; we use cooking videos and name the dataset YouCook2. We propose a segment-level recurrent network for generating procedure segments by modeling the dependencies across segments. The generated segments can be used as pre-processing for other tasks, such as dense video captioning and event parsing. We show in our experiments that the proposed model outperforms competitive baselines in procedure segmentation. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Automatic procedures\n",
      "KW  - Automatic-learning\n",
      "KW  - Evaluation phase\n",
      "KW  - Instructional videos\n",
      "KW  - Large-scale dataset\n",
      "KW  - Real-world scenario\n",
      "KW  - Recurrent networks\n",
      "KW  - Visual evidence\n",
      "KW  - Software agents\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735800-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI Conf. Artif. Intell., AAAI\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 517; Conference name: 32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 2 February 2018 through 7 February 2018; Conference code: 143510\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 273 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Huang, J.\n",
      "AU  - Li, N.\n",
      "AU  - Zhang, T.\n",
      "AU  - Li, G.\n",
      "AU  - Huang, T.\n",
      "AU  - Gao, W.\n",
      "TI  - SAP: Self-adaptive proposal model for temporal action detection based on reinforcement learning\n",
      "PY  - 2018\n",
      "T2  - 32nd AAAI Conference on Artificial Intelligence, AAAI 2018\n",
      "SP  - 6951\n",
      "EP  - 6958\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054879163&partnerID=40&md5=0e3094925c531b669cc1533bb2e3290e\n",
      "AD  - School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University, Lishui Road 2199, Nanshan District, Shenzhen, 518055, China\n",
      "AB  - Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose a Self-Adaptive Proposal (SAP) model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at the beginning of the video and traverse the whole video by adopting a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS'14 validate the effectiveness of SAP, which can achieve competitive performance with current action detection algorithms via much fewer proposals. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n",
      "KW  - Artificial intelligence\n",
      "KW  - Deep learning\n",
      "KW  - Learning algorithms\n",
      "KW  - Signal detection\n",
      "KW  - Competitive performance\n",
      "KW  - Computational overheads\n",
      "KW  - Detection algorithm\n",
      "KW  - Feature representation\n",
      "KW  - Human perception\n",
      "KW  - Q-learning algorithms\n",
      "KW  - Temporal pooling\n",
      "KW  - Temporal windows\n",
      "KW  - Reinforcement learning\n",
      "PB  - AAAI press\n",
      "SN  - 978-157735800-8 (ISBN)\n",
      "LA  - English\n",
      "J2  - AAAI Conf. Artif. Intell., AAAI\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 25; Conference name: 32nd AAAI Conference on Artificial Intelligence, AAAI 2018; Conference date: 2 February 2018 through 7 February 2018; Conference code: 143510\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 274 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shapovalova, N.\n",
      "AU  - Raptis, M.\n",
      "AU  - Sigal, L.\n",
      "AU  - Mori, G.\n",
      "TI  - Action is in the eye of the beholder: Eye-gaze driven model for spatio-temporal action localization\n",
      "PY  - 2013\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899004990&partnerID=40&md5=e41cbe3e15bb63db0ebafefaa7e0db6d\n",
      "AD  - Simon Fraser University, Canada\n",
      "AD  - Comcast, Canada\n",
      "AD  - Disney Research, Canada\n",
      "AB  - We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efficiently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.\n",
      "KW  - Benchmark datasets\n",
      "KW  - Classification labels\n",
      "KW  - Context information\n",
      "KW  - Latent models\n",
      "KW  - Learning frameworks\n",
      "KW  - Search Algorithms\n",
      "KW  - Spatio-temporal\n",
      "KW  - Structured learning\n",
      "KW  - Classification (of information)\n",
      "PB  - Neural information processing systems foundation\n",
      "SN  - 10495258 (ISSN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 28; Conference name: 27th Annual Conference on Neural Information Processing Systems, NIPS 2013; Conference date: 5 December 2013 through 10 December 2013; Conference code: 104690\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 275 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Tran, D.\n",
      "AU  - Yuan, J.\n",
      "TI  - Max-margin structured output regression for spatio-temporal action localization\n",
      "PY  - 2012\n",
      "T2  - Advances in Neural Information Processing Systems\n",
      "VL  - 1\n",
      "SP  - 350\n",
      "EP  - 358\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877744900&partnerID=40&md5=9836288a6778d638108315e60596d169\n",
      "AD  - School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore\n",
      "AB  - Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efficient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods.\n",
      "KW  - Benchmark datasets\n",
      "KW  - Learning problem\n",
      "KW  - Object localization\n",
      "KW  - Search method\n",
      "KW  - Spatio-temporal\n",
      "KW  - State-of-the-art methods\n",
      "KW  - Structured learning\n",
      "KW  - Structured video\n",
      "SN  - 10495258 (ISSN); 978-162748003-1 (ISBN)\n",
      "LA  - English\n",
      "J2  - Adv. neural inf. proces. syst.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 52; Conference name: 26th Annual Conference on Neural Information Processing Systems 2012, NIPS 2012; Conference date: 3 December 2012 through 6 December 2012; Conference code: 96883\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 276 without DOI:\n",
      "TY  - CONF\n",
      "AU  - Shamsipour, G.\n",
      "AU  - Shanbehzadeh, J.\n",
      "AU  - Sarrafzadeh, A.\n",
      "TI  - Human Action Recognition by Conceptual Features\n",
      "PY  - 2017\n",
      "T2  - Lecture Notes in Engineering and Computer Science\n",
      "VL  - 2227\n",
      "SP  - 7\n",
      "EP  - 13\n",
      "UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042202648&partnerID=40&md5=b053c8c706b1535a23164b1507a4e689\n",
      "AD  - Department of Computer Engineering, Islamic Azad University of Najafabad, Esfahan, Iran\n",
      "AD  - Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tarbiat Moallem University of Tehran, Tehran, Iran\n",
      "AD  - High Tech Research Network, Unitec Institute of Technology, Auckland, New Zealand\n",
      "AD  - Kharazmi University, Iran\n",
      "AB  - Human action recognition is the process of labeling a video according to human behavior. This process requires a large set of labeled video and analyzing all the frames of a video. The consequence is high computation and memory requirement. This paper solves these problems by focusing on a limited set rather than all the human action and considering the human-object interaction. This paper employs three randomly selected video frames instead of employing all the frames and, Convolutional Neural Network extracts conceptual features and recognize the video objects. Finally, support vector machine determines the relation between these objects and labels the video. The proposed method have been tested on two popular dataset; UCF Sports Action and Olympic Sports. The results show improvements over state-of-the-art algorithms.\n",
      "KW  - Computer Vision\n",
      "KW  - Convolutional neural networks\n",
      "KW  - Human Activity Recognition\n",
      "KW  - Support vector machine\n",
      "KW  - Computer vision\n",
      "KW  - Convolution\n",
      "KW  - Image recognition\n",
      "KW  - Neural networks\n",
      "KW  - Sports\n",
      "KW  - Support vector machines\n",
      "KW  - Convolutional neural network\n",
      "KW  - Human activity recognition\n",
      "KW  - Human behaviors\n",
      "KW  - Human-action recognition\n",
      "KW  - Human-object interaction\n",
      "KW  - Memory requirements\n",
      "KW  - State-of-the-art algorithms\n",
      "KW  - Video objects\n",
      "KW  - Behavioral research\n",
      "A2  - Castillo O.\n",
      "A2  - Korsunsky A.M.\n",
      "A2  - Feng D.D.\n",
      "A2  - Ao S.I.\n",
      "A2  - Douglas C.\n",
      "PB  - Newswood Limited\n",
      "SN  - 20780958 (ISSN); 978-988140473-2 (ISBN)\n",
      "LA  - English\n",
      "J2  - Lect. Notes Eng. Comput. Sci.\n",
      "M3  - Conference paper\n",
      "DB  - Scopus\n",
      "N1  - Export Date: 15 May 2025; Cited By: 2; Conference name: 2017 International MultiConference of Engineers and Computer Scientists, IMECS 2017; Conference date: 15 March 2017 through 17 March 2017; Conference code: 133365\n",
      "ER  -\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 筛选出没有DO的条目\n",
    "def parse_ris_entries(ris_text):\n",
    "    entries = ris_text.strip().split('\\nER  -')\n",
    "    entries = [entry.strip() + '\\nER  -' for entry in entries if entry.strip()]\n",
    "    return entries\n",
    "\n",
    "def filter_entries_without_doi(entries):\n",
    "    no_doi_entries = []\n",
    "    for entry in entries:\n",
    "        if 'DO  -' not in entry:\n",
    "            no_doi_entries.append(entry)\n",
    "    return no_doi_entries\n",
    "\n",
    "# 示例 RIS 内容（可替换为从文件读取）\n",
    "with open(R'D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\scopus\\3892_tad_tal_scopus_20250515.ris', 'r', encoding='utf-8') as f:\n",
    "    ris_content = f.read()\n",
    "\n",
    "# 处理和筛选\n",
    "entries = parse_ris_entries(ris_content)\n",
    "no_doi_entries = filter_entries_without_doi(entries)\n",
    "\n",
    "# 打印结果\n",
    "for i, entry in enumerate(no_doi_entries, 1):\n",
    "    print(f\"Entry {i} without DOI:\\n{entry}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossref DOI found: SA-DETR:Span Aware Detection Transformer for Moment Retrieval -> 10.1016/j.compag.2025.110373\n",
      "Crossref DOI found: VideoQA-TA: Temporal-Aware Multi-Modal Video Question Answering -> 10.1109/tcsvt.2024.3490665\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: 14th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2025 -> 10.48550/arXiv.2502.01379v1\n",
      "arXiv DOI found: 2024 7th International Conference on Mechatronics, Robotics and Automation, ICMRA 2024 -> 10.48550/arXiv.2504.19498v2\n",
      "arXiv DOI found: Micro-gesture Online Recognition using Learnable Query Points -> 10.48550/arXiv.2407.04490v1\n",
      "arXiv DOI found: Development of a versatile and sophisticated benchmark dataset for the detection of pigs in images -> 10.1093/pasj/63.sp2.S613\n",
      "arXiv DOI found: Elderly People's Abnormal Behavior Detection Using HAR and CNN Algorithms -> 10.48550/arXiv.1410.5600v1\n",
      "Crossref DOI found: VideoGLUE: Video General Understanding Evaluation of Foundation Models -> 10.1101/2024.07.30.605655\n",
      "arXiv DOI found: Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation -> 10.48550/arXiv.2405.04405v2\n",
      "arXiv DOI found: Class-Specific Semantic Generation and Reconstruction Learning for Open Set Recognition -> 10.48550/arXiv.1410.4521v1\n",
      "arXiv DOI found: Relabeling Abnormal Videos via Intra-Video Label Propagation for Weakly Supervised Video Anomaly Detection -> 10.48550/arXiv.2311.15367v1\n",
      "Crossref DOI found: VideoPrism: A Foundational Visual Encoder for Video Understanding -> 10.1145/3627673.3680011\n",
      "arXiv DOI found: Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation -> 10.48550/arXiv.2303.16635v1\n",
      "Crossref DOI found: Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis -> 10.24963/ijcai.2024/46\n",
      "arXiv DOI found: Learning Human Action Representations from Temporal Context in Lifestyle Vlogs -> 10.18653/v1/P19-1643\n",
      "arXiv DOI found: 18th European Conference on Computer Vision, ECCV 2024 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: A UNIQUE M-PATTERN FOR MICRO-EXPRESSION SPOTTING IN LONG VIDEOS -> 10.48550/arXiv.1911.01519v2\n",
      "arXiv DOI found: BOUNDARY DENOISING FOR VIDEO ACTIVITY LOCALIZATION -> 10.48550/arXiv.2304.02934v1\n",
      "Crossref DOI found: R-EDL: RELAXING NONESSENTIAL SETTINGS OF EVIDENTIAL DEEP LEARNING -> 10.1145/3664647.3681629\n",
      "arXiv DOI found: STRUCTURED VIDEO-LANGUAGE MODELING WITH TEMPORAL GROUPING AND SPATIAL GROUNDING -> 10.48550/arXiv.2303.16341v3\n",
      "arXiv DOI found: A Fast and High-quality Text-to-Speech Method with Compressed Auxiliary Corpus and Limited Target Speaker Corpus -> 10.48550/arXiv.2111.00320v1\n",
      "Crossref DOI found: AUC-CL: A BATCHSIZE-ROBUST FRAMEWORK FOR SELF-SUPERVISED CONTRASTIVE REPRESENTATION LEARNING -> 10.1007/s10618-024-01006-1\n",
      "arXiv DOI found: Does Video-Text Pretraining Help Open-Vocabulary Online Action Detection? -> 10.48550/arXiv.2207.06010v2\n",
      "Crossref DOI found: FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification -> 10.1109/cvpr52733.2024.01069\n",
      "arXiv DOI found: Streaming Detection of Queried Event Start -> 10.48550/arXiv.2412.03567v1\n",
      "arXiv DOI found: Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network -> 10.48550/arXiv.2312.02224v3\n",
      "arXiv DOI found: Recovering Complete Actions for Cross-dataset Skeleton Action Recognition -> 10.48550/arXiv.2410.23641v1\n",
      "arXiv DOI found: Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection -> 10.48550/arXiv.2501.15271v1\n",
      "Crossref DOI found: MambaTree: Tree Topology is All You Need in State Space Model -> 10.1080/15481603.2024.2380126\n",
      "Crossref DOI found: QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model -> 10.1109/bibm62325.2024.10822789\n",
      "Crossref DOI found: ViLCo-Bench: VIdeo Language COntinual learning Benchmark -> 10.18653/v1/2024.emnlp-main.190\n",
      "arXiv DOI found: Activating Self-Attention for Multi-Scene Absolute Pose Regression -> 10.48550/arXiv.2411.01443v2\n",
      "Crossref DOI found: SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM -> 10.3390/app15010004\n",
      "Crossref DOI found: OnlineTAS: An Online Baseline for Temporal Action Segmentation -> 10.1109/tcsvt.2023.3288878\n",
      "Crossref DOI found: MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding -> 10.1145/3664647.3681394\n",
      "arXiv DOI found: Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New UAV-View Benchmark -> 10.48550/arXiv.2505.07818v1\n",
      "arXiv DOI found: Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey -> 10.48550/arXiv.2305.02176v2\n",
      "Crossref DOI found: ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation -> 10.1145/3700410.3702130\n",
      "Crossref DOI found: VFIMamba: Video Frame Interpolation with State Space Models -> 10.1609/aaai.v38i2.27912\n",
      "Crossref DOI found: VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding -> 10.1109/tmm.2023.3316025\n",
      "arXiv DOI found: E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding -> 10.48550/arXiv.1708.06992v2\n",
      "arXiv DOI found: Exploiting Representation Curvature for Boundary Detection in Time Series -> 10.48550/arXiv.0507450v1\n",
      "Crossref DOI found: AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation -> 10.1109/icsece61636.2024.10729298\n",
      "Crossref DOI found: HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model -> 10.1016/j.eswa.2023.123045\n",
      "Crossref DOI found: MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset -> 10.1109/icassp48485.2024.10445862\n",
      "Crossref DOI found: EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views -> 10.1109/cvpr52733.2024.01541\n",
      "arXiv DOI found: Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models -> 10.48550/arXiv.2410.08611v1\n",
      "arXiv DOI found: Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023 -> 10.48550/arXiv.1810.06018v1\n",
      "Crossref DOI found: Meta-Adapter: An Online Few-shot Learner for Vision-Language Model -> 10.1109/tgrs.2022.3233591\n",
      "Crossref DOI found: NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding -> 10.1109/cvpr52729.2023.01254\n",
      "arXiv DOI found: Temporal RPN Learning for Weakly-Supervised Temporal Action Localization -> 10.48550/arXiv.1708.00042v1\n",
      "Crossref DOI found: Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition -> 10.1109/saner56733.2023.00052\n",
      "arXiv DOI found: On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes -> 10.48550/arXiv.2410.19553v1\n",
      "Crossref DOI found: EgoEnv: Human-centric environment representations from egocentric video -> 10.34293/sijash.v11i2.6651\n",
      "arXiv DOI found: Glance and Focus: Memory Prompting for Multi-Event Video Question Answering -> 10.48550/arXiv.2401.01529v1\n",
      "arXiv DOI found: Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities -> 10.48550/arXiv.2110.07058v3\n",
      "Crossref DOI found: VidChapters-7M: Video Chapters at Scale -> 10.1109/mssc.2022.3217986\n",
      "arXiv DOI found: Mask Propagation for Efficient Video Semantic Segmentation -> 10.48550/arXiv.2310.18954v1\n",
      "Crossref DOI found: NEUCORE: Neural Concept Reasoning for Composed Image Retrieval -> 10.1109/icmew59549.2023.00077\n",
      "Crossref DOI found: Auslan-Daily: Australian Sign Language Translation for Daily Communication and News -> 10.1080/02188791.2023.2219414\n",
      "Crossref DOI found: DBQ-SSD: DYNAMIC BALL QUERY FOR EFFICIENT 3D OBJECT DETECTION -> 10.1007/s11760-023-02726-5\n",
      "Crossref DOI found: RankSEG: A Consistent Ranking-based Framework for Segmentation -> 10.1109/qrs60937.2023.00016\n",
      "Crossref DOI found: AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation -> 10.1109/iccv51070.2023.00106\n",
      "Crossref DOI found: DVSOD: RGB-D Video Salient Object Detection -> 10.1016/j.dsp.2023.104095\n",
      "arXiv DOI found: CONTINUAL TRANSFORMERS: REDUNDANCY-FREE ATTENTION FOR ONLINE INFERENCE -> 10.48550/arXiv.2412.03214v3\n",
      "arXiv DOI found: Spectral Subgraph Localization -> 10.48550/arXiv.2203.11847v2\n",
      "arXiv DOI found: 18th International Symposium on Visual Computing, ISVC 2023 -> 10.4204/EPTCS.282\n",
      "Crossref DOI found: VideoComposer: Compositional Video Synthesis with Motion Controllability -> 10.24963/ijcai.2023/122\n",
      "arXiv DOI found: 18th International Symposium on Visual Computing, ISVC 2023 -> 10.4204/EPTCS.282\n",
      "arXiv DOI found: EXPLORING TEMPORALLY DYNAMIC DATA AUGMENTATION FOR VIDEO RECOGNITION -> 10.48550/arXiv.2206.15015v1\n",
      "arXiv DOI found: Context Consistency Regularization for Label Sparsity in Time Series -> 10.48550/arXiv.2304.07453v1\n",
      "arXiv DOI found: Uncertainty Estimation by Fisher Information-based Evidential Deep Learning -> 10.48550/arXiv.2303.02045v3\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: 23nd Scandinavian Conference on Image Analysis, SCIA 2023 -> 10.1007/978-3-031-31438-4_21\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "Crossref DOI found: TAEC: Unsupervised action segmentation with temporal-Aware embedding and clustering -> 10.1145/3539618.3592081\n",
      "arXiv DOI found: 23nd Scandinavian Conference on Image Analysis, SCIA 2023 -> 10.1007/978-3-031-31438-4_21\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: COMPOSITIONAL PROMPT TUNING WITH MOTION CUES FOR OPEN-VOCABULARY VIDEO RELATION DETECTION -> 10.48550/arXiv.2302.00268v1\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: Memory-efficient Temporal Moment Localization in Long Videos -> 10.48550/arXiv.2412.07157v2\n",
      "arXiv DOI found: 29th International Conference on Neural Information Processing, ICONIP 2022 -> 10.4204/EPTCS.368\n",
      "arXiv DOI found: Recent trends in 2D object detection and applications in video event recognition -> 10.48550/arXiv.2202.03206v1\n",
      "arXiv DOI found: Robust Action Segmentation from Timestamp Supervision -> 10.48550/arXiv.2210.06501v1\n",
      "arXiv DOI found: Is Out-of-Distribution Detection Learnable? -> 10.48550/arXiv.1507.05733v1\n",
      "arXiv DOI found: Efficient Test-Time Model Adaptation without Forgetting -> 10.48550/arXiv.2410.08020v3\n",
      "arXiv DOI found: References -> 10.48550/arXiv.2105.10914v3\n",
      "arXiv DOI found: Probing Visual-Audio Representation for Video Highlight Detection via Hard-Pairs Guided Contrastive Learning -> 10.48550/arXiv.2206.10157v1\n",
      "arXiv DOI found: COHERENCE-BASED LABEL PROPAGATION OVER TIME SERIES FOR ACCELERATED ACTIVE LEARNING -> 10.48550/arXiv.2110.03006v4\n",
      "arXiv DOI found: On Temporal Granularity in Self-Supervised Video Representation Learning -> 10.48550/arXiv.2112.04480v1\n",
      "arXiv DOI found: Two-Stream Transformer Architecture for Long Form Video Understanding -> 10.48550/arXiv.2106.11310v1\n",
      "arXiv DOI found: Learning to Focus on the Foreground for Temporal Sentence Grounding -> 10.48550/arXiv.2305.04123v1\n",
      "arXiv DOI found: Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network -> 10.48550/arXiv.2001.00335v1\n",
      "Crossref DOI found: MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing -> 10.1109/wacv51458.2022.00273\n",
      "Crossref DOI found: ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning -> 10.1117/12.2623448\n",
      "Crossref DOI found: PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points -> 10.1016/j.patcog.2021.108216\n",
      "arXiv DOI found: Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning -> 10.48550/arXiv.2212.00986v2\n",
      "arXiv DOI found: Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs -> 10.48550/arXiv.2406.12925v2\n",
      "arXiv DOI found: Are all Frames Equal? Active Sparse Labeling for Video Action Detection -> 10.48550/arXiv.2210.04331v1\n",
      "arXiv DOI found: An In-depth Study of Stochastic Backpropagation -> 10.48550/arXiv.2210.00129v1\n",
      "arXiv DOI found: Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation -> 10.48550/arXiv.2206.10095v1\n",
      "arXiv DOI found: Semi-Supervised Video Salient Object Detection Based on Uncertainty-Guided Pseudo Labels -> 10.48550/arXiv.1908.04051v2\n",
      "arXiv DOI found: End-to-end Dense Video Captioning as Sequence Generation -> 10.48550/arXiv.2204.08121v2\n",
      "arXiv DOI found: Don't Pour Cereal into Coffee: Differentiable Temporal Logic for Temporal Action Segmentation -> 10.48550/arXiv.2211.12803v2\n",
      "arXiv DOI found: Egocentric Video-Language Pretraining -> 10.48550/arXiv.2206.01670v2\n",
      "arXiv DOI found: Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space -> 10.48550/arXiv.2206.11895v4\n",
      "arXiv DOI found: Video-based Human-Object Interaction Detection from Tubelet Tokens -> 10.48550/arXiv.2206.01908v1\n",
      "arXiv DOI found: Self-Attention Refinement for Actionness Temporal Action Localization -> 10.48550/arXiv.2111.01673v1\n",
      "arXiv DOI found: Research on Special Vehicle Detection and Passenger Elevator Docking Behavior Recognition in Intelligent Monitoring -> 10.1109/ISC253183.2021.9562809\n",
      "arXiv DOI found: Parametric and geometric PDE-based models for automatic image segmentation -> 10.48550/arXiv.2211.08783v1\n",
      "arXiv DOI found: Masked Autoencoders As Spatiotemporal Learners -> 10.48550/arXiv.2205.09113v2\n",
      "arXiv DOI found: Piecewise linear prediction model for action tracking in sports -> 10.48550/arXiv.1806.10270v4\n",
      "arXiv DOI found: Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation -> 10.48550/arXiv.2302.06195v1\n",
      "arXiv DOI found: TADA! TEMPORALLY-ADAPTIVE CONVOLUTIONS FOR VIDEO UNDERSTANDING -> 10.48550/arXiv.2110.06178v4\n",
      "arXiv DOI found: Action recognition application using artificial intelligence for smart social surveillance system -> 10.48550/arXiv.2208.09588v1\n",
      "arXiv DOI found: CROSS-ATTENTIONAL AUDIO-VISUAL FUSION FOR WEAKLY-SUPERVISED ACTION LOCALIZATION -> 10.48550/arXiv.1907.12223v1\n",
      "arXiv DOI found: Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network -> 10.48550/arXiv.1812.03813v2\n",
      "arXiv DOI found: Revisiting spatio-temporal layouts for compositional action recognition -> 10.48550/arXiv.2111.01936v1\n",
      "arXiv DOI found: Fine-grained Multi-Modal Self-Supervised Learning -> 10.48550/arXiv.2410.05323v1\n",
      "arXiv DOI found: Intraoperative Adverse Event Detection in Laparoscopic Surgery: Stabilized Multi-Stage Temporal Convolutional Network with Focal-Uncertainty Loss -> 10.48550/arXiv.2504.16749v1\n",
      "arXiv DOI found: With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition -> 10.48550/arXiv.2111.01024v1\n",
      "arXiv DOI found: Spatiotemporal Deformable Scene Graphs for Complex Activity Detection -> 10.48550/arXiv.2312.07621v1\n",
      "Crossref DOI found: ASFormer: Transformer for Action Segmentation -> 10.1109/iccvw54120.2021.00301\n",
      "arXiv DOI found: Video Action Recognition with Neural Architecture Search -> 10.48550/arXiv.1907.04632v1\n",
      "Crossref DOI found: AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation -> 10.1109/icassp39728.2021.9415101\n",
      "Crossref DOI found: Point3D: tracking actions as moving points with 3D CNNs -> 10.1109/iccv48922.2021.00674\n",
      "Crossref DOI found: CTRN: Class Temporal Relational Network for Action Detection -> 10.1109/iccv48922.2021.00792\n",
      "arXiv DOI found: Unsupervised Discovery of Actions in Instructional Videos -> 10.48550/arXiv.2106.14733v1\n",
      "arXiv DOI found: Conditional Model Selection for Efficient Video Understanding -> 10.48550/arXiv.2104.13400v1\n",
      "arXiv DOI found: Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers -> 10.48550/arXiv.2106.05392v2\n",
      "arXiv DOI found: Foreground Mining via Contrastive Guidance for Weakly Supervised Object Localization -> 10.48550/arXiv.2108.00379v1\n",
      "arXiv DOI found: Low-Fidelity Video Encoder Optimization for Temporal Action Localization -> 10.48550/arXiv.2103.15233v3\n",
      "arXiv DOI found: Few-Shot Temporal Action Localization with Query Adaptive Transformer -> 10.48550/arXiv.2110.10552v1\n",
      "arXiv DOI found: Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing -> 10.48550/arXiv.2401.02138v1\n",
      "arXiv DOI found: MMSys 2021 - Proceedings of the 2021 Multimedia Systems Conference -> 10.48550/arXiv.2107.05297v1\n",
      "Crossref DOI found: MOMA: Multi-Object Multi-Actor Activity Parsing -> 10.1109/iscipt53667.2021.00096\n",
      "arXiv DOI found: Weakly-Supervised Dense Action Anticipation -> 10.48550/arXiv.2309.06130v1\n",
      "arXiv DOI found: Long Short-Term Transformer for Online Action Detection -> 10.48550/arXiv.2107.03377v3\n",
      "arXiv DOI found: Learning Background Suppression Model for Weakly-supervised Temporal Action Localization -> 10.48550/arXiv.2008.13705v1\n",
      "arXiv DOI found: Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos -> 10.48550/arXiv.2110.10596v2\n",
      "arXiv DOI found: Proceedings - 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021 -> 10.1109/CVPR42600.2020.00961\n",
      "arXiv DOI found: 11th International Conference on Image and Graphics, ICIG 2021 -> 10.4204/EPTCS.334\n",
      "arXiv DOI found: Dynamic Grained Encoder for Vision Transformers -> 10.48550/arXiv.2301.03831v1\n",
      "arXiv DOI found: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021 -> 10.48550/arXiv.2004.14619v1\n",
      "Crossref DOI found: QVHIGHLIGHTS: Detecting Moments and Highlights in Videos via Natural Language Queries -> 10.18653/v1/2021.naacl-main.193\n",
      "arXiv DOI found: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021 -> 10.48550/arXiv.2004.14619v1\n",
      "arXiv DOI found: Uncertain process data with probabilistic knowl- edge: Problem characterization and challenges -> 10.48550/arXiv.2106.03324v1\n",
      "arXiv DOI found: 11th International Conference on Image and Graphics, ICIG 2021 -> 10.4204/EPTCS.334\n",
      "arXiv DOI found: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021 -> 10.48550/arXiv.2004.14619v1\n",
      "arXiv DOI found: 11th International Conference on Image and Graphics, ICIG 2021 -> 10.4204/EPTCS.334\n",
      "arXiv DOI found: 4th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2021 -> 10.48550/arXiv.2004.14619v1\n",
      "arXiv DOI found: 15th Asian Conference on Computer Vision, ACCV 2020 -> 10.4204/EPTCS.55\n",
      "arXiv DOI found: Accurate temporal action proposal generation with relation-aware pyramid network -> 10.48550/arXiv.2003.04145v1\n",
      "arXiv DOI found: Graph attention based proposal 3D convnets for action detection -> 10.48550/arXiv.2004.08915v1\n",
      "arXiv DOI found: Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering -> 10.1017/jsl.2024.20\n",
      "arXiv DOI found: Learning 2D temporal adjacent networks for moment localization with natural language -> 10.48550/arXiv.1912.03590v3\n",
      "Crossref DOI found: TSPNet: Hierarchical feature learning via temporal semantic pyramid for sign language translation -> 10.1109/tip.2019.2941267\n",
      "arXiv DOI found: Breaking the curse of space explosion: Towards effcient NAS with curriculum search -> 10.48550/arXiv.2007.07197v2\n",
      "arXiv DOI found: Location-aware graph convolutional networks for video question answering -> 10.48550/arXiv.2008.09105v1\n",
      "arXiv DOI found: Reasoning Step-by-Step: Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network -> 10.1145/3469877.3490595\n",
      "arXiv DOI found: Mid-level Fusion for End-to-End Temporal Activity Detection in Untrimmed Video -> 10.48550/arXiv.2110.00111v1\n",
      "arXiv DOI found: Learning to segment actions from observation and narration -> 10.48550/arXiv.2005.03684v2\n",
      "arXiv DOI found: Self-supervised learning by cross-modal audio-video clustering -> 10.48550/arXiv.2312.08662v1\n",
      "arXiv DOI found: Progressive boundary refinement network for temporal action detection -> 10.48550/arXiv.2308.09268v1\n",
      "arXiv DOI found: A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer -> 10.48550/arXiv.2410.03051v4\n",
      "arXiv DOI found: Relational prototypical network for weakly supervised temporal action localization -> 10.48550/arXiv.1807.10982v1\n",
      "arXiv DOI found: Review of improving healthcare services through human activity recognition -> 10.1016/j.patcog.2014.11.016\n",
      "arXiv DOI found: Refinement of Boundary Regression Using Uncertainty in Temporal Action Localization -> 10.48550/arXiv.2103.13141v1\n",
      "arXiv DOI found: Weakly-supervised video re-localization with multiscale attention model -> 10.48550/arXiv.2409.03890v1\n",
      "arXiv DOI found: Unsupervised Domain Adaptation for Spatio-Temporal Action Localization -> 10.48550/arXiv.2312.13377v4\n",
      "arXiv DOI found: An efficient framework for dense video captioning -> 10.48550/arXiv.2404.08229v1\n",
      "arXiv DOI found: Constrained Video Face Clustering using 1NN Relations -> 10.48550/arXiv.1909.00632v1\n",
      "Crossref DOI found: TEINet: Towards an efficient architecture for video recognition -> 10.1609/aaai.v34i07.6836\n",
      "arXiv DOI found: FASTER recurrent networks for efficient video classification -> 10.48550/arXiv.1906.04226v2\n",
      "arXiv DOI found: Fine-grained dynamic head for object detection -> 10.1609/aaai.v39i12.33445\n",
      "arXiv DOI found: Tripping through time: Efficient Localization of Activities in Videos -> 10.48550/arXiv.1904.09936v5\n",
      "arXiv DOI found: Dense procedure captioning in narrated instructional videos -> 10.48550/arXiv.2311.16444v4\n",
      "arXiv DOI found: Finding action tubes with a sparse-to-dense framework -> 10.48550/arXiv.2008.13196v1\n",
      "arXiv DOI found: Rethinking learnable tree filter for generic feature transform -> 10.48550/arXiv.2012.03482v1\n",
      "arXiv DOI found: Object affordances graph network for action recognition -> 10.48550/arXiv.2309.10426v4\n",
      "arXiv DOI found: A spatiotemporal pre-processing network for activity recognition under rain -> 10.1145/3343031.3350883\n",
      "arXiv DOI found: Multi-instance multi-label action recognition and localization based on spatio-temporal pre-trimming for untrimmed videos -> 10.48550/arXiv.2110.00111v1\n",
      "arXiv DOI found: Anti-Litter Surveillance based on Person Understanding via Multi-Task Learning -> 10.48550/arXiv.2110.04764v5\n",
      "Crossref DOI found: POST: POlicy-based switch tracking -> 10.1609/aaai.v34i07.6899\n",
      "arXiv DOI found: An efficient 3D CNN for action/object segmentation in video -> 10.48550/arXiv.1907.08895v1\n",
      "arXiv DOI found: Mutual suppression network for video prediction using disentangled features -> 10.48550/arXiv.1804.04810v2\n",
      "arXiv DOI found: Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks -> 10.48550/arXiv.1905.02419v2\n",
      "arXiv DOI found: Traffic sign recognition based on up-sampling convolution -> 10.48550/arXiv.2311.06651v1\n",
      "Crossref DOI found: MindSpaces: Art-driven Adaptive Outdoors and Indoors Design -> 10.55630/dipp.2019.9.43\n",
      "arXiv DOI found: Video summarization using adaptive thresholding by machine learning for distributed cloud storage -> 10.48550/arXiv.1908.00812v2\n",
      "arXiv DOI found: Understanding Teacher Gaze Patterns for Robot Learning -> 10.48550/arXiv.1907.07202v4\n",
      "Crossref DOI found: S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Network -> 10.1109/isbi.2019.8759286\n",
      "arXiv DOI found: Why Can't I dance in the mall? learning to mitigate scene bias in action recognition -> 10.48550/arXiv.1912.05534v1\n",
      "arXiv DOI found: Incremental tube construction for human action detection -> 10.48550/arXiv.1704.01358v2\n",
      "arXiv DOI found: 14th Asian Conference on Computer Vision, ACCV 2018 -> 10.4204/EPTCS.266\n",
      "arXiv DOI found: Ntt_cqupt@trecVid2019 actev：activities in extended video -> 10.48550/arXiv.2407.13219v1\n",
      "Crossref DOI found: AHA-3D: A labelled dataset for senior fitness exercise recognition and segmentation from 3D skeletal data -> 10.3390/s19163503\n",
      "arXiv DOI found: 2nd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2019 -> 10.48550/arXiv.1906.03990v2\n",
      "arXiv DOI found: Weakly-supervised temporal localization via occurrence count learning -> 10.48550/arXiv.1905.07293v1\n",
      "arXiv DOI found: Practical action recognition with manifold regularized sparse representations -> 10.48550/arXiv.2408.08202v2\n",
      "arXiv DOI found: Budget-aware activity detection with a recurrent policy network -> 10.48550/arXiv.1712.00097v2\n",
      "arXiv DOI found: Semantic conditioned dynamic modulation for temporal sentence grounding in videos -> 10.48550/arXiv.1910.14303v1\n",
      "arXiv DOI found: Multi-marginal wasserstein GAN -> 10.48550/arXiv.2403.15312v2\n",
      "Crossref DOI found: WSLLN: Weakly supervised natural language localization networks -> 10.18653/v1/d19-1157\n",
      "arXiv DOI found: Action completion: A temporal model for moment detection -> 10.48550/arXiv.2202.04132v1\n",
      "arXiv DOI found: Temporal Gaussian mixture layer for videos -> 10.48550/arXiv.1803.06316v6\n",
      "arXiv DOI found: VireoJD-MM @ TRECVID 2019: Activities in extended video (ACTEV) -> 10.48550/arXiv.2009.09984v1\n",
      "arXiv DOI found: Marginalized average attentional network for weakly-supervised learning -> 10.48550/arXiv.1810.03773v3\n",
      "arXiv DOI found: Human motion parsing by hierarchical dynamic clustering -> 10.48550/arXiv.2302.11306v2\n",
      "arXiv DOI found: Action recognition with the augmented MoCap data using neural data translation -> 10.48550/arXiv.1605.00392v1\n",
      "arXiv DOI found: An online system for real-time activity detection in untrimmed surveillance videos -> 10.48550/arXiv.2203.03796v2\n",
      "arXiv DOI found: Spatio-temporal action detection in untrimmed videos -> 10.48550/arXiv.2110.00111v1\n",
      "arXiv DOI found: Object-centric spatio-temporal activity detection and recognition -> 10.48550/arXiv.1807.00486v3\n",
      "arXiv DOI found: Sequence-to-segments networks for segment detection -> 10.48550/arXiv.1902.00125v1\n",
      "Crossref DOI found: VideocapsuleNet: A simplified network for action detection -> 10.1109/comst.2018.2812641\n",
      "arXiv DOI found: BUPt-MCPRL at TRECVID 2018: AcTEV and INS -> 10.48550/arXiv.2109.01774v2\n",
      "arXiv DOI found: A flexible model for training action localization with varying levels of supervision -> 10.48550/arXiv.1806.11328v2\n",
      "arXiv DOI found: An examination of proposal-based approaches to fine-grained activity detection in untrimmed surveillance videos -> 10.48550/arXiv.2203.03796v2\n",
      "arXiv DOI found: VANT at TRECVID 2018 -> 10.48550/arXiv.1908.07933v1\n",
      "arXiv DOI found: Spatial temporal graph convolutional networks for skeleton-based action recognition -> 10.48550/arXiv.1801.07455v2\n",
      "arXiv DOI found: Exploring temporal preservation networks for precise temporal action localization -> 10.48550/arXiv.1708.03280v2\n",
      "arXiv DOI found: Towards automatic learning of procedures from web instructional videos -> 10.48550/arXiv.1703.09788v3\n",
      "Crossref DOI found: SAP: Self-adaptive proposal model for temporal action detection based on reinforcement learning -> 10.1609/aaai.v32i1.12229\n",
      "arXiv DOI found: Action is in the eye of the beholder: Eye-gaze driven model for spatio-temporal action localization -> 10.48550/arXiv.1512.00795v2\n",
      "arXiv DOI found: Max-margin structured output regression for spatio-temporal action localization -> 10.48550/arXiv.2103.04612v3\n",
      "arXiv DOI found: Human Action Recognition by Conceptual Features -> 10.48550/arXiv.2307.00464v2\n",
      "\n",
      "处理完成。\n",
      "总共处理条目数：3891\n",
      "无DOI且获取失败的条目数：0\n",
      "超时/需重试条目数：0\n",
      "输出文件路径：D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\scopus\\3892_tad_tal_scopus_20250515_updated.ris\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "import time\n",
    "\n",
    "input_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\scopus\\3892_tad_tal_scopus_20250515.ris\"\n",
    "output_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\scopus\\3892_tad_tal_scopus_20250515_updated.ris\"\n",
    "log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\"\n",
    "error_log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo_error.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"TY  -\"):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith(\"ER  -\"):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith(\"TI  -\") and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith(\"AU  -\"):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith(\"PY  -\") and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "        elif line.startswith(\"PB  -\") and 'publisher' not in info:\n",
    "            info['publisher'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    # Encode the title for URL safety and construct the query\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1'\n",
    "    \n",
    "    try:\n",
    "        # Send request to arXiv API\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(data)\n",
    "        \n",
    "        # Define namespaces\n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "        \n",
    "        # Find the first entry\n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract arXiv ID\n",
    "        id_element = entry.find('atom:id', ns)\n",
    "        if id_element is None or not id_element.text:\n",
    "            return None\n",
    "        arxiv_id = id_element.text.split('/')[-1]  # e.g., 2504.13460v3\n",
    "        \n",
    "        # Check for existing DOI\n",
    "        doi_element = entry.find('arxiv:doi', ns)\n",
    "        if doi_element is not None and doi_element.text:\n",
    "            return doi_element.text\n",
    "        \n",
    "        # Construct DOI from arXiv ID\n",
    "        return f\"10.48550/arXiv.{arxiv_id}\"\n",
    "            \n",
    "    except (urllib.error.URLError, ET.ParseError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_crossref(title, authors=None, year=None):\n",
    "    params = {'query.title': title, 'rows': 1}\n",
    "    if year:\n",
    "        params['filter'] = f'from-pub-date:{year},until-pub-date:{year}'\n",
    "    if authors:\n",
    "        params['query.author'] = authors[0]\n",
    "    try:\n",
    "        response = requests.get(\"https://api.crossref.org/works\", params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except ReadTimeout:\n",
    "        raise\n",
    "    except RequestException:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def insert_doi(entry, doi):\n",
    "    for idx, line in enumerate(entry):\n",
    "        if line.startswith(\"ER  -\"):\n",
    "            entry.insert(idx, f\"DO  - {doi}\")\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    error_log = []\n",
    "    missing = []\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, \\\n",
    "         open(log_path, 'w', encoding='utf-8') as flog:\n",
    "\n",
    "        for entry in entries:\n",
    "            # 检查是否已有 DO\n",
    "            has_doi = any(line.startswith('DO  -') for line in entry)\n",
    "            \n",
    "            if has_doi:\n",
    "                # 直接写入已有 DOI 的条目\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "                continue\n",
    "\n",
    "            # 只有没有 DOI 的条目才进行处理\n",
    "            info = extract_key_info(entry)\n",
    "            title = info.get('title', '<无标题>')\n",
    "            authors = info.get('authors', [])\n",
    "            year = info.get('year', '')\n",
    "            publisher = info.get('publisher', '')\n",
    "\n",
    "            doi = None\n",
    "            # 首先尝试 arXiv\n",
    "            if 1:\n",
    "                doi = get_arxiv_doi(title)\n",
    "                if doi:\n",
    "                    print(f\"arXiv DOI found: {title} -> {doi}\")\n",
    "                    insert_doi(entry, doi)\n",
    "                else:\n",
    "                    time.sleep(0.1)  # 遵守 arXiv 请求频率限制\n",
    "\n",
    "            # 如果 arXiv 没找到，尝试 Crossref\n",
    "            if not doi:\n",
    "                try:\n",
    "                    doi = query_crossref(title, authors, year)\n",
    "                    if doi:\n",
    "                        print(f\"Crossref DOI found: {title} -> {doi}\")\n",
    "                        insert_doi(entry, doi)\n",
    "                except ReadTimeout:\n",
    "                    error_log.append(info)\n",
    "                    print(f\"Timeout error for: {title}\")\n",
    "                    flog.write(f\"Timeout: {title} | Authors: {'; '.join(authors)} | Year: {year}\\n\")\n",
    "                    # 即使超时，也写入原条目\n",
    "\n",
    "            # 如果仍然没有 DOI，记录并输出\n",
    "            if not doi:\n",
    "                print(f\"Failed to find DOI for: {title}\")\n",
    "                missing.append(title)\n",
    "                flog.write(f\"Missing: {title} | Authors: {'; '.join(authors)} | Year: {year}\\n\")\n",
    "\n",
    "            # 写入条目（无论是否找到 DOI）\n",
    "            for ln in entry:\n",
    "                fout.write(ln + '\\n')\n",
    "            fout.write('\\n')\n",
    "\n",
    "    # 写入错误日志\n",
    "    if error_log:\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as ef:\n",
    "            for idx, info in enumerate(error_log, 1):\n",
    "                ef.write(f\"{idx}. 标题: {info.get('title', '<无标题>')}\\n\")\n",
    "                ef.write(f\"   作者: {'; '.join(info.get('authors', ['<未知作者>']))}\\n\")\n",
    "                ef.write(f\"   出版年: {info.get('year', '<未知年份>')}\\n---\\n\")\n",
    "\n",
    "    print(\"\\n处理完成。\")\n",
    "    print(f\"总共处理条目数：{len(entries)}\")\n",
    "    print(f\"无DOI且获取失败的条目数：{len(missing)}\")\n",
    "    print(f\"超时/需重试条目数：{len(error_log)}\")\n",
    "    print(f\"输出文件路径：{output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
