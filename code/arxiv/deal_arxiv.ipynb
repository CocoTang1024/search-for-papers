{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功处理 1 个文件，共导出 7 条文献。\n"
     ]
    }
   ],
   "source": [
    "# 1.将SingFile导出的html文件进行解析生成ris文件\n",
    "# 首先鉴于当前的scopus并不支持对于arxiv的批量导出，同时arxiv尽管是预印版，但是有着一些高引的文章，值得参考\n",
    "# 51s 1326篇文献\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 主函数：多线程读取多个文件并合并写入 RIS 文件\n",
    "def main():\n",
    "    html_dir = R\"../../data/arxiv/html\"\n",
    "    ris_output_path = \"../../data/arxiv/html_result/arxiv_results_multi.ris\"\n",
    "    \n",
    "    filenames = [\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：23).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：02).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：37).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：11).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：47).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：20).html\",\n",
    "        # \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：49：05).html\",\n",
    "        R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html\\Scopus - 文献搜索结果 ｜ 已登录 (2025_5_27 17：08：04).html\"\n",
    "    ]\n",
    "\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 写入 RIS\n",
    "    write_ris(all_documents, ris_output_path)\n",
    "    print(f\"已成功处理 {len(filepaths)} 个文件，共导出 {len(all_documents)} 条文献。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1 个 HTML 文件，共提取 200 条文献。\n",
      "发现 193 条重复文献。\n",
      "导出了 7 条不重复文献到 ../../data/arxiv/html_result/unique_arxiv_results.ris。\n"
     ]
    }
   ],
   "source": [
    "# 2.对于每次scopus更新的arxiv多出来的条目列出来，并且生成相应的ris文件\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 解析 RIS 文件，提取条目\n",
    "def parse_ris_file(ris_filepath):\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(ris_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"TY  -\"):\n",
    "                current_entry = {}\n",
    "            elif line.startswith(\"ER  -\"):\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "            elif line:\n",
    "                key, value = map(str.strip, line.split(\" - \", 1))\n",
    "                if key in current_entry:\n",
    "                    if isinstance(current_entry[key], list):\n",
    "                        current_entry[key].append(value)\n",
    "                    else:\n",
    "                        current_entry[key] = [current_entry[key], value]\n",
    "                else:\n",
    "                    current_entry[key] = value\n",
    "    return entries\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 检测重复并返回不重复的文献\n",
    "def deduplicate_documents(new_documents, existing_ris_filepath):\n",
    "    existing_entries = parse_ris_file(existing_ris_filepath)\n",
    "    existing_titles = {entry['TI'].lower() for entry in existing_entries if 'TI' in entry}\n",
    "    \n",
    "    unique_documents = []\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for doc in new_documents:\n",
    "        if doc['title'].lower() not in existing_titles:\n",
    "            unique_documents.append(doc)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    return unique_documents, duplicate_count\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 配置路径（需要用户修改）\n",
    "    html_dir = \"../../data/arxiv/html\"  # HTML 文件目录\n",
    "    existing_ris_filepath = \"../../data/arxiv/html_result/arxiv_results_multi_1647_tad_tal.ris\"  # 现有 RIS 文件\n",
    "    output_ris_filepath = \"../../data/arxiv/html_result/unique_arxiv_results.ris\"  # 输出不重复 RIS 文件\n",
    "    \n",
    "    # HTML 文件列表（示例，用户需替换为实际文件名）\n",
    "    filenames = [\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_21 16：10：18).html\",\n",
    "    ]\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理 HTML 文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 去重\n",
    "    unique_documents, duplicate_count = deduplicate_documents(all_documents, existing_ris_filepath)\n",
    "\n",
    "    # 写入不重复的 RIS 文件\n",
    "    if unique_documents:\n",
    "        write_ris(unique_documents, output_ris_filepath)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"处理了 {len(filepaths)} 个 HTML 文件，共提取 {len(all_documents)} 条文献。\")\n",
    "    print(f\"发现 {duplicate_count} 条重复文献。\")\n",
    "    print(f\"导出了 {len(unique_documents)} 条不重复文献到 {output_ris_filepath}。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成：共 1647 条文献；缺失 DOI 的有 110 条，成功补全 DOI 的有 1537 条。\n",
      "有 50 条因网络超时或错误未处理，将记录至 ../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log 以便后续重试。\n",
      "输出文件：D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal_withdoi.ris，日志文件：../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "\n",
    "# 用户直接输入文件路径\n",
    "input_path = R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal.ris\"\n",
    "output_path = R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_1647_tad_tal_withdoi.ris\"\n",
    "log_path = R\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\"\n",
    "error_log_path = R\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line.startswith('TY  -'):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith('ER  -'):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith('TI  -') and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith('AU  -'):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith('PY  -') and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def query_crossref(title, authors=None, year=None):\n",
    "    params = {'query.title': title, 'rows': 1}\n",
    "    if year:\n",
    "        params['filter'] = f'from-pub-date:{year},until-pub-date:{year}'\n",
    "    if authors:\n",
    "        params['query.author'] = authors[0]\n",
    "    try:\n",
    "        response = requests.get('https://api.crossref.org/works', params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except ReadTimeout:\n",
    "        # 超时，不影响主流程，将在后续重新处理\n",
    "        raise\n",
    "    except RequestException:\n",
    "        # 其他网络错误\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def enrich_entry_with_doi(entry, error_log):\n",
    "    info = extract_key_info(entry)\n",
    "    title = info.get('title')\n",
    "    authors = info.get('authors')\n",
    "    year = info.get('year')\n",
    "    try:\n",
    "        doi = query_crossref(title, authors, year)\n",
    "    except ReadTimeout:\n",
    "        # 记录超时条目以便后续重试\n",
    "        error_log.append(info)\n",
    "        return None\n",
    "    if doi:\n",
    "        for idx, line in enumerate(entry):\n",
    "            if line.startswith('ER  -'):\n",
    "                entry.insert(idx, f'DO  - {doi}')\n",
    "                break\n",
    "    return doi\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    missing_count = 0\n",
    "    enriched_count = 0\n",
    "    error_log = []  # 存放超时或 API 错误需后续处理的条目信息\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, \\\n",
    "         open(log_path, 'w', encoding='utf-8') as flog:\n",
    "\n",
    "        for entry in entries:\n",
    "            if any(line.startswith('DO  -') for line in entry):\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "            else:\n",
    "                doi = enrich_entry_with_doi(entry, error_log)\n",
    "                if doi:\n",
    "                    enriched_count += 1\n",
    "                    for ln in entry:\n",
    "                        fout.write(ln + '\\n')\n",
    "                    fout.write('\\n')\n",
    "                else:\n",
    "                    missing_count += 1\n",
    "                    info = extract_key_info(entry)\n",
    "                    title = info.get('title', '<无标题>')\n",
    "                    authors = '; '.join(info.get('authors', ['<未知作者>']))\n",
    "                    year = info.get('year', '<未知年份>')\n",
    "                    flog.write(f\"{missing_count}. 标题: {title}\\n\")\n",
    "                    flog.write(f\"   作者: {authors}\\n\")\n",
    "                    flog.write(f\"   出版年: {year}\\n\")\n",
    "                    flog.write(f\"   原始条目行数: {len(entry)}\\n\")\n",
    "                    flog.write(\"---\\n\")\n",
    "\n",
    "    # 将需后续重试的条目写入单独日志\n",
    "    if error_log:\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as ef:\n",
    "            for idx, info in enumerate(error_log, 1):\n",
    "                ef.write(f\"{idx}. 标题: {info.get('title','<无标题>')}\\n\")\n",
    "                ef.write(f\"   作者: {'; '.join(info.get('authors', ['<未知作者>']))}\\n\")\n",
    "                ef.write(f\"   出版年: {info.get('year','<未知年份>')}\\n\")\n",
    "                ef.write(\"---\\n\")\n",
    "\n",
    "    print(f\"处理完成：共 {len(entries)} 条文献；缺失 DOI 的有 {missing_count} 条，成功补全 DOI 的有 {enriched_count} 条。\")\n",
    "    if error_log:\n",
    "        print(f\"有 {len(error_log)} 条因网络超时或错误未处理，将记录至 {error_log_path} 以便后续重试。\")\n",
    "    print(f\"输出文件：{output_path}，日志文件：{log_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成：共 4500 条文献；缺失 DOI 的有 806 条，已记录至 ../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.log。输出文件：../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.ris\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "脚本功能：\n",
    "1. 读取一个 .ris 文件，\n",
    "2. 提取所有条目（以 'TY  -' 开始，以 'ER  -' 结束），\n",
    "3. 判断每个条目中是否包含 DOI（以 'DO  -' 开头的行），\n",
    "4. 将包含 DOI 的条目写入新的 .ris 文件，\n",
    "5. 将缺失 DOI 的条目的关键信息（如 TI、AU、PY）记录到日志文件。\n",
    "\n",
    "使用方法：\n",
    "    直接运行脚本，脚本会提示输入文件路径和输出文件路径，无需命令行参数。\n",
    "\"\"\"\n",
    "\n",
    "# 用户直接输入文件路径\n",
    "input_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep.ris\"\n",
    "output_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.ris\"\n",
    "log_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    \"\"\"\n",
    "    将 RIS 文件内容分割成若干条目，每条目是若干行列表。\n",
    "    依据：条目以 'TY  -' 行开始，以 'ER  -' 行结束。\"\"\"\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line.startswith('TY  -'):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith('ER  -'):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    \"\"\"\n",
    "    从条目中提取关键信息，用于日志：\n",
    "    标题 (TI)、作者 (AU)、出版年份 (PY)。\"\"\"\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith('TI  -') and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith('AU  -'):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith('PY  -') and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    missing_count = 0\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, open(log_path, 'w', encoding='utf-8') as flog:\n",
    "        for entry in entries:\n",
    "            has_doi = any(line.startswith('DO  -') for line in entry)\n",
    "            if has_doi:\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "            else:\n",
    "                info = extract_key_info(entry)\n",
    "                title = info.get('title', '<无标题>')\n",
    "                authors = '; '.join(info.get('authors', ['<未知作者>']))\n",
    "                year = info.get('year', '<未知年份>')\n",
    "                flog.write(f\"{missing_count+1}. 标题: {title}\\n\")\n",
    "                flog.write(f\"   作者: {authors}\\n\")\n",
    "                flog.write(f\"   出版年: {year}\\n\")\n",
    "                flog.write(f\"   原始条目行数: {len(entry)}\\n\")\n",
    "                flog.write(\"---\\n\")\n",
    "                missing_count += 1\n",
    "\n",
    "    print(f\"处理完成：共 {len(entries)} 条文献；缺失 DOI 的有 {missing_count} 条，已记录至 {log_path}。输出文件：{output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个主要是对于LNCS已经归档的文献找到其归属的会议\n",
    "# 去除Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)的杂项\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 扩展会议缩写列表\n",
    "abbrs = [\n",
    "    'ECCV', 'ACCV', 'ACPR', 'CGI', 'DAGM-GCPR', 'ICANN', 'ICIAP', 'ICIC',\n",
    "    'ICIG', 'ICIRA', 'ICONIP', 'ICPR', 'ISVC', 'MMM', 'PCM', 'PRCV',\n",
    "    'PRICAI', 'SCIA', 'CAAI', 'NPC', 'ADMA', 'CVM', 'HCC', 'ICSI',\n",
    "    'CCBR', 'ICFEM', 'MICCAI', 'DMAH', 'MICCAI', 'CAAI',\n",
    "    'ICR', 'Euro-Par', 'MLDM', 'IbPRIA', 'ICPRAI',\n",
    "    'ICCSA', 'ICPRAI', 'CAIP', 'ICDAR', 'CICAI', 'BDA'\n",
    "    \n",
    "]\n",
    "\n",
    "# 处理单个 RIS 条目的函数\n",
    "def process_ris_entry(entry: str) -> str:\n",
    "    # 匹配 T2 和 N1 字段\n",
    "    t2_match = re.search(r'^(T2  - .+)$', entry, re.MULTILINE)\n",
    "    n1_match = re.search(r'^N1  - (.*?)$', entry, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    if t2_match and n1_match:\n",
    "        t2_value = t2_match.group(1)\n",
    "        n1_content = n1_match.group(1)\n",
    "\n",
    "        # 仅处理 Lecture Notes in Computer Science 条目\n",
    "        if 'Lecture Notes in Computer Science' in t2_value:\n",
    "            # 在 N1 中寻找缩写和年份，例如 'ECCV 2018'\n",
    "            pattern = r'\\b(' + '|'.join(map(re.escape, abbrs)) + r')\\s+(\\d{4})\\b'\n",
    "            match = re.search(pattern, n1_content)\n",
    "            if match:\n",
    "                abbr = match.group(1)\n",
    "                year = match.group(2)\n",
    "                new_t2 = f'T2  - {abbr} {year}'\n",
    "                # 替换原有 T2 行\n",
    "                entry = re.sub(r'^(T2  - .+)$', new_t2, entry, flags=re.MULTILINE)\n",
    "\n",
    "    return entry\n",
    "\n",
    "# 主处理函数\n",
    "def process_ris_file(input_path: str, output_path: str):\n",
    "    text = Path(input_path).read_text(encoding='utf-8')\n",
    "    # 保留 ER 结束标记并分割条目\n",
    "    raw_entries = re.split(r'\\nER  -\\s*\\n', text.strip(), flags=re.DOTALL)\n",
    "    processed = []\n",
    "\n",
    "    for raw in raw_entries:\n",
    "        entry = raw.strip()\n",
    "        if not entry:\n",
    "            continue\n",
    "        # 恢复 ER 标记，处理后再添加\n",
    "        processed_entry = process_ris_entry(entry + '\\nER  -\\n')\n",
    "        processed.append(processed_entry)\n",
    "\n",
    "    # 写入输出文件\n",
    "    result = '\\n'.join(processed).strip() + '\\n'\n",
    "    Path(output_path).write_text(result, encoding='utf-8')\n",
    "\n",
    "# 命令行执行示例\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Process RIS file, updating T2 based on conference abbreviations.')\n",
    "    # parser.add_argument('input', help='Input RIS file path')\n",
    "    # parser.add_argument('output', help='Output RIS file path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    process_ris_file(R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\all\\20250516_3892_888_1647-6427.ris\", R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\all\\20250516_3892_888_1647-6427_withoutLNCS.ris\")\n",
    "    # print(f\"Processed RIS file saved to D:\\Users\\tang\\Desktop\\LNCS_20250511_303\\LNCS_20250511_303.ris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Output written to D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\all\\20250516_3892_888_1647-6427_withoutLNCS_deplication.ris\n",
      "Log written to deduplication_log_*.txt\n"
     ]
    }
   ],
   "source": [
    "# 3.处理重复的条目，保留最详细的，并且在日志里面输出当前去重过的期刊 二重 去重逻辑 题目 DOI 现在存在bug -和空格区分不开\n",
    "# 20250508最好的条目是arxiv\\data\\20250508_scopus_3837_tad_tal 20250508_wos_886_tad_tal_3891-3_3888_deduplication_end.ris\n",
    "import uuid\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"Normalize title for comparison by removing case and punctuation.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', title.lower()).strip()\n",
    "\n",
    "def parse_ris_file(file_path):\n",
    "    \"\"\"Parse RIS file and return a list of entries.\"\"\"\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == 'ER  -':\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "                    current_entry = {}\n",
    "            elif line:\n",
    "                tag, value = line.split('  - ', 1) if '  - ' in line else (line, '')\n",
    "                current_entry[tag] = current_entry.get(tag, []) + [value]\n",
    "    return entries\n",
    "\n",
    "def count_entry_fields(entry):\n",
    "    \"\"\"Count the number of fields (data lines) in an entry.\"\"\"\n",
    "    return sum(len(values) for values in entry.values())\n",
    "\n",
    "def deduplicate_by_field(entries, field, normalize=False):\n",
    "    \"\"\"Deduplicate entries based on a specified field, keeping the one with most fields.\"\"\"\n",
    "    field_to_entries = defaultdict(list)\n",
    "    for entry in entries:\n",
    "        field_value = entry.get(field, [''])[0]\n",
    "        if field_value:  # Only process entries with the field\n",
    "            key = normalize_title(field_value) if normalize else field_value\n",
    "            field_to_entries[key].append(entry)\n",
    "    \n",
    "    deduplicated = []\n",
    "    log_messages = []\n",
    "    \n",
    "    for key, entries_group in field_to_entries.items():\n",
    "        if len(entries_group) > 1:\n",
    "            # Sort by number of fields (descending) and keep the one with most fields\n",
    "            entries_group.sort(key=count_entry_fields, reverse=True)\n",
    "            kept_entry = entries_group[0]\n",
    "            deduplicated.append(kept_entry)\n",
    "            # Log removed entries\n",
    "            for removed_entry in entries_group[1:]:\n",
    "                log_messages.append(\n",
    "                    f\"Removed duplicate entry with {field} '{key}' \"\n",
    "                    f\"(kept {count_entry_fields(kept_entry)} fields, \"\n",
    "                    f\"removed {count_entry_fields(removed_entry)} fields, \"\n",
    "                    f\"title: '{removed_entry.get('TI', [''])[0]}')\"\n",
    "                )\n",
    "        else:\n",
    "            deduplicated.append(entries_group[0])\n",
    "    \n",
    "    # Add entries that didn't have the field\n",
    "    for entry in entries:\n",
    "        if not entry.get(field, [''])[0]:\n",
    "            deduplicated.append(entry)\n",
    "    \n",
    "    return deduplicated, log_messages\n",
    "\n",
    "def deduplicate_entries(entries):\n",
    "    \"\"\"Deduplicate entries first by TI, then by DO.\"\"\"\n",
    "    # Step 1: Deduplicate by TI\n",
    "    entries, ti_log_messages = deduplicate_by_field(entries, 'TI', normalize=True)\n",
    "    \n",
    "    # Step 2: Deduplicate by DO\n",
    "    entries, do_log_messages = deduplicate_by_field(entries, 'DO', normalize=False)\n",
    "    \n",
    "    return entries, ti_log_messages + do_log_messages\n",
    "\n",
    "def write_ris_file(entries, output_path):\n",
    "    \"\"\"Write deduplicated entries to a new RIS file with a blank line between entries.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for i, entry in enumerate(entries):\n",
    "            for tag, values in entry.items():\n",
    "                for value in values:\n",
    "                    file.write(f\"{tag}  - {value}\\n\")\n",
    "            file.write(\"ER  -\\n\")\n",
    "            if i < len(entries) - 1:  # Add blank line between entries, but not after the last\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging to a file.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        filename=f'deduplication_log_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    setup_logging()\n",
    "    \n",
    "    # Parse RIS file\n",
    "    entries = parse_ris_file(input_file)\n",
    "    \n",
    "    # Deduplicate entries\n",
    "    deduplicated_entries, log_messages = deduplicate_entries(entries)\n",
    "    \n",
    "    # Write to output file\n",
    "    write_ris_file(deduplicated_entries, output_file)\n",
    "    \n",
    "    # Log results\n",
    "    for message in log_messages:\n",
    "        logging.info(message)\n",
    "    \n",
    "    logging.info(f\"Processed {len(entries)} entries, kept {len(deduplicated_entries)} entries\")\n",
    "    print(f\"Deduplication complete. Output written to {output_file}\")\n",
    "    print(f\"Log written to deduplication_log_*.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\all\\20250516_3892_888_1647-6427_withoutLNCS.ris\"  # Replace with your input RIS file path\n",
    "    output_file = R\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\all\\20250516_3892_888_1647-6427_withoutLNCS_deplication.ris\"  # Replace with your desired output RIS file path\n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI: No DOI available for this article.\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    # Encode the title for URL safety and construct the query\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1'\n",
    "    \n",
    "    try:\n",
    "        # Send request to arXiv API\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(data)\n",
    "        \n",
    "        # Define namespaces\n",
    "        ns = {\n",
    "            'atom': 'http://www.w3.org/2005/Atom',\n",
    "            'arxiv': 'http://arxiv.org/schemas/atom'\n",
    "        }\n",
    "        \n",
    "        # Find the first entry\n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return \"No articles found for the given title.\"\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi_element = entry.find('arxiv:doi', ns)\n",
    "        if doi_element is not None and doi_element.text:\n",
    "            return doi_element.text\n",
    "        else:\n",
    "            return \"No DOI available for this article.\"\n",
    "            \n",
    "    except urllib.error.URLError as e:\n",
    "        return f\"Error fetching data: {e}\"\n",
    "    except ET.ParseError:\n",
    "        return \"Error parsing XML response.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get title input from user\n",
    "    title = input(\"Enter the article title: \")\n",
    "    result = get_arxiv_doi(\"Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization\")\n",
    "    print(f\"DOI: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing DOI: Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos\n",
      "Missing DOI: Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature\n",
      "Missing DOI: Self-Supervised Video Action Localization with Adversarial Temporal Transforms\n",
      "Missing DOI: Learning Disentangled Classification and Localization Representations for Temporal Action Localization\n",
      "Missing DOI: Large Receptive Field Boundary Matching Networks for Generating Better Proposals\n",
      "Missing DOI: Temporal Perception and Reasoning in Videos\n",
      "Missing DOI: Deep Learning for Action Understanding in Video\n",
      "Missing DOI: Frequency Selective Augmentation for Video Representation Learning\n",
      "Missing DOI: Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition\n",
      "Missing DOI: Anchor-Free Pipeline Temporal Action Localisation\n",
      "Missing DOI: Visual Representation Learning with Progressive Data Scarcity\n",
      "\n",
      "处理完成。\n",
      "总共处理条目数：887\n",
      "获取失败（无DOI）的条目数：11\n",
      "超时/需重试条目数：45\n",
      "输出文件路径：D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\wos\\889_tad_tal_wos_20250515_888_updatedoi.ris\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "import time\n",
    "\n",
    "input_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\wos\\889_tad_tal_wos_20250515_888.ris\"\n",
    "output_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\wos\\889_tad_tal_wos_20250515_888_updatedoi.ris\"\n",
    "log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\"\n",
    "error_log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo_error.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"TY  -\"):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith(\"ER  -\"):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith(\"TI  -\") and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith(\"AU  -\"):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith(\"PY  -\") and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "        elif line.startswith(\"PB  -\") and 'publisher' not in info:\n",
    "            info['publisher'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    if ':' in title:\n",
    "        title = title.split(':', 1)[1].strip()\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1\"\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        root = ET.fromstring(data)\n",
    "        ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}\n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return None\n",
    "        id_elem = entry.find('atom:id', ns)\n",
    "        if id_elem is None or not id_elem.text:\n",
    "            return None\n",
    "        arxiv_id = id_elem.text.split('/')[-1]\n",
    "        doi_elem = entry.find('arxiv:doi', ns)\n",
    "        if doi_elem is not None and doi_elem.text:\n",
    "            return doi_elem.text\n",
    "        return f\"10.48550/arXiv.{arxiv_id}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_crossref(title, authors=None, year=None):\n",
    "    params = {'query.title': title, 'rows': 1}\n",
    "    if year:\n",
    "        params['filter'] = f'from-pub-date:{year},until-pub-date:{year}'\n",
    "    if authors:\n",
    "        params['query.author'] = authors[0]\n",
    "    try:\n",
    "        response = requests.get(\"https://api.crossref.org/works\", params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except ReadTimeout:\n",
    "        raise\n",
    "    except RequestException:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def insert_doi(entry, doi):\n",
    "    for idx, line in enumerate(entry):\n",
    "        if line.startswith(\"ER  -\"):\n",
    "            entry.insert(idx, f\"DO  - {doi}\")\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    error_log = []\n",
    "    enriched = []\n",
    "    missing = []\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, \\\n",
    "         open(log_path, 'w', encoding='utf-8') as flog:\n",
    "\n",
    "        for entry in entries:\n",
    "            if any(line.startswith('DO  -') for line in entry):\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "                continue\n",
    "\n",
    "            info = extract_key_info(entry)\n",
    "            title = info.get('title', '<无标题>')\n",
    "            authors = info.get('authors', [])\n",
    "            year = info.get('year', '')\n",
    "            publisher = info.get('publisher', '')\n",
    "\n",
    "            doi = None\n",
    "            if publisher.lower() == 'arxiv':\n",
    "                doi = get_arxiv_doi(title)\n",
    "                if doi:\n",
    "                    print(f\"arXiv DOI: {title}\")\n",
    "                    insert_doi(entry, doi)\n",
    "                else:\n",
    "                    time.sleep(0.1)  # 遵守 arXiv 请求频率限制\n",
    "\n",
    "            if not doi:\n",
    "                try:\n",
    "                    doi = query_crossref(title, authors, year)\n",
    "                    if doi:\n",
    "                        print(f\"Crossref DOI: {title}\")\n",
    "                        insert_doi(entry, doi)\n",
    "                except ReadTimeout:\n",
    "                    error_log.append(info)\n",
    "                    continue\n",
    "\n",
    "            if not doi:\n",
    "                print(f\"Missing DOI: {title}\")\n",
    "                missing.append(title)\n",
    "                flog.write(f\"Missing: {title} | Authors: {'; '.join(authors)} | Year: {year}\\n\")\n",
    "\n",
    "            for ln in entry:\n",
    "                fout.write(ln + '\\n')\n",
    "            fout.write('\\n')\n",
    "\n",
    "    if error_log:\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as ef:\n",
    "            for idx, info in enumerate(error_log, 1):\n",
    "                ef.write(f\"{idx}. 标题: {info.get('title', '<无标题>')}\\n\")\n",
    "                ef.write(f\"   作者: {'; '.join(info.get('authors', ['<未知作者>']))}\\n\")\n",
    "                ef.write(f\"   出版年: {info.get('year', '<未知年份>')}\\n---\\n\")\n",
    "\n",
    "    print(\"\\n处理完成。\")\n",
    "    print(f\"总共处理条目数：{len(entries)}\")\n",
    "    print(f\"获取失败（无DOI）的条目数：{len(missing)}\")\n",
    "    print(f\"超时/需重试条目数：{len(error_log)}\")\n",
    "    print(f\"输出文件路径：{output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Gwon, Huiwon\n",
      "AU  - Jo, Hyejeong\n",
      "AU  - Jo, Sunhee\n",
      "AU  - Jung, Chanho\n",
      "TI  - A Bi-directional Information Learning Method Using Reverse Playback Video for Fully Supervised Temporal Action Localization\n",
      "TI  - 완전지도 시간적 행동 검출에서역재생 비디오를 이용한 양방향 정보 학습 방법\n",
      "T2  - Journal of IKEEE\n",
      "T2  - 전기전자학회논문지\n",
      "M3  - research-article\n",
      "AB  - Recently, research on temporal action localization has been actively conducted. In this paper, unlike existingmethods, we propose two approaches for learning bidirectional information by creating reverse playback videosfor fully supervised temporal action localization. One approach involves creating training data by combiningreverse playback videos and forward playback videos, while the other approach involves training separate modelson videos with different playback directions. Experiments were conducted on the THUMOS-14 dataset usingTALLFormer. When using both reverse and forward playback videos as training data, the performance was 5.1%lower than that of the existing method. On the other hand, using a model ensemble shows a 1.9% improvementin performance.\n",
      "PU  - Institute of Korean Electrical and Electronics Engineers\n",
      "SN  - 1226-7244\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "VL  - 28\n",
      "IS  - 2\n",
      "SP  - 145\n",
      "EP  - 149\n",
      "AN  - KJD:ART003097617\n",
      "Y2  - 2024-07-26\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 2 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Vasileiou, Vasiliki, I\n",
      "AU  - Kardaris, Nikolaus\n",
      "AU  - Maragos, Petros\n",
      "A1  - EURASIP\n",
      "TI  - Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos\n",
      "T2  - 29TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO 2021)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 29th European Signal Processing Conference (EUSIPCO)\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Nowadays, the interaction between humans and robots is constantly expanding, requiring more and more human motion recognition applications to operate in real time. However, most works on temporal action detection and recognition perform these tasks in offline manner, i.e. temporally segmented videos are classified as a whole. In this paper, based on the recently proposed framework of Temporal Recurrent Networks, we explore how temporal context and human movement dynamics can be effectively employed for online action detection. Our approach uses various state-of-the-art architectures and appropriately combines the extracted features in order to improve action detection. We evaluate our method on a challenging but widely used dataset for temporal action localization, THUMOS'14. Our experiments show significant improvement over the baseline method, achieving state-of-the art results on THUMOS'14.\n",
      "PU  - EUROPEAN ASSOC SIGNAL SPEECH & IMAGE PROCESSING-EURASIP\n",
      "PI  - KESSARIANI\n",
      "PA  - PO BOX 74251, KESSARIANI, 151 10, GREECE\n",
      "SN  - 2076-1465\n",
      "SN  - 978-9-0827-9706-0\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "SP  - 1431\n",
      "EP  - 1435\n",
      "AN  - WOS:000764066600285\n",
      "Y2  - 2022-04-17\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 3 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Liu, Mengxue\n",
      "AU  - Gao, Xiangjun\n",
      "AU  - Ge, Fangzhen\n",
      "AU  - Liu, Huaiyu\n",
      "AU  - Li, Wenjing\n",
      "ED  - Peng, C\n",
      "ED  - Sun, J\n",
      "TI  - Weakly-Supervised Temporal Action Localization by Background Suppression\n",
      "T2  - 2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 40th Chinese Control Conference (CCC)\n",
      "CL  - Shanghai, PEOPLES R CHINA\n",
      "AB  - We propose a novel method of background suppression to solve the issue that background regions are recognized as actions in weakly-supervised temporal action localization. The general attention-based action localization methods tend to use the attention module to generate segment-level attention weights. But there is little difference between the attentions from the background segments similar to the target actions and the attentions of the action segments, which causes the result that many background segments related to the target actions are still recognized as actions. To address this issue, a weakly-supervised temporal action localization network by background suppression (BS-WTAL) is designed. It introduces a filtering module for suppressing the background features and encouraging the action features, a classification module for identifying action categories and a generative attention module for segment-wise representation modeling. This enables BS-WTAL to suppress background to improve localization performance. Furthermore, we conduct ablation studies from different perspectives. Extensive experiments were performed on two datasets - THUMOS14 and ActivityNet1.2. Our approach shows better performance on these two datasets, even comparable with state-of-the-art fully-supervised methods.\n",
      "PU  - IEEE\n",
      "PI  - NEW YORK\n",
      "PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA\n",
      "SN  - 2161-2927\n",
      "SN  - 978-988-15638-0-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "SP  - 7074\n",
      "EP  - 7081\n",
      "AN  - WOS:000931046707034\n",
      "Y2  - 2021-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 4 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yun, Wulian\n",
      "AU  - Qi, Mengshi\n",
      "AU  - Wang, Chuanming\n",
      "AU  - Ma, Huadong\n",
      "ED  - Wooldridge, M\n",
      "ED  - Dy, J\n",
      "ED  - Natarajan, S\n",
      "TI  - Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature\n",
      "T2  - THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 7\n",
      "M3  - Proceedings Paper\n",
      "CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Vancouver, CANADA\n",
      "AB  - Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods. Our source code is available at https://github.com/wuli55555/ISSF.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - *****************\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "SP  - 6908\n",
      "EP  - 6916\n",
      "AN  - WOS:001239937300046\n",
      "Y2  - 2024-08-15\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 5 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Huang, Linjiang\n",
      "AU  - Huang, Yan\n",
      "AU  - Ouyang, Wanli\n",
      "AU  - Wang, Liang\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Relational Prototypical Network for Weakly Supervised Temporal Action Localization\n",
      "T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - New York, NY\n",
      "AB  - In this paper, we propose a weakly supervised temporal action localization method on untrimmed videos based on prototypical networks. We observe two challenges posed by weakly supervision, namely action-background separation and action relation construction. Unlike the previous method, we propose to achieve action-background separation only by the original videos. To achieve this, a clustering loss is adopted to separate actions from backgrounds and learn intra-compact features, which helps in detecting complete action instances. Besides, a similarity weighting module is devised to further separate actions from backgrounds. To effectively identify actions, we propose to construct relations among actions for prototype learning. A GCN-based prototype embedding module is introduced to generate relational prototypes. Experiments on THUMOS14 and ActivityNet1.2 datasets show that our method outperforms the state-of-the-art methods.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-835-0\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "VL  - 34\n",
      "SP  - 11053\n",
      "EP  - 11060\n",
      "AN  - WOS:000668126803062\n",
      "Y2  - 2020-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 6 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Jo, Hyejeong\n",
      "AU  - Gwon, Huiwon\n",
      "AU  - Jo, Sunhee\n",
      "AU  - Chanho, Jung\n",
      "TI  - A Study on Kernel Size Variations in 1D Convolutional Layer for Single-Frame supervised Temporal Action Localization\n",
      "TI  - 단일 프레임 지도 시간적 행동 지역화에서1D 합성곱 층의 커널 사이즈 변화 연구\n",
      "T2  - Journal of IKEEE\n",
      "T2  - 전기전자학회논문지\n",
      "M3  - research-article\n",
      "AB  - In this paper, we propose variations in the kernel size of 1D convolutional layers for single-frame supervisedtemporal action localization. Building upon the existing method, which utilizes two 1D convolutional layers withkernel sizes of 3 and 1, we introduce an approach that adjusts the kernel sizes of each 1D convolutional layer. Tovalidate the efficiency of our proposed approach, we conducted comparative experiments using the THUMOS’14dataset. Additionally, we use overall video classification accuracy, mAP (mean Average Precision), and AveragemAP as performance metrics for evaluation. According to the experimental results, our proposed approachdemonstrates higher accuracy in terms of mAP and Average mAP compared to the existing method. The methodwith variations in kernel size of 7 and 1 further demonstrates an 8.0% improvement in overall video classificationaccuracy.\n",
      "PU  - Institute of Korean Electrical and Electronics Engineers\n",
      "SN  - 1226-7244\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "VL  - 28\n",
      "IS  - 2\n",
      "SP  - 199\n",
      "EP  - 203\n",
      "AN  - KJD:ART003097624\n",
      "Y2  - 2024-07-26\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 7 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Huang, Jing\n",
      "AU  - Kong, Ming\n",
      "AU  - Chen, Luyuan\n",
      "AU  - Liang, Tian\n",
      "AU  - Zhu, Qiang\n",
      "ED  - Yanikoglu, B\n",
      "ED  - Buntine, W\n",
      "TI  - Temporal RPN Learning for Weakly-Supervised Temporal Action Localization\n",
      "T2  - ASIAN CONFERENCE ON MACHINE LEARNING, VOL 222\n",
      "M3  - Proceedings Paper\n",
      "CP  - 15th Asian Conference on Machine Learning (ACML)\n",
      "CL  - Istanbul, TURKEY\n",
      "AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model's learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: https://github.com/ZJUHJ/TRPN.\n",
      "PU  - JMLR-JOURNAL MACHINE LEARNING RESEARCH\n",
      "PI  - SAN DIEGO\n",
      "PA  - 1269 LAW ST, SAN DIEGO, CA, UNITED STATES\n",
      "SN  - 2640-3498\n",
      "SN  - *****************\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "VL  - 222\n",
      "AN  - WOS:001221095300031\n",
      "Y2  - 2024-08-13\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 8 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Gong, Guoqiang\n",
      "AU  - Zheng, Liangfeng\n",
      "AU  - Jiang, Wenhao\n",
      "AU  - Mu, Yadong\n",
      "ED  - Zhou, ZH\n",
      "TI  - Self-Supervised Video Action Localization with Adversarial Temporal Transforms\n",
      "T2  - PROCEEDINGS OF THE THIRTIETH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2021\n",
      "M3  - Proceedings Paper\n",
      "CP  - 30th International Joint Conference on Artificial Intelligence (IJCAI)\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Weakly-supervised temporal action localization aims to locate intervals of action instances with only video-level action labels for training. However, the localization results generated from video classification networks are often not accurate due to the lack of temporal boundary annotation of actions. Our motivating insight is that the temporal boundary of action should be stably predicted under various temporal transforms. This inspires a self-supervised equivariant transform consistency constraint. We design a set of temporal transform operations, including naive temporal down-sampling to learnable attention-piloted time warping. In our model, a localization network aims to perform well under all transforms, and another policy network is designed to choose a temporal transform at each iteration that adversarially brings localization result inconsistent with the localization network's. Additionally, we devise a self-refine module to enhance the completeness of action intervals harnessing temporal and semantic contexts. Experimental results on THUMOS14 and ActivityNet demonstrate that our model consistently outperforms the state-of-the-art weakly-supervised temporal action localization methods.\n",
      "PU  - IJCAI-INT JOINT CONF ARTIF INTELL\n",
      "PI  - FREIBURG\n",
      "PA  - ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY\n",
      "SN  - 978-0-9992411-9-6\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "SP  - 693\n",
      "EP  - 699\n",
      "AN  - WOS:001202335500096\n",
      "Y2  - 2024-11-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 9 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Lee, Pilhyeon\n",
      "AU  - Uh, Youngjung\n",
      "AU  - Byun, Hyeran\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Background Suppression Network for Weakly-Supervised Temporal Action Localization\n",
      "T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - New York, NY\n",
      "AB  - Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaS-Net) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at https://github.com/Pilhyeon/BaSNet-pytorch.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-835-0\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "VL  - 34\n",
      "SP  - 11320\n",
      "EP  - 11327\n",
      "AN  - WOS:000668126803095\n",
      "Y2  - 2020-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 10 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yu, Jun\n",
      "AU  - Zheng, Yingshuai\n",
      "AU  - Ruan, Shulan\n",
      "AU  - Liu, Qi\n",
      "AU  - Cheng, Zhiyuan\n",
      "AU  - Wu, Jinze\n",
      "ED  - Elkind, E\n",
      "TI  - Actor-Multi-Scale Context Bidirectional Higher Order Interactive Relation Network for Spatial-Temporal Action Localization\n",
      "T2  - PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023\n",
      "M3  - Proceedings Paper\n",
      "CP  - 32nd International Joint Conference on Artificial Intelligence (IJCAI)\n",
      "CL  - Macao, PEOPLES R CHINA\n",
      "AB  - The key to video action detection lies in the understanding of interaction between persons and background objects in a video. Current methods usually employ object detectors to extract objects directly or use grid features to represent objects in the environment, which underestimate the great potential of multi-scale context information (e.g., objects and scenes of different sizes). How to exactly represent the multi-scale context and make full utilization of it still remains an unresolved challenge for spatial-temporal action localization. In this paper, we propose a novel Actor-Multi-Scale Context Bidirectional Higher Order Interactive Relation Network (AMCRNet) that extracts multi-scale context through multiple pooling layers with different sizes. Specifically, we develop an Interactive Relation Extraction Module to model the higher-order relation between the target person and the context (e.g., other persons and objects). Along this line, we further propose a History Feature Bank and Interaction Module to achieve better performance by modeling such relation across continuing video clips. Extensive experimental results on AVA2.2 and UCF101-24 demonstrate the superiority and rationality of our proposed AMCRNet.\n",
      "PU  - IJCAI-INT JOINT CONF ARTIF INTELL\n",
      "PI  - FREIBURG\n",
      "PA  - ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY\n",
      "SN  - 978-1-956792-03-4\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "SP  - 1676\n",
      "EP  - 1685\n",
      "AN  - WOS:001202344201085\n",
      "Y2  - 2024-07-27\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 11 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yang, Zichen\n",
      "AU  - Qin, Jie\n",
      "AU  - Huang, Di\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - ACGNet: Action Complement Graph Network for Weakly-Supervised Temporal Action Localization\n",
      "T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS' 14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-876-3\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "SP  - 3090\n",
      "EP  - 3098\n",
      "AN  - WOS:000893636203020\n",
      "Y2  - 2023-02-17\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 12 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Chen, Guo\n",
      "AU  - Zheng, Yin-Dong\n",
      "AU  - Wang, Limin\n",
      "AU  - Lu, Tong\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - DCAN: Improving Temporal Action Detection via Dual Context Aggregation\n",
      "T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-Fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.1% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-876-3\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "SP  - 248\n",
      "EP  - 257\n",
      "AN  - WOS:000893636200028\n",
      "Y2  - 2023-02-17\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 13 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Gao, Zhanning\n",
      "AU  - Wang, Le\n",
      "AU  - Zhang, Qilin\n",
      "AU  - Niu, Zhenxing\n",
      "AU  - Zheng, Nanning\n",
      "AU  - Hua, Gang\n",
      "A1  - AAAI\n",
      "TI  - Video Imprint Segmentation for Temporal Action Detection in Untrimmed Videos\n",
      "T2  - THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 33rd AAAI Conference on Artificial Intelligence / 31st Innovative Applications of Artificial Intelligence Conference / 9th AAAI Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Honolulu, HI\n",
      "AB  - We propose a temporal action detection by spatial segmentation framework, which simultaneously categorize actions and temporally localize action instances in untrimmed videos. The core idea is the conversion of temporal detection task into a spatial semantic segmentation task. Firstly, the video imprint representation is employed to capture the spatial/temporal interdependences within/among frames and represent them as spatial proximity in a feature space. Subsequently, the obtained imprint representation is spatially segmented by a fully convolutional network. With such segmentation labels projected back to the video space, both temporal action boundary localization and per-frame spatial annotation can be obtained simultaneously. The proposed framework is robust to variable lengths of untrimmed videos, due to the underlying fixed-size imprint representations. The efficacy of the framework is validated in two public action detection datasets.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-809-1\n",
      "DA  - 2019 \n",
      "PY  - 2019\n",
      "SP  - 8328\n",
      "EP  - 8335\n",
      "AN  - WOS:000486572502105\n",
      "Y2  - 2019-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 14 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Lee, Pilhyeon\n",
      "AU  - Wang, Jinglu\n",
      "AU  - Lu, Yan\n",
      "AU  - Byun, Hyeran\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Weakly-supervised Temporal Action Localization by Uncertainty Modeling\n",
      "T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THU-MOS' 14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL- Uncertainty- Modeling.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-866-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 35\n",
      "SP  - 1854\n",
      "EP  - 1862\n",
      "AN  - WOS:000680423501105\n",
      "Y2  - 2021-09-02\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 15 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Liu, Ziyi\n",
      "AU  - Wang, Le\n",
      "AU  - Tang, Wei\n",
      "AU  - Yuan, Junsong\n",
      "AU  - Zheng, Nanning\n",
      "AU  - Hua, Gang\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Weakly Supervised Temporal Action Localization Through Learning Explicit Subspaces for Action and Context\n",
      "T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to localize temporal starts and ends of action instances in a video under only video-level supervision. Existing WS-TAL methods rely on deep features learned for action recognition. However, due to the mismatch between classification and localization, these features cannot distinguish the frequently co-occurring contextual background, i.e., the context, and the actual action instances. We term this challenge action-context confusion, and it will adversely affect the action localization accuracy. To address this challenge, we introduce a framework that learns two feature subspaces respectively for actions and their context. By explicitly accounting for action visual elements, the action instances can be localized more precisely without the distraction from the context. To facilitate the learning of these two feature subspaces with only video-level categorical labels, we leverage the predictions from both spatial and temporal streams for snippets grouping. In addition, an unsupervised learning task is introduced to make the proposed module focus on mining temporal information. The proposed approach outperforms state-of-the-art WS-TAL methods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3 datasets.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-866-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 35\n",
      "SP  - 2242\n",
      "EP  - 2250\n",
      "AN  - WOS:000680423502038\n",
      "Y2  - 2021-09-02\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 16 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Vo, Khoa\n",
      "Z2  -  \n",
      "TI  - Towards Comprehensive and Interpretable Video Understanding\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798302874283\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:121296856\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 17 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Tang, Xiaojun\n",
      "AU  - Fan, Junsong\n",
      "AU  - Luo, Chuanchen\n",
      "AU  - Zhang, Zhaoxiang\n",
      "AU  - Zhang, Man\n",
      "AU  - Yang, Zongyuan\n",
      "A1  - IEEE\n",
      "TI  - DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization\n",
      "T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION, ICCV\n",
      "M3  - Proceedings Paper\n",
      "CP  - IEEE/CVF International Conference on Computer Vision (ICCV)\n",
      "CL  - Paris, FRANCE\n",
      "AB  - Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them omit that ambiguous snippets deliver contradictory information, which would reduce the discriminability of linked snippets. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at https://github. com/XiaojunTang22/ICCV2023-DDGNet.\n",
      "PU  - IEEE COMPUTER SOC\n",
      "PI  - LOS ALAMITOS\n",
      "PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA\n",
      "SN  - 1550-5499\n",
      "SN  - 979-8-3503-0718-4\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "SP  - 6599\n",
      "EP  - 6609\n",
      "AN  - WOS:001159644306082\n",
      "Y2  - 2024-04-14\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 18 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Li, Zhilin\n",
      "AU  - Wang, Zilei\n",
      "AU  - Liu, Qinying\n",
      "ED  - Williams, B\n",
      "ED  - Chen, Y\n",
      "ED  - Neville, J\n",
      "TI  - Actionness Inconsistency-Guided Contrastive Learning for Weakly-Supervised Temporal Action Localization\n",
      "T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 2\n",
      "M3  - Proceedings Paper\n",
      "CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Washington, DC\n",
      "AB  - Weakly-supervised temporal action localization (WTAL) aims to detect action instances given only video-level labels. To address the challenge, recent methods commonly employ a two-branch framework, consisting of a class-aware branch and a class-agnostic branch. In principle, the two branches are supposed to produce the same actionness activation. However, we observe that there are actually many inconsistent activation regions. These inconsistent regions usually contain some challenging segments whose semantic information (action or background) is ambiguous. In this work, we propose a novel Actionness Inconsistency-guided Contrastive Learning (AICL) method which utilizes the consistent segments to boost the representation learning of the inconsistent segments. Specifically, we first define the consistent and inconsistent segments by comparing the predictions of two branches and then construct positive and negative pairs between consistent segments and inconsistent segments for contrastive learning. In addition, to avoid the trivial case where there is no consistent sample, we introduce an action consistency constraint to control the difference between the two branches. We conduct extensive experiments on THUMOS14, ActivityNet v1.2, and ActivityNet v1.3 datasets, and the results show the effectiveness of AICL with state-of-the-art performance. Our code is available at https://github.com/lizhilin-ustc/AAAI2023-AICL.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - *****************\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "SP  - 1513\n",
      "EP  - 1521\n",
      "AN  - WOS:001243761100012\n",
      "Y2  - 2024-08-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 19 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Tan, Jing\n",
      "AU  - Zhao, Xiaotong\n",
      "AU  - Shi, Xintian\n",
      "AU  - Kang, Bin\n",
      "AU  - Wang, Limin\n",
      "ED  - Koyejo, S\n",
      "ED  - Mohamed, S\n",
      "ED  - Agarwal, A\n",
      "ED  - Belgrave, D\n",
      "ED  - Cho, K\n",
      "ED  - Oh, A\n",
      "TI  - PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points\n",
      "T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35 (NEURIPS 2022)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 36th Conference on Neural Information Processing Systems (NeurIPS)\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for finegrained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detectionmAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD.\n",
      "PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)\n",
      "PI  - LA JOLLA\n",
      "PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA\n",
      "SN  - 1049-5258\n",
      "SN  - 978-1-7138-7108-8\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "AN  - WOS:001213811606056\n",
      "Y2  - 2024-11-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 20 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Islam, Ashraful\n",
      "AU  - Long, Chengjiang\n",
      "AU  - Radke, Richard\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization\n",
      "T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THU-MOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-866-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 35\n",
      "SP  - 1637\n",
      "EP  - 1645\n",
      "AN  - WOS:000680423501082\n",
      "Y2  - 2021-09-02\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 21 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Liu, Ziyi\n",
      "AU  - Wang, Le\n",
      "AU  - Zhang, Qilin\n",
      "AU  - Tang, Wei\n",
      "AU  - Yuan, Junsong\n",
      "AU  - Zheng, Nanning\n",
      "AU  - Hua, Gang\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization\n",
      "T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - The object of Weakly-supervised Temporal Action Localization (WS -TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS -TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS -TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground-Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-866-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 35\n",
      "SP  - 2233\n",
      "EP  - 2241\n",
      "AN  - WOS:000680423502037\n",
      "Y2  - 2021-09-02\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 22 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Zhang, Huaxin\n",
      "AU  - Wang, Xiang\n",
      "AU  - Xu, Xiaohao\n",
      "AU  - Qing, Zhiwu\n",
      "AU  - Gao, Changxin\n",
      "AU  - Sang, Nong\n",
      "ED  - Wooldridge, M\n",
      "ED  - Dy, J\n",
      "ED  - Natarajan, S\n",
      "TI  - HR-Pro: Point-Supervised Temporal Action Localization via Hierarchical Reliability Propagation\n",
      "T2  - THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 7\n",
      "M3  - Proceedings Paper\n",
      "CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Vancouver, CANADA\n",
      "AB  - Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HRPro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - *****************\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "SP  - 7115\n",
      "EP  - 7123\n",
      "AN  - WOS:001239937300069\n",
      "Y2  - 2024-08-15\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 23 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Zhu, Zixin\n",
      "AU  - Wang, Le\n",
      "AU  - Tang, Wei\n",
      "AU  - Liu, Ziyi\n",
      "AU  - Zheng, Nanning\n",
      "AU  - Hua, Gang\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Learning Disentangled Classification and Localization Representations for Temporal Action Localization\n",
      "T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - A common approach to Temporal Action Localization (TAL) is to generate action proposals and then perform action classification and localization on them. For each proposal, existing methods universally use a shared proposal-level representation for both tasks. However, our analysis indicates that this shared representation focuses on the most discriminative frames for classification, e.g., \"take-offs\" rather than \"runups\" in distinguishing \"high jump\" and \"long jump\", while frames most relevant to localization, such as the start and end frames of an action, are largely ignored. In other words, such a shared representation can not simultaneously handle both classification and localization tasks well, and it makes precise TAL difficult. To address this challenge, this paper disentangles the shared representation into classification and localization representations. The disentangled classification representation focuses on the most discriminative frames, and the disentangled localization representation focuses on the action phase as well as the action start and end. Our model can be divided into two sub-networks, i.e., the disentanglement network and the context-based aggregation network. The disentanglement network is an autoencoder to learn orthogonal hidden variables of classification and localization. The context-based aggregation network aggregates the classification and localization representations by modeling local and global contexts. We evaluate our proposed method on two popular benchmarks for TAL, which outperforms all state-of-the-art methods.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-876-3\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "SP  - 3644\n",
      "EP  - 3652\n",
      "AN  - WOS:000893636203081\n",
      "Y2  - 2023-02-17\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 24 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Liu, Qinying\n",
      "AU  - Wang, Zilei\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Progressive Boundary Refinement Network for Temporal Action Detection\n",
      "T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - New York, NY\n",
      "AB  - Temporal action detection is a challenging task due to vagueness of action boundaries. To tackle this issue, we propose an end-to-end progressive boundary refinement network (PBRNet) in this paper. PBRNet belongs to the family of one-stage detectors and is equipped with three cascaded detection modules for localizing action boundary more and more precisely. Specifically, PBRNet mainly consists of coarse pyramidal detection, refined pyramidal detection, and fine-grained detection. The first two modules build two feature pyramids to perform the anchor-based detection, and the third one explores the frame-level features to refine the boundaries of each action instance. In the fined-grained detection module, three frame-level classification branches are proposed to augment the frame-level features and update the confidence scores of action instances. Evidently, PBRNet integrates the anchor-based and frame-level methods. We experimentally evaluate the proposed PBRNet and comprehensively investigate the effect of the main components. The results show PBRNet achieves the state-of-the-art detection performances on two popular benchmarks: THUMOS'14 and ActivityNet, and meanwhile possesses a high inference speed.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-835-0\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "VL  - 34\n",
      "SP  - 11612\n",
      "EP  - 11619\n",
      "AN  - WOS:000668126804008\n",
      "Y2  - 2020-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 25 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Xu, Mengmeng\n",
      "AU  - Perez-Rua, Juan-Manuel\n",
      "AU  - Zhu, Xiatian\n",
      "AU  - Ghanem, Bernard\n",
      "AU  - Martinez, Brais\n",
      "ED  - Ranzato, M\n",
      "ED  - Beygelzimer, A\n",
      "ED  - Dauphin, Y\n",
      "ED  - Liang, PS\n",
      "ED  - Vaughan, JW\n",
      "TI  - Low-Fidelity Video Encoder Optimization for Temporal Action Localization\n",
      "T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th Annual Conference on Neural Information Processing Systems (NeurIPS)\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline, first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder - trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical flow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-fi/lofi_action_localization.\n",
      "PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)\n",
      "PI  - LA JOLLA\n",
      "PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA\n",
      "SN  - 1049-5258\n",
      "SN  - *****************\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 34\n",
      "AN  - WOS:000922928207014\n",
      "Y2  - 2021-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 26 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Moniruzzaman, Md.\n",
      "Z2  -  \n",
      "TI  - Human Action Analysis From Cameras and Wearable Sensors: Recognition, Localization, Anticipation, Online Detection, and Pose Estimation\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798302178398\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:120967512\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 27 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Xu, Lei\n",
      "AU  - Liu, Jiexi\n",
      "AU  - Chen, Bo\n",
      "ED  - Peng, C\n",
      "ED  - Sun, J\n",
      "TI  - Large Receptive Field Boundary Matching Networks for Generating Better Proposals\n",
      "T2  - 2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 40th Chinese Control Conference (CCC)\n",
      "CL  - Shanghai, PEOPLES R CHINA\n",
      "AB  - Temporal Action Detection is a research hotspot in Video Understanding. It is necessary not only to give the specific moments of the beginning and end of each action instance in the video, but also to give the category of the action instance. At present, most of the methods in temporal action detection are divided into two steps: The video is divided into a series of video clips to determine whether each clip is an action instance, and the fragments that are not an action instance are deleted; Segments that may be action instances are then classified to get the fmal result. At present, the difficulties in action detection research mainly include the following two points: 1) The boundary is not clear. Different from action recognition, action detection requires precise positioning, but an action in life is often not very defmite. It is also the reason why the mean average precision(mAP) of action detection is low at present. 2) The time span is large. In life, an action often spans a very long scale. Short movements such as waving can last for a few seconds, while long movements such as rock climbing or cycling can last for tens of minutes, which makes it extremely difficult for us to extract proposals. In view of the above difficulties, this paper proposes a large receptive field boundary matching network(LRFBMN) model which takes advantage of the relationship between proposals to improve the accuracy of proposal generation. The model is mainly divided into two parts: 1) Clipping feature map is processed by large kernel convolution, and then proposal feature is generated by ROI-pooling; 2) The proposals are arranged in a certain order to form a fixed graph, and the information exchange between graph nodes is realized by using convolution with dilation. Through experiments, this model is 2.35% higher than baseline and 1.06% higher than state-of-the-art in THUMOS14 data set.\n",
      "PU  - IEEE\n",
      "PI  - NEW YORK\n",
      "PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA\n",
      "SN  - 2161-2927\n",
      "SN  - 978-988-15638-0-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "SP  - 7196\n",
      "EP  - 7200\n",
      "AN  - WOS:000931046707054\n",
      "Y2  - 2021-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 28 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Gao, Jiyang\n",
      "Z2  -  \n",
      "TI  - Temporal Perception and Reasoning in Videos\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9781658407045\n",
      "DA  - 2018 \n",
      "PY  - 2018\n",
      "AN  - PQDT:67825436\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 29 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Shou, Zheng\n",
      "Z2  -  \n",
      "TI  - Deep Learning for Action Understanding in Video\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 978-1-392-07873-0\n",
      "DA  - 2019 \n",
      "PY  - 2019\n",
      "AN  - PQDT:61115043\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 30 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Kim, Jinhyung\n",
      "AU  - Kim, Taeoh\n",
      "AU  - Shim, Minho\n",
      "AU  - Han, Dongyoon\n",
      "AU  - Wee, Dongyoon\n",
      "AU  - Kim, Junmo\n",
      "ED  - Williams, B\n",
      "ED  - Chen, Y\n",
      "ED  - Neville, J\n",
      "TI  - Frequency Selective Augmentation for Video Representation Learning\n",
      "T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 1\n",
      "M3  - Proceedings Paper\n",
      "CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Washington, DC\n",
      "AB  - Recent self-supervised video representation learning methods focus on maximizing the similarity between multiple augmented views from the same video and largely rely on the quality of generated views. However, most existing methods lack a mechanism to prevent representation learning from bias towards static information in the video. In this paper, we propose frequency augmentation (FreqAug), a spatio-temporal data augmentation method in the frequency domain for video representation learning. FreqAug stochastically removes specific frequency components from the video so that learned representation captures essential features more from the remaining information for various downstream tasks. Specifically, FreqAug pushes the model to focus more on dynamic features rather than static features in the video via dropping spatial or temporal low-frequency components. To verify the generality of the proposed method, we experiment with FreqAug on multiple self-supervised learning frameworks along with standard augmentations. Transferring the improved representation to five video action recognition and two temporal action localization downstream tasks shows consistent improvements over baselines.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-880-0\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "SP  - 1124\n",
      "EP  - 1132\n",
      "AN  - WOS:001243759700125\n",
      "Y2  - 2024-09-15\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 31 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Dong, Yiting\n",
      "AU  - Li, Yang\n",
      "AU  - Zhao, Dongcheng\n",
      "AU  - Shen, Guobin\n",
      "AU  - Zeng, Yi\n",
      "ED  - Oh, A\n",
      "ED  - Neumann, T\n",
      "ED  - Globerson, A\n",
      "ED  - Saenko, K\n",
      "ED  - Hardt, M\n",
      "ED  - Levine, S\n",
      "TI  - Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition\n",
      "T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 36 (NEURIPS 2023)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 37th Conference on Neural Information Processing Systems (NeurIPS)\n",
      "CL  - New Orleans, LA\n",
      "AB  - The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment. To address the problem, we leverage Dynamic Vision Sensors (DVS) camera to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.\n",
      "PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)\n",
      "PI  - LA JOLLA\n",
      "PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA\n",
      "SN  - 1049-5258\n",
      "SN  - *****************\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "AN  - WOS:001220600001003\n",
      "Y2  - 2024-07-10\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 32 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Lee, Jiyong\n",
      "Z2  -  \n",
      "TI  - Anchor-Free Pipeline Temporal Action Localisation\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798382637242\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "AN  - PQDT:89145012\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 33 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Sun, Hao\n",
      "Z2  -  \n",
      "TI  - Human Behavior Understanding and Intention Prediction\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798834023586\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "AN  - PQDT:68545795\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 34 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yang, Haosen\n",
      "AU  - Wu, Wenhao\n",
      "AU  - Wang, Lining\n",
      "AU  - Jin, Sheng\n",
      "AU  - Xia, Boyang\n",
      "AU  - Yao, Hongxun\n",
      "AU  - Huang, Hujie\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Temporal Action Proposal Generation with Background Constraint\n",
      "T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (e.g., BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, i.e., ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-876-3\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "SP  - 3054\n",
      "EP  - 3062\n",
      "AN  - WOS:000893636203016\n",
      "Y2  - 2023-02-17\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 35 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yu, Zefang\n",
      "AU  - Xie, Mingye\n",
      "AU  - Gao, Jingsheng\n",
      "AU  - Liu, Ting\n",
      "AU  - Fu, Yuzhuo\n",
      "ED  - Wooldridge, M\n",
      "ED  - Dy, J\n",
      "ED  - Natarajan, S\n",
      "TI  - From Raw Video to Pedagogical Insights: A Unified Framework for Student Behavior Analysis\n",
      "T2  - THIRTY-EIGTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 21\n",
      "M3  - Proceedings Paper\n",
      "CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Vancouver, CANADA\n",
      "AB  - Understanding student behavior in educational settings is critical in improving both the quality of pedagogy and the level of student engagement. While various AI-based models exist for classroom analysis, they tend to specialize in limited tasks and lack generalizability across diverse educational environments. Additionally, these models often fall short in ensuring student privacy and in providing actionable insights accessible to educators. To bridge this gap, we introduce a unified, end-to-end framework by leveraging temporal action detection techniques and advanced large language models for a more nuanced student behavior analysis. Our proposed framework provides an end-to-end pipeline that starts with raw classroom video footage and culminates in the autonomous generation of pedagogical reports. It offers a comprehensive and scalable solution for student behavior analysis. Experimental validation confirms the capability of our framework to accurately identify student behaviors and to produce pedagogically meaningful insights, thereby setting the stage for future AI-assisted educational assessments.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - *****************\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "SP  - 23241\n",
      "EP  - 23249\n",
      "AN  - WOS:001239989100067\n",
      "Y2  - 2024-08-25\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 36 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Choi, Jinwoo\n",
      "AU  - Gao, Chen\n",
      "AU  - Messou, Joseph C. E.\n",
      "AU  - Huang, Jia-Bin\n",
      "ED  - Wallach, H\n",
      "ED  - Larochelle, H\n",
      "ED  - Beygelzimer, A\n",
      "ED  - d'Alche-Buc, F\n",
      "ED  - Fox, E\n",
      "ED  - Garnett, R\n",
      "TI  - Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition\n",
      "T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)\n",
      "M3  - Proceedings Paper\n",
      "CP  - 33rd Conference on Neural Information Processing Systems (NeurIPS)\n",
      "CL  - Vancouver, CANADA\n",
      "AB  - Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.\n",
      "PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)\n",
      "PI  - LA JOLLA\n",
      "PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA\n",
      "SN  - 1049-5258\n",
      "SN  - *****************\n",
      "DA  - 2019 \n",
      "PY  - 2019\n",
      "VL  - 32\n",
      "AN  - WOS:000534424300077\n",
      "Y2  - 2019-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 37 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Su, Haisheng\n",
      "AU  - Gan, Weihao\n",
      "AU  - Wu, Wei\n",
      "AU  - Qiao, Yu\n",
      "AU  - Yan, Junjie\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - BSN plus plus : Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation\n",
      "T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - ELECTR NETWORK\n",
      "AB  - Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-866-4\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "VL  - 35\n",
      "SP  - 2602\n",
      "EP  - 2610\n",
      "AN  - WOS:000680423502078\n",
      "Y2  - 2021-01-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 38 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Wang, Danxu\n",
      "Z2  -  \n",
      "TI  - Automated Measurement of the Water Drop Penetration Time for the Analysis of Soil Water Repellency\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798384438854\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:100655799\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 39 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Xie, Ting-Ting\n",
      "Z2  -  \n",
      "TI  - Learning to Localize the Temporal Extent of Human Activities in Videos\n",
      "M3  - Dissertation/Thesis\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "AN  - PQDT:81837698\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 40 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Li, Yuxi\n",
      "AU  - Lin, Weiyao\n",
      "AU  - Wang, Tao\n",
      "AU  - See, John\n",
      "AU  - Qian, Rui\n",
      "AU  - Xu, Ning\n",
      "AU  - Wang, Limin\n",
      "AU  - Xu, Shugong\n",
      "A1  - Assoc Advancement Artificial Intelligence\n",
      "TI  - Finding Action Tubes with a Sparse-to-Dense Framework\n",
      "T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE\n",
      "M3  - Proceedings Paper\n",
      "CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - New York, NY\n",
      "AB  - The task of spatial-temporal action detection has attracted increasing attention among researchers. Existing dominant methods solve this problem by relying on short-term information and dense serial-wise detection on each individual frames or clips. Despite their effectiveness, these methods showed inadequate use of long-term information and are prone to inefficiency. In this paper, we propose for the first time, an efficient framework that generates action tube proposals from video streams with a single forward pass in a sparse-to-dense manner. There are two key characteristics in this framework: (1) Both long-term and short-term sampled information are explicitly utilized in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS) is designed to effectively approximate the tube output while keeping the system tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and UCFSports benchmark datasets, achieving promising results that are competitive to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our framework about 7.6 times more efficient than the nearest competitor.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - 978-1-57735-835-0\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "VL  - 34\n",
      "SP  - 11466\n",
      "EP  - 11473\n",
      "AN  - WOS:000668126803113\n",
      "Y2  - 2021-08-18\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 41 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Cheng, Feng\n",
      "Z2  -  \n",
      "TI  - Efficient Unimodal and Multimodal Video Understanding\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798346872443\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:120143203\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 42 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Yu, Qing\n",
      "AU  - Fujiwara, Kent\n",
      "ED  - Williams, B\n",
      "ED  - Chen, Y\n",
      "ED  - Neville, J\n",
      "TI  - Frame-Level Label Refinement for Skeleton-Based Weakly-Supervised Action Recognition\n",
      "T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 3\n",
      "M3  - Proceedings Paper\n",
      "CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence\n",
      "CL  - Washington, DC\n",
      "AB  - In recent years, skeleton-based action recognition has achieved remarkable performance in understanding human motion from sequences of skeleton data, which is an important medium for synthesizing realistic human movement in various applications. However, existing methods assume that each action clip is manually trimmed to contain one specific action, which requires a significant amount of effort for an-notation. To solve this problem, we consider a novel problem of skeleton-based weakly-supervised temporal action localization (S-WTAL), where we need to recognize and localize human action segments in untrimmed skeleton videos given only the video-level labels. Although this task is challenging due to the sparsity of skeleton data and the lack of contextual clues from interaction with other objects and the environment, we present a frame-level label refinement frame-work based on a spatio-temporal graph convolutional network (ST-GCN) to overcome these difficulties. We use multiple instance learning (MIL) with video-level labels to generate the frame-level predictions. Inspired by advances in handling the noisy label problem, we introduce a label cleaning strategy of the frame-level pseudo labels to guide the learning pro-cess. The network parameters and the frame-level predictions are alternately updated to obtain the final results. We extensively evaluate the effectiveness of our learning approach on skeleton-based action recognition benchmarks. The state-of-the-art experimental results demonstrate that the proposed method can recognize and localize action segments of the skeleton data.\n",
      "PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE\n",
      "PI  - PALO ALTO\n",
      "PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA\n",
      "SN  - 2159-5399\n",
      "SN  - 2374-3468\n",
      "SN  - *****************\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "SP  - 3322\n",
      "EP  - 3330\n",
      "AN  - WOS:001243762100076\n",
      "Y2  - 2024-10-02\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 43 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Vaishnavi, Pratik\n",
      "Z2  -  \n",
      "TI  - Generating Temporal Action Proposals in Long Untrimmed Videos\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 978-0-438-20699-1\n",
      "DA  - 2018 \n",
      "PY  - 2018\n",
      "AN  - PQDT:60870160\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 44 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Bency, Archith John\n",
      "Z2  -  \n",
      "TI  - Search Strategies for Localization in Images and Videos\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 978-0-438-89659-8\n",
      "DA  - 2018 \n",
      "PY  - 2018\n",
      "AN  - PQDT:68776511\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 45 without DOI:\n",
      "TY  - CPAPER\n",
      "AU  - Wang, Shuoping\n",
      "AU  - Xiao, Youan\n",
      "AU  - Wang, Tengfei\n",
      "AU  - Li, Zhuo\n",
      "A1  - AMER SOC MECHANICAL ENGINEERS\n",
      "TI  - THE REAL-TIME HUMAN RELIABILITY DETECTION SYSTEM BASED ON SHIP BRIDGE VIDEOS\n",
      "T2  - PROCEEDINGS OF ASME 2022 INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, IMECE2022, VOL 9\n",
      "M3  - Proceedings Paper\n",
      "CP  - ASME International Mechanical Engineering Congress and Exposition (IMECE)\n",
      "CL  - Columbus, OH\n",
      "AB  - Human reliability analysis in assessment of ship collision risk has always been a concern for shipping practitioners. The risk of ship collision is largely related to the driving state of crew. At the same time, although surveillance cameras have been deployed in key locations such as ship bridge on most of the current operating ships, the monitoring information is only used as evidence for daily evaluation or responsibility determination after an accident, further values are not discovered. How to make full use of existing ship bridge video resources to assess the real-time state of crew, and then assess the risk of collisions, is of great significance and academic value.Therefore, this paper proposes a real-time human reliability detection system based on ship bridge videos. Object detection model trained on COCO[1] dataset using Faster-RCNN [2] algorithm and spatio-temporal action detection model trained on AVA[3] dataset using SlowFast[4] algorithm are used to perform video understanding work on ship bridge videos, obtaining the actions of crew and corresponding probabilities. This paper introduces performance impact factors (PIFs) and Information, Decision and Action in a Crew content cognitive (IDAC) model to build a three-layer mapping model which clarifies how to use the results of video understanding work to calculate relevant PIF scores and explain the impact of the state of crew on various processes in IDAC model.\n",
      "PU  - AMER SOC MECHANICAL ENGINEERS\n",
      "PI  - NEW YORK\n",
      "PA  - THREE PARK AVENUE, NEW YORK, NY 10016-5990 USA\n",
      "SN  - 978-0-7918-8671-7\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "AN  - WOS:001215443800103\n",
      "Y2  - 2024-11-01\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 46 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Agarwal, Nakul\n",
      "Z2  -  \n",
      "TI  - Learning to Recognise Objects and Actions for Intelligent Agents\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9781687947031\n",
      "DA  - 2019 \n",
      "PY  - 2019\n",
      "AN  - PQDT:68749550\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 47 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Ignat, Oana\n",
      "Z2  -  \n",
      "TI  - Towards Human Action Understanding in Social Media Videos Using Multimodal Models\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798845465511\n",
      "DA  - 2022 \n",
      "PY  - 2022\n",
      "AN  - PQDT:68525083\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 48 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Tang, Yue\n",
      "Z2  -  \n",
      "TI  - Efficient Hardware and Software Design for On-Device Learning of Video Streams\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798310324022\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:123174130\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 49 without DOI:\n",
      "TY  - JOUR\n",
      "AU  - Kalogeiton, Vasiliki\n",
      "Z2  -  \n",
      "TI  - Localizing spatially and temporally objects and actions in videos\n",
      "M3  - Dissertation/Thesis\n",
      "DA  - 2018 \n",
      "PY  - 2018\n",
      "AN  - PQDT:67496094\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 50 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Hwang, Pin-Jui\n",
      "Z2  -  \n",
      "TI  - Vision-Based Learning From Demonstration and Collaborative Robotic Systems\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798342736862\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "AN  - PQDT:119727417\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 51 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Yang, Xitong\n",
      "Z2  -  \n",
      "TI  - Long-Term Temporal Modeling for Video Action Understanding\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798460452743\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "AN  - PQDT:64557566\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 52 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Yang, Hongtao\n",
      "Z2  -  \n",
      "TI  - Visual Representation Learning with Progressive Data Scarcity\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798684618987\n",
      "DA  - 2020 \n",
      "PY  - 2020\n",
      "AN  - PQDT:66916516\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 53 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Truong, Quang Sang\n",
      "Z2  -  \n",
      "TI  - Towards Multi-Modal Interpretable Video Understanding\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798381193367\n",
      "DA  - 2023 \n",
      "PY  - 2023\n",
      "AN  - PQDT:87150088\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 54 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Asnani, Vishal\n",
      "Z2  -  \n",
      "TI  - Proactive Schemes: Adversarial Attacks for Social Good\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798310195790\n",
      "DA  - 2025 \n",
      "PY  - 2025\n",
      "AN  - PQDT:122820913\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 55 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Monteiro, Carlos Filipe Batista Cardoso\n",
      "Z2  -  \n",
      "TI  - Spatio-Temporal Action Localization with Deep Learning\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798381495973\n",
      "DA  - 2021 \n",
      "PY  - 2021\n",
      "AN  - PQDT:87640062\n",
      "ER  -\n",
      "----------------------------------------\n",
      "Entry 56 without DOI:\n",
      "TY  - BOOK\n",
      "AU  - Wang, Xijun\n",
      "Z2  -  \n",
      "TI  - Deep Learning Applied to Image and Video Processing\n",
      "M3  - Dissertation/Thesis\n",
      "SN  - 9798381977776\n",
      "DA  - 2024 \n",
      "PY  - 2024\n",
      "AN  - PQDT:88341751\n",
      "ER  -\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 筛选出没有DO的条目\n",
    "def parse_ris_entries(ris_text):\n",
    "    entries = ris_text.strip().split('\\nER  -')\n",
    "    entries = [entry.strip() + '\\nER  -' for entry in entries if entry.strip()]\n",
    "    return entries\n",
    "\n",
    "def filter_entries_without_doi(entries):\n",
    "    no_doi_entries = []\n",
    "    for entry in entries:\n",
    "        if 'DO  -' not in entry:\n",
    "            no_doi_entries.append(entry)\n",
    "    return no_doi_entries\n",
    "\n",
    "# 示例 RIS 内容（可替换为从文件读取）\n",
    "with open(R'D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\wos\\889_tad_tal_wos_20250515_888.ris', 'r', encoding='utf-8') as f:\n",
    "    ris_content = f.read()\n",
    "\n",
    "# 处理和筛选\n",
    "entries = parse_ris_entries(ris_content)\n",
    "no_doi_entries = filter_entries_without_doi(entries)\n",
    "\n",
    "# 打印结果\n",
    "for i, entry in enumerate(no_doi_entries, 1):\n",
    "    print(f\"Entry {i} without DOI:\\n{entry}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv DOI found: Learning Streaming Video Representation via Multitask Training -> 10.48550/arXiv.2504.20041v1\n",
      "arXiv DOI found: Live: Learning Video LLM with Streaming Speech Transcription at Scale -> 10.48550/arXiv.2504.16030v1\n",
      "arXiv DOI found: Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer -> 10.48550/arXiv.2504.14860v1\n",
      "arXiv DOI found: Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection -> 10.48550/arXiv.2408.16990v2\n",
      "arXiv DOI found: HDBFormer: Efficient RGB-D Semantic Segmentation with A Heterogeneous Dual-Branch Framework -> 10.1109/LSP.2024.3496588\n",
      "Timeout error for: Msvt: Multi-Grained Spatial and Vmamba Temporal for Few-Shot Action Recognition\n",
      "Failed to find DOI for: Msvt: Multi-Grained Spatial and Vmamba Temporal for Few-Shot Action Recognition\n",
      "Timeout error for: F3SET: TOWARDS ANALYZING FAST, FREQUENT, AND FINE-GRAINED EVENTS FROM VIDEOS\n",
      "Failed to find DOI for: F3SET: TOWARDS ANALYZING FAST, FREQUENT, AND FINE-GRAINED EVENTS FROM VIDEOS\n",
      "\n",
      "处理完成。\n",
      "总共处理条目数：7\n",
      "无DOI且获取失败的条目数：2\n",
      "超时/需重试条目数：2\n",
      "输出文件路径：D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_withdoi.ris\n"
     ]
    }
   ],
   "source": [
    "# 查找完整的DOI号的完整代码\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "import time\n",
    "\n",
    "input_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi.ris\"\n",
    "output_path = r\"D:\\Programs\\Codes\\Skill-Up\\search-for-papers\\data\\arxiv\\html_result\\arxiv_results_multi_withdoi.ris\"\n",
    "log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo.log\"\n",
    "error_log_path = r\"../../data/all/arxiv_results_multi_1647_tad_tal_withdo_error.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"TY  -\"):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith(\"ER  -\"):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith(\"TI  -\") and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith(\"AU  -\"):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith(\"PY  -\") and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "        elif line.startswith(\"PB  -\") and 'publisher' not in info:\n",
    "            info['publisher'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_arxiv_doi(title):\n",
    "    if ':' in title:\n",
    "        title = title.split(':', 1)[1].strip()\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=ti:{encoded_title}&max_results=1\"\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read()\n",
    "        root = ET.fromstring(data)\n",
    "        ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}\n",
    "        entry = root.find('atom:entry', ns)\n",
    "        if entry is None:\n",
    "            return None\n",
    "        id_elem = entry.find('atom:id', ns)\n",
    "        if id_elem is None or not id_elem.text:\n",
    "            return None\n",
    "        arxiv_id = id_elem.text.split('/')[-1]\n",
    "        doi_elem = entry.find('arxiv:doi', ns)\n",
    "        if doi_elem is not None and doi_elem.text:\n",
    "            return doi_elem.text\n",
    "        return f\"10.48550/arXiv.{arxiv_id}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_crossref(title, authors=None, year=None):\n",
    "    params = {'query.title': title, 'rows': 1}\n",
    "    if year:\n",
    "        params['filter'] = f'from-pub-date:{year},until-pub-date:{year}'\n",
    "    if authors:\n",
    "        params['query.author'] = authors[0]\n",
    "    try:\n",
    "        response = requests.get(\"https://api.crossref.org/works\", params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except ReadTimeout:\n",
    "        raise\n",
    "    except RequestException:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def insert_doi(entry, doi):\n",
    "    for idx, line in enumerate(entry):\n",
    "        if line.startswith(\"ER  -\"):\n",
    "            entry.insert(idx, f\"DO  - {doi}\")\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    error_log = []\n",
    "    missing = []\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, \\\n",
    "         open(log_path, 'w', encoding='utf-8') as flog:\n",
    "\n",
    "        for entry in entries:\n",
    "            # 检查是否已有 DO\n",
    "            has_doi = any(line.startswith('DO  -') for line in entry)\n",
    "            \n",
    "            if has_doi:\n",
    "                # 直接写入已有 DOI 的条目\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "                continue\n",
    "\n",
    "            # 只有没有 DOI 的条目才进行处理\n",
    "            info = extract_key_info(entry)\n",
    "            title = info.get('title', '<无标题>')\n",
    "            authors = info.get('authors', [])\n",
    "            year = info.get('year', '')\n",
    "            publisher = info.get('publisher', '')\n",
    "\n",
    "            doi = None\n",
    "            # 首先尝试 arXiv\n",
    "            if publisher.lower() == 'arxiv':\n",
    "                doi = get_arxiv_doi(title)\n",
    "                if doi:\n",
    "                    print(f\"arXiv DOI found: {title} -> {doi}\")\n",
    "                    insert_doi(entry, doi)\n",
    "                else:\n",
    "                    time.sleep(0.1)  # 遵守 arXiv 请求频率限制\n",
    "\n",
    "            # 如果 arXiv 没找到，尝试 Crossref\n",
    "            if not doi:\n",
    "                try:\n",
    "                    doi = query_crossref(title, authors, year)\n",
    "                    if doi:\n",
    "                        print(f\"Crossref DOI found: {title} -> {doi}\")\n",
    "                        insert_doi(entry, doi)\n",
    "                except ReadTimeout:\n",
    "                    error_log.append(info)\n",
    "                    print(f\"Timeout error for: {title}\")\n",
    "                    flog.write(f\"Timeout: {title} | Authors: {'; '.join(authors)} | Year: {year}\\n\")\n",
    "                    # 即使超时，也写入原条目\n",
    "\n",
    "            # 如果仍然没有 DOI，记录并输出\n",
    "            if not doi:\n",
    "                print(f\"Failed to find DOI for: {title}\")\n",
    "                missing.append(title)\n",
    "                flog.write(f\"Missing: {title} | Authors: {'; '.join(authors)} | Year: {year}\\n\")\n",
    "\n",
    "            # 写入条目（无论是否找到 DOI）\n",
    "            for ln in entry:\n",
    "                fout.write(ln + '\\n')\n",
    "            fout.write('\\n')\n",
    "\n",
    "    # 写入错误日志\n",
    "    if error_log:\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as ef:\n",
    "            for idx, info in enumerate(error_log, 1):\n",
    "                ef.write(f\"{idx}. 标题: {info.get('title', '<无标题>')}\\n\")\n",
    "                ef.write(f\"   作者: {'; '.join(info.get('authors', ['<未知作者>']))}\\n\")\n",
    "                ef.write(f\"   出版年: {info.get('year', '<未知年份>')}\\n---\\n\")\n",
    "\n",
    "    print(\"\\n处理完成。\")\n",
    "    print(f\"总共处理条目数：{len(entries)}\")\n",
    "    print(f\"无DOI且获取失败的条目数：{len(missing)}\")\n",
    "    print(f\"超时/需重试条目数：{len(error_log)}\")\n",
    "    print(f\"输出文件路径：{output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
