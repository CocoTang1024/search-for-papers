{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功处理 7 个文件，共导出 1326 条文献。\n"
     ]
    }
   ],
   "source": [
    "# 1.将SingFile导出的html文件进行解析生成ris文件\n",
    "# 首先鉴于当前的scopus并不支持对于arxiv的批量导出，同时arxiv尽管是预印版，但是有着一些高引的文章，值得参考\n",
    "# 51s 1326篇文献\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 主函数：多线程读取多个文件并合并写入 RIS 文件\n",
    "def main():\n",
    "    html_dir = R\"../../data/arxiv/html\"\n",
    "    ris_output_path = \"../../data/arxiv/html_result/arxiv_results_multi.ris\"\n",
    "    \n",
    "    filenames = [\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：23).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：02).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：37).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：11).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：47).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：20).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：49：05).html\",\n",
    "    ]\n",
    "\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 写入 RIS\n",
    "    write_ris(all_documents, ris_output_path)\n",
    "    print(f\"已成功处理 {len(filepaths)} 个文件，共导出 {len(all_documents)} 条文献。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1 个 HTML 文件，共提取 200 条文献。\n",
      "发现 195 条重复文献。\n",
      "导出了 5 条不重复文献到 ../../data/arxiv/html_result/unique_arxiv_results.ris。\n"
     ]
    }
   ],
   "source": [
    "# 2.对于每次scopus更新的arxiv多出来的条目列出来，并且生成相应的ris文件\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 解析 RIS 文件，提取条目\n",
    "def parse_ris_file(ris_filepath):\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(ris_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"TY  -\"):\n",
    "                current_entry = {}\n",
    "            elif line.startswith(\"ER  -\"):\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "            elif line:\n",
    "                key, value = map(str.strip, line.split(\" - \", 1))\n",
    "                if key in current_entry:\n",
    "                    if isinstance(current_entry[key], list):\n",
    "                        current_entry[key].append(value)\n",
    "                    else:\n",
    "                        current_entry[key] = [current_entry[key], value]\n",
    "                else:\n",
    "                    current_entry[key] = value\n",
    "    return entries\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 检测重复并返回不重复的文献\n",
    "def deduplicate_documents(new_documents, existing_ris_filepath):\n",
    "    existing_entries = parse_ris_file(existing_ris_filepath)\n",
    "    existing_titles = {entry['TI'].lower() for entry in existing_entries if 'TI' in entry}\n",
    "    \n",
    "    unique_documents = []\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for doc in new_documents:\n",
    "        if doc['title'].lower() not in existing_titles:\n",
    "            unique_documents.append(doc)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    return unique_documents, duplicate_count\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 配置路径（需要用户修改）\n",
    "    html_dir = \"../../data/arxiv/html\"  # HTML 文件目录\n",
    "    existing_ris_filepath = \"../../data/arxiv/html_result/arxiv_results_multi_1635_tad_tal.ris\"  # 现有 RIS 文件\n",
    "    output_ris_filepath = \"../../data/arxiv/html_result/unique_arxiv_results.ris\"  # 输出不重复 RIS 文件\n",
    "    \n",
    "    # HTML 文件列表（示例，用户需替换为实际文件名）\n",
    "    filenames = [\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_9 16：50：37).html\",\n",
    "    ]\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理 HTML 文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 去重\n",
    "    unique_documents, duplicate_count = deduplicate_documents(all_documents, existing_ris_filepath)\n",
    "\n",
    "    # 写入不重复的 RIS 文件\n",
    "    if unique_documents:\n",
    "        write_ris(unique_documents, output_ris_filepath)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"处理了 {len(filepaths)} 个 HTML 文件，共提取 {len(all_documents)} 条文献。\")\n",
    "    print(f\"发现 {duplicate_count} 条重复文献。\")\n",
    "    print(f\"导出了 {len(unique_documents)} 条不重复文献到 {output_ris_filepath}。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Output written to ../../data/all/6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep.ris\n",
      "Log written to deduplication_log_*.txt\n"
     ]
    }
   ],
   "source": [
    "# 3.处理重复的条目，保留最详细的，并且在日志里面输出当前去重过的期刊 二重 去重逻辑 题目 DOI 现在存在bug -和空格区分不开\n",
    "import uuid\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"Normalize title for comparison by removing case and punctuation.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', title.lower()).strip()\n",
    "\n",
    "def parse_ris_file(file_path):\n",
    "    \"\"\"Parse RIS file and return a list of entries.\"\"\"\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == 'ER  -':\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "                    current_entry = {}\n",
    "            elif line:\n",
    "                tag, value = line.split('  - ', 1) if '  - ' in line else (line, '')\n",
    "                current_entry[tag] = current_entry.get(tag, []) + [value]\n",
    "    return entries\n",
    "\n",
    "def count_entry_fields(entry):\n",
    "    \"\"\"Count the number of fields (data lines) in an entry.\"\"\"\n",
    "    return sum(len(values) for values in entry.values())\n",
    "\n",
    "def deduplicate_by_field(entries, field, normalize=False):\n",
    "    \"\"\"Deduplicate entries based on a specified field, keeping the one with most fields.\"\"\"\n",
    "    field_to_entries = defaultdict(list)\n",
    "    for entry in entries:\n",
    "        field_value = entry.get(field, [''])[0]\n",
    "        if field_value:  # Only process entries with the field\n",
    "            key = normalize_title(field_value) if normalize else field_value\n",
    "            field_to_entries[key].append(entry)\n",
    "    \n",
    "    deduplicated = []\n",
    "    log_messages = []\n",
    "    \n",
    "    for key, entries_group in field_to_entries.items():\n",
    "        if len(entries_group) > 1:\n",
    "            # Sort by number of fields (descending) and keep the one with most fields\n",
    "            entries_group.sort(key=count_entry_fields, reverse=True)\n",
    "            kept_entry = entries_group[0]\n",
    "            deduplicated.append(kept_entry)\n",
    "            # Log removed entries\n",
    "            for removed_entry in entries_group[1:]:\n",
    "                log_messages.append(\n",
    "                    f\"Removed duplicate entry with {field} '{key}' \"\n",
    "                    f\"(kept {count_entry_fields(kept_entry)} fields, \"\n",
    "                    f\"removed {count_entry_fields(removed_entry)} fields, \"\n",
    "                    f\"title: '{removed_entry.get('TI', [''])[0]}')\"\n",
    "                )\n",
    "        else:\n",
    "            deduplicated.append(entries_group[0])\n",
    "    \n",
    "    # Add entries that didn't have the field\n",
    "    for entry in entries:\n",
    "        if not entry.get(field, [''])[0]:\n",
    "            deduplicated.append(entry)\n",
    "    \n",
    "    return deduplicated, log_messages\n",
    "\n",
    "def deduplicate_entries(entries):\n",
    "    \"\"\"Deduplicate entries first by TI, then by DO.\"\"\"\n",
    "    # Step 1: Deduplicate by TI\n",
    "    entries, ti_log_messages = deduplicate_by_field(entries, 'TI', normalize=True)\n",
    "    \n",
    "    # Step 2: Deduplicate by DO\n",
    "    entries, do_log_messages = deduplicate_by_field(entries, 'DO', normalize=False)\n",
    "    \n",
    "    return entries, ti_log_messages + do_log_messages\n",
    "\n",
    "def write_ris_file(entries, output_path):\n",
    "    \"\"\"Write deduplicated entries to a new RIS file with a blank line between entries.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for i, entry in enumerate(entries):\n",
    "            for tag, values in entry.items():\n",
    "                for value in values:\n",
    "                    file.write(f\"{tag}  - {value}\\n\")\n",
    "            file.write(\"ER  -\\n\")\n",
    "            if i < len(entries) - 1:  # Add blank line between entries, but not after the last\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging to a file.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        filename=f'../../data/log/deduplication_log_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    setup_logging()\n",
    "    \n",
    "    # Parse RIS file\n",
    "    entries = parse_ris_file(input_file)\n",
    "    \n",
    "    # Deduplicate entries\n",
    "    deduplicated_entries, log_messages = deduplicate_entries(entries)\n",
    "    \n",
    "    # Write to output file\n",
    "    write_ris_file(deduplicated_entries, output_file)\n",
    "    \n",
    "    # Log results\n",
    "    for message in log_messages:\n",
    "        logging.info(message)\n",
    "    \n",
    "    logging.info(f\"Processed {len(entries)} entries, kept {len(deduplicated_entries)} entries\")\n",
    "    print(f\"Deduplication complete. Output written to {output_file}\")\n",
    "    print(f\"Log written to deduplication_log_*.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = '../../data/all/6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885.ris'\n",
    "    output_file = '../../data/all/6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep.ris'\n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
