{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功处理 7 个文件，共导出 1326 条文献。\n"
     ]
    }
   ],
   "source": [
    "# 1.将SingFile导出的html文件进行解析生成ris文件\n",
    "# 首先鉴于当前的scopus并不支持对于arxiv的批量导出，同时arxiv尽管是预印版，但是有着一些高引的文章，值得参考\n",
    "# 51s 1326篇文献\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 主函数：多线程读取多个文件并合并写入 RIS 文件\n",
    "def main():\n",
    "    html_dir = R\"../../data/arxiv/html\"\n",
    "    ris_output_path = \"../../data/arxiv/html_result/arxiv_results_multi.ris\"\n",
    "    \n",
    "    filenames = [\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：23).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：53：02).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：37).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：52：11).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：47).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：51：20).html\",\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_5 21：49：05).html\",\n",
    "    ]\n",
    "\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 写入 RIS\n",
    "    write_ris(all_documents, ris_output_path)\n",
    "    print(f\"已成功处理 {len(filepaths)} 个文件，共导出 {len(all_documents)} 条文献。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1 个 HTML 文件，共提取 200 条文献。\n",
      "发现 195 条重复文献。\n",
      "导出了 5 条不重复文献到 ../../data/arxiv/html_result/unique_arxiv_results.ris。\n"
     ]
    }
   ],
   "source": [
    "# 2.对于每次scopus更新的arxiv多出来的条目列出来，并且生成相应的ris文件\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 解析 RIS 文件，提取条目\n",
    "def parse_ris_file(ris_filepath):\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(ris_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"TY  -\"):\n",
    "                current_entry = {}\n",
    "            elif line.startswith(\"ER  -\"):\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "            elif line:\n",
    "                key, value = map(str.strip, line.split(\" - \", 1))\n",
    "                if key in current_entry:\n",
    "                    if isinstance(current_entry[key], list):\n",
    "                        current_entry[key].append(value)\n",
    "                    else:\n",
    "                        current_entry[key] = [current_entry[key], value]\n",
    "                else:\n",
    "                    current_entry[key] = value\n",
    "    return entries\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 检测重复并返回不重复的文献\n",
    "def deduplicate_documents(new_documents, existing_ris_filepath):\n",
    "    existing_entries = parse_ris_file(existing_ris_filepath)\n",
    "    existing_titles = {entry['TI'].lower() for entry in existing_entries if 'TI' in entry}\n",
    "    \n",
    "    unique_documents = []\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for doc in new_documents:\n",
    "        if doc['title'].lower() not in existing_titles:\n",
    "            unique_documents.append(doc)\n",
    "        else:\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    return unique_documents, duplicate_count\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 配置路径（需要用户修改）\n",
    "    html_dir = \"../../data/arxiv/html\"  # HTML 文件目录\n",
    "    existing_ris_filepath = \"../../data/arxiv/html_result/arxiv_results_multi_1635_tad_tal.ris\"  # 现有 RIS 文件\n",
    "    output_ris_filepath = \"../../data/arxiv/html_result/unique_arxiv_results.ris\"  # 输出不重复 RIS 文件\n",
    "    \n",
    "    # HTML 文件列表（示例，用户需替换为实际文件名）\n",
    "    filenames = [\n",
    "        \"Scopus - 文献搜索结果 ｜ 已登录 (2025_5_9 16：50：37).html\",\n",
    "    ]\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理 HTML 文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 去重\n",
    "    unique_documents, duplicate_count = deduplicate_documents(all_documents, existing_ris_filepath)\n",
    "\n",
    "    # 写入不重复的 RIS 文件\n",
    "    if unique_documents:\n",
    "        write_ris(unique_documents, output_ris_filepath)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"处理了 {len(filepaths)} 个 HTML 文件，共提取 {len(all_documents)} 条文献。\")\n",
    "    print(f\"发现 {duplicate_count} 条重复文献。\")\n",
    "    print(f\"导出了 {len(unique_documents)} 条不重复文献到 {output_ris_filepath}。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成：共 4500 条文献；缺失 DOI 的有 30 条，成功补全 DOI 的有 776 条。\n",
      "有 1 条因网络超时或错误未处理，将记录至 ../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_errors.log 以便后续重试。\n",
      "输出文件：../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_enriched.ris，日志文件：../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_enriched.log\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.exceptions import ReadTimeout, RequestException\n",
    "\n",
    "# 用户直接输入文件路径\n",
    "input_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep.ris\"\n",
    "output_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_enriched.ris\"\n",
    "log_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_enriched.log\"\n",
    "error_log_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi_errors.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line.startswith('TY  -'):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith('ER  -'):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith('TI  -') and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith('AU  -'):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith('PY  -') and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def query_crossref(title, authors=None, year=None):\n",
    "    params = {'query.title': title, 'rows': 1}\n",
    "    if year:\n",
    "        params['filter'] = f'from-pub-date:{year},until-pub-date:{year}'\n",
    "    if authors:\n",
    "        params['query.author'] = authors[0]\n",
    "    try:\n",
    "        response = requests.get('https://api.crossref.org/works', params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        items = response.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except ReadTimeout:\n",
    "        # 超时，不影响主流程，将在后续重新处理\n",
    "        raise\n",
    "    except RequestException:\n",
    "        # 其他网络错误\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def enrich_entry_with_doi(entry, error_log):\n",
    "    info = extract_key_info(entry)\n",
    "    title = info.get('title')\n",
    "    authors = info.get('authors')\n",
    "    year = info.get('year')\n",
    "    try:\n",
    "        doi = query_crossref(title, authors, year)\n",
    "    except ReadTimeout:\n",
    "        # 记录超时条目以便后续重试\n",
    "        error_log.append(info)\n",
    "        return None\n",
    "    if doi:\n",
    "        for idx, line in enumerate(entry):\n",
    "            if line.startswith('ER  -'):\n",
    "                entry.insert(idx, f'DO  - {doi}')\n",
    "                break\n",
    "    return doi\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    missing_count = 0\n",
    "    enriched_count = 0\n",
    "    error_log = []  # 存放超时或 API 错误需后续处理的条目信息\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, \\\n",
    "         open(log_path, 'w', encoding='utf-8') as flog:\n",
    "\n",
    "        for entry in entries:\n",
    "            if any(line.startswith('DO  -') for line in entry):\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "            else:\n",
    "                doi = enrich_entry_with_doi(entry, error_log)\n",
    "                if doi:\n",
    "                    enriched_count += 1\n",
    "                    for ln in entry:\n",
    "                        fout.write(ln + '\\n')\n",
    "                    fout.write('\\n')\n",
    "                else:\n",
    "                    missing_count += 1\n",
    "                    info = extract_key_info(entry)\n",
    "                    title = info.get('title', '<无标题>')\n",
    "                    authors = '; '.join(info.get('authors', ['<未知作者>']))\n",
    "                    year = info.get('year', '<未知年份>')\n",
    "                    flog.write(f\"{missing_count}. 标题: {title}\\n\")\n",
    "                    flog.write(f\"   作者: {authors}\\n\")\n",
    "                    flog.write(f\"   出版年: {year}\\n\")\n",
    "                    flog.write(f\"   原始条目行数: {len(entry)}\\n\")\n",
    "                    flog.write(\"---\\n\")\n",
    "\n",
    "    # 将需后续重试的条目写入单独日志\n",
    "    if error_log:\n",
    "        with open(error_log_path, 'w', encoding='utf-8') as ef:\n",
    "            for idx, info in enumerate(error_log, 1):\n",
    "                ef.write(f\"{idx}. 标题: {info.get('title','<无标题>')}\\n\")\n",
    "                ef.write(f\"   作者: {'; '.join(info.get('authors', ['<未知作者>']))}\\n\")\n",
    "                ef.write(f\"   出版年: {info.get('year','<未知年份>')}\\n\")\n",
    "                ef.write(\"---\\n\")\n",
    "\n",
    "    print(f\"处理完成：共 {len(entries)} 条文献；缺失 DOI 的有 {missing_count} 条，成功补全 DOI 的有 {enriched_count} 条。\")\n",
    "    if error_log:\n",
    "        print(f\"有 {len(error_log)} 条因网络超时或错误未处理，将记录至 {error_log_path} 以便后续重试。\")\n",
    "    print(f\"输出文件：{output_path}，日志文件：{log_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成：共 4500 条文献；缺失 DOI 的有 806 条，已记录至 ../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.log。输出文件：../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.ris\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "脚本功能：\n",
    "1. 读取一个 .ris 文件，\n",
    "2. 提取所有条目（以 'TY  -' 开始，以 'ER  -' 结束），\n",
    "3. 判断每个条目中是否包含 DOI（以 'DO  -' 开头的行），\n",
    "4. 将包含 DOI 的条目写入新的 .ris 文件，\n",
    "5. 将缺失 DOI 的条目的关键信息（如 TI、AU、PY）记录到日志文件。\n",
    "\n",
    "使用方法：\n",
    "    直接运行脚本，脚本会提示输入文件路径和输出文件路径，无需命令行参数。\n",
    "\"\"\"\n",
    "\n",
    "# 用户直接输入文件路径\n",
    "input_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep.ris\"\n",
    "output_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.ris\"\n",
    "log_path = \"../../data/all/4500_4514_6415_arxiv_results_multi_1640_tad_tal_3890_tad_tal_scopus_20250509_886_tad_tal_wos_20250509_885_dep_doi.log\"\n",
    "\n",
    "\n",
    "def parse_ris_entries(lines):\n",
    "    \"\"\"\n",
    "    将 RIS 文件内容分割成若干条目，每条目是若干行列表。\n",
    "    依据：条目以 'TY  -' 行开始，以 'ER  -' 行结束。\"\"\"\n",
    "    entries = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line.startswith('TY  -'):\n",
    "            if current:\n",
    "                entries.append(current)\n",
    "            current = [line]\n",
    "        elif current:\n",
    "            current.append(line)\n",
    "            if line.startswith('ER  -'):\n",
    "                entries.append(current)\n",
    "                current = []\n",
    "    if current:\n",
    "        entries.append(current)\n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_key_info(entry):\n",
    "    \"\"\"\n",
    "    从条目中提取关键信息，用于日志：\n",
    "    标题 (TI)、作者 (AU)、出版年份 (PY)。\"\"\"\n",
    "    info = {}\n",
    "    for line in entry:\n",
    "        if line.startswith('TI  -') and 'title' not in info:\n",
    "            info['title'] = line[6:].strip()\n",
    "        elif line.startswith('AU  -'):\n",
    "            info.setdefault('authors', []).append(line[6:].strip())\n",
    "        elif line.startswith('PY  -') and 'year' not in info:\n",
    "            info['year'] = line[6:].strip()\n",
    "    return info\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"无法找到输入文件: {input_path}\")\n",
    "        return\n",
    "\n",
    "    entries = parse_ris_entries(lines)\n",
    "    missing_count = 0\n",
    "    with open(output_path, 'w', encoding='utf-8') as fout, open(log_path, 'w', encoding='utf-8') as flog:\n",
    "        for entry in entries:\n",
    "            has_doi = any(line.startswith('DO  -') for line in entry)\n",
    "            if has_doi:\n",
    "                for ln in entry:\n",
    "                    fout.write(ln + '\\n')\n",
    "                fout.write('\\n')\n",
    "            else:\n",
    "                info = extract_key_info(entry)\n",
    "                title = info.get('title', '<无标题>')\n",
    "                authors = '; '.join(info.get('authors', ['<未知作者>']))\n",
    "                year = info.get('year', '<未知年份>')\n",
    "                flog.write(f\"{missing_count+1}. 标题: {title}\\n\")\n",
    "                flog.write(f\"   作者: {authors}\\n\")\n",
    "                flog.write(f\"   出版年: {year}\\n\")\n",
    "                flog.write(f\"   原始条目行数: {len(entry)}\\n\")\n",
    "                flog.write(\"---\\n\")\n",
    "                missing_count += 1\n",
    "\n",
    "    print(f\"处理完成：共 {len(entries)} 条文献；缺失 DOI 的有 {missing_count} 条，已记录至 {log_path}。输出文件：{output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个主要是对于LNCS已经归档的文献找到其归属的会议\n",
    "# 去除Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)的杂项\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 扩展会议缩写列表\n",
    "abbrs = [\n",
    "    'ECCV', 'ACCV', 'ACPR', 'CGI', 'DAGM-GCPR', 'ICANN', 'ICIAP', 'ICIC',\n",
    "    'ICIG', 'ICIRA', 'ICONIP', 'ICPR', 'ISVC', 'MMM', 'PCM', 'PRCV',\n",
    "    'PRICAI', 'SCIA', 'CAAI', 'NPC', 'ADMA', 'CVM', 'HCC', 'ICSI',\n",
    "    'CCBR', 'ICFEM', 'MICCAI', 'DMAH', 'MICCAI', 'CAAI',\n",
    "    'ICR', 'Euro-Par', 'MLDM', 'IbPRIA', 'ICPRAI',\n",
    "    'ICCSA', 'ICPRAI', 'CAIP', 'ICDAR', 'CICAI'\n",
    "    \n",
    "]\n",
    "\n",
    "# 处理单个 RIS 条目的函数\n",
    "def process_ris_entry(entry: str) -> str:\n",
    "    # 匹配 T2 和 N1 字段\n",
    "    t2_match = re.search(r'^(T2  - .+)$', entry, re.MULTILINE)\n",
    "    n1_match = re.search(r'^N1  - <p>(.*?)</p>$', entry, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    if t2_match and n1_match:\n",
    "        t2_value = t2_match.group(1)\n",
    "        n1_content = n1_match.group(1)\n",
    "\n",
    "        # 仅处理 Lecture Notes in Computer Science 条目\n",
    "        if 'Lecture Notes in Computer Science' in t2_value:\n",
    "            # 在 N1 中寻找缩写和年份，例如 'ECCV 2018'\n",
    "            pattern = r'\\b(' + '|'.join(map(re.escape, abbrs)) + r')\\s+(\\d{4})\\b'\n",
    "            match = re.search(pattern, n1_content)\n",
    "            if match:\n",
    "                abbr = match.group(1)\n",
    "                year = match.group(2)\n",
    "                new_t2 = f'T2  - {abbr} {year}'\n",
    "                # 替换原有 T2 行\n",
    "                entry = re.sub(r'^(T2  - .+)$', new_t2, entry, flags=re.MULTILINE)\n",
    "\n",
    "    return entry\n",
    "\n",
    "# 主处理函数\n",
    "def process_ris_file(input_path: str, output_path: str):\n",
    "    text = Path(input_path).read_text(encoding='utf-8')\n",
    "    # 保留 ER 结束标记并分割条目\n",
    "    raw_entries = re.split(r'\\nER  -\\s*\\n', text.strip(), flags=re.DOTALL)\n",
    "    processed = []\n",
    "\n",
    "    for raw in raw_entries:\n",
    "        entry = raw.strip()\n",
    "        if not entry:\n",
    "            continue\n",
    "        # 恢复 ER 标记，处理后再添加\n",
    "        processed_entry = process_ris_entry(entry + '\\nER  -')\n",
    "        processed.append(processed_entry)\n",
    "\n",
    "    # 写入输出文件\n",
    "    result = '\\n'.join(processed).strip() + '\\n'\n",
    "    Path(output_path).write_text(result, encoding='utf-8')\n",
    "\n",
    "# 命令行执行示例\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Process RIS file, updating T2 based on conference abbreviations.')\n",
    "    # parser.add_argument('input', help='Input RIS file path')\n",
    "    # parser.add_argument('output', help='Output RIS file path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    process_ris_file(R\"D:\\Users\\tang\\Desktop\\LNCS_20250511_303\\LNCS_20250511_303.ris\", R\"D:\\Users\\tang\\Desktop\\LNCS_20250511_303\\LNCS_20250511_303.ris\")\n",
    "    # print(f\"Processed RIS file saved to D:\\Users\\tang\\Desktop\\LNCS_20250511_303\\LNCS_20250511_303.ris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
