TY  - CPAPER
AU  - Li, Zhihui
AU  - Yao, Lina
A1  - IEEE COMP SOC
TI  - Three Birds with One Stone: Multi-Task Temporal Action Detection via Recycling Temporal Annotations
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Temporal action detection on unconstrained videos has seen significant research progress in recent years. Deep learning has achieved enormous success in this direction. However, collecting large-scale temporal detection datasets to ensuring promising performance in the real-world is a laborious, impractical and time consuming process. Accordingly, we present a novel improved temporal action localization model that is better able to take advantage of limited labeled data available. Specifically, we design two auxiliary tasks by reconstructing the available label information and then facilitate the learning of the temporal action detection model. Each task generates their supervision signal by recycling the original annotations, and are jointly trained with the temporal action detection model in a multitask learning fashion. Note that the proposed approach can be pluggable to any region proposal based temporal action detection models. We conduct extensive experiments on three benchmark datasets, namely THUMOS' 14 [15], Charades [35] and ActivityNet [14]. Our experimental results confirm the effectiveness of the proposed model.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 4749
EP  - 4758
DO  - 10.1109/CVPR46437.2021.00472
AN  - WOS:000739917304092
AD  - Univ New South Wales, Sch Comp Sci & Engn, Kensington, NSW, Australia
Y2  - 2022-02-03
ER  -

TY  - JOUR
AU  - Gwon, Huiwon
AU  - Jo, Hyejeong
AU  - Jo, Sunhee
AU  - Jung, Chanho
TI  - A Bi-directional Information Learning Method Using Reverse Playback Video for Fully Supervised Temporal Action Localization
TI  - 완전지도 시간적 행동 검출에서역재생 비디오를 이용한 양방향 정보 학습 방법
T2  - Journal of IKEEE
T2  - 전기전자학회논문지
M3  - research-article
AB  - Recently, research on temporal action localization has been actively conducted. In this paper, unlike existingmethods, we propose two approaches for learning bidirectional information by creating reverse playback videosfor fully supervised temporal action localization. One approach involves creating training data by combiningreverse playback videos and forward playback videos, while the other approach involves training separate modelson videos with different playback directions. Experiments were conducted on the THUMOS-14 dataset usingTALLFormer. When using both reverse and forward playback videos as training data, the performance was 5.1%lower than that of the existing method. On the other hand, using a model ensemble shows a 1.9% improvementin performance.
PU  - Institute of Korean Electrical and Electronics Engineers
SN  - 1226-7244
DA  - 2024 
PY  - 2024
VL  - 28
IS  - 2
SP  - 145
EP  - 149
AN  - KJD:ART003097617
Y2  - 2024-07-26
ER  -

TY  - JOUR
AU  - Kang, Tae-Kyung
AU  - Lee, Gun-Hee
AU  - Lee, Seong-Whan
AU  - Lee, Seong-Whan
TI  - HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization (TAL) is a task of identifying a set of actions in a video, which involves localizing the start and end frames and classifying each action instance. Existing methods have addressed this task by using predefined anchor windows or heuristic bottom-up boundary-matching strategies, which are major bottlenecks in inference time. Additionally, the main challenge is the inability to capture long-range actions due to a lack of global contextual information. In this paper, we present a novel anchor-free framework, referred to as HTNet, which predicts a set of triplets from a video based on a Transformer architecture. After the prediction of coarse boundaries, we refine it through a background feature sampling (BFS) module and hierarchical Transformers, which enables our model to aggregate global contextual information and effectively exploit the inherent semantic relationships in a video. We demonstrate how our method localizes accurate action instances and achieves state-of-the-art performance on two TAL benchmark datasets: THUMOS14 and ActivityNet 1.3.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.09662
AN  - PPRN:11120582
AD  - Korea Univ, Dept Artificial Intelligence, Seoul, South Korea
AD  - Korea Univ, Dept Comp Engn, Seoul, South Korea
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Liu, Zihao
AU  - Yan, Danfeng
AU  - Cai, Yuanqiang
AU  - Song, Yan
TI  - Spatio-temporal human action localization in indoor surveillances
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Spatio-temporal action localization is a crucial and challenging task in the field of video understanding. Existing benchmarks for spatio-temporal action detection are limited by factors such as incomplete annotations, highlevel non-universal actions, and uncommon scenarios. To address these limitations and facilitate research in real-world security applications, we introduce a novel human-centric dataset for spatio-temporal localization of atomic actions in indoor surveillance settings, termed as HIA (Human-centric Indoor Actions). The HIA dataset is constructed by selecting 30 atomic action classes, compiling 100 surveillance videos, and annotating 219,225 frames with 370,937 bounding boxes. The primary characteristics of HIA include (1) accurate spatiotemporal annotations for atomic actions, (2) human-centric annotations at the frame level, (3) temporal linking of persons across discontinuous tracks, and (4) utilization of indoor surveillance videos. Our HIA, with its realistic settings in indoor surveillance scenes and comprehensive annotations, presents a valuable and novel challenge to the spatio-temporal action localization domain. To establish a benchmark, we evaluate various methods and provide an in-depth analysis of the HIA dataset. The HIA dataset will be made available soon, and we anticipate that it will serve as a standard and practical benchmark for the research community.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2024 MAR
PY  - 2024
VL  - 147
C7  - 110087
DO  - 10.1016/j.patcog.2023.110087
AN  - WOS:001125603900001
C6  - NOV 2023
AD  - Beijing Univ Posts & Telecommun, Sch Comp Sci, Natl Pilot Software Engn Sch, Beijing 100876, Peoples R China
AD  - Shanghai Int Studies Univ, Sch Business & Management, Shanghai 200083, Peoples R China
Y2  - 2024-01-06
ER  -

TY  - CPAPER
AU  - Vasileiou, Vasiliki, I
AU  - Kardaris, Nikolaus
AU  - Maragos, Petros
A1  - EURASIP
TI  - Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos
T2  - 29TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO 2021)
M3  - Proceedings Paper
CP  - 29th European Signal Processing Conference (EUSIPCO)
CL  - ELECTR NETWORK
AB  - Nowadays, the interaction between humans and robots is constantly expanding, requiring more and more human motion recognition applications to operate in real time. However, most works on temporal action detection and recognition perform these tasks in offline manner, i.e. temporally segmented videos are classified as a whole. In this paper, based on the recently proposed framework of Temporal Recurrent Networks, we explore how temporal context and human movement dynamics can be effectively employed for online action detection. Our approach uses various state-of-the-art architectures and appropriately combines the extracted features in order to improve action detection. We evaluate our method on a challenging but widely used dataset for temporal action localization, THUMOS'14. Our experiments show significant improvement over the baseline method, achieving state-of-the art results on THUMOS'14.
PU  - EUROPEAN ASSOC SIGNAL SPEECH & IMAGE PROCESSING-EURASIP
PI  - KESSARIANI
PA  - PO BOX 74251, KESSARIANI, 151 10, GREECE
SN  - 2076-1465
SN  - 978-9-0827-9706-0
DA  - 2021 
PY  - 2021
SP  - 1431
EP  - 1435
AN  - WOS:000764066600285
AD  - Natl Tech Univ Athens, Sch ECE, Athens 15773, Greece
Y2  - 2022-04-17
ER  -

TY  - CPAPER
AU  - Kong, Weijie
AU  - Li, Nannan
AU  - Liu, Shan
AU  - Li, Thomas
AU  - Li, Ge
A1  - IEEE
TI  - BLP - BOUNDARY LIKELIHOOD PINPOINTING NETWORKS FOR ACCURATE TEMPORAL ACTION LOCALIZATION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)
M3  - Proceedings Paper
CP  - 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - Brighton, ENGLAND
AB  - Despite tremendous progress achieved in temporal action detection, state-of-the-art methods still suffer from the sharp performance deterioration when localizing the starting and ending temporal action boundaries. Although most methods apply boundary regression paradigm to tackle this problem, we argue that the direct regression lacks detailed enough information to yield accurate temporal boundaries. In this paper, we propose a novel Boundary Likelihood Pinpointing (BLP) network to alleviate this deficiency of boundary regression and improve the localization accuracy. Given a loosely localized search interval that contains an action instance, BLP casts the problem of localizing temporal boundaries as that of assigning probabilities on each equally divided unit of this interval. These generated probabilities provide useful information regarding the boundary location of the action inside this search interval. Based on these probabilities, we introduce a boundary pinpointing paradigm to pinpoint the accurate boundaries under a simple probabilistic framework. Compared with other C3D feature based detectors, extensive experiments demonstrate that BLP significantly improves the localization performance of recent state-of-the-art detectors, and achieves competitive detection mAP on both THUMOS' 14 and ActivityNet datasets, particularly when the evaluation tIoU is high.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-4799-8131-1
DA  - 2019 
PY  - 2019
SP  - 1647
EP  - 1651
DO  - 10.1109/icassp.2019.8682466
AN  - WOS:000482554001175
AD  - Peking Univ, Sch Elect & Comp Engn, Shenzhen, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Tencent, Media Lab, Shenzhen, Peoples R China
AD  - Peking Univ, Adv Inst Informat Technol, Hangzhou, Zhejiang, Peoples R China
Y2  - 2019-09-30
ER  -

TY  - JOUR
AU  - Song, Qing
AU  - Zhou, Yang
AU  - Hu, Mengjie
AU  - Liu, Chun
TI  - Faster Learning of Temporal Action Proposal via Sparse Multilevel Boundary Generator
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization in videos presents significant challenges in the field of computer vision. While the boundary-sensitive method has been widely adopted, its limitations include incomplete use of intermediate and global information, as well as an inefficient proposal feature generator. To address these challenges, we propose a novel framework, Sparse Multilevel Boundary Generator (SMBG), which enhances the boundary-sensitive method with boundary classification and action completeness regression. SMBG features a multi-level boundary module that enables faster processing by gathering boundary information at different lengths. Additionally, we introduce a sparse extraction confidence head that distinguishes information inside and outside the action, further optimizing the proposal feature generator. To improve the synergy between multiple branches and balance positive and negative samples, we propose a global guidance loss. Our method is evaluated on two popular benchmarks, ActivityNet-1.3 and THUMOS14, and is shown to achieve state-of-the-art performance, with a better inference speed (2.47xBSN++, 2.12xDBG). These results demonstrate that SMBG provides a more efficient and simple solution for generating temporal action proposals. Our proposed framework has the potential to advance the field of computer vision and enhance the accuracy and speed of temporal action localization in video analysis.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.03166
AN  - PPRN:43573305
AD  - Beijing Univ Posts & Telecommun, Pattern Recognit & Intelligent Vis, Xi Tu Cheng Rd, Beijing 100876, Peoples R China
M2  - Beijing Univ Posts & Telecommun
Y2  - 2023-03-17
ER  -

TY  - JOUR
AU  - Megrhi, Sameh
AU  - Jmal, Marwa
AU  - Souidene, Wided
AU  - Beghdadi, Azeddine
TI  - Spatio-temporal action localization and detection for human recognition in big dataset
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - Human action recognition is still attracting the computer vision research community due to its various applications. However, despite the variety of methods proposed to solve this problem, some issues still need to be addressed. In this paper, we present a human action detection and recognition process on large datasets based on Interest Points trajectories. In order to detect moving humans in moving field of views, a spatio-temporal action detection is performed basing on optical flow and dense speed-up-robust features (SURF). Then, a video description based on a fusion process that combines motion, trajectory and visual descriptors is proposed. Features within each bounding box are extracted by exploiting the bag-of-words approach. Finally, a support-vector-machine is employed to classify the detected actions. Experimental results on the complex benchmark UCF101, KTH and HMDB51 datasets reveal that the proposed technique achieves better performances compared to some of the existing state-of-the-art action recognition approaches. (C) 2016 Elsevier Inc. All rights reserved.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2016 NOV
PY  - 2016
VL  - 41
SP  - 375
EP  - 390
DO  - 10.1016/j.jvcir.2016.10.016
AN  - WOS:000389169000033
AD  - Univ Paris 13, Inst Galilee, L2TI, 99 Ave Jean Baptiste Clement, F-93430 Villetaneuse, France
AD  - Univ Carthage, SERCom Lab, Ecole Polytech Tunisie, BP 743, La Marsa 2078, Tunisia
AD  - Telnet Holding, Telnet Innovat Labs, Ariana, Tunisia
M2  - Telnet Holding
Y2  - 2017-01-11
ER  -

TY  - JOUR
AU  - Xia, Kun
AU  - Wang, Le
AU  - Shen, Yichao
AU  - Zhou, Sanpin
AU  - Hua, Gang
AU  - Tang, Wei
TI  - Exploring Action Centers for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization aims at detecting the temporal intervals of human actions in untrimmed videos. Most previous methods rely on locating and matching the start and end times of actions. However, action boundaries are ambiguous and uncertain in nature, which leads to inaccurate action localization and a lot of false positives. In this paper, we introduce a new framework for temporal action localization. It explicitly models temporal action centers to reduce unreliable action detection results caused by ambiguous action boundaries. Since action centers are highly related to semantic actions, they can be detected more reliably than the conventional action boundaries. As a result, our framework can exclude false positives and promote high-quality proposals. Based on action centers, we propose a triplet feature fusion mechanism. It performs neural message passing among the boundaries and the center as well as contextual regions outside of the proposal to enrich its representation. In addition, we introduce a centerness scoring method to suppress proposals deviating from the centers of action instances. Consequently, our network can retrieve high-quality action proposals and locate actions more precisely. Experimental results show our method outperforms state-of-the-art methods on the THUMOS14 and ActivityNet v1.3 datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 9425
EP  - 9436
DO  - 10.1109/TMM.2023.3252176
AN  - WOS:001133324200012
AD  - Xi An Jiao Tong Univ, Natl Key Lab Human Machine Hybrid Augmented Intell, Natl Engn Res Ctr Visual Informat & Applicat, Xian, Shaanxi, Peoples R China
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Shaanxi, Peoples R China
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA
M2  - Wormpex AI Res
Y2  - 2024-01-24
ER  -

TY  - JOUR
AU  - Chen, Yunze
AU  - Jiang, He
AU  - Xiao, Junrui
AU  - Li, Ding
AU  - Gu, Qingyi
TI  - Temporal action detection with dynamic weights based on curriculum learning
T2  - NEUROCOMPUTING
M3  - Article
AB  - To enable temporal action localization, the computer needs to recognize the locations and classes of action instances in a video. The main challenge to temporal action detection is that the videos are often long and untrimmed, consisting of varying action content. Existing temporal action detection frameworks exhibit a gap between the training and testing phases, which is detrimental to model performance. Specifically, all positive samples are trained identically in the training phase. By contrast, in the testing phase, the positive samples with the best classification and localization scores are selected, while all others are suppressed. To mitigate this issue, we build an auxiliary branch to unify the training and test-ing procedures. In the construction of the auxiliary branch, we design a dynamic weighting strategy based on curriculum learning, where the weights of training samples are a combination of their classifi-cation and localization scores. Motivated by the speculation of curriculum learning, we emphasize the importance of classification and localization scores in different training stages. The classification score accounts for a higher proportion of the combined score in the early stages of the training process. As the epoch increases, the localization score gradually increases in proportion as well. The experimental results demonstrate that our methodology of curriculum-based learning enhances the performance of current action localization techniques. On THUMOS14, our technique outperforms the existing state-of-the-art technique (57.6% vs 55.5%). And the performance on ActivityNet v1.3 (mAP@Avg) reaches 35.4%. (c) 2022 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2023 MAR 1
PY  - 2023
VL  - 524
SP  - 106
EP  - 116
DO  - 10.1016/j.neucom.2022.12.049
AN  - WOS:000990128300001
C6  - DEC 2022
AD  - Chinese Acad Sci, Inst Automat, East Zhongguancun Rd, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Jingjia Rd, Beijing, Peoples R China
Y2  - 2023-05-31
ER  -

TY  - JOUR
AU  - He, Yilong
AU  - Zhong, Yong
AU  - Wang, Lishun
AU  - Dang, Jiachen
TI  - GLFormer: Global and Local Context Aggregation Network for Temporal Action Detection
T2  - APPLIED SCIENCES-BASEL
M3  - Article
AB  - As the core component of video analysis, Temporal Action Localization (TAL) has experienced remarkable success. However, some issues are not well addressed. First, most of the existing methods process the local context individually, without explicitly exploiting the relations between features in an action instance as a whole. Second, the duration of different actions varies widely; thus, it is difficult to choose the proper temporal receptive field. To address these issues, this paper proposes a novel network, GLFormer, which can aggregate short, medium, and long temporal contexts. Our method consists of three independent branches with different ranges of attention, and these features are then concatenated along the temporal dimension to obtain richer features. One is multi-scale local convolution (MLC), which consists of multiple 1D convolutions with varying kernel sizes to capture the multi-scale context information. Another is window self-attention (WSA), which tries to explore the relationship between features within the window range. The last is global attention (GA), which is used to establish long-range dependencies across the full sequence. Moreover, we design a feature pyramid structure to be compatible with action instances of various durations. GLFormer achieves state-of-the-art performance on two challenging video benchmarks, THUMOS14 and ActivityNet 1.3. Our performance is 67.2% and 54.5% AP@0.5 on the datasets THUMOS14 and ActivityNet 1.3, respectively.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
DA  - 2022 SEP
PY  - 2022
VL  - 12
IS  - 17
C7  - 8557
DO  - 10.3390/app12178557
AN  - WOS:000850983200001
AD  - Chinese Acad Sci, Chengdu Inst Comp Applicat, Chengdu 610081, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China
Y2  - 2022-09-14
ER  -

TY  - JOUR
AU  - Liu, Xiaolong
AU  - Sun, Yuchao
AU  - Lu, Jianghu
AU  - Yao, Cong
AU  - Zhou, Yu
TI  - Self-Similarity Action Proposal
T2  - IEEE SIGNAL PROCESSING LETTERS
M3  - Article
AB  - Temporal action proposal generation, which aims to locate temporal segments that may contain actions, is a key prepositive step of various video analysis tasks, like temporal action detection. In this letter, we present Self-Similarity Action Proposal (SSAP), a simple method that generates action proposals using the self-similarity of videos. Specifically, a basic low-level index, structural similarity, is adopted to measure the similarity between adjacent frames. Potential action boundaries are located by thresholding the similarity values and candidate action segments are successively generated by grouping the boundaries. A segment evaluation module (SEM) is further employed to score and refine the segments. The framework achieves state-of-the-art performance on THUMOS14 and competitive results on ActivityNet v1.3. Notably, on THUMOS14, it achieves over 4% improvement on the average recall at 50 proposals and 3.3% gain in mAP@0.7 when combined with an existing action classifier for temporal action detection.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1070-9908
SN  - 1558-2361
DA  - 2020 
PY  - 2020
VL  - 27
SP  - 2064
EP  - 2068
DO  - 10.1109/LSP.2020.3037796
AN  - WOS:000597748200005
AD  - Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China
AD  - Megvii Co Ltd, Beijing 100190, Peoples R China
AD  - Bigo Technol Pte Ltd, Beijing 100086, Peoples R China
M2  - Megvii Co Ltd
M2  - Bigo Technol Pte Ltd
Y2  - 2020-12-31
ER  -

TY  - JOUR
AU  - Liu, Xiaolong
AU  - Wang, Qimeng
AU  - Hu, Yao
AU  - Tang, Xu
AU  - Zhang, Shiwei
AU  - Bai, Song
AU  - Bai, Xiang
TI  - End-to-end Temporal Action Detection with Transformer
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2106.10271
AN  - PPRN:12056954
AD  - Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan, Peoples R China
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - ByteDance Inc, Beijing, Peoples R China
M2  - Huazhong Univ Sci & Technol
M2  - Huazhong Univ Sci & Technol
M2  - ByteDance Inc
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Shi, Dingfeng
AU  - Cao, Qiong
AU  - Zhong, Yujie
AU  - An, Shan
AU  - Cheng, Jian
AU  - Zhu, Haogang
AU  - Tao, Dacheng
TI  - Temporal Action Localization with Enhanced Instant Discriminability
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD) aims to detect all action boundaries and their corresponding categories in an untrimmed video. The unclear boundaries of actions in videos often result in imprecise predictions of action boundaries by existing methods. To resolve this issue, we propose a one-stage framework named TriDet. First, we propose a Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. Then, we analyze the rank-loss problem (i.e. instant discriminability deterioration) in transformer-based methods and propose an efficient scalable-granularity perception (SGP) layer to mitigate this issue. To further push the limit of instant discriminability in the video backbone, we leverage the strong representation capability of pretrained large models and investigate their performance on TAD. Last, considering the adequate spatial-temporal context for classification, we design a decoupled feature pyramid network with separate feature pyramids to incorporate rich spatial context from the large model for localization. Experimental results demonstrate the robustness of TriDet and its state-of-the-art performance on multiple TAD datasets, including hierarchical (multilabel) TAD datasets.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.05590
AN  - PPRN:84950613
AD  - Beihang Univ, Beijing, Peoples R China
AD  - JDcom, Beijing, Peoples R China
AD  - Meituran Inc, Beijing, Peoples R China
AD  - Zhongguancun Lab, Beijing, Peoples R China
AD  - Univ Sydney, Camperdown, Australia
M2  - Beihang Univ
M2  - JDcom
M2  - Meituran Inc
M2  - Univ Sydney
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Xiang, Wangmeng
AU  - Li, Chao
AU  - Li, Ke
AU  - Wang, Biao
AU  - Hua, Xian-Sheng
AU  - Zhang, Lei
A1  - IEEE
TI  - CDAD: A Common Daily Action Dataset with Collected Hard Negative Samples
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - The research on action understanding has achieved significant progress with the establishment of various benchmark datasets. However, the results of action understanding are far from satisfactory in practice. One reason is that the existing action datasets ignore the existence of many hard negative samples in real-world scenarios, which are usually undefined confusion actions, e.g., holding a pen near the mouth vs. smoking. In this work, we focus on the common actions in our daily life and present a novel Common Daily Action Dataset (CDAD), which consists of 57,824 video clips of 23 well-defined common daily actions with rich manual annotations. Particularly, for each daily action, we collect not only diverse positive samples but also various hard negative samples that have minor differences (share similarities) in action with the positive ones. The established CDAD dataset could not only serve as a benchmark for several important daily action understanding tasks, including multi-label action recognition, temporal action localization, and spatial-temporal action detection, but also provide a testbed for researchers to investigate the influence of highly similar negative samples in learning action understanding models.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3920
EP  - 3929
DO  - 10.1109/CVPRW56347.2022.00437
AN  - WOS:000861612704002
AD  - Hong Kong Polytech Univ, Hong Kong, Peoples R China
AD  - Alibaba Grp, DAMO Acad, Hangzhou, Peoples R China
Y2  - 2022-12-07
ER  -

TY  - CPAPER
AU  - Gritsenko, Alexey A.
AU  - Xiong, Xuehan
AU  - Djolonga, Josip
AU  - Dehghani, Mostafa
AU  - Sun, Chen
AU  - Lucic, Mario
AU  - Schmid, Cordelia
AU  - Arnab, Anurag
A1  - IEEE
TI  - End-to-End Spatio-Temporal Action Localisation with Video Transformers
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - The most performant spatio-temporal action localisation models use external person proposals and complex external memory banks. We propose a fully end-to-end, purely-transformer based model that directly ingests an input video, and outputs tubelets - a sequence of bounding boxes and the action classes at each frame. Our flexible model can be trained with either sparse bounding-box supervision on individual frames, or full tubelet annotations. And in both cases, it predicts coherent tubelets as the output. Moreover, our end-to-end model requires no additional pre-processing in the form of proposals, or post-processing in terms of non-maximal suppression. We perform extensive ablation experiments, and significantly advance the state-of-the-art on five different spatio-temporal action localisation benchmarks with both sparse keyframes and full tubelet annotations.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18373
EP  - 18383
DO  - 10.1109/CVPR52733.2024.01739
AN  - WOS:001342515501068
AD  - Google, Mountain View, CA 94043 USA
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
TI  - Zero-Shot Temporal Action Detection via Vision-Language Prompting
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing temporal action detection (TAD) methods rely on large training data including segment-level annotations, limited to recog-nizing previously seen classes alone during inference. Collecting and an-notating a large training set for each class of interest is costly and hence unscalable. Zero-shot TAD (ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with significantly less investiga-tion. Inspired by the success of zero-shot image classification aided by vision-language (ViL) models such as CLIP, we aim to tackle the more complex TAD task. An intuitive method is to integrate an off-the-shelf proposal detector with CLIP style classification. However, due to the se-quential localization (e.g., proposal generation) and classification design, it is prone to localization error propagation. To overcome this problem, in this paper we propose a novel zero-Shot Temporal Action detection model via Vision-LanguagE prompting (STALE). Such a novel design effec-tively eliminates the dependence between localization and classification by breaking the route for error propagation in-between. We further in-troduce an interaction mechanism between classification and localization for improved optimization. Extensive experiments on standard ZS-TAD video benchmarks show that our STALE significantly outperforms state -of-the-art alternatives. Besides, our model also yields superior results on supervised TAD over recent strong competitors. The PyTorch implemen-tation of STALE is available on https://github.com/sauradip/STALE.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.08184
AN  - PPRN:10828005
AD  - Univ Surrey, CVSSP, Guildford, England
AD  - FlyTek Surrey Joint Res Ctr Artificial Intelligence, Guildford, England
AD  - Univ Surrey, Surrey Inst People Ctr Artificial Intelligence, Guildford, England
M2  - FlyTek Surrey Joint Res Ctr Artificial Intelligence
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Tang, Yepeng
AU  - Wang, Weining
AU  - Zhang, Chunjie
AU  - Liu, Jing
AU  - Zhao, Yao
TI  - Learnable Feature Augmentation Framework for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Temporal action localization (TAL) has drawn much attention in recent years, however, the performance of previous methods is still far from satisfactory due to the lack of annotated untrimmed video data. To deal with this issue, we propose to improve the utilization of current data through feature augmentation. Given an input video, we first extract video features with pre-trained video encoders, and then randomly mask various semantic contents of video features to consider different views of video features. To avoid damaging important action-related semantic information, we further develop a learnable feature augmentation framework to generate better views of videos. In particular, a Mask-based Feature Augmentation Module (MFAM) is proposed. The MFAM has three advantages: 1) it captures the temporal and semantic relationships of original video features, 2) it generates masked features with indispensable action-related information, and 3) it randomly recycles some masked information to ensure diversity. Finally, we input the masked features and the original features into shared action detectors respectively, and perform action classification and localization jointly for model learning. The proposed framework can improve the robustness and generalization of action detectors by learning more and better views of videos. In the testing stage, the MFAM can be removed, which does not bring extra computational costs. Extensive experiments are conducted on four TAL benchmark datasets. Our proposed framework significantly improves different TAL models and achieves the state-of-the-art performances.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2024 
PY  - 2024
VL  - 33
SP  - 4002
EP  - 4015
DO  - 10.1109/TIP.2024.3413599
AN  - WOS:001258857900003
AD  - Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci, Sch Comp Sci & Technol, Beijing 100044, Peoples R China
AD  - Inst Informat Sci, Beijing Jiaotong Univ, Sch Comp Sci & Technol, Beijing 100044, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Lab Cognit & Decis Intelligence Complex Syst, Beijing 100190, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Lab Cognit & Decis Intelligence Complex Syst, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China
Y2  - 2024-07-08
ER  -

TY  - JOUR
AU  - Shen, Zhengyang
AU  - Wang, Feng
AU  - Dai, Jin
TI  - Weakly Supervised Temporal Action Localization by Multi-Stage Fusion Network
T2  - IEEE ACCESS
M3  - Article
AB  - Most temporal action localization methods are usually trained using video data-sets with frame-wise annotations which are expensive and time-consuming to acquire. To alleviate this problem, many weakly supervised temporal action localization methods which only leverage video-level annotations during training are proposed. In this paper, we first analyze three problems of weakly supervised temporal action localization, namely feature similarity, action completeness, and weak annotation. Based on these three problems, we propose a novel network called multi-stage fusion network, which decomposes the problems into three different modules within the network, namely feature, sub-action, and action modules. Specifically, for feature similarity, a Triplet Loss was introduced to ensure the action instances from the same class having similar feature sequences and expand the margin of the action instance from different classes in the feature module. For action completeness, each stage of the sub-action module can discover the different sub-actions. The complete action instances can be localized in the action module by fusing multiple sub-actions from the sub-action module. To alleviate weak annotation, we localize multiple action proposals from multi-stage outputs of the network in the action module and select the action proposals with higher confidence scores as predicted action instances. Extensive experiment results on data-sets Thumos'14 and ActivityNet1.2 demonstrate that our method outperforms the state-of-the-art methods and the average mean Average Precision (mAP) on Thumos'14 is significantly improved from 40.9% to 43.3%.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2020 
PY  - 2020
VL  - 8
SP  - 17287
EP  - 17298
DO  - 10.1109/ACCESS.2020.2967627
AN  - WOS:000524753200064
AD  - Chinese Acad Sci, Hefei Inst Phys Sci, Inst Plasma Phys, Hefei 230031, Peoples R China
AD  - Univ Sci & Technol China, Grad Sch, Sci Isl Branch, Hefei 230026, Peoples R China
Y2  - 2020-04-24
ER  -

TY  - CPAPER
AU  - Burdukovskaya, Galina
AU  - Shadrin, Dmitrii
AU  - Ovchinnikov, George
AU  - Fedorov, Maxim
A1  - IEEE
TI  - Improving of Action Localization in Videos Using the Novel Feature Extraction
T2  - PROCEEDINGS OF 2021 IEEE 30TH INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS (ISIE)
M3  - Proceedings Paper
CP  - 30th IEEE International Symposium on Industrial Electronics (ISIE)
CL  - Kyoto, JAPAN
AB  - Nowadays, automatic video analysis using state-of-the-art machine learning techniques is highly relevant and finds lots of industrial applications. This approach gives the possibility to solve a set of the problems such as finding the main part of the video, tracking in time by the duration of an action that is labor-intensive and almost impossible to be solved manually or by old-fashioned methods. One of the most promising approaches is the Weakly-supervised method. The Weakly-supervised technique can be used for temporal action localization, which is a challenging task because it uses only video-level labels during the training. The main benefit of this method is that it is not required to create large-scale datasets with temporal annotations, which are quite time-consuming. In this work, we propose to improve the current state-of-the-art in temporal action localization by introducing the new feature extraction procedure. We achieved better results on the RGB stream by making the new feature extraction using more powerful neural network models on the benchmark THUMOS'14 dataset [1]. We also provide a deep analysis of the importance of the feature extraction procedure in the whole workflow for temporal action detection. The results of our investigations can be applied directly for solving a wide range of industrial problems in a robust and accurate manner.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2163-5137
SN  - 978-1-7281-9023-5
DA  - 2021 
PY  - 2021
DO  - 10.1109/ISIE45552.2021.9576318
AN  - WOS:000779299900132
AD  - Skolkovo Inst Sci & Technol, Ctr Computat & Data Intens Sci & Engn CDISE, Moscow, Russia
AD  - Skolkovo Inst Sci & Technol, Digital Agr Lab DAL, Moscow, Russia
Y2  - 2022-04-28
ER  -

TY  - JOUR
AU  - Tang, Yiping
AU  - Ge, Junyao
AU  - Guo, Kaitai
AU  - Zheng, Yang
AU  - Hu, Haihong
AU  - Liang, Jimin
TI  - Towards better utilization of pseudo labels for weakly supervised temporal action localization
T2  - INFORMATION SCIENCES
M3  - Article
AB  - Weakly supervised temporal action localization (WS-TAL) aims to simultaneously recognize and localize action instances of interest in untrimmed videos with the use of the video-level label only. Some works have demonstrated that pseudo labels play an important role for performance improvement in WS-TAL. Since pseudo labels are inevitably inaccurate, direct adoption of noisy labels can lead to inappropriate knowledge transfer. Although some previous studies have shown the benefits of using only "reliable" pseudo labels, performance improvement is still limited. In this work, we experimentally analyze how the noise in pseudo labels affects model performance within the self-distillation framework. Motivated by the finding that incorrect pseudo labels with large confidence scores have a significant impact on performance, we propose the overconfidence suppression (OCS) strategy to mitigate the effect of the overconfident pseudo labels, and thus prevent over-fitting of the student model. In addition, a simplified contrast learning method is utilized to fine-tune the feature representation by increasing the separation of the foreground and background snippets. Equipped with the proposed methods, the benefits of pseudo labels can be better exploited and allow the model to achieve state-of-the-art performance on THUMOS'14 and ActivityNet-1.2 benchmarks. (C) 2022 Elsevier Inc. All rights reserved.
PU  - ELSEVIER SCIENCE INC
PI  - NEW YORK
PA  - STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN  - 0020-0255
SN  - 1872-6291
DA  - 2023 APR
PY  - 2023
VL  - 623
SP  - 693
EP  - 708
DO  - 10.1016/j.ins.2022.12.044
AN  - WOS:000913249900001
C6  - DEC 2022
AD  - Xidian Univ, Sch Elect Engn, Xian, Peoples R China
Y2  - 2023-02-02
ER  -

TY  - CPAPER
AU  - Qing, Zhiwu
AU  - Su, Haisheng
AU  - Gan, Weihao
AU  - Wang, Dongliang
AU  - Wu, Wei
AU  - Wang, Xiang
AU  - Qiao, Yu
AU  - Yan, Junjie
AU  - Gao, Changxin
AU  - Sang, Nong
A1  - IEEE COMP SOC
TI  - Temporal Context Aggregation Network for Temporal Action Proposal Refinement
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through "local and global" temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both "local and global" temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 485
EP  - 494
DO  - 10.1109/CVPR46437.2021.00055
AN  - WOS:000739917300046
AD  - Huazhong Univ Sci & Technol, Key Lab Image Proc & Intelligent Control, Sch Artificial Intelligence & Automat, Wuhan, Peoples R China
AD  - SenseTime Res, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - SenseTime Res
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Vasileiou, Vasiliki I
AU  - Kardaris, Nikolaos
AU  - Maragos, Petros
TI  - Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Nowadays, the interaction between humans and robots is constantly expanding, requiring more and more human motion recognition applications to operate in real time. However, most works on temporal action detection and recognition perform these tasks in offline manner, i.e. temporally segmented videos are classified as a whole. In this paper, based on the recently proposed framework of Temporal Recurrent Networks, we explore how temporal context and human movement dynamics can be effectively employed for online action detection. Our approach uses various state-of-the-art architectures and appropriately combines the extracted features in order to improve action detection. We evaluate our method on a challenging but widely used dataset for temporal action localization, THUMOS&#39;14. Our experiments show significant improvement over the baseline method, achieving state-of-the art results on THUMOS&#39;14.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2106.13967
AN  - PPRN:12352328
AD  - Natl Tech Univ Athens, Sch E.C.E, Athens 15773, Greece
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Xia, Huifen
AU  - Zhan, Yongzhao
TI  - A Survey on Temporal Action Localization
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action localization is one of the most crucial and challenging problems for video understanding in computer vision. It has received a lot of attention in recent years because of the extensive application of daily life. Temporal action localization has made some significant progress, especially with the development of deep learning recently. And more demand is for temporal action localization in untrimmed videos. In this paper, our target is to survey the state-of-the-art techniques and models for video temporal action localization. It mainly includes the related techniques, some benchmark datasets and the evaluation metrics of temporal action localization. In addition, we summarize temporal action localization from two aspects: fully-supervised learning and weakly-supervised learning. And we list several representative works and compare their performances respectively. Finally, we make some deep analysis and propose potential research directions, and conclude the survey.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2020 
PY  - 2020
VL  - 8
SP  - 70477
EP  - 70487
DO  - 10.1109/ACCESS.2020.2986861
AN  - WOS:000549832700004
AD  - Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China
AD  - Changzhou Vocat Inst Mechatron Technol, Changzhou 213164, Jiangsu, Peoples R China
Y2  - 2020-07-28
ER  -

TY  - JOUR
AU  - Vahdani, Elahe
AU  - Tian, Yingli
TI  - POTLoc: Pseudo-label Oriented Transformer for point-supervised temporal Action Localization
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a P seudo-label O riented T ransformer for weakly-supervised Action Loc alization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of 'pseudo-labels' to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state -of -the -art point-supervised methods on THUMOS'14 and ActivityNet-v1.2 datasets.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2024 SEP
PY  - 2024
VL  - 246
C7  - 104044
DO  - 10.1016/j.cviu.2024.104044
AN  - WOS:001258313300001
C6  - JUN 2024
AD  - CUNY, City Coll, New York, NY 10031 USA
AD  - CUNY, Grad Ctr, New York, NY 10031 USA
Y2  - 2024-07-10
ER  -

TY  - JOUR
AU  - Song, Qing
AU  - Zhou, Yang
AU  - Hu, Mengjie
AU  - Liu, Chun
TI  - Faster learning of temporal action proposal via sparse multilevel boundary generator
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Temporal action localization in videos presents significant challenges in the field of computer vision. While the boundary-sensitive method has been widely adopted, its limitations include incomplete use of intermediate and global information, as well as an inefficient proposal feature generator. To address these challenges, we propose a novel framework, Sparse Multilevel Boundary Generator (SMBG), which enhances the boundary-sensitive method with boundary classification and action completeness regression. SMBG features a multi-level boundary module that enables faster processing by gathering boundary information at different lengths. Additionally, we introduce a sparse extraction confidence head that distinguishes information inside and outside the action, further optimizing the proposal feature generator. To improve the synergy between multiple branches and balance positive and negative samples, we propose a global guidance loss. Our method is evaluated on two popular benchmarks, ActivityNet-1.3 and THUMOS14, and is shown to achieve state-of-the-art performance, with a better inference speed (2.47xBSN++, 2.12xDBG). These results demonstrate that SMBG provides a more efficient and simple solution for generating temporal action proposals. Our proposed framework has the potential to advance the field of computer vision and enhance the accuracy and speed of temporal action localization in video analysis.The code and models are made available at .
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2024 JAN
PY  - 2024
VL  - 83
IS  - 3
SP  - 9121
EP  - 9136
DO  - 10.1007/s11042-023-15308-x
AN  - WOS:001000907700005
C6  - JUN 2023
AD  - Beijing Univ Posts & Telecommun, Pattern Recognit & Intelligent Vis, Xi Tu Cheng Rd, Beijing 100876, Peoples R China
Y2  - 2023-06-21
ER  -

TY  - JOUR
AU  - Li, Bairong
AU  - Pan, Yifan
AU  - Liu, Ruixin
AU  - Zhu, Yuesheng
TI  - Separately Guided Context-Aware Network for Weakly Supervised Temporal Action Detection
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Weakly supervised temporal action detection uses the extracted appearance and motion features to localize the action segments in untrimmed videos with only action category labels. Most previous methods detect action segments based on temporally local features, and employ the early fusion or the late fusion machine to combine the knowledge of two feature modalities. However, the temporally local features generally lead to incomplete detection results, and the above-mentioned fusion machines cannot fully use the complementary information between different modalities. In this paper, we propose the separately guided context-aware network to exploit the global contexts and sufficiently leverage different modality information to detect action segments. Specifically, we propose to construct graphs by modeling the co-occurrence relations between frames to gather the global contexts. To fully combine the complementary information of two modalities, the separately guided scheme is proposed, which utilizes two graphs for each feature modality to integrate the contexts revealed by the intra-modality and the cross-modality information respectively. This scheme sufficiently enhances frame representations based on two modalities and facilitates the detection of action frames. And we also present the co-occurrence relation learning strategy under weak supervision to better guide graphs in gathering contexts. Extensive experiments on the THUMOS14 dataset and the ActivityNet dataset demonstrate the superior performance of the proposed method. Particularly, the proposed method achieves a mean average precision of 39.1% and 42.0% on the THUMOS14 and the ActivityNet dataset respectively under the IoU threshold 0.5.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2023 OCT
PY  - 2023
VL  - 55
IS  - 5
SP  - 6269
EP  - 6288
DO  - 10.1007/s11063-022-11138-4
AN  - WOS:000952080600002
C6  - MAR 2023
AD  - Peking Univ, Shenzhen Grad Sch, Shenzhen, Peoples R China
Y2  - 2023-04-06
ER  -

TY  - JOUR
AU  - Liu, Xiaolong
AU  - Wang, Qimeng
AU  - Hu, Yao
AU  - Tang, Xu
AU  - Zhang, Shiwei
AU  - Bai, Song
AU  - Bai, Xiang
TI  - End-to-End Temporal Action Detection With Transformer
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 5427
EP  - 5441
DO  - 10.1109/TIP.2022.3195321
AN  - WOS:000842776300012
AD  - Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China
AD  - Alibaba Grp, Beijing 100102, Peoples R China
AD  - Alibaba Grp, Hangzhou 311121, Peoples R China
AD  - ByteDance Inc, Singapore 048583, Singapore
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China
M2  - ByteDance Inc
Y2  - 2022-08-28
ER  -

TY  - CPAPER
AU  - Zhang, Pengfei
AU  - Cao, Yu
AU  - Liu, Benyuan
A1  - IEEE
TI  - MULTI-STREAM SINGLE SHOT SPATIAL-TEMPORAL ACTION DETECTION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (i) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (ii) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71:30% on the challenging UCF101-24 [1] actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 3691
EP  - 3695
DO  - 10.1109/icip.2019.8803564
AN  - WOS:000521828603166
AD  - Univ Massachusetts, Dept Comp Sci, Lowell, MA 01854 USA
Y2  - 2020-04-15
ER  -

TY  - JOUR
AU  - Gritsenko, Alexey
AU  - Xiong, Xuehan
AU  - Djolonga, Josip
AU  - Dehghani, Mostafa
AU  - Sun, Chen
AU  - Lucic, Mario
AU  - Schmid, Cordelia
AU  - Arnab, Anurag
TI  - End-to-End Spatio-Temporal Action Localisation with Video Transformers
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The most performant spatio-temporal action localisation models use external person proposals and complex external memory banks. We propose a fully end-to-end, purely-transformer based model that directly ingests an input video, and outputs tubelets -- a sequence of bounding boxes and the action classes at each frame. Our flexible model can be trained with either sparse bounding-box supervision on individual frames, or full tubelet annotations. And in both cases, it predicts coherent tubelets as the output. Moreover, our end-to-end model requires no additional pre-processing in the form of proposals, or post-processing in terms of non-maximal suppression. We perform extensive ablation experiments, and significantly advance the state-of-the-art results on four different spatio-temporal action localisation benchmarks with both sparse keyframes and full tubelet annotations.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.12160
AN  - PPRN:65237874
AD  - Google Res, New York, NY 10011, USA
M2  - Google Res
Y2  - 2023-08-20
ER  -

TY  - JOUR
AU  - Wang, Chuanxu
AU  - Wang, Jing
AU  - Xu, Wenting
TI  - Double branch synergies with modal reinforcement for weakly supervised temporal action detection
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - Weakly supervised Temporal Action localization (WTAL) aims to locate the action instances and identify their corresponding labels. Most current methods rely on a Multi-Instance Learning (MIL) framework to predict start and end boundaries of each action in a video. However, they have shortcomings of incomplete positioning and context confusion. Therefore, we propose an algorithm of Double Branch Synergies with Modal Reinforcement (DBSMR), which utilizes long-short temporal attention to model contextual relationships and refines segmental features to encourage more distinguishable segment classification. In terms of blur boundaries between actions and camouflage background in complex scenes and easily resulting in wrong positioning, we construct a sparse graph focusing on the effective representation of context motion by optical flow modal learning, further enhancing the representation of the active region to be examined, and suppressing the interference from the background. Finally, with the idea of "All roads lead to Rome", we design motion-guided loss constraints to balance the long-short temporal module and graph reinforcement module, by which the two branches can converge to almost the same detection goal, thus to achieve an agreement of near ground truth localization result. The algorithm achieves mAP of 69.1% and 42.0% detection performances on the THUMOS14 and ActivityNet1.2 datasets respectively. We also verify its effectiveness by comparing it with the state-of-the-art methods.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2024 MAR
PY  - 2024
VL  - 99
C7  - 104090
DO  - 10.1016/j.jvcir.2024.104090
AN  - WOS:001186577200001
C6  - FEB 2024
AD  - Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266001, Peoples R China
Y2  - 2024-03-29
ER  -

TY  - JOUR
AU  - Liu, Yi
AU  - Wang, Limin
AU  - Wang, Yali
AU  - Ma, Xiao
AU  - Qiao, Yu
TI  - FineAction: A Fine-Grained Video Dataset for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond. The dataset is available at https://deeperaction.github.io/datasets/fineaction.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 6937
EP  - 6950
DO  - 10.1109/TIP.2022.3217368
AN  - WOS:000880642200002
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Prov Key Lab Comp Vis & Virtual Real Te, Shenzhen 518055, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
AD  - Shenzhen Inst Artificial Intelligence & Robot Soc, SIAT Branch, Shenzhen 518172, Peoples R China
AD  - Shanghai AI Lab, Shanghai 202150, Peoples R China
Y2  - 2022-11-22
ER  -

TY  - JOUR
AU  - Liu, Yi
AU  - Wang, Limin
AU  - Wang, Yali
AU  - Ma, Xiao
AU  - Qiao, Yu
TI  - FineAction: A Fine-Grained Video Dataset for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2105.11107
AN  - PPRN:22123333
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Chinese Acad Sci
M2  - Nanjing Univ
Y2  - 2023-11-23
ER  -

TY  - CPAPER
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - Zero-Shot Temporal Action Detection via Vision-Language Prompting
T2  - COMPUTER VISION - ECCV 2022, PT III
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Existing temporal action detection (TAD) methods rely on large training data including segment-level annotations, limited to recognizing previously seen classes alone during inference. Collecting and annotating a large training set for each class of interest is costly and hence unscalable. Zero-shot TAD (ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with significantly less investigation. Inspired by the success of zero-shot image classification aided by vision-language (ViL) models such as CLIP, we aim to tackle the more complex TAD task. An intuitive method is to integrate an off-the-shelf proposal detector with CLIP style classification. However, due to the sequential localization (e.g., proposal generation) and classification design, it is prone to localization error propagation. To overcome this problem, in this paper we propose a novel zero-Shot Temporal Action detection model via Vision-LanguagE prompting (STALE). Such a novel design effectively eliminates the dependence between localization and classification by breaking the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for improved optimization. Extensive experiments on standard ZS-TAD video benchmarks show that our STALE significantly outperforms state-of-the-art alternatives. Besides, our model also yields superior results on supervised TAD over recent strong competitors. The PyTorch implementation of STALE is available on https://github.com/sauradip/STALE.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20061-8
SN  - 978-3-031-20062-5
DA  - 2022 
PY  - 2022
VL  - 13663
SP  - 681
EP  - 697
DO  - 10.1007/978-3-031-20062-5_39
AN  - WOS:000899240500039
AD  - Univ Surrey, CVSSP, Guildford, Surrey, England
AD  - IFlyTek Surrey Joint Res Ctr Artificial Intellige, London, England
AD  - Univ Surrey, Surrey Inst People Ctr Artificial Intelligence, Guildford, Surrey, England
M2  - IFlyTek Surrey Joint Res Ctr Artificial Intellige
Y2  - 2023-01-25
ER  -

TY  - JOUR
AU  - Wang, Le
AU  - Duan, Xuhuan
AU  - Zhang, Qilin
AU  - Niu, Zhenxing
AU  - Hua, Gang
AU  - Zheng, Nanning
TI  - Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation
T2  - SENSORS
M3  - Article
AB  - Inspired by the recent spatio-temporal action localization efforts with tubelets (sequences of bounding boxes), we present a new spatio-temporal action localization detector Segment-tube, which consists of sequences of per-frame segmentation masks. The proposed Segment-tube detector can temporally pinpoint the starting/ending frame of each action category in the presence of preceding/subsequent interference actions in untrimmed videos. Simultaneously, the Segment-tube detector produces per-frame segmentation masks instead of bounding boxes, offering superior spatial accuracy to tubelets. This is achieved by alternating iterative optimization between temporal action localization and spatial action segmentation. Experimental results on three datasets validated the efficacy of the proposed method, including (1) temporal action localization on the THUMOS 2014 dataset; (2) spatial action segmentation on the Segtrack dataset; and (3) joint spatio-temporal action localization on the newly proposed ActSeg dataset. It is shown that our method compares favorably with existing state-of-the-art methods.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
DA  - 2018 MAY
PY  - 2018
VL  - 18
IS  - 5
C7  - 1657
DO  - 10.3390/s18051657
AN  - WOS:000435580300356
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - HERE Technol, Chicago, IL 60606 USA
AD  - Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China
AD  - Microsoft Res, Redmond, WA 98052 USA
M2  - HERE Technol
Y2  - 2018-08-20
ER  -

TY  - CPAPER
AU  - Yang, Jiantao
AU  - Liu, Sheng
AU  - Feng, Yuan
AU  - Tian, Xiaopeng
AU  - Zhang, Yineng
AU  - Pan, Songqi
A1  - IEEE
TI  - Supervised Temporal 2024InternationalJointConferenceonNeuralNetworks( IJCNN)| 979-8-3503-5931-2/24/$31.00©2024IEEE| DOI:ATCE: Adaptive Temporal Context Exploitation for Weakly-Supervised Temporal Action Localization
T2  - 2024 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN 2024
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - Yokohama, JAPAN
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 979-8-3503-5932-9
SN  - 979-8-3503-5931-2
DA  - 2024 
PY  - 2024
DO  - 10.1109/IJCNN60899.2024.10650778
AN  - WOS:001392668200049
AD  - Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China
Y2  - 2025-03-05
ER  -

TY  - JOUR
AU  - Su, Haisheng
AU  - Feng, Jinyuan
AU  - Shao, Hao
AU  - Jiang, Zhenyu
AU  - Zhang, Manyuan
AU  - Wu, Wei
AU  - Liu, Yu
AU  - Li, Hongsheng
AU  - Yan, Junjie
TI  - Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2020 Task 1 (\textbf{temporal action localization/detection}). Temporal action localization requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. In this paper, we decouple the temporal action localization task into two stages (i.e. proposal generation and classification) and enrich the proposal diversity through exhaustively exploring the influences of multiple components from different but complementary perspectives. Specifically, in order to generate high-quality proposals, we consider several factors including the video feature encoder, the proposal generator, the proposal-proposal relations, the scale imbalance, and ensemble strategy. Finally, in order to obtain accurate detections, we need to further train an optimal video classifier to recognize the generated proposals. Our proposed scheme achieves the state-of-the-art performance on the temporal action localization task with \textbf{42.26} average mAP on the challenge testing set.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2007.09883
AN  - PPRN:13152225
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Yao, Ting
AU  - Li, Xue
TI  - YH Technologies at ActivityNet Challenge 2018
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This notebook paper presents an overview and comparative analysis of our systems designed for the following five tasks in ActivityNet Challenge 2018: temporal action proposals, temporal action localization, dense-captioning events in videos, trimmed action recognition, and spatio-temporal action localization.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.00686
AN  - PPRN:22700579
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Yu, Jiaruo
AU  - Ge, Yongxin
AU  - Qin, Xiaolei
AU  - Li, Ziqiang
AU  - Huang, Sheng
AU  - Chen, Feiyu
TI  - Deep feature enhancing and selecting network for weakly supervised temporal action localization
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - Weakly supervised temporal action localization is a challenging computer vision problem that uses only video-level labels and lacks the supervision of temporal annotations. In this task, the majority of existing methods usually identify the most discriminative snippets and ignore other relevant snippets. To address this problem, we propose a deep feature enhancing and selecting network. It generates multiple masks for both capturing more complete temporal interval of actions and keeping its high classification accuracy. After that, we further propose a novel selection strategy to balance the influence of multiple masks and improve the model performance. In the experiments, we evaluate the proposed method on the THUMOS'14 and ActivityNet datasets, and the results show the effectiveness of our approach for weakly supervised temporal action localization.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2021 OCT
PY  - 2021
VL  - 80
C7  - 103276
DO  - 10.1016/j.jvcir.2021.103276
AN  - WOS:000703429600004
C6  - AUG 2021
AD  - Chongqing Univ, Sch Big Data & Software Engn, Chongqing, Peoples R China
AD  - Natl Ctr Appl Math Chongqing, Chongqing, Peoples R China
M2  - Natl Ctr Appl Math Chongqing
Y2  - 2021-10-13
ER  -

TY  - JOUR
AU  - Qin, Yefeng
AU  - Chen, Lei
AU  - Ben, Xianye
AU  - Yang, Mingqiang
TI  - You watch once more: a more effective CNN architecture for video spatio-temporal action localization
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - The task of spatio-temporal action localization (STAL) needs to detect the action and position of individuals in the scene. Many works cannot model spatio-temporal information well, and they usually ignore inference speed and practical applications. To address the above problems, we propose a new end-to-end spatio-temporal action localization network called You Watch Once More (YWOM). Two backbones are applied to extract spatio-temporal information effectively. In this work, there are three measures proposed to improve the accuracy of positioning and recognition while guaranteeing the inference speed. First, a new feature fusion mechanism based on frequency channel attention (FCA) is proposed, which can effectively fuse the features extracted by different backbones. In addition, a new loss function is proposed to speed up the regression and convergence of the bounding box. Specifically, the SIOU regression loss function instead of the smooth L1 loss function is applied to help the model converge stably. Moreover, a lateral connection mechanism is designed to apply more backbones to our network structure. The experimental results demonstrate that YWOM can achieve online inference speed and has good performance in spatio-temporal action localization tasks. YWOM has superiority over other related works including YOWO on the UCF101-24 dataset. The frame-mAP and the video-mAP (0.2) are improved by 4.23% and 1.63% respectively.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2024 FEB
PY  - 2024
VL  - 30
IS  - 1
C7  - 53
DO  - 10.1007/s00530-023-01254-z
AN  - WOS:001151990500002
AD  - Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Shandong, Peoples R China
Y2  - 2024-02-06
ER  -

TY  - JOUR
AU  - Qiu, Zhaofan
AU  - Li, Dong
AU  - Li, Yehao
AU  - Cai, Qi
AU  - Pan, Yingwei
AU  - Yao, Ting
TI  - Trimmed Action Recognition, Dense-Captioning Events in Videos, and Spatio-temporal Action Localization with Focus on ActivityNet Challenge 2019
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This notebook paper presents an overview and comparative analysis of our systems designed for the following three tasks in ActivityNet Challenge 2019: trimmed action recognition, dense-captioning events in videos, and spatio-temporal action localization.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1906.07016
AN  - PPRN:22601029
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Sun, Jingtao
AU  - Shi, Weipeng
AU  - Hao, Shaoyang
AU  - Wang, Jialin
TI  - Weakly Supervised Temporal Action Localization With Contrastive Learning-Based Action Salience Network
T2  - EUROPEAN JOURNAL ON ARTIFICIAL INTELLIGENCE
M3  - Article
AB  - In recent years, the wide application of weakly supervised temporal action localization (WTAL) technology has accelerated the efficiency of video analysis. However, this domain continues to confront numerous challenges, especially due to the lack of precise temporal annotations. Consequently, this technique becomes highly susceptible to contextual background noise and overly reliant on prominent action segments, leading to less-than-ideal action localization. To alleviate this problem, we propose the contrastive learning-based action salience network (CLASNet), comprising two pivotal modules: feature contrast separation module (FCSM) and boundary refinement module (BRM). FCSM utilizes a contrastive learning approach to effectively separate action features from background features, thereby enhancing the discriminability of features. Concurrently, BRM introduces boundary refinement loss to rectify the temporal boundaries of actions, thereby further elevating the precision of temporal localization. The collaborative functioning of these two key modules effectively resolves the ambiguity issues in temporal action localization under weak supervision, markedly enhancing localization accuracy. Furthermore, CLASNet is versatile and can be integrated into different WTAL frameworks, achieving enhanced localization performance while preserving the original end-to-end training manner. Utilizing three large-scale benchmark action localization datasets, THUMOS14, ActivityNet v1.2, and ActivityNet v1.3, we embed CLASNet into various cutting-edge weakly supervised temporal action localization methods, such as CO2-Net, DELU, and ACRNet, for empirical substantiation. The experimental outcomes reveal that CLASNet significantly enhances the efficacy of these methods in action localization, offering novel perspectives for the advancement of temporal action localization technology.
PU  - SAGE PUBLICATIONS LTD
PI  - LONDON
PA  - 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN  - 3050-4554
SN  - 3050-4546
DA  - 2025 FEB
PY  - 2025
VL  - 38
IS  - 1
SP  - 64
EP  - 78
DO  - 10.1177/30504554241301394
AN  - WOS:001456093600001
AD  - Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian, Shaanxi, Peoples R China
AD  - Xian Univ Posts & Telecommun, Shaanxi Key Lab Network Data Anal & Intelligent Pr, Xian, Shaanxi, Peoples R China
Y2  - 2025-04-05
ER  -

TY  - CPAPER
AU  - Park, Jungin
AU  - Lee, Jiyoung
AU  - Jeon, Sangryul
AU  - Kim, Seungryong
AU  - Sohn, Kwanghoon
A1  - IEEE
TI  - GRAPH REGULARIZATION NETWORK WITH SEMANTIC AFFINITY FOR WEAKLY-SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - This paper presents a novel deep architecture for weakly-supervised temporal action localization that not only generates segment-level action responses but also propagates segment-level responses to the neighborhood in a form of graph Laplacian regularization. Specifically, our approach consists of two sub-modules; a class activation module to estimate the action score map over time through the action classifiers, and a graph regularization module to refine the estimated action score map by solving a quadratic programming problem with the predicted segment-level semantic affinities. Since these two modules are integrated with fully differentiable layers, the proposed networks can be jointly trained in an end-to-end manner. Experimental results on Thumos14 and ActivityNet1.2 demonstrate that the proposed method provides outstanding performances in weakly-supervised temporal action localization.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 3701
EP  - 3705
DO  - 10.1109/icip.2019.8803589
AN  - WOS:000521828603168
AD  - Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea
AD  - Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, Lausanne, Switzerland
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Li, Ronglu
AU  - Zhang, Tianyi
AU  - Zhang, Rubo
TI  - Weakly supervised temporal action localization: a survey
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
M3  - Early Access
AB  - Temporal action localization (TAL) is one of the most important tasks in video understanding. Weakly supervised temporal action localization (WTAL) involves classifying and localizing all the action instances in untrimmed videos under the supervision of only video-level category labels, which is a challenging task because of the absence of frame-level annotations. In this study, first, we review the development process of the WTAL task in recent years, summarize and analyze the main problems of WTAL. Second, we classify and compare the research approaches of existing models and thoroughly discuss methods based on multiple instance learning (MIL), feature erasing, the attention mechanism, similarity propagation, pseudo-ground truth generation, contrastive learning, and adversarial learning. Then, we present the datasets and evaluation criteria for the WTAL task. Finally, we discuss the main application areas and further developments in WTAL.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2024 FEB 22
PY  - 2024
DO  - 10.1007/s11042-024-18554-9
AN  - WOS:001168583600013
C6  - FEB 2024
AD  - Dalian Minzu Univ, Coll Mech & Elect Engn, Dalian 116600, Liaoning, Peoples R China
AD  - Beihang Univ, Sch Cyber Sci & Technol, Beijing 100191, Peoples R China
Y2  - 2024-03-20
ER  -

TY  - CPAPER
AU  - Li, Hongru
AU  - Yang, Jianxing
AU  - Zhou, Yuan
AU  - Li, Sumei
A1  - IEEE
TI  - RETHINKING TEMPORAL STRUCTURE MODELING METHOD FOR TEMPORAL ACTION LOCALIZATION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling temporal structures of videos. In the present study, we developed a novel method, referred to as Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are attributable to three major factors. First, the developed network utilizes two sub-nets for effective modeling of temporal structures. Second, three parallel feature extraction pipelines are used to prevent interference between the extractions of different stage features. Third, the proposed method utilizes auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network was used to achieve state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 3676
EP  - 3680
DO  - 10.1109/icip.2019.8803628
AN  - WOS:000521828603163
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China
Y2  - 2020-04-15
ER  -

TY  - JOUR
AU  - Lin, Tianwei
AU  - Zhao, Xu
AU  - Shou, Zheng
TI  - Temporal Convolution Based Action Proposal: Submission to ActivityNet 2017
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this notebook paper, we describe our approach in the submission to the temporal action proposal (task 3) and temporal action localization (task 4) of ActivityNet Challenge hosted at CVPR 2017. Since the accuracy in action classification task is already very high (nearly 90% in ActivityNet dataset), we believe that the main bottleneck for temporal action localization is the quality of action proposals. Therefore, we mainly focus on the temporal action proposal task and propose a new proposal model based on temporal convolutional network. Our approach achieves the state-of-the-art performances on both temporal action proposal task and temporal action localization task.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1707.06750
AN  - PPRN:21093470
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Degiorgio, Kurt
AU  - Cuzzolin, Fabio
TI  - Spatio-Temporal Action Localization in a Weakly Supervised Setting
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Enabling computational systems with the ability to localize actions in video-based content has manifold applications. Traditionally, such a problem is approached in a fully-supervised setting where video-clips with complete frame-by-frame annotations around the actions of interest are provided for training. However, the data requirements needed to achieve adequate generalization in this setting is prohibitive. In this work, we circumvent this issue by casting the problem in a weakly supervised setting, i.e., by considering videos as labelled `sets' of unlabelled video segments. Firstly, we apply unsupervised segmentation to take advantage of the elementary structure of each video. Subsequently, a convolutional neural network is used to extract RGB features from the resulting video segments. Finally, Multiple Instance Learning (MIL) is employed to predict labels at the video segment level, thus inherently performing spatio-temporal action detection. In contrast to previous work, we make use of a different MIL formulation in which the label of each video segment is continuous rather then discrete, making the resulting optimization function tractable. Additionally, we utilize a set splitting technique for regularization. Experimental results considering multiple performance indicators on the UCF-Sports data-set support the effectiveness of our approach.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1905.02171
AN  - PPRN:19392601
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Liu, Yifan
AU  - Tang, Youbao
AU  - Zhang, Ning
AU  - Lin, Ruei-Sung
AU  - Wang, Haoqian
TI  - Prior-enhanced Temporal Action Localization using Subject-aware Spatial Attention
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization (TAL) aims to detect the boundary and identify the class of each action instance in a long untrimmed video. Current approaches treat video frames homogeneously, and tend to give background and key objects excessive attention. This limits their sensitivity to localize action boundaries. To this end, we propose a prior enhanced temporal action localization method (PETAL), which only takes in RGB input and incorporates action subjects as priors. This proposal leverages action subjects&rsquo; information with a plug-and-play subject-aware spatial attention module (SA-SAM) to generate an aggregated and subject-prioritized representation. Experimental results on THUMOS-14 and ActivityNet-1.3 datasets demonstrate that the proposed PETAL achieves competitive performance using only RGB features, e.g., boosting mAP by 2.41% or 0.25% over the state-of-the-art approach that uses RGB features or with additional optical flow features on the THUMOS-14 dataset.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2211.05299
AN  - PPRN:22712685
AD  - Tsinghua Univ, Shenzhen, Peoples R China
AD  - PAII Inc, Palo Alto, CA 94306, USA
M2  - PAII Inc
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Zhou, Yuan
AU  - Wang, Ruolin
AU  - Li, Hongru
AU  - Kung, Sun-Yuan
TI  - Temporal Action Localization Using Long Short-Term Dependency
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling the temporal structures of videos. In the present study, we develop a novel method, referred to as the Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are due to three major factors. First, temporal dependencies are explicitly distinguished as long-term temporal dependencies and short-term temporal dependencies and are separately captured by two dedicated subnets. Second, a long-range temporal dependency capture module combined with a self-adaptive pooling module is proposed to capture long-term temporal dependency. Third, the proposed method uses auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network is used to achieve a state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2021 
PY  - 2021
VL  - 23
SP  - 4363
EP  - 4375
DO  - 10.1109/TMM.2020.3042077
AN  - WOS:000720519900035
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China
AD  - Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Dong, Peixiang
AU  - Zhu, Lisong
AU  - Zhang, Yong
A1  - IEEE
TI  - Category-Level Multi-Attention based Boundary Refinement for Action Detection
T2  - 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP 2019)
M3  - Proceedings Paper
CP  - 4th IEEE International Conference on Signal and Image Processing (ICSIP)
CL  - SE Univ, Wuxi, PEOPLES R CHINA
AB  - Temporal action localization has attracted considerable attention in recent years. Most previous work focused on a fully supervised setting leveraging frame-level action labels. However, it is usually expensive and time-consuming to acquire precise temporal annotations. In this paper, we focus on a weakly supervised setting using only video level labels to address the temporal action localization task. A category-level multi-attention based method is proposed to provide a comprehensive representation for each action category and obtain much more precise temporal intervals. Our method is evaluated on the untrimmed THUMOS14 video dataset, and the experiments have provided very positive results.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-3660-8
DA  - 2019 
PY  - 2019
SP  - 230
EP  - 234
DO  - 10.1109/siprocess.2019.8868533
AN  - WOS:000557898200045
AD  - CCTV Int Network Wuxi Co Ltd, Wuxi, Jiangsu, Peoples R China
AD  - Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing, Peoples R China
M2  - CCTV Int Network Wuxi Co Ltd
Y2  - 2020-08-27
ER  -

TY  - JOUR
AU  - Wang, Shaomeng
AU  - Yan, Rui
AU  - Huang, Peng
AU  - Dai, Guangzhao
AU  - Song, Yan
AU  - Shu, Xiangbo
TI  - Com-STAL: Compositional Spatio-Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Spatio-temporal action localization aims to locate the spatial and temporal positions of actors and classify their actions. However, prior research overlooks the fact that human actions often interact with novel objects in real-world scenarios, which neglects the various combinations of action-object, and considerably limits the generalization of the developed models. In this paper, we study the action-object combinations by researching multi-modal vision information of them. To this end, we propose a novel compositional spatio-temporal action localization (Com-STAL) task, which features non-overlapping action-object combinations in their training and test sets. Based on this, we construct a compositional action localization dataset (Com-AD). Beyond that, we propose a simple yet effective framework, Instance-Centric Interaction Network (ICIN), to reduce invalid induction biases within the visual modality and alleviate the combined distribution bias issue by leveraging additional modal information. The extensive experiment results on Com-AD demonstrate superior action localization performance of ICIN.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2023 DEC
PY  - 2023
VL  - 33
IS  - 12
SP  - 7645
EP  - 7657
DO  - 10.1109/TCSVT.2023.3276979
AN  - WOS:001121618300015
AD  - Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China
AD  - Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210023, Peoples R China
Y2  - 2024-03-02
ER  -

TY  - JOUR
AU  - Eun, Hyunjun
AU  - Moon, Jinyoung
AU  - Park, Jongyoul
AU  - Chanho, Jung
AU  - Kim, Changick
TI  - Temporal Action Detection: A Survey
TI  - 시간적 행동 검출: 서베이
T2  - The Journal of Korean Institute of Communications and Information Sciences
T2  - 한국통신학회논문지
M3  - research-article
AB  - In the field of computer vision, action recognition for video understanding has been studied for a long time.However, the videos used in action recognition are trimmed videos processed by professionals for well representing predefined actions. In recent, many people have been able to upload and watch real-world videos from the development of many media platforms. These platforms also make it easier to collect and access such untrimmed videos. As a result, for video understanding, research on temporal action detection on untrimmed videos has been actively studied recently, as well as research on action recognition on trimmed videos. Temporal action detection can be categorized into offline and online action detection, and many temporal action detection methods have been proposed in both fields over the last few years. In addition, due to the recent promising results of deep learning in computer vision, the performance of temporal action detection approaches has been remarkably improved. In this paper, we introduce deep learning-based temporal action detection methods that have recently attracted attention.
PU  - 한국통신학회
SN  - 1226-4717
DA  - 2020 
PY  - 2020
VL  - 45
IS  - 7
SP  - 1152
EP  - 1165
DO  - 10.7840/kics.2020.45.7.1152
AN  - KJD:ART002610007
Y2  - 2021-07-07
ER  -

TY  - JOUR
AU  - Ge, Yongxin
AU  - Qin, Xiaolei
AU  - Yang, Dan
AU  - Jagersand, Martin
TI  - Deep snippet selective network for weakly supervised temporal action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Temporal action localization has been a hot topic in video analyzation. In this paper, we propose a novel method called deep snippet selective network (DSSN) to address two key problems in weak supervision for temporal action localization, which are separability and integrality. Specifically, we employ two eras-ing branches to ensure the integrality, which can force the network to select other complementary snippets by erasing the most discriminative snippets. It is worth mentioning that a ternary mask is utilized to provide erasing branches with a background prior to enhance the separability of the model. Besides, we design a background suppression branch to further reduce the effect of background snippets. Extensive experiments on dataset THUMOS'14 and ActivityNet show the effectiveness of our method. (c) 2020 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2021 FEB
PY  - 2021
VL  - 110
C7  - 107686
DO  - 10.1016/j.patcog.2020.107686
AN  - WOS:000585303400010
AD  - Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China
AD  - Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China
AD  - Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada
Y2  - 2020-12-11
ER  -

TY  - JOUR
AU  - Qin, Xin
AU  - Zhao, Hanbin
AU  - Lin, Guangchen
AU  - Zeng, Hao
AU  - Xu, Songcen
AU  - Li, Xi
TI  - PcmNet: Position-sensitive context modeling network for temporal action localization
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal action localization, which aims to locate temporal regions where actions take place and recog-nize their corresponding classes in untrimmed real-world videos, is a challenging task. As a critical cue to video understanding, exploiting the video context has become an important strategy to boost the local-ization performance. However, previous methods mainly focus on exploring semantic context which cap-tures the feature similarity among frames or proposals. The temporal position context which is also vital for temporal action localization is less explored. In this paper, we propose a position-sensitive context modeling approach to fuse both semantic and position context for more precise action localization. Specifically, we first propose a position encoding method tailored for temporal action localization on both frame-level and proposal-level, which ensures that the generated position representations can model the distance and chronological relationships among frames or proposals. Then we conduct attention-based context aggregation to produce discriminative features and help with precise boundary detection and proposal evaluation. Our method achieves state-of-the-art performance on two widely used datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the effectiveness and generalizability of our method.(c) 2022 Published by Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2022 OCT 21
PY  - 2022
VL  - 510
SP  - 48
EP  - 58
DO  - 10.1016/j.neucom.2022.08.040
AN  - WOS:000862258000005
C6  - SEP 2022
AD  - Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China
AD  - Zhejiang Univ, Shanghai Inst Adv Study, Shanghai, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
AD  - Huawei Technol, Noahs Ark Lab, Beijing, Peoples R China
Y2  - 2022-10-15
ER  -

TY  - JOUR
AU  - Joyce, Eric C.
AU  - Chen, Yao
AU  - Neeter, Eduardo
AU  - Mordohai, Philippos
TI  - SmallTAL: Real-Time Egocentric Online Temporal Action Localization for the Data-Impoverished
T2  - PRESENCE-VIRTUAL AND AUGMENTED REALITY
M3  - Article
AB  - We propose a real-time, online temporal action localization system that requires a small amount of annotated data. The main challenges we address are high intra-class variability and a large and diverse background class. We address these using a flexible frame descriptor, dynamic time warping, and a novel approach to database construction. Our solution receives egocentric RGB-D streams as input and makes predictions at regular temporal intervals. We validate our approach by localizing actions in a digital twin of an electrical substation, in which certain objects have been replaced by functional virtual replicas.
PU  - MIT PRESS
PI  - CAMBRIDGE
PA  - ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN  - 1054-7460
SN  - 1531-3263
DA  - 2023 DEC 1
PY  - 2023
VL  - 32
SP  - 179
EP  - 203
DO  - 10.1162/pres_a_00408
AN  - WOS:001214533100001
AD  - Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA
AD  - HyperTunnel, Basingstoke, England
M2  - HyperTunnel
Y2  - 2024-05-12
ER  -

TY  - JOUR
AU  - Chen, Shimin
AU  - Li, Wei
AU  - Gu, Jianyang
AU  - Chen, Chen
AU  - Guo, Yandong
TI  - Technical Report for ActivityNet Challenge 2022- Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2411.00883
AN  - PPRN:119022346
AD  - OPPO Res Inst, Chengdu, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
M2  - OPPO Res Inst
Y2  - 2024-12-09
ER  -

TY  - JOUR
AU  - Tang, Yepeng
AU  - Wang, Weining
AU  - Yang, Yanwu
AU  - Zhang, Chunjie
AU  - Liu, Jing
TI  - Anchor-free temporal action localization via Progressive Boundary-aware Boosting
T2  - INFORMATION PROCESSING & MANAGEMENT
M3  - Article
AB  - Enormous untrimmed videos from the real world are difficult to analyze and manage. Temporal action localization algorithms can help us to locate and recognize human activity clips in untrimmed videos. Recently, anchor-free temporal action localization methods have gained increasing attention due to small computational costs and no complex hyperparameters of pre-set anchors. Although the performance has been significantly improved, most existing anchor-free temporal action localization methods still suffer from inaccurate action boundary predictions. In this paper, we want to alleviate the above problem through boundary refinement and temporal context aggregation. To this end, a novel Progressive Boundary-aware Boosting Network (PBBNet) is proposed for anchor-free temporal action localization. The PBBNet consists of three main modules: Temporal Context-aware Module (TCM), Instance-wise Boundaryaware Module (IBM), and Frame-wise Progressive Boundary-aware Module (FPBM). The TCM aggregates the temporal context information and provides features for the IBM and the FPBM. The IBM generates multi-scale video features to predict action results coarsely. Compared with IBM, the FPBM focuses on instance features corresponding to action predictions and uses more supervision information for boundary regression. Given action results from IBM, the FPBM uses a progressive boosting strategy to refine the boundary predictions multiple times with supervision from weak to strong. Extensive experiments on three benchmark datasets THUMOS14, ActivityNet-v1.3 and HACS show our PPBNet outperforms all existing anchor-free methods. Further, our PPBNet achieves state-of-the-art performance (72.5% mAP at tIoU = 0.5) on THUMOS14 dataset.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0306-4573
SN  - 1873-5371
DA  - 2023 JAN
PY  - 2023
VL  - 60
IS  - 1
C7  - 103141
DO  - 10.1016/j.ipm.2022.103141
AN  - WOS:000934053500007
C6  - NOV 2022
AD  - Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China
AD  - Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China
AD  - Huazhong Univ Sci & Technol, Sch Management, Wuhan 430074, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China
Y2  - 2023-03-15
ER  -

TY  - JOUR
AU  - Baraka, AbdulRahman
AU  - Noor, Mohd Halim Mohd
TI  - Weakly-supervised temporal action localization: a survey
T2  - NEURAL COMPUTING & APPLICATIONS
M3  - Review
AB  - Temporal Action Localization (TAL) is an important task of various computer vision topics such as video understanding, summarization, and analysis. In the real world, the videos are long untrimmed and contain multiple actions, where the temporal boundaries annotations are required in the fully-supervised learning setting for classification and localization tasks. Since the annotation task is costly and time-consuming, the trend is moving toward the weakly-supervised setting, which depends on the video-level labels only without any additional information, and this approach is called weakly-supervised Temporal Action Localization (WTAL). In this survey, we review the concepts, strategies, and techniques related to the WTAL in order to clarify all aspects of the problem and review the state-of-the-art frameworks of WTAL according to their challenges. Furthermore, a comparison of models' performance and results based on benchmark datasets is presented. Finally, we summarize the future works to allow the researchers to improve the model's performance.
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 0941-0643
SN  - 1433-3058
DA  - 2022 JUN
PY  - 2022
VL  - 34
IS  - 11
SP  - 8479
EP  - 8499
DO  - 10.1007/s00521-022-07102-x
AN  - WOS:000765194500003
C6  - MAR 2022
AD  - Univ Sains Malaysia, Sch Comp Sci, George Town 11800, Malaysia
AD  - Al Quds Open Univ, Ramallah, Palestine
M2  - Al Quds Open Univ
Y2  - 2022-03-16
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Han, Junwei
AU  - Zhao, Tao
AU  - Liu, Nian
AU  - Zhang, Dingwen
TI  - Structured Attention Composition for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2205.09956
AN  - PPRN:12349504
AD  - Northwestern Polytech Univ, Xian, Peoples R China
AD  - Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates
M2  - Incept Inst Artificial Intelligence
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Zeng, Huanbin
AU  - Zhu, Suguo
AU  - Yu, Jun
ED  - Peng, Y
ED  - Liu, Q
ED  - Lu, H
ED  - Sun, Z
ED  - Liu, C
ED  - Chen, X
ED  - Zha, H
ED  - Yang, J
TI  - Discriminative Regions Erasing Strategy for Weakly-Supervised Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2020, PT II
M3  - Proceedings Paper
CP  - 3rd Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Nanjing Univ Sci & Tech, Nanjing, PEOPLES R CHINA
AB  - Weakly-supervised temporal action localization (WTAL) has recently attracted attentions. Many of the state-of-the-art methods usually utilize temporal class activation map (T-CAM) to obtain target action temporal regions. However, class-specific T-CAM tends to cover only the most discriminative part of the actions, not the entire action. In this paper, we propose an erasing strategy for mining discriminative regions in weakly-supervised temporal action localization (DRES). DRES achieves better performance with action localization, which can be attribute to two aspects. First, we employ the salient detection module, which suppresses the background to obtain the most discriminative regions. Second, we design the eraser module to discover the missed action regions by the salient detection module, which complements action regions. Based on experiments, we demonstrate that DRES improve the state-of-the-art performance on THUMOS'14.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-60638-1
SN  - 978-3-030-60639-8
DA  - 2020 
PY  - 2020
VL  - 12306
SP  - 639
EP  - 651
DO  - 10.1007/978-3-030-60639-8_53
AN  - WOS:001425845400053
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - CPAPER
AU  - Zhai, Yuanhao
AU  - Wang, Le
AU  - Liu, Ziyi
AU  - Zhang, Qilin
AU  - Hua, Gang
AU  - Zheng, Nanning
A1  - IEEE
TI  - ACTION COHERENCE NETWORK FOR WEAKLY SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - Most prominent temporal action localization methods are of the fully-supervised type, which rely heavily on frame-level labels, which could be prohibitively expensive to annotate. Thanks to recent developments on the Weakly-supervised Temporal Action Localization (W-TAL), this alternative paradigm requires only video-level labels in training, alleviating such annotation efforts. Specifically, we present Action Coherence Network (ACN) for W-TAL, which features a new coherence loss that better supervises action boundary learning and facilitate proposal regression. In addition, a purpose-built fusion module is proposed for localization inference based on features extracted by two streams of convolutional neural network. Overall, the proposed ACN achieves state-of-the-art W-TAL performance on two challenging datasets (THUMOS14 and ActivityNet1.2, particularly ACN attains mAP of 24.2% on THUMOS14 under IoU threshold 0.5), which is approaching some recent fully-supervised TAL methods.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 3696
EP  - 3700
DO  - 10.1109/icip.2019.8803447
AN  - WOS:000521828603167
AD  - Xi An Jiao Tong Univ, Xian, Peoples R China
AD  - HERE Technol, Sunnyvale, CA USA
AD  - Wormpex AI Res, Bellevue, WA USA
M2  - HERE Technol
M2  - Wormpex AI Res
Y2  - 2020-04-15
ER  -

TY  - JOUR
AU  - Zhao, Yibo
AU  - Zhang, Hua
AU  - Gao, Zan
AU  - Gao, Wenjie
AU  - Wang, Meng
AU  - Chen, Shengyong
TI  - A Novel Action Saliency and Context-Aware Network for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization is a challenging task in computer vision, and it tries to find the start time and the end time of the actions and predict their categories. However, compared to temporal action localization, weakly supervised temporal action localization (WTAL) is a more challenging task due to its poor annotations. With only video-level annotation, some background frames, similar to actions, would be classified as actions and produce inaccurate results. In addition, the two-stream fusion problem, ignored previously, also needs to be further considered. To resolve these issues, we propose a novel action saliency and context-aware network (ASCN) for WTAL tasks. Specifically, the temporal saliency and context module is designed to enhance the global saliency and context information of the RGB and flow features to suppress the backgrounds and enhance the actions. In addition, a hybrid attention mechanism using frame differences and two-stream attention is designed to model the local action context information and further enlarge the scores of the potential action regions and suppress the background regions. Finally, to obtain two-stream consistency and solve the fusion problem, we use the similarity loss and a channel self-attention module to adaptively fuse the enhanced RGB and flow features. Extensive experiments demonstrate that ASCN can outperform all of the SOTA WTAL methods on THUMOS14 dataset and ActivityNet1.3 dataset with an average mAP that can reach 37.2% on THUMOS14 dataset and attains an average mAP of 26.3% on ActivityNet1.3 dataset. On ActivityNet1.2 dataset, ASCN can also obtain comparable results.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 8253
EP  - 8266
DO  - 10.1109/TMM.2023.3234362
AN  - WOS:001125902000047
AD  - Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China
AD  - Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 300384, Peoples R China
Y2  - 2024-01-14
ER  -

TY  - CPAPER
AU  - Chen, Guo
AU  - Zheng, Yin-Dong
AU  - Chen, Zhe
AU  - Wang, Jiahao
AU  - Lu, Tong
A1  - IEEE
TI  - ELAN: ENHANCING TEMPORAL ACTION DETECTION WITH LOCATION AWARENESS
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Brisbane, AUSTRALIA
AB  - Current query-based temporal action detection methods lack multiple levels of location awareness, leading to performance degradation. In this paper, we present a novel query-based method called Enhanced Location-Aware Network (ELAN) for temporal action detection. ELAN adopts a lightweight convolution-based encoder, termed Temporal Location-Aware (TLA) encoder, to model temporal continuous location-aware context. Moreover, ELAN can re-aware the location-related context inside and between queries through our proposed Instance Location-Aware (ILA) decoder. As a result, ELAN can learn strong position discrimination of actions and effectively eliminates the ambiguity caused by sparse action decoding, yielding significant improvement in detection performance. ELAN achieves state-of-the-art performance on two temporal action detection benchmarks, including THUMOS-14 and ActivityNet-1.3.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-6654-6891-6
DA  - 2023 
PY  - 2023
SP  - 1020
EP  - 1025
DO  - 10.1109/ICME55011.2023.00179
AN  - WOS:001062707300165
AD  - Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2023-11-05
ER  -

TY  - JOUR
AU  - Kim, Young Hwi
AU  - Nam, Seonghyeon
AU  - Kim, Seon Joo
TI  - 2PESNet: Towards online processing of temporal action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Existing online video processing methods such as online action detection focus on a frame-level under-standing for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the on-line settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to fur-ther improve the performance. Extensive experiments on THUMOS'14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.(c) 2022 Published by Elsevier Ltd.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2022 NOV
PY  - 2022
VL  - 131
C7  - 108871
DO  - 10.1016/j.patcog.2022.108871
AN  - WOS:000834134500008
C6  - JUN 2022
AD  - Yonsei Univ, Dept Comp Sci, 50 Yonsei Ro, Seoul, South Korea
AD  - York Univ, Dept Elect Engn & Comp Sci, 4700 Keele St, Toronto, ON, Canada
Y2  - 2022-08-18
ER  -

TY  - JOUR
AU  - Yang, Ke
AU  - Qiao, Peng
AU  - Li, Dongsheng
AU  - Lv, Shaohe
AU  - Dou, Yong
TI  - Exploring Temporal Preservation Networks for Precise Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments pre-determined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement on per-frame action prediction and competing results on segment-level temporal action localization.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1708.03280
AN  - PPRN:12739735
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Wang, Lijuan
AU  - Zhu, Suguo
AU  - Li, Zhihao
AU  - Fang, Zhenying
ED  - Ma, H
ED  - Wang, L
ED  - Zhang, C
ED  - Wu, F
ED  - Tan, T
ED  - Wang, Y
ED  - Lai, J
ED  - Zhao, Y
TI  - Complementary Temporal Classification Activation Maps in Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2021, PT II
M3  - Proceedings Paper
CP  - 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Univ Sci & Technol Beijing, Zhuhai, PEOPLES R CHINA
AB  - Weakly-supervised temporal action localization aims to correctly predict the categories and temporal intervals of actions in an untrimmed video by using only video-level labels. Previous methods aggregate category scores through a classification network to generate temporal class activation map (T-CAM), and obtain the temporal regions of the object action by using a predetermined threshold on generated T-CAM. However, class-specific T-CAM pays too much attention to those regions that are more discriminative for classification tasks, which ultimately leads to fragmentation of localization results. In this paper, we propose a complementary learning strategy for weakly-supervised temporal action localization. It obtains the erasure feature by masking the high activation value position of the original temporal class activation map, and takes it as input to train an additional classification network to produce complementary temporal class activation map. Finally, the fragmentation problem is alleviated by merging two temporal class activation map. We have conduct sufficient experiments on the THUMOS'14 and ActivityNet1.2, and the experimental results show that the localization performance of the proposed method has been greatly improved compared with the existing methods.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-88007-1
SN  - 978-3-030-88006-4
DA  - 2021 
PY  - 2021
VL  - 13020
SP  - 373
EP  - 384
DO  - 10.1007/978-3-030-88007-1_31
AN  - WOS:000846897000031
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Media Intelligence Lab, Hangzhou 310018, Peoples R China
Y2  - 2022-09-07
ER  -

TY  - CPAPER
AU  - Sun, Mengbo
AU  - Song, Yonghong
AU  - Wang, Hongda
A1  - IEEE
TI  - Efficient temporal action localization with temporal attention and gaussian weight
T2  - 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - Broadbeach, AUSTRALIA
AB  - The task of temporal action localization is to recognize the action categories and meanwhile detect the start and end time of each action instance. In this paper, we propose a temporal attention and gaussian weighted anchor-free method, named TG-TAL, for temporal action localization. Rather than using anchors, our method regresses action instances directly with video frames as samples. To better address the variable length of action instance, we introduce a multi-level prediction framework with temporal attention. An additional gaussian weight branch is also defined to enhance the classification performance on low-quality temporal segments. Extensive experiments demonstrate that our method is effective on various datasets. In particular, on THUMOS14, our method outperforms one-stage temporal action localization methods and establishes a new state-of-the-art performance with an mAP(%) of 41.9 at tIoU threshold 0.5. Our method also works with two-stages methods and proposal postprocessing methods. Combined with PGCN, our method surpasses the state-of-the-art methods at tIoU threshold 0.7 and achieves a new state-of-the-art performance of 24.1 in terms of mAP(%) on THUMOS14.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-1-6654-8867-9
DA  - 2023 
PY  - 2023
DO  - 10.1109/IJCNN54540.2023.10191428
AN  - WOS:001046198703011
AD  - Xi An Jiao Tong Univ, Sch Software Engn, Xian, Peoples R China
Y2  - 2023-09-30
ER  -

TY  - JOUR
AU  - Zhang, Tianyi
AU  - Li, Ronglu
AU  - Feng, Pengming
AU  - Zhang, Rubo
TI  - Integration of Global and Local Knowledge for Foreground Enhancing in Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly Supervised Temporal Action Localization (WTAL) aims to identify the temporal duration of actions and classify the action categories with only video-level labels in the training stage. Motivated by the intuition that the attention maps generated from various views will assist in enhancing the foreground action temporal segments, in this paper we propose a WTAL pipeline based on a novel attention mechanism that effectively integrates global and local knowledge. Our attention mechanism is mainly composed of a global attention branch and a local attention branch. Specifically, the global attention branch is built on the inter-segment similarity to sparsely mine out the correlation knowledge within the entire video, while the local attention branch is built on the convolutional structure to densely aggregate the information within the fixed local respective field. Experiments on THUMOS14 and ActivityNet v1.3 datasets demonstrate the effectiveness of our proposed WTAL pipeline compared to state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 8476
EP  - 8487
DO  - 10.1109/TMM.2024.3379887
AN  - WOS:001297535300013
AD  - Beihang Univ, Sch Cyber Sci & Technol, Beijing 100191, Peoples R China
AD  - Dalian Minzu Univ, Coll Mech & Elect Engn, Dalian 116600, Peoples R China
AD  - CAST, State Key Lab Space Ground Integrated Informat Tec, Beijing 100095, Peoples R China
M2  - CAST
Y2  - 2024-09-04
ER  -

TY  - CPAPER
AU  - Li, Wei
AU  - Chen, Shimin
AU  - Gu, Jianyang
AU  - Wang, Ning
AU  - Chen, Chen
AU  - Guo, Yandong
A1  - IEEE
TI  - MV-TAL: Mulit-view Temporal Action Localization in Naturalistic Driving
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Human risky behavior in driving is an important visual recognition problem. In this paper, we propose a multi-view temporal action localization system based on the grayscale video to achieve action recognition in naturalistic driving. Specifically, we adopted SwinTransformer as feature extractor, and a single framework to detect boundary and class at the same time. Also, we improve multiple loss function for explicit constraints of embedded feature distributions. Our proposed framework achieves the overall F1 -score of 0.3154 on A2 dataset.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3241
EP  - 3247
DO  - 10.1109/CVPRW56347.2022.00366
AN  - WOS:000861612703039
AD  - OPPO Res Inst, Beijing, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
AD  - East China Univ Sci & Technol, Shanghai, Peoples R China
M2  - OPPO Res Inst
Y2  - 2022-12-07
ER  -

TY  - JOUR
AU  - Shen, Junyi
AU  - Ma, Li
AU  - Zhang, Jikai
TI  - Temporal Action Detection Methods Based on Deep Learning
T2  - INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
M3  - Article
AB  - Temporal action detection is one of the most important and challenging tasks in video analysis. Due to its wide application prospects, it has received extensive attention in recent years. With the development of deep learning, great progress has been made in temporal behavior detection, but there are still many difficulties to be solved, such as accurate proposal generation and high computational cost. In this paper, deep learning-based temporal action detection methods are classified according to full supervision and weak supervision, and then the representative models of the two methods are summarized in detail, and the ideas, advantages and disadvantages of different models and the evolution between different models are analyzed. At the same time, the performance of different models on mainstream datasets is compared. The mainstream dataset and evaluation index used in temporal action detection are introduced in detail, and the calculation method of evaluation index is also elaborated. Finally, through in-depth analysis, the possible future research directions of temporal action detection and the whole review are summarized.
PU  - WORLD SCIENTIFIC PUBL CO PTE LTD
PI  - SINGAPORE
PA  - 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN  - 0218-0014
SN  - 1793-6381
DA  - 2022 MAR 15
PY  - 2022
VL  - 36
IS  - 03
C7  - 2252005
DO  - 10.1142/S021800142252005X
AN  - WOS:000773769300001
AD  - Inner Mongolia Univ Sci & Technol, Sch Informat Engn, Baotou 014010, Inner Mongolia, Peoples R China
AD  - Ordos Inst Technol, Dept Math & Comp Engn, Ordos 017000, Inner Mongolia, Peoples R China
Y2  - 2022-04-07
ER  -

TY  - JOUR
AU  - Han, Yinan
AU  - Jiang, Qingyuan
AU  - Mei, Hongming
AU  - Yang, Yang
AU  - Tang, Jinhui
TI  - The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This report presents our method for Temporal Action Localisation (TAL), which focuses on identifying and classifying actions within specific time intervals throughout a video sequence. We employ a data augmentation technique by expanding the training dataset using overlapping labels from the Something-SomethingV2 dataset, enhancing the model's ability to generalize across various action classes. For feature extraction, we utilize state-of-the-art models, including UMT, VideoMAEv2 for video features, and BEATs and CAV-MAE for audio features. Our approach involves training both multimodal (video and audio) and unimodal (video only) models, followed by combining their predictions using the Weighted Box Fusion (WBF) method. This fusion strategy ensures robust action localisation. our overall approach achieves a score of 0.5498, securing first place in the competition.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2410.09088
AN  - PPRN:112576408
AD  - Nanjing Univ Sci & Technol, Nanjing, Peoples R China
AD  - Univ Toronto, Toronto, ON, Canada
M2  - Nanjing Univ Sci & Technol
Y2  - 2024-11-05
ER  -

TY  - JOUR
AU  - Wang, Zhuoyao
AU  - Zhao, Rui-Wei
AU  - Feng, Rui
AU  - Jin, Cheng
TI  - Toward Causal and Evidential Open-Set Temporal Action Detection
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action detection (TAD) is a critical task in video understanding. Nevertheless, most existing closed-set TAD methods often struggle to replicate their high performance when completely unseen or unknown actions emerge in an open-world test environment. To this end, the open-set temporal action detection (OSTAD) task has been recently proposed to relax the closed-set TAD condition to the unknown-aware open-set detection. Given only a limited number of known action classes available in model training, precisely localizing and rejecting the unknown action instances is extremely difficult and requires strong model generalization abilities. However, existing approaches are yet far from optimal in discriminative action feature learning and prediction uncertainty estimation, which may hamper model generalization to unknown action detection. To address these issues, this paper proposes a novel Causal and Evidential Open-set Temporal Action Detection model named CEO-TAD for improved OSTAD performance. It accomplishes expressive video feature pyramid extraction, discriminative causal action feature representation learning, and reliable EDL-based prediction uncertainty estimation with our tailored network architectures and modified loss functions. Experimental results show that our proposed method achieves state-of-the-art open-set temporal action detection performance on the THUMOS14 and ActivityNet1.3 benchmarks. Ablation studies verify the effectiveness of the proposed model components.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2025 
PY  - 2025
VL  - 13
SP  - 51440
EP  - 51457
DO  - 10.1109/ACCESS.2025.3553717
AN  - WOS:001455525600001
AD  - Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China
AD  - Fudan Univ, Acad Engn & Technol, Shanghai 200433, Peoples R China
AD  - Fudan Zhangjiang Inst, Shanghai 201438, Peoples R China
AD  - Fudan Univ, Innovat Ctr Callig & Painting Creat Technol, MCT, Shanghai 200433, Peoples R China
M2  - Fudan Zhangjiang Inst
Y2  - 2025-04-08
ER  -

TY  - JOUR
AU  - Gleason, Joshua
AU  - Ranjan, Rajeev
AU  - Schwarcz, Steven
AU  - D. Castillo, Carlos
AU  - Cheng, Jun-Chen
AU  - Chellappa, Rama
TI  - A Proposal-Based Solution to Spatio-Temporal Action Detection in Untrimmed Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing approaches for spatio-temporal action detection in videos are limited by the spatial extent and temporal duration of the actions. In this paper, we present a modular system for spatio-temporal action detection in untrimmed security videos. We propose a two stage approach. The first stage generates dense spatio-temporal proposals using hierarchical clustering and temporal jittering techniques on frame-wise object detections. The second stage is a Temporal Refinement I3D (TRI-3D) network that performs action classification and temporal refinement on the generated proposals. The object detection-based proposal generation step helps in detecting actions occurring in a small spatial region of a video frame, while temporal jittering and refinement helps in detecting actions of variable lengths. Experimental results on the spatio-temporal action detection dataset - DIVA - show the effectiveness of our system. For comparison, the performance of our system is also evaluated on the THUMOS14 temporal action detection dataset.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1811.08496
AN  - PPRN:22152882
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Dong, Cerui
AU  - Liu, Qinying
AU  - Wang, Zilei
AU  - Zhang, Yixin
AU  - Zhao, Feng
TI  - Context Sensitive Network for weakly-supervised fine-grained temporal action localization
T2  - NEURAL NETWORKS
M3  - Article
AB  - Weakly-supervised fine-grained temporal action localization seeks to identify fine-grained action instances in untrimmed videos using only video-level labels. The primary challenge in this task arises from the subtle distinctions among various fine-grained action categories, which complicate the accurate localization of specific action instances. In this paper, we note that the context information embedded within the videos plays a crucial role in overcoming this challenge. However, we also find that effectively integrating context information across different scales is non-trivial, as not all scales provide equally valuable information for distinguishing fine-grained actions. Based on these observations, we propose a weakly-supervised fine-grained temporal action localization approach termed the Context Sensitive Network, which aims to fully leverage context information. Specifically, we first introduce a multi-scale context extraction module designed to efficiently capture multi- scale temporal contexts. Subsequently, we develop a scale-sensitive context gating module that facilitates interaction among multi-scale contexts and adaptively selects informative contexts based on varying video content. Extensive experiments conducted on two benchmark datasets, FineGym and FineAction, demonstrate that our approach achieves state-of-the-art performance.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0893-6080
SN  - 1879-2782
DA  - 2025 MAY
PY  - 2025
VL  - 185
C7  - 107140
DO  - 10.1016/j.neunet.2025.107140
AN  - WOS:001421721400001
C6  - JAN 2025
AD  - Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China
Y2  - 2025-02-20
ER  -

TY  - JOUR
AU  - Liu, Yuanyuan
AU  - Zhou, Ning
AU  - Zhang, Fayong
AU  - Wang, Wenbin
AU  - Wang, Yu
AU  - Liu, Kejun
AU  - Liu, Ziyuan
TI  - APSL: Action-positive separation learning for unsupervised temporal action localization
T2  - INFORMATION SCIENCES
M3  - Article
AB  - Unsupervised temporal action localization in untrimmed videos is a challenging and open issue. Existing works focus on the "clustering + localization" framework for unsupervised temporal action localization. However, it heavily relies on features used for clustering and localization, e.g., features implying potential background information would degrade the localization performance. To address this problem, we propose a novel Action-positive Separation Learning (APSL) method. APSL follows a novel "feature separation + clustering + localization" iterative procedure. First, we introduce a novel feature separation learning (FSL) module. FSL employs separation learning to identify action and background features in a video, and then refines and removes potential action-negative and background-negative features (hard-to-locate) from the identified features employing contrastive learning, thus obtaining action-positive features (easy-to-locate). Next, in "clustering" step, we apply clustering to the separated action-positive features to obtain action pseudo-labels. In "localization" step, with action pseudo-labels and action-positive features, we employ a temporal action localization module to locate action instance regions, in turn, improving the performance of clustering and FSL. The three steps learn iteratively and reinforce each other during training. Comprehensive evaluations conducted on the THUMOS'14 and ActivityNet v1.2 datasets demonstrate that our method outperforms cutting-edge weakly supervised and unsupervised methods, obtaining state-of-the-art performance.
PU  - ELSEVIER SCIENCE INC
PI  - NEW YORK
PA  - STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN  - 0020-0255
SN  - 1872-6291
DA  - 2023 JUN
PY  - 2023
VL  - 630
SP  - 206
EP  - 221
DO  - 10.1016/j.ins.2023.02.047
AN  - WOS:000962452400001
C6  - FEB 2023
AD  - China Univ Geosci Wuhan, Sch Comp Sci, Wuhan, Peoples R China
AD  - China Univ Geosci Wuhan, Sch Geog & Informat Engn, Wuhan, Peoples R China
AD  - China Univ Geosci Wuhan, Sch Econ & Management, Wuhan, Peoples R China
Y2  - 2024-02-11
ER  -

TY  - JOUR
AU  - Zhao, Yibo
AU  - Zhang, Hua
AU  - Gao, Zan
AU  - Guan, Weili
AU  - Nie, Jie
AU  - Liu, Anan
AU  - Wang, Meng
AU  - Chen, Shengyong
TI  - A Temporal-Aware Relation and Attention Network for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Temporal action localization is currently an active research topic in computer vision and machine learning due to its usage in smart surveillance. It is a challenging problem since the categories of the actions must be classified in untrimmed videos and the start and end of the actions need to be accurately found. Although many temporal action localization methods have been proposed, they require substantial amounts of computational resources for the training and inference processes. To solve these issues, in this work, a novel temporal-aware relation and attention network (abbreviated as TRA) is proposed for the temporal action localization task. TRA has an anchor-free and end-to-end architecture that fully uses temporal-aware information. Specifically, a temporal self-attention module is first designed to determine the relationship between different temporal positions, and more weight is given to features within the actions. Then, a multiple temporal aggregation module is constructed to aggregate the temporal domain information. Finally, a graph relation module is designed to obtain the aggregated graph features, which are used to refine the boundaries and classification results. Most importantly, these three modules are jointly explored in a unified framework, and temporal awareness is always fully used. Extensive experiments demonstrate that the proposed method can outperform all state-of-the-art methods on the THUMOS14 dataset with an average mAP that reaches 67.6% and obtain a comparable result on the ActivityNet1.3 dataset with an average mAP that reaches 34.4%. Compared with A2Net (TIP20), PCG-TAL (TIP21), and AFSD (CVPR21) TRA can achieve improvements of 11.7%, 4.4%, and 1.8%, respectively on the THUMOS14 dataset.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 JUL 8
PY  - 2022
VL  - 31
SP  - 4746
EP  - 4760
DO  - 10.1109/TIP.2022.3182866
AN  - WOS:000825967200001
AD  - Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China
AD  - Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China
AD  - Monash Univ, Fac Informat Technol, Clayton Campus, Clayton, Vic 3800, Australia
AD  - Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China
Y2  - 2022-07-22
ER  -

TY  - JOUR
AU  - Fang, Zhenying
AU  - Fan, Jianping
AU  - Yu, Jun
TI  - LPR: learning point-level temporal action localization through re-training
T2  - MULTIMEDIA SYSTEMS
M3  - Article
M3  - Early Access
AB  - Point-level temporal action localization (PTAL) aims to locate action instances in untrimmed videos with only one timestamp annotation for each action instance. Existing methods adopt the localization-by-classification paradigm to locate action boundaries in the temporal class activation map (TCAM) by thresholding, also known as TCAM-based method. However, TCAM-based methods are limited by the gap between classification and localization tasks, since TCAM is generated by a classification network. To address this issue, we propose a re-training framework for the PTAL task, also known as LPR. This framework consists of two stages: pseudo-label generation and re-training. In the pseudo-label generation stage, we propose a feature embedding module based on a transformer encoder to capture global context features and optimize pseudo-labels' quality by leveraging point-level annotations. In the re-training stage, LPR uses the above pseudo-labels as supervision to locate action instances with a temporal action localization network rather than generating TCAMs. Furthermore, to alleviate the effects of label noise in the pseudo-labels, we propose a joint learning classification module (JLCM) in the re-training stage. This module contains two classification sub-modules that simultaneously predict action categories and are guided by a jointly determined clean set for network training. The proposed framework achieves state-of-the-art localization performance on both the THUMOS'14 and BEOID datasets.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2023 JUL 27
PY  - 2023
DO  - 10.1007/s00530-023-01128-4
AN  - WOS:001037340700001
C6  - JUL 2023
AD  - Hangzhou Dianzi Univ, Comp & Software Sch, Hangzhou 310018, Peoples R China
AD  - ZJU, Hangzhou Global Sci & Technol Innovat Ctr, Hangzhou 310018, Peoples R China
AD  - Lenovo Res, AI Lab, Beijing 100094, Peoples R China
Y2  - 2023-08-10
ER  -

TY  - JOUR
AU  - Kim, Ji-Hwan
AU  - Heo, Jae-Pil
TI  - Learning Coarse and Fine Features for Precise Temporal Action Localization
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action localization from untrimmed videos is a fundamental task for real-world computer vision applications such as video surveillance systems. Even though a great deal of research attention has been paid to the problem, precise localization of human activities at a frame level still remains as a challenge. In this paper, we propose CoarseFine networks that learn highly discriminative features without loss of time granularity with two streams: the coarse and fine networks. The coarse network aims to classify the action category based on the global context of a video by taking advantage of the description power of successful action recognition models. On the other hand, the fine network does not deploy temporal pooling constrained with a low channel capacity. The fine network is specialized to identify the per-frame location of actions based on local semantics. This approach enables CoarseFine networks to learn find-grained representations without any temporal information loss. Our extensive experiments on two challenging benchmarks, THUMOS14 and ActivityNet-v1.3, validate that our proposed method provides a higher accuracy compared to the state-of-the-art by a remarkable margin in per-frame labeling and temporal action localization tasks while the computational cost is significantly reduced.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2019 
PY  - 2019
VL  - 7
SP  - 149797
EP  - 149809
DO  - 10.1109/ACCESS.2019.2946898
AN  - WOS:000497160500066
AD  - Sungkyunkwan Univ, Dept Elect & Comp Engn, Suwon 16419, South Korea
AD  - Sungkyunkwan Univ, Dept Comp Sci & Engn, Suwon 16419, South Korea
AD  - Sungkyunkwan Univ, Dept Artificial Intelligence, Suwon 16419, South Korea
Y2  - 2019-12-06
ER  -

TY  - CPAPER
AU  - Liu, Peng
AU  - Wang, Chuanxu
AU  - Zhao, Min
A1  - IEEE
TI  - MODAL CONSENSUS AND CONTEXTUAL SEPARATION FOR WEAKLY SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING, ICASSP 2024
M3  - Proceedings Paper
CP  - 49th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
CL  - Seoul, SOUTH KOREA
AB  - Weakly-supervised Temporal Action Localization (W-TAL) is a challenging task aiming to achieve both action class identification and localization of temporal boundaries using video-level label learning. Recent methods resort to basic cascading or integration of appearance and optical flow features, often resulting in incomplete action localization and ambiguity distinguishing foreground from background. Therefore, this paper introduces the Modal Consensus and Context Separation (MCCS) approach to address these complexities. First, the modal collaboration module proposes to enhance action feature representation by synergizing appearance and optical flow features while discarding redundant elements to eschew suboptimal outcomes. Further, these augmented bimodal streams are meticulously fused via the spatiotemporal self-attention module, which adeptly fuses spatial and temporal relationships of action snippets. In addition, the hybrid modeling mechanism is employed for foreground-background separation, focusing on local action attributes within hybrid features to refine the differentiation between foreground and background. This paper substantiates the efficacy of the MCCS method through rigorous testing on the THUMOS14 and ActivityNet1.3 datasets, demonstrating its superiority in tackling the intricate facets of W-TAL.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 979-8-3503-4486-8
SN  - 979-8-3503-4485-1
DA  - 2024 
PY  - 2024
SP  - 4220
EP  - 4224
DO  - 10.1109/ICASSP48485.2024.10446233
AN  - WOS:001285850004094
AD  - Qingdao Univ Sci & Technol, Qingdao 266061, Peoples R China
Y2  - 2025-03-02
ER  -

TY  - JOUR
AU  - Su, Rui
AU  - Xu, Dong
AU  - Sheng, Lu
AU  - Ouyang, Wanli
TI  - PCG-TAL: Progressive Cross-Granularity Cooperation for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - There are two major lines of works, i.e., anchor-based and frame-based approaches, in the field of temporal action localization. But each line of works is inherently limited to a certain detection granularity and cannot simultaneously achieve high recall rates with accurate action boundaries. In this work, we propose a progressive cross-granularity cooperation (PCG-TAL) framework to effectively take advantage of complementarity between the anchor-based and frame-based paradigms, as well as between two-view clues (i.e., appearance and motion). Specifically, our new Anchor-Frame Cooperation (AFC) module can effectively integrate both two-granularity and two-stream knowledge at the feature and proposal levels, as well as within each AFC module and across adjacent AFC modules. Specifically, the RGB-stream AFC module and the flow-stream AFC module are stacked sequentially to form a progressive localization framework. The whole framework can be learned in an end-to-end fashion, whilst the temporal action localization performance can be gradually boosted in a progressive manner. Our newly proposed framework outperforms the state-of-the-art methods on three benchmark datasets the THUMOS14, ActivityNet v1.3 and UCF-101-24, which clearly demonstrates the effectiveness of our framework.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 2103
EP  - 2113
DO  - 10.1109/TIP.2020.3044218
AN  - WOS:000613403600001
AD  - Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia
AD  - Beihang Univ, Coll Software, Beijing 100083, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Chen, Changjian
AU  - Chen, Jiashu
AU  - Yang, Weikai
AU  - Wang, Haoze
AU  - Knittel, Johannes
AU  - Zhao, Xibin
AU  - Koch, Steffen
AU  - Ertl, Thomas
AU  - Liu, Shixia
TI  - Enhancing Single-Frame Supervision for Better Temporal Action Localization
T2  - IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
M3  - Article
AB  - Temporal action localization aims to identify the boundaries and categories of actions in videos, such as scoring a goal in a football match. Single-frame supervision has emerged as a labor-efficient way to train action localizers as it requires only one annotated frame per action. However, it often suffers from poor performance due to the lack of precise boundary annotations. To address this issue, we propose a visual analysis method that aligns similar actions and then propagates a few user-provided annotations (e.g., boundaries, category labels) to similar actions via the generated alignments. Our method models the alignment between actions as a heaviest path problem and the annotation propagation as a quadratic optimization problem. As the automatically generated alignments may not accurately match the associated actions and could produce inaccurate localization results, we develop a storyline visualization to explain the localization results of actions and their alignments. This visualization facilitates users in correcting wrong localization results and misalignments. The corrections are then used to improve the localization results of other actions. The effectiveness of our method in improving localization performance is demonstrated through quantitative evaluation and a case study.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 1077-2626
SN  - 1941-0506
DA  - 2024 JUN
PY  - 2024
VL  - 30
IS  - 6
SP  - 2903
EP  - 2915
DO  - 10.1109/TVCG.2024.3388521
AN  - WOS:001252775500008
AD  - Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410012, Hunan, Peoples R China
AD  - Tsinghua Univ, Sch Software, BNRist, Beijing 100190, Peoples R China
AD  - Univ Stuttgart, D-70174 Stuttgart, Germany
Y2  - 2024-07-04
ER  -

TY  - JOUR
AU  - Yang, Jin
AU  - Wei, Ping
AU  - Ren, Ziyang
AU  - Zheng, Nanning
TI  - Gated Multi-Scale Transformer for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization (TAL) is a critical task in video understanding. Effectively utilizing multi-scale information and handling interactions across various scales have consistently posed challenging issues within the realm of TAL. In this article, we propose a novel gated multi-scale Transformer model (TransGMC) for temporal action localization. A gated control mechanism is designed to filter and aggregate the information at different scales, by which the contributions of contexts at different temporal scales are well characterized. To enhance the feature representation at each temporal scale, the rich global-local contexts are extracted at each temporal scale. A cascade attention module that contains two seamlessly integrated channel attention and moment attention is proposed for capturing global temporal contexts. We utilize a new regression loss function for locating the time boundaries. We conducted experiments on four challenging benchmark datasets, including two third-person view datasets and two first-person view datasets. Our method achieves an average mAP of 67.5% on THUMOS14, 36.1% on ActivityNet v1.3, 24.9% on EPIC-Kitchens 100, and 23.2% on Ego4D, which all outperform the previous state-of-the-arts methods. Extensive ablation studies also validate the effectiveness of the proposed method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 5705
EP  - 5717
DO  - 10.1109/TMM.2023.3338082
AN  - WOS:001197874100010
AD  - Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
Y2  - 2024-05-01
ER  -

TY  - JOUR
AU  - Xu, Hongsheng
AU  - Chen, Zihan
AU  - Zhang, Yu
AU  - Geng, Xin
AU  - Mi, Siya
AU  - Yang, Zhihong
TI  - Weakly supervised temporal action localization with proxy metric modeling
T2  - FRONTIERS OF COMPUTER SCIENCE
M3  - Article
AB  - Temporal localization is crucial for action video recognition. Since the manual annotations are expensive and time-consuming in videos, temporal localization with weak video-level labels is challenging but indispensable. In this paper, we propose a weakly-supervised temporal action localization approach in untrimmed videos. To settle this issue, we train the model based on the proxies of each action class. The proxies are used to measure the distances between action segments and different original action features. We use a proxy-based metric to cluster the same actions together and separate actions from backgrounds. Compared with state-of-the-art methods, our method achieved competitive results on the THUMOS14 and ActivityNet1.2 datasets.
PU  - HIGHER EDUCATION PRESS
PI  - BEIJING
PA  - CHAOYANG DIST, 4, HUIXINDONGJIE, FUSHENG BLDG, BEIJING 100029, PEOPLES R CHINA
SN  - 2095-2228
SN  - 2095-2236
DA  - 2023 APR
PY  - 2023
VL  - 17
IS  - 2
C7  - 172309
DO  - 10.1007/s11704-022-1154-1
AN  - WOS:000837700800009
AD  - NARI Grp Corp, State Grid Elect Power Res Inst, Nanjing 211106, Peoples R China
AD  - Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China
AD  - Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, Nanjing 211189, Peoples R China
AD  - Southeast Univ, Sch Cyber Sci & Engn, Nanjing 211189, Peoples R China
AD  - Purple Mt Labs, Nanjing 211111, Peoples R China
Y2  - 2022-08-18
ER  -

TY  - CPAPER
AU  - Li, Mengzhu
AU  - Wu, Hongjun
AU  - Liu, Yongcheng
AU  - Liu, Hongzhe
AU  - Xu, Cheng
AU  - Li, Xuewei
A1  - IEEE
TI  - W-ART: ACTION RELATION TRANSFORMER FOR WEAKLY-SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)
M3  - Proceedings Paper
CP  - 47th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - Singapore, SINGAPORE
AB  - Weakly-supervised temporal action localization (WTAL) is a long-standing and challenging research problem in video signal analysis. It is to localize the action segments in the video given only video-level labels. The key to this task is understanding how the diverse actions interact. In this paper, we propose W-ART, a relation Transformer to explicitly capture the relationships between action segments. We devise a new effective Transformer architecture and construct new training loss functions for WTAL. Further, we propose a dedicated query mechanism to satisfy the different feature preferences between classification and localization. Thanks to these designs, our W-ART can accurately localize the diverse actions even in weakly-supervised setting. Extensive evaluation and empirical analysis show that our method outperforms the state of the arts on two challenging benchmarks, Charades and THUMOS14.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-6654-0540-9
DA  - 2022 
PY  - 2022
SP  - 2195
EP  - 2199
DO  - 10.1109/ICASSP43922.2022.9747389
AN  - WOS:000864187902094
AD  - Beijing Union Univ, Beijing Key Lab Informat Serv Engn, Beijing, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
Y2  - 2023-01-06
ER  -

TY  - CPAPER
AU  - Xie, Tingting
AU  - Yang, Xiaoshan
AU  - Zhang, Tianzhu
AU  - Xu, Changsheng
AU  - Patras, Ioannis
A1  - IEEE
TI  - EXPLORING FEATURE REPRESENTATION AND TRAINING STRATEGIES IN TEMPORAL ACTION LOCALIZATION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - Temporal action localization has recently attracted significant interest in the Computer Vision community. However, despite the great progress, it is hard to identify which aspects of the proposed methods contribute most to the increase in localization performance. To address this issue, we conduct ablative experiments on feature extraction methods, fixed-size feature representation methods and training strategies, and report how each influences the overall performance. Based on our findings, we propose a two-stage detector that outperforms the state of the art in THUMOS14, achieving a mAP@tIoU=0.5 equal to 44.20%.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 1605
EP  - 1609
DO  - 10.1109/icip.2019.8803745
AN  - WOS:000521828601147
AD  - Queen Mary Univ London, EECS, London, England
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
Y2  - 2019-01-01
ER  -

TY  - CPAPER
AU  - Warchocki, Jan
AU  - Oprescu, Teodor
AU  - Wang, Yunhan
AU  - Damacus, Alexandru
AU  - Misterka, Paul
AU  - Bruintjes, Robert-Jan
AU  - Lengyel, Attila
AU  - Strafforello, Ombretta
AU  - van Gemert, Jan
A1  - IEEE
TI  - Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS, ICCVW
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - In temporal action localization, given an input video, the goal is to predict which actions it contains, where they begin, and where they end. Training and testing current state-of-the-art deep learning models requires access to large amounts of data and computational power. However, gathering such data is challenging and computational resources might be limited. This work explores and measures how current deep temporal action localization models perform in settings constrained by the amount of data or computational power. We measure data efficiency by training each model on a subset of the training set. We find that TemporalMaxer outperforms other models in data-limited settings. Furthermore, we recommend TriDet when training time is limited. To test the efficiency of the models during inference, we pass videos of different lengths through each model. We find that TemporalMaxer requires the least computational resources, likely due to its simple architecture.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2473-9936
SN  - 979-8-3503-0744-3
DA  - 2023 
PY  - 2023
SP  - 3000
EP  - 3008
DO  - 10.1109/ICCVW60793.2023.00323
AN  - WOS:001156680303010
AD  - Delft Univ Technol, Comp Vis Lab, Delft, Netherlands
Y2  - 2024-03-21
ER  -

TY  - CPAPER
AU  - Gleason, Joshua
AU  - Ranjan, Rajeev
AU  - Schwarcz, Steven
AU  - Castillo, Carlos D.
AU  - Chen, Jun-Cheng
AU  - Chellappa, Rama
A1  - IEEE
TI  - A Proposal-Based Solution to Spatio-Temporal Action Detection in Untrimmed Videos
T2  - 2019 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - 19th IEEE Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa Village, HI
AB  - Existing approaches for spatio-temporal action detection in videos are limited by the spatial extent and temporal duration of the actions. In this paper, we present a modular system for spatio-temporal action detection in untrimmed security videos. We propose a two stage approach. The first stage generates dense spatio-temporal proposals using hierarchical clustering and temporal jittering techniques on frame-wise object detections. The second stage is a Temporal Refinement I3D (TRI-3D) network that performs action classification and temporal refinement on the generated proposals. The object detection-based proposal generation step helps in detecting actions occurring in a small spatial region of a video frame, while temporal jittering and refinement helps in detecting actions of variable lengths. Experimental results on the spatio-temporal action detection dataset - DIVA - show the effectiveness of our system. For comparison, the performance of our system is also evaluated on the THUMOS'14 temporal action detection dataset.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2472-6737
SN  - 978-1-7281-1975-5
DA  - 2019 
PY  - 2019
SP  - 141
EP  - 150
DO  - 10.1109/WACV.2019.00021
AN  - WOS:000469423400014
AD  - Univ Maryland, College Pk, MD 20742 USA
Y2  - 2019-06-18
ER  -

TY  - JOUR
AU  - Reza, Sakib
AU  - Zhang, Yuexi
AU  - Moghaddam, Mohsen
AU  - Camps, Octavia
TI  - HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Online video understanding often relies on individual frames, leading to frame-by-frame predictions. Recent advancements such as Online Temporal Action Localization (OnTAL), extend this approach to instance-level predictions. However, existing methods mainly focus on short-term context, neglecting historical information. To address this, we introduce the History-Augmented Anchor Transformer (HAT) Framework for OnTAL. By integrating historical context, our framework enhances the synergy between long-term and short-term information, improving the quality of anchor features crucial for classification and localization. We evaluate our model on both procedural egocentric (PREGO) datasets (EGTEA and EPIC) and standard non-PREGO OnTAL datasets (THUMOS and MUSES). Results show that our model outperforms state-of-the-art approaches significantly on PREGO datasets and achieves comparable or slightly superior performance on non-PREGO datasets, underscoring the importance of leveraging long-term history, especially in procedural and egocentric action scenarios. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.06437
AN  - PPRN:91369430
AD  - Northeastern Univ, Boston, MA 02115, USA
Y2  - 2024-08-22
ER  -

TY  - JOUR
AU  - Song, Youngkil
AU  - Kim, Dongkeun
AU  - Cho, Minsu
AU  - Kwak, Suha
TI  - Online Temporal Action Localization with Memory-Augmented Transformer
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.02957
AN  - PPRN:91255368
AD  - Pohang Univ Sci & Technol POSTECH, Pohang, South Korea
M2  - Pohang Univ Sci & Technol POSTECH
Y2  - 2024-08-11
ER  -

TY  - JOUR
AU  - Warchocki, Jan
AU  - Oprescu, Teodor
AU  - Wang, Yunhan
AU  - Damacus, Alexandru
AU  - Misterka, Paul
AU  - Bruintjes, Robert-Jan
AU  - Lengyel, Attila
AU  - Strafforello, Ombretta
AU  - van Gemert, Jan
TI  - Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In temporal action localization, given an input video, the goal is to predict which actions it contains, where they begin, and where they end. Training and testing current state-of-the-art deep learning models requires access to large amounts of data and computational power. However, gathering such data is challenging and computational resources might be limited. This work explores and measures how current deep temporal action localization models perform in settings constrained by the amount of data or computational power. We measure data efficiency by training each model on a subset of the training set. We find that TemporalMaxer outperforms other models in data-limited settings. Furthermore, we recommend TriDet when training time is limited. To test the efficiency of the models during inference, we pass videos of different lengths through each model. We find that TemporalMaxer requires the least computational resources, likely due to its simple architecture.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2308.13082
AN  - PPRN:83835772
AD  - Delft Univ Technol, Comp Vis Lab, Delft, Netherlands
Y2  - 2023-09-13
ER  -

TY  - JOUR
AU  - Zhang, Hao
AU  - Feng, Chunyan
AU  - Yang, Jiahui
AU  - Li, Zheng
AU  - Guo, Caili
TI  - Boundary-Aware Proposal Generation Method for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The goal of Temporal Action Localization (TAL) is to find the categories and temporal boundaries of actions in an untrimmed video. Most TAL methods rely heavily on action recognition models that are sensitive to action labels rather than temporal boundaries. More importantly, few works consider the background frames that are similar to action frames in pixels but dissimilar in semantics, which also leads to inaccurate temporal boundaries. To address the challenge above, we propose a Boundary-Aware Proposal Generation (BAPG) method with contrastive learning. Specifically, we define the above background frames as hard negative samples. Contrastive learning with hard negative mining is introduced to improve the discrimination of BAPG. BAPG is independent of the existing TAL network architecture, so it can be applied plug-and-play to mainstream TAL models. Extensive experimental results on THUMOS14 and ActivityNet-1.3 demonstrate that BAPG can significantly improve the performance of TAL.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.13810
AN  - PPRN:85200260
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
M2  - Beijing Univ Posts & Telecommun
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Ren, Hao
AU  - Ran, Wu
AU  - Liu, Xingson
AU  - Ren, Haoran
AU  - Lu, Hong
AU  - Zhang, Rui
AU  - Jin, Cheng
A1  - IEEE
TI  - Weakly-supervised Temporal Action Localization with Adaptive Clustering and Refining Network
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Brisbane, AUSTRALIA
AB  - Weakly-supervised temporal action localization task aims to localize temporal boundaries of action instances by using only video-level labels. Existing methods primarily adopt Multi-Instance-Learning (MIL) scheme to handle this task. The effectiveness of MIL scheme depends heavily on the selection of top-k action snippets, which is unstable and requires manual tuning. To address these deficiencies, we propose an Adaptive Clustering and Refining Network (ACRNet). Specifically, we present an action-aware clustering strategy that is adaptable and requires no manual tuning to separate action and background snippets of diverse videos based on intra-class activation distribution. And a cluster refining step is included to eliminate false action snippets by considering inter-class activation distribution, which greatly improves robustness and localization accuracy. Extensive experiments on THUMOS14, ActivityNet 1.2&1.3 benchmarks show that our method achieves state-of-the-art performance.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-6654-6891-6
DA  - 2023 
PY  - 2023
SP  - 1008
EP  - 1013
DO  - 10.1109/ICME55011.2023.00177
AN  - WOS:001062707300163
AD  - Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
Y2  - 2023-11-05
ER  -

TY  - CPAPER
AU  - Wang, Danxu
AU  - Regentova, Emma
AU  - Muthukumar, Venkatesan
AU  - Berli, Markus
AU  - Harris, Frederick C., Jr.
ED  - Bebis, G
ED  - Patel, V
ED  - Gu, J
ED  - Panetta, J
ED  - Gingold, Y
ED  - Johnsen, K
ED  - Arefin, MS
ED  - Dutta, S
ED  - Biswas, A
TI  - Video Analysis of Water Drop Penetration Time Using Temporal Action Localization for Evaluating Soil Water Repellency
T2  - ADVANCES IN VISUAL COMPUTING, ISVC 2024, PT II
M3  - Proceedings Paper
CP  - 19th International Symposium on Visual Computing
CL  - NV
AB  - We present a system for the automated measurement of Water Drop Penetration Time (WDPT) that is commonly used for evaluating soil water repellency (SWR). Conventional manual measurements of WDPT are labor-intensive, subjective, tend to produce variability of outcomes, and are not available in remote and difficult-to-reach locations. The developed system performs a standardized WDPT test, estimates WDPT, and evaluates SWR. The entire process from drop landing on the soil surface to its complete absorption is modeled as a temporal action, and consequently, the WDPT estimation is solved using Temporal Action Localization (TAL) models. Two state-of-the-art TAL models are explored and compared: ActionFormer and TriDet. Although TriDet has a lower mean absolute error for fast absorption events, ActionFormer outperforms it by the average AP metric and the accuracy of SWR characterization.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-77388-4
SN  - 978-3-031-77389-1
DA  - 2025 
PY  - 2025
VL  - 15047
SP  - 159
EP  - 170
DO  - 10.1007/978-3-031-77389-1_13
AN  - WOS:001447738500013
AD  - Univ Nevada, Dept Elect & Comp Engn, 4505 S Maryland Pkwy, Las Vegas, NV 89154 USA
AD  - Desert Res Inst, Div Hydrol Sci, 755 E Flamingo Rd, Las Vegas, NV 89119 USA
AD  - Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89557 USA
Y2  - 2025-04-11
ER  -

TY  - JOUR
AU  - Wang, Wen
AU  - Wu, Yongjian
AU  - Liu, Haijun
AU  - Wang, Shiguang
AU  - Cheng, Jian
TI  - Temporal Action Detection by Joint Identification-Verification
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection aims at not only recognizing action category but also detecting start time and end time for each action instance in an untrimmed video. The key challenge of this task is to accurately classify the action and determine the temporal boundaries of each action instance. In temporal action detection benchmark: THUMOS 2014, large variations exist in the same action category while many similarities exist in different action categories, which always limit the performance of temporal action detection. To address this problem, we propose to use joint Identification-Verification network to reduce the intra-action variations and enlarge inter-action differences. The joint Identification-Verification network is a siamese network based on 3D ConvNets, which can simultaneously predict the action categories and the similarity scores for the input pairs of video proposal segments. Extensive experimental results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of our proposed method compared to the existing state-of-art methods for temporal action detection in untrimmed videos.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1810.08375
AN  - PPRN:22626272
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Liu, Daochang
AU  - Jiang, Tingting
AU  - Wang, Yizhou
A1  - IEEE Comp Soc
TI  - Completeness Modeling and Context Separation for Weakly Supervised Temporal Action Localization
T2  - 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)
M3  - Proceedings Paper
CP  - 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Long Beach, CA
AB  - Temporal action localization is crucial for understanding untrimmed videos. In this work, we first identify two underexplored problems posed by the weak supervision for temporal action localization, namely action completeness modeling and action-context separation. Then by presenting a novel network architecture and its training strategy, the two problems are explicitly looked into. Specifically, to model the completeness of actions, we propose a multi-branch neural network in which branches are enforced to discover distinctive action parts. Complete actions can be therefore localized by fusing activations from different branches. And to separate action instances from their surrounding context, we generate hard negative data for training using the prior that motionless video clips are unlikely to be actions. Experiments performed on datasets THUMOS'14 and ActivityNet show that our framework outperforms state-of-the-art methods. In particular, the average mAP on ActivityNet v1.2 is significantly improved from 18.0% to 22.4%. Our code will be released soon.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-7281-3293-8
DA  - 2019 
PY  - 2019
SP  - 1298
EP  - 1307
DO  - 10.1109/CVPR.2019.00139
AN  - WOS:000529484001046
AD  - Peking Univ, Sch EECS, Cooperat Medianet Innovat Ctr, NELVT, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China
AD  - Deepwise AI Lab, Beijing, Peoples R China
M2  - Deepwise AI Lab
Y2  - 2020-07-08
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Han, Junwei
AU  - Zhao, Tao
AU  - Liu, Nian
AU  - Zhang, Dingwen
TI  - Structured Attention Composition for Temporal Action Localization.
T2  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
M3  - Journal Article
AB  - Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.
SN  - 1941-0042
DA  - 2022 Jun 13 (Epub 2022 Jun 13)
PY  - 2022
VL  - PP
DO  - 10.1109/TIP.2022.3180925
AN  - MEDLINE:35696480
Y2  - 2022-06-15
ER  -

TY  - CPAPER
AU  - Islam, Ashraful
AU  - Radke, Richard J.
A1  - IEEE Comp Soc
TI  - Weakly Supervised Temporal Action Localization Using Deep Metric Learning
T2  - 2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Snowmass, CO
AB  - Temporal action localization is an important step towards video understanding. Most current action localization methods depend on untrimmed videos with full temporal annotations of action instances. However, it is expensive and time-consuming to annotate both action labels and temporal boundaries of videos. To this end, we propose a weakly supervised temporal action localization method that only requires video-level action instances as supervision during training. We propose a classification module to generate action labels for each segment in the video, and a deep metric learning module to learn the similarity between different action instances. We jointly optimize a balanced binary cross-entropy loss and a metric loss using a standard backpropagation algorithm. Extensive experiments demonstrate the effectiveness of both of these components in temporal localization. We evaluate our algorithm on two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our approach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP at IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-7281-6553-0
DA  - 2020 
PY  - 2020
SP  - 536
EP  - 545
DO  - 10.1109/wacv45572.2020.9093620
AN  - WOS:000578444800056
AD  - Rensselaer Polytech Inst, Troy, NY 12181 USA
Y2  - 2020-11-09
ER  -

TY  - JOUR
AU  - Liu, Yifu
AU  - Li, Xiaoxia
AU  - Luo, Zhiling
AU  - Zhou, Wei
TI  - JCDNet: Joint of Common and Definite phases Network for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to localize action instances in untrimmed videos with only video-level supervision. We witness that different actions record common phases, e.g., the run-up in the HighJump and LongJump. These different actions are defined as conjoint actions, whose rest parts are definite phases, e.g., leaping over the bar in a HighJump. Compared with the common phases, the definite phases are more easily localized in existing researches. Most of them formulate this task as a Multiple Instance Learning paradigm, in which the common phases are tended to be confused with the background, and affect the localization completeness of the conjoint actions. To tackle this challenge, we propose a Joint of Common and Definite phases Network (JCDNet) by improving feature discriminability of the conjoint actions. Specifically, we design a Class-Aware Discriminative module to enhance the contribution of the common phases in classification by the guidance of the coarse definite-phase features. Besides, we introduce a temporal attention module to learn robust action-ness scores via modeling temporal dependencies, distinguishing the common phases from the background. Extensive experiments on three datasets (THUMOS14, ActivityNetv1.2, and a conjoint-action subset) demonstrate that JCDNet achieves competitive performance against the state-of-the-art methods. Keywords: weakly-supervised learning, temporal action localization, conjoint action
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.17294
AN  - PPRN:52072255
AD  - Alibaba DAMO Acad, Hangzhou, Peoples R China
M2  - Alibaba DAMO Acad
Y2  - 2023-04-08
ER  -

TY  - JOUR
AU  - Dou, Peng
AU  - Hu, Haifeng
TI  - Complementary Attention Network for Weakly Supervised Temporal Action Localization
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Weakly supervised temporal action localization effectively reduces the expensive cost of manual labeling. Some works are implemented based on the attention framework. However, we observe that attention-based methods can only pay attention to local segments, ignoring the dependencies between individual segments. To address this issue, we propose complementary attention network (CAN) to capture dependencies between segments. Specifically, we design a global attention branch and a channel attention branch, the former is used to exploit inter-segment information and the latter is used to enhance video features. Based on the channel attention branch, sparse loss and similarity loss are proposed to identify actions from sparse subsets of video segments and summarize action features, respectively. Combining the above designs, the CAN model effectively optimizes the network and improves the localization accuracy. Our model achieves excellent results on THUMOS14 and ActivityNet1.2 datasets.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2023 OCT
PY  - 2023
VL  - 55
IS  - 5
SP  - 6713
EP  - 6732
DO  - 10.1007/s11063-023-11156-w
AN  - WOS:000918495400001
C6  - JAN 2023
AD  - Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou, Peoples R China
Y2  - 2023-02-12
ER  -

TY  - JOUR
AU  - Vahdani, Elahe
AU  - Tian, Yingli
TI  - ADM-Loc: Actionness Distribution Modeling for Point-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper addresses the challenge of point-supervised temporal action detection, in which only one frame per action instance is annotated in the training set. Self-training aims to provide supplementary supervision for the training process by generating pseudo-labels (action proposals) from a base model. However, most current methods generate action proposals by applying manually designed thresholds to action classification probabilities and treating adjacent snippets as independent entities. As a result, these methods struggle to generate complete action proposals, exhibit sensitivity to fluctuations in action classification scores, and generate redundant and overlapping action proposals. This paper proposes a novel framework termed ADM-Loc, which stands for Actionness Distribution Modeling for point-supervised action Localization. ADM-Loc generates action proposals by fitting a composite distribution, comprising both Gaussian and uniform distributions, to the action classification signals. This fitting process is tailored to each action class present in the video and is applied separately for each action instance, ensuring the distinctiveness of their distributions. ADM-Loc significantly enhances the alignment between the generated action proposals and ground-truth action instances and offers high-quality pseudo-labels for self-training. Moreover, to model action boundary snippets, it enforces consistency in action classification scores during training by employing Gaussian kernels, supervised with the proposed loss functions. ADM-Loc outperforms the state-of-the-art point-supervised methods on THUMOS14 and ActivityNet-v1.2 datasets.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2311.15916
AN  - PPRN:86295521
AD  - CUNY, New York, NY 10017, USA
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Vahdani, Elahe
AU  - Tian, Yingli
TI  - POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for weakly-supervised Action Localization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of `pseudo-labels' to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14 and ActivityNet-v1.2 datasets, showing a significant improvement of 5% average mAP on the former.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2310.13585
AN  - PPRN:85744606
AD  - CUNY, New York City, NY 10031, USA
M2  - CUNY
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Qing, Zhiwu
AU  - Su, Haisheng
AU  - Gan, Weihao
AU  - Wang, Dongliang
AU  - Wu, Wei
AU  - Wang, Xiang
AU  - Qiao, Yu
AU  - Yan, Junjie
AU  - Gao, Changxin
AU  - Sang, Nong
TI  - Temporal Context Aggregation Network for Temporal Action Proposal Refinement
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through &ldquo;local and global&rdquo; temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both &ldquo;local and global&rdquo; temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 HACS challenge leaderboard on temporal action localization task.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.13141
AN  - PPRN:11468454
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Key Lab Image Proc & Intelligent Control, Wuhan, Peoples R China
AD  - SenseTime Res, Hong kong, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Huazhong Univ Sci & Technol
M2  - SenseTime Res
M2  - Shanghai AI Lab
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Hong, Fa-Ting
AU  - Feng, Jia-Chang
AU  - Xu, Dan
AU  - Shan, Ying
AU  - Zheng, Wei-Shi
A1  - ACM
TI  - Cross-modal Consensus Network forWeakly Supervised Temporal Action Localization
T2  - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021
M3  - Proceedings Paper
CP  - 29th ACM International Conference on Multimedia (MM)
CL  - ELECTR NETWORK
AB  - Weakly supervised temporal action localization (WS-TAL) is a challenging task that aims to localize action instances in the given video with video-level categorical supervision. Previous works use the appearance and motion features extracted from pre-trained feature encoder directly, e.g., feature concatenation or score-level fusion. In this work, we argue that the features extracted from the pre-trained extractors, e.g., I3D, which are trained for trimmed video action classification, but not specific for WS-TAL task, leading to inevitable redundancy and sub-optimization. Therefore, the feature re-calibration is needed for reducing the task-irrelevant information redundancy. Here, we propose a cross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we mainly introduce two identical proposed cross-modal consensus modules (CCM) that design a cross-modal attention mechanism to filter out the task-irrelevant information redundancy using the global information from the main modality and the cross-modal local information from the auxiliary modality. Moreover, we further explore inter-modality consistency, where we treat the attention weights derived from each CCM as the pseudo targets of the attention weights derived from another CCM to maintain the consistency between the predictions derived from two CCMs, forming a mutual learning manner. Finally, we conduct extensive experiments on two commonly used temporal action localization datasets, THUMOS14 and ActivityNet1.2, to verify our method, which we achieve the stateof-the-art results. The experimental results show that our proposed cross-modal consensus module can produce more representative features for temporal action localization.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8651-7
DA  - 2021 
PY  - 2021
SP  - 1591
EP  - 1599
DO  - 10.1145/3474085.3475298
AN  - WOS:001147786901077
AD  - Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Tencent PCG, Appl Res Ctr ARC, Shenzhen, Peoples R China
AD  - Minist Educ, Key Lab Machine Intelligence & Adv Comp, Beijing, Peoples R China
AD  - HKUST, Dept Comp Sci & Engn, Hk, Peoples R China
AD  - Pazhou Lab, Guangzhou, Peoples R China
AD  - Sun Yat Sen Univ, Guangdong Key Lab Informat Secur Technol, Guangzhou, Peoples R China
M2  - Tencent PCG
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Zhang, Yaru
AU  - Zhang, Xiao-Yu
AU  - Shi, Haichao
TI  - OW-TAL: Learning Unknown Human Activities for Open-World Temporal Action Localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Current temporal action localization methods work well on a closed-world assumption, in which all ac-tion categories to be localized are known as a priori. However, this assumption doesn't apply to open -world scenarios, as novel categories that never appeared in the training stage will be encountered with-out explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and local-izing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two -branch framework with Unknown and Known action modeling Networks, a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline. Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowl-edge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forget-ting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competi-tive performance of our method when compared with traditional closed-world counterparts.(c) 2022 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2023 JAN
PY  - 2023
VL  - 133
C7  - 109027
DO  - 10.1016/j.patcog.2022.109027
AN  - WOS:000861386400002
C6  - SEP 2022
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100093, Peoples R China
Y2  - 2022-10-05
ER  -

TY  - JOUR
AU  - Liu, Mengxue
AU  - Li, Wenjing
AU  - Ge, Fangzhen
AU  - Gao, Xiangjun
TI  - Weakly-supervised temporal action localization using multi-branch attention weighting
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - Weakly-supervised temporal action localization aims to train an accurate and robust localization model using only video-level labels. Due to the lack of frame-level temporal annotations, existing weakly-supervised temporal action localization methods typically rely on multiple instance learning mechanisms to localize and classify all action instances in an untrimmed video. However, these methods focus only on the most discriminative regions that contribute to the classification task, neglecting a large number of ambiguous background and context snippets in the video. We believe that these controversial snippets have a significant impact on the localization results. To mitigate this issue, we propose a multi-branch attention weighting network (MAW-Net), which introduces an additional non-action class and integrates a multi-branch attention module to generate action and background attention, respectively. In addition, considering the correlation among context, action, and background, we use the difference of action and background attention to construct context attention. Finally, based on these three types of attention values, we obtain three new class activation sequences that distinguish action, background, and context. This enables our model to effectively remove background and context snippets in the localization results. Extensive experiments were performed on the THUMOS-14 and Activitynet1.3 datasets. The experimental results show that our method is superior to other state-of-the-art methods, and its performance is comparable to those of fully-supervised approaches.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2024 OCT
PY  - 2024
VL  - 30
IS  - 5
C7  - 260
DO  - 10.1007/s00530-024-01445-2
AN  - WOS:001302500500003
AD  - Huaibei Normal Univ, Sch Comp Sci & Technol, Huaibei 235000, Anhui, Peoples R China
AD  - Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China
AD  - Huaibei Inst Technol, Sch Elect & Informat Engn, Huaibei 235000, Anhui, Peoples R China
M2  - Huaibei Inst Technol
Y2  - 2024-09-06
ER  -

TY  - JOUR
AU  - Du, Jia-Run
AU  - Feng, Jia-Chang
AU  - Lin, Kun-Yu
AU  - Hong, Fa-Ting
AU  - Qi, Zhongang
AU  - Shan, Ying
AU  - Hu, Jian-Fang
AU  - Zheng, Wei-Shi
TI  - Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize and classify action instances in long untrimmed videos with only video-level category labels as supervision. A critical challenge of WSTAL is the large gap between video-level supervision and unavailable snippet-level supervision. Prevailing methods typically assign pseudo labels to snippets, but these methods suffer from significant noise caused by the pseudo snippet-level labels. In this work, we address the WSTAL from a novel category exclusion perspective, which gradually enhances the snippet-level supervision to bridge the gap. Our proposed Progressive Complementary Learning (ProCL) is inspired by the fact that, video-level labels precisely indicate the categories that all snippets surely do not belong to, which is ignored by previous works. Accordingly, we first exclude these surely non-existent categories by the deterministic complementary learning. And then, we introduce the entropy-based pseudo complementary learning that is able to exclude more categories for snippets of less ambiguity. Furthermore, for the remaining ambiguous snippets, we attempt to reduce the ambiguity by distinguishing foreground actions from the background. Extensive experimental results show that our method achieves new state-of-the-art performance on THUMOS14, ActivityNet1.3, and MultiTHUMOS benchmarks.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2025 JAN
PY  - 2025
VL  - 35
IS  - 1
SP  - 938
EP  - 952
DO  - 10.1109/TCSVT.2024.3456795
AN  - WOS:001410649200014
AD  - Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China
AD  - Tencent PCG, Appl Res Ctr ARC, Shenzhen 518057, Peoples R China
M2  - Tencent PCG
Y2  - 2025-02-09
ER  -

TY  - JOUR
AU  - Sun, Che
AU  - Song, Hao
AU  - Wu, Xinxiao
AU  - Jia, Yunde
AU  - Luo, Jiebo
TI  - Exploiting Informative Video Segments for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognize actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks, i.e., THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with existing state-of-the-art methods. Moreover, compared with the baseline method that treats video segments equally, STAN achieves significant improvements with an increase of the mean average precision from 30.4% to 39.8% on the THUMOS2014 dataset, and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2022 
PY  - 2022
VL  - 24
SP  - 274
EP  - 287
DO  - 10.1109/TMM.2021.3050067
AN  - WOS:000745524300021
AD  - Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 10081, Peoples R China
AD  - Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA
Y2  - 2022-02-01
ER  -

TY  - JOUR
AU  - Li, Yueyang
AU  - Hou, Yonghong
AU  - Li, Wanqing
TI  - Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Point-level weakly-supervised temporal action localization (PWTAL) aims to localize actions with only a single timestamp annotation for each action instance. Existing methods tend to mine dense pseudo labels to alleviate the label sparsity, but overlook the potential sub-action temporal structures, resulting in inferior performance. To tackle this problem, we propose a novel sub-action prototype learning framework (SPL-Loc) which comprises Sub-action Prototype Clustering (SPC) and Ordered Prototype Alignment (OPA). SPC adaptively extracts representative sub-action prototypes which are capable to perceive the temporal scale and spatial content variation of action instances. OPA selects relevant prototypes to provide completeness clue for pseudo label generation by applying a temporal alignment loss. As a result, pseudo labels are derived from alignment results to improve action boundary prediction. Extensive experiments on three popular benchmarks demonstrate that the proposed SPL-Loc significantly outperforms existing SOTA PWTAL methods.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.09060
AN  - PPRN:85033199
Y2  - 2023-10-27
ER  -

TY  - JOUR
AU  - Zhang, Songyang
AU  - Peng, Houwen
AU  - Yang, Le
AU  - Fu, Jianlong
AU  - Luo, Jiebo
TI  - Learning Sparse 2D Temporal Adjacent Networks for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this report, we introduce the Winner method for HACS Temporal Action Localization Challenge 2019. Temporal action localization is challenging since a target proposal may be related to several other candidate proposals in an untrimmed video. Existing methods cannot tackle this challenge well since temporal proposals are considered individually and their temporal dependencies are neglected. To address this issue, we propose sparse 2D temporal adjacent networks to model the temporal relationship between candidate proposals. This method is built upon the recent proposed 2D-TAN approach. The sampling strategy in 2D-TAN introduces the unbalanced context problem, where short proposals can perceive more context than long proposals. Therefore, we further propose a Sparse 2D Temporal Adjacent Network (S-2D-TAN). It is capable of involving more context information for long proposals and further learning discriminative features from them. By combining our S-2D-TAN with a simple action classifier, our method achieves a mAP of 23.49 on the test set, which win the first place in the HACS challenge.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1912.03612
AN  - PPRN:12922731
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Islam, Ashraful
AU  - J. Radke, Richard
TI  - Weakly Supervised Temporal Action Localization Using Deep Metric Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is an important step towards video understanding. Most current action localization methods depend on untrimmed videos with full temporal annotations of action instances. However, it is expensive and time-consuming to annotate both action labels and temporal boundaries of videos. To this end, we propose a weakly supervised temporal action localization method that only requires video-level action instances as supervision during training. We propose a classification module to generate action labels for each segment in the video, and a deep metric learning module to learn the similarity between different action instances. We jointly optimize a balanced binary cross-entropy loss and a metric loss using a standard backpropagation algorithm. Extensive experiments demonstrate the effectiveness of both of these components in temporal localization. We evaluate our algorithm on two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our approach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP at IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2001.07793
AN  - PPRN:22905583
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Liu, Mengxue
AU  - Gao, Xiangjun
AU  - Ge, Fangzhen
AU  - Liu, Huaiyu
AU  - Li, Wenjing
ED  - Peng, C
ED  - Sun, J
TI  - Weakly-Supervised Temporal Action Localization by Background Suppression
T2  - 2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)
M3  - Proceedings Paper
CP  - 40th Chinese Control Conference (CCC)
CL  - Shanghai, PEOPLES R CHINA
AB  - We propose a novel method of background suppression to solve the issue that background regions are recognized as actions in weakly-supervised temporal action localization. The general attention-based action localization methods tend to use the attention module to generate segment-level attention weights. But there is little difference between the attentions from the background segments similar to the target actions and the attentions of the action segments, which causes the result that many background segments related to the target actions are still recognized as actions. To address this issue, a weakly-supervised temporal action localization network by background suppression (BS-WTAL) is designed. It introduces a filtering module for suppressing the background features and encouraging the action features, a classification module for identifying action categories and a generative attention module for segment-wise representation modeling. This enables BS-WTAL to suppress background to improve localization performance. Furthermore, we conduct ablation studies from different perspectives. Extensive experiments were performed on two datasets - THUMOS14 and ActivityNet1.2. Our approach shows better performance on these two datasets, even comparable with state-of-the-art fully-supervised methods.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-2927
SN  - 978-988-15638-0-4
DA  - 2021 
PY  - 2021
SP  - 7074
EP  - 7081
AN  - WOS:000931046707034
AD  - Huaibei Normal Univ, Sch Comp Sci & Technol, Huaibei 235000, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Su, Haisheng
AU  - Zhao, Xu
AU  - Liu, Shuming
TI  - Multi-Granularity Fusion Network for Proposal and Activity Localization: Submission to ActivityNet Challenge 2019 Task 1 and Task 2
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2019 Task 1 (\textbf{temporal action proposal generation}) and Task 2 (\textbf{temporal action localization/detection}). Temporal action proposal indicates the temporal intervals containing the actions and plays an important role in temporal action localization. Top-down and bottom-up methods are the two main categories used for proposal generation in the existing literature. In this paper, we devise a novel Multi-Granularity Fusion Network (MGFN) to combine the proposals generated from different frameworks for complementary filtering and confidence re-ranking. Specifically, we consider the diversity comprehensively from multiple perspectives, e.g. the characteristic aspect, the data aspect, the model aspect and the result aspect. Our MGFN achieves the state-of-the-art performance on the temporal action proposal task with 69.85 AUC score and the temporal action localization task with 38.90 mAP on the challenge testing set.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1907.12223
AN  - PPRN:22455106
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Wang, Binglu
AU  - Zhao, Yongqiang
AU  - Yang, Le
AU  - Long, Teng
AU  - Li, Xuelong
TI  - Temporal Action Localization in the Deep Learning Era: A Survey
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - The temporal action localization research aims to discover action instances from untrimmed videos, representing a fundamental step in the field of intelligent video understanding. With the advent of deep learning, backbone networks have been instrumental in providing representative spatiotemporal features, while the end-to-end learning paradigm has enabled the development of high-quality models through data-driven training. Both supervised and weakly supervised learning approaches have contributed to the rapid progress of temporal action localization, resulting in a multitude of methods and a large body of literature, making a comprehensive survey a pressing necessity. This paper presents a thorough analysis of existing action localization works, offering a well-organized taxonomy that highlights the strengths and weaknesses of each strategy. In the realm of supervised learning, in addition to the anchor mechanism, we introduce a novel classification mechanism to categorize and summarize existing works. Similarly, for weakly supervised learning, we extend the traditional pre-classification and post-classification mechanisms by providing a fresh perspective on enhancement strategies. Furthermore, we shed light on the bottleneck of confidence estimation, a critical yet overlooked aspect of current works. By conducting detailed analyses, this survey serves as a valuable resource for researchers, providing beneficial guidance to newcomers and inspiring seasoned researchers alike.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2024 APR
PY  - 2024
VL  - 46
IS  - 4
SP  - 2171
EP  - 2190
DO  - 10.1109/TPAMI.2023.3330794
AN  - WOS:001180891600034
AD  - Beijing Inst Technol, Sch Informat & Elect, Beijing 100811, Peoples R China
AD  - Northwestern Polytech Univ, Xian 710072, Peoples R China
Y2  - 2024-04-17
ER  -

TY  - JOUR
AU  - Sheng, Jinrong
AU  - Li, Ao
AU  - Ge, Yongxin
TI  - Summarized knowledge guidance for single-frame temporal action localization
T2  - PATTERN RECOGNITION LETTERS
M3  - Article
AB  - Single-frame temporal action localization has garnered attention in the computer vision community. Existing methods address annotation sparsity by generating dense pseudo labels within individual videos, but disregard the variable representation from intra-class action instances, resulting in inferior completeness localization. In this paper, we propose to model intra-class relationships by using Summarized Knowledge Guidance (SKG). Specifically, we initially design a learnable memory bank to summarize annotated single-frame knowledge for each class. Then, we introduce two corresponding components, i.e., the knowledge propagation module (KPM) and the knowledge refinement module (KRM), for intra-class guidance. In KPM, we propagate summarized knowledge for feature-level enhancement through bipartite matching. In KRM, summarized knowledge is presented as confident pseudo positive samples for label-level refinement in a contrastive learning manner. Extensive experiments and ablation studies on the THUMOS14, GTEA and BEOID reveal that our method significantly outperforms state-of-the-art methods.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0167-8655
SN  - 1872-7344
DA  - 2025 MAY
PY  - 2025
VL  - 191
SP  - 31
EP  - 36
DO  - 10.1016/j.patrec.2025.02.027
AN  - WOS:001442061400001
C6  - MAR 2025
AD  - Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ China, Chongqing 400044, Peoples R China
AD  - Chongqing Univ, Sch Big Data & Software Engn, Daxuecheng South Rd 55, Chognqing 401331, Peoples R China
Y2  - 2025-03-18
ER  -

TY  - JOUR
AU  - Sun, Hao
AU  - Ning, Guanghan
AU  - Zhao, Zhiqun
AU  - Huang, Zhongchao
AU  - He, Zhihai
TI  - Automated work efficiency analysis for smart manufacturing using human pose tracking and temporal action localization
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - In this paper, we aim to develop an automatic system to monitor and evaluate worker's efficiency for smart manufacturing based on human pose tracking and temporal action localization. First, we explore the generative adversarial networks (GANs) to achieve significantly improved estimation of human body joints. Second, we formulate the automated worker efficiency analysis into a temporal action localization problem in which the action video performed by the worker is matched against a reference video performed by a teacher. We extract invariant spatio-temporal features from the human body pose sequences and perform cross-video matching using dynamic time warping. Our proposed human pose estimation method achieves state-of-the-art performance on the benchmark dataset. Our automated work efficiency analysis is able to achieve action localization with an average IoU (intersection over union) score large than 0.9. This represents one of the first systems to provide automated worker efficiency evaluation.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2020 NOV
PY  - 2020
VL  - 73
C7  - 102948
DO  - 10.1016/j.jvcir.2020.102948
AN  - WOS:000598557000001
AD  - Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA
AD  - Cent South Univ, Dept Biomed Engn, Sch Basic Med Sci, Changsha, Hunan, Peoples R China
Y2  - 2021-01-12
ER  -

TY  - JOUR
AU  - Tang, Haoyu
AU  - Jiang, Han
AU  - Xu, Mingzhu
AU  - Hu, Yupeng
AU  - Zhu, Jihua
AU  - Nie, Liqiang
TI  - Unsupervised Temporal Action Localization via Self-paced Incremental Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recently, temporal action localization (TAL) has garnered significant interest in information retrieval community. However, existing supervised/weakly supervised methods are heavily dependent on extensive labeled temporal boundaries and action categories, which is labor-intensive and time-consuming. Although some unsupervised methods have utilized the "iteratively clustering and localization'' paradigm for TAL, they still suffer from two pivotal impediments: 1) unsatisfactory video clustering confidence, and 2) unreliable video pseudolabels for model training. To address these limitations, we present a novel self-paced incremental learning model to enhance clustering and localization training simultaneously, thereby facilitating more effective unsupervised TAL. Concretely, we improve the clustering confidence through exploring the contextual feature-robust visual information. Thereafter, we design two (constant- and variable- speed) incremental instance learning strategies for easy-to-hard model training, thus ensuring the reliability of these video pseudolabels and further improving overall localization performance. Extensive experiments on two public datasets have substantiated the superiority of our model over several state-of-the-art competitors.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2312.07384
AN  - PPRN:86556286
AD  - Shandong Univ, Sch Software, Jinan 250101, Peoples R China
AD  - Xian Jiaotong Univ, Sch Software Engn, Xian 710049, Peoples R China
AD  - Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China
M2  - Shandong Univ
M2  - Harbin Inst Technol Shenzhen
Y2  - 2023-12-22
ER  -

TY  - JOUR
AU  - Sun, Weiqi
AU  - Su, Rui
AU  - Yu, Qian
AU  - Xu, Dong
TI  - Slow Motion Matters: A Slow Motion Enhanced Network for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly supervised temporal action localization (WTAL) aims to localize actions in untrimmed videos with only weak supervision information (e.g., video-level labels). Most existing models handle all input videos with a fixed temporal scale. However, such models are not sensitive to actions whose pace of the movements is different from the "normal " speed, especially slow-motion action instances, which complete the movements with a much slower speed than their counterparts with a "normal " speed. Here arises the slow-motion blurred issue: It is hard to explore salient slow-motion information from videos at normal speed. In this paper, we propose a novel framework termed Slow Motion Enhanced Network (SMEN) to improve the ability of a WTAL network by compensating its sensitivity on slow-motion action segments. The proposed SMEN comprises a Mining module and a Localization module. The mining module generates mask to mine slow-motion-related features by utilizing the relationships between the normal motion and slow motion; while the localization module leverages the mined slow-motion features as complementary information to improve the temporal action localization results. Our proposed framework can be easily adapted by existing WTAL networks and enable them be more sensitive to slow-motion actions. Extensive experiments on three benchmarks are conducted, which demonstrate the high performance of our proposed framework.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2023 JAN
PY  - 2023
VL  - 33
IS  - 1
SP  - 354
EP  - 366
DO  - 10.1109/TCSVT.2022.3201540
AN  - WOS:000911746000026
AD  - Beihang Univ, Coll Software, Beijing 100191, Peoples R China
AD  - Shanghai Artificial Intelligence Lab, Shanghai 200000, Peoples R China
AD  - Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China
M2  - Shanghai Artificial Intelligence Lab
Y2  - 2023-02-12
ER  -

TY  - JOUR
AU  - Liu, Shuai
AU  - Zhang, Yang
AU  - Srivastava, Gautam
TI  - Point-supervised temporal action localisation based on multi-branch attention
T2  - ENTERPRISE INFORMATION SYSTEMS
M3  - Article
AB  - Temporal action localisation is a key research direction for video understanding in the field of computer vision. Current methods of using an attention mechanism only divides the video frame into an action instance frame and a background frame. As a result, the action context, which should belong to the background is misclassified into an action instance In addition, during the training phase of using point-supervised frame-level labels, action samples and background samples are unbalanced. The lack of background samples leads to the reduction of the activation score of the background so that the imbalance of samples will affect the separation of action examples from the background. All these reduce the accuracy of action classification and temporal localisation. Therefore, this paper proposesa multi-branch attention network and a pseudo-background label generation method. Experimental results show that the proposed method can improve the separation effect of action instances, background, and action context. Moreover, the proposed model achieves excellent performance on the THUMOS-14 dataset.
PU  - TAYLOR & FRANCIS LTD
PI  - ABINGDON
PA  - 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN  - 1751-7575
SN  - 1751-7583
DA  - 2023 NOV 2
PY  - 2023
VL  - 17
IS  - 11
DO  - 10.1080/17517575.2023.2197318
AN  - WOS:000972516900001
C6  - APR 2023
AD  - Hunan Normal Univ, Sch Educ Sci, Changsha, Peoples R China
AD  - Hunan Normal Univ, Key Lab Big Data Res & Applicat Basic Educ, Changsha, Peoples R China
AD  - Hunan Normal Univ, Coll Comp Sci & Engn, Changsha, Peoples R China
AD  - Brandon Univ, Dept Math & Comp Sci, Brandon, MB, Canada
AD  - China Med Univ, Res Ctr Interneural Comp, Taichung, Taiwan
AD  - Lebanese Amer Univ, Dept Comp Sci & Math, Beirut, Lebanon
AD  - Brandon Univ, 270 18th St, Brandon, MB R7A 6A9, Canada
Y2  - 2023-05-03
ER  -

TY  - JOUR
AU  - Zhao, Peisen
AU  - Xie, Lingxi
AU  - Zhang, Ya
AU  - Tian, Qi
TI  - Actionness-Guided Transformer for Anchor-Free Temporal Action Localization
T2  - IEEE SIGNAL PROCESSING LETTERS
M3  - Article
AB  - Temporal action localization, detecting actions in untrimmed videos, is widely studied by anchor-based approaches that first generate excessive action proposals, i.e., temporal windows, then evaluate and classify these proposals. To reduce the number of action proposals, recent studies use an anchor-free approach that leverages each time point rather than a temporal window to represent an action instance. However, this point representation, usually modeled by temporal convolutions, may have the fixed and limited receptive field to detect an entire action. So we propose an Actionness-guided Transformer (Ag-Trans) model to learn representations for each point proposal. Ag-Trans first predicts the actionness, i.e., time sequences of the action starting, continuing, and ending phases, then the corresponding action phase can be embedded to model the point representation. Experimental results show that the Ag-Trans model outperforms the CNN-based model under the same experiment settings, especially for long-duration actions.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1070-9908
SN  - 1558-2361
DA  - 2022 
PY  - 2022
VL  - 29
SP  - 194
EP  - 198
DO  - 10.1109/LSP.2021.3132287
AN  - WOS:000747445300020
AD  - Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China
AD  - Huawei Inc, Shenzhen 518129, Guangdong, Peoples R China
Y2  - 2022-02-03
ER  -

TY  - CPAPER
AU  - Li, Jingjing
AU  - Yang, Tianyu
AU  - Ji, Wei
AU  - Wang, Jue
AU  - Cheng, Li
A1  - IEEE COMP SOC
TI  - Exploring Denoised Cross-video Contrast for Weakly-supervised Temporal Action Localization
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Weakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level labels. Most existing methods address this problem with a "localization-by-classification" pipeline that localizes action regions based on snippet-wise classification sequences. Snippet-wise classifications are unfortunately error prone due to the sparsity of video-level labels. Inspired by recent success in unsupervised contrastive representation learning, we propose a novel denoised cross-video contrastive algorithm, aiming to enhance the feature discrimination ability of video snippets for accurate temporal action localization in the weakly-supervised setting. This is enabled by three key designs: I) an effective pseudo-label denoising module to alleviate the side effects caused by noisy contrastive features, 2) an efficient region-level feature contrast strategy with a region-level memory bank to capture "global" contrast across the entire dataset, and 3) a diverse contrastive learning strategy to enable action-background separation as well as intra-class compactness & inter-class separability. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate the superior performance of our approach.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 19882
EP  - 19892
DO  - 10.1109/CVPR52688.2022.01929
AN  - WOS:000870783005070
AD  - Univ Alberta, Edmonton, AB, Canada
AD  - Tencent AI Lab, Shenzhen, Peoples R China
Y2  - 2023-01-05
ER  -

TY  - JOUR
AU  - Chen, Lin
AU  - Zhang, Jing
AU  - Zhang, Yian
AU  - Kang, Junpeng
AU  - Zhuo, Li
TI  - MKP-Net: Memory knowledge propagation network for point-supervised temporal action localization in livestreaming
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - Standardized regulation of livestreaming is an important element of cyberspace governance. Temporal action localization (TAL) can localize the occurrence of specific actions to better understand human activities. Due to the short duration and inconspicuous boundaries of human-specific actions, it is very cumbersome to obtain sufficient labeled data for training in untrimmed livestreaming. The point-supervised approach requires only a single-frame annotation for each action instance and can effectively balance cost and performance. Therefore, we propose a memory knowledge propagation network (MKP-Net) for point-supervised temporal action localization in livestreaming, including (1) a plug-and-play memory module is introduced to model prototype features of foreground actions and background knowledge using point-level annotations, (2) the memory knowledge propagation mechanism is used to generate discriminative feature representation in a multi-instance learning pipeline, and (3) localization completeness learning is performed by designing a dual optimization loss for refining and localizing temporal actions. Experimental results show that our method achieves 61.4% and 49.1% SOTAs on THUMOS14 and self-built BJUT-PTAL datasets, respectively, with an inference speed of 711 FPS.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2024 NOV
PY  - 2024
VL  - 248
C7  - 104109
DO  - 10.1016/j.cviu.2024.104109
AN  - WOS:001301487100001
C6  - AUG 2024
AD  - Beijing Univ Technol, Sch Informat Sci & Technol, Beijing 100124, Peoples R China
AD  - Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intelligen, Beijing 100124, Peoples R China
Y2  - 2024-09-06
ER  -

TY  - JOUR
AU  - Xu, Huijuan
AU  - Yang, Lizhi
AU  - Sclaroff, Stan
AU  - Saenko, Kate
AU  - Darrell, Trevor
TI  - Spatio-Temporal Action Detection with Multi-Object Interaction
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Spatio-temporal action detection in videos requires localizing the action both spatially and temporally in the form of an "action tube". Nowadays, most spatio-temporal action detection datasets (e.g. UCF101-24, AVA, DALY) are annotated with action tubes that contain a single person performing the action, thus the predominant action detection models simply employ a person detection and tracking pipeline for localization. However, when the action is defined as an interaction between multiple objects, such methods may fail since each bounding box in the action tube contains multiple objects instead of one person. In this paper, we study the spatio-temporal action detection problem with multi-object interaction. We introduce a new dataset that is annotated with action tubes containing multi-object interactions. Moreover, we propose an end-to-end spatio-temporal action detection model that performs both spatial and temporal regression simultaneously. Our spatial regression may enclose multiple objects participating in the action. During test time, we simply connect the regressed bounding boxes within the predicted temporal duration using a simple heuristic. We report the baseline results of our proposed model on this new dataset, and also show competitive results on the standard benchmark UCF101-24 using only RGB input.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2004.00180
AN  - PPRN:14904157
Y2  - 2023-04-02
ER  -

TY  - JOUR
AU  - Zhu, Anlei
AU  - Wang, Yinghui
AU  - Yang, Jinlong
AU  - Yan, Tao
AU  - Ma, Haomiao
AU  - Li, Wei
TI  - YOWOv3: A Lightweight Spatio-Temporal Joint Network for Video Action Detection
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Spatio-temporal action detection networks, which need to simultaneously extract and fuse spatial and temporal features, often result in existing models becoming bloated and difficult to run in real-time and deploy on edge devices. This paper introduces an efficient and real-time spatio-temporal action detection model, YOWOv3. This model uses efficient 3D and 2D backbone networks to separately extract spatial and spatial-temporal features from sequential information. A lightweight spatio-temporal feature fusion module, designed by deeply integrating convolution and self-attention mechanisms, further enhances the extraction of spatio-temporal features. We refer to this module as the CFACM (Channel Fusion & Attention Convolution Mix) module. Our approach not only outperforms the latest efficient spatio-temporal action detection models in terms of lightness, reducing the model size by 24% compared to the latter, but also improves the mAP accuracy on the UCF101-24 dataset by 1.35%, while maintaining excellent speed performance, thus achieving a balance between accuracy and speed. Furthermore, existing models often use 3D convolutions to extract temporal information, which may be limited on certain devices, such as Apple's M series processors. To mitigate the potential issue of 3D convolution operators not being supported during edge deployment of spatio-temporal action detection models, we employ a spatio-temporal shift module containing only 2D convolutions. This enables the model to acquire temporal information and inject the obtained temporal features into multi-level spatio-temporal feature extraction models. This not only liberates the model from the constraints of 3D convolution operations but also enhances the model's balance between accuracy and speed. This results in state-of-the-art performance in lightweight networks using only 2D convolutions.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 SEP
PY  - 2024
VL  - 34
IS  - 9
SP  - 8148
EP  - 8160
DO  - 10.1109/TCSVT.2024.3387933
AN  - WOS:001409508700026
AD  - Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China
AD  - Shaanxi Normal Univ, Sch Comp Sci, Xian 710119, Peoples R China
Y2  - 2025-02-07
ER  -

TY  - JOUR
AU  - Xie, Tingting
AU  - Yang, Xiaoshan
AU  - Zhang, Tianzhu
AU  - Xu, Changsheng
AU  - Patras, Ioannis
TI  - Exploring Feature Representation and Training strategies in Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization has recently attracted significant interest in the Computer Vision community. However, despite the great progress, it is hard to identify which aspects of the proposed methods contribute most to the increase in localization performance. To address this issue, we conduct ablative experiments on feature extraction methods, fixed-size feature representation methods and training strategies, and report how each influences the overall performance. Based on our findings, we propose a two-stage detector that outperforms the state of the art in THUMOS14, achieving a mAP@tIoU=0.5 equal to 44.2%.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1905.10608
AN  - PPRN:21741399
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Gan, Ming-Gang
AU  - Zhang, Yan
TI  - Temporal Attention-Pyramid Pooling for Temporal Action Detection
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action detection is a challenging task in video understanding, which is usually divided into two stages: proposal generation and classification. Learning proposal features is a crucial step for both stages. However, most methods ignore temporal information of proposals and consider background and action frames in proposals equally, leading to poor proposal features. In this paper, we propose a novel Temporal Attention-Pyramid Pooling (TAPP) method to learn proposal features of arbitrary length action proposals. The TAPP method exploits the attention mechanism to focus on the discriminative part of proposals, suppressing background influence on proposal features. It constructs a temporal pyramid structure to convert arbitrary length proposal feature sequences to multiple fixed-length sequences while retaining the temporal information. In the TAPP method, we design a multi-scale temporal function and apply it to the temporal pyramid to generate final proposal features. Based on the TAPP method, we construct a temporal action proposal generation model and an action proposal classification model, and then we perform extensive experiments on two mainstream temporal action detection datasets for the temporal action proposal and temporal action detection tasks to verify our models. On the THUMOS'14 dataset, our models based on the TAPP significantly outperform the previous state-of-the-art methods for both tasks.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 3799
EP  - 3810
DO  - 10.1109/TMM.2022.3166025
AN  - WOS:001144015500020
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2024-01-27
ER  -

TY  - JOUR
AU  - Gan, Ming-Gang
AU  - Zhang, Yan
TI  - Improving accuracy of temporal action detection by deep hybrid convolutional network
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Temporal action detection, a fundamental yet challenging task in understanding human actions, is usually divided into two stages: temporal action proposal generation and proposal classification. Classifying action proposals is always considered an action recognition task and receives little attention. However, compared with action classification, classifying action proposals has more large intra-class variations and subtle inter-class differences, making it more difficult to classify accurately. In this paper, we propose a novel end-to-end framework called Deep Hybrid Convolutional Network (DHCNet) to classify action proposals and achieve high-performance temporal action detection. DHCNet improves temporal action detection performance from three aspects. First, DHCNet utilizes Subnet I to effectively model the temporal structure of proposals and generate discriminative proposal features. Second, the Subnet II of DHCNet exploits Graph Convolution (GConv) to acquire information from other proposals and obtains much semantic information to enhance the proposal feature. Third, DHCNet adopts a coarse-to-fine cascaded classification, where the influence of large intra-class variations and subtle inter-class differences are reduced significantly at different granularities. Besides, we design an iterative boundary regression method based on closed-loop feedback to refine the temporal boundaries of proposals. Extensive experiments demonstrate the effectiveness of our approach. Furthermore, DHCNet achieves the state-of-the-art performance on the THUMOS'14 dataset(59.9% on mAP@0.5).
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2023 MAY
PY  - 2023
VL  - 82
IS  - 11
SP  - 16127
EP  - 16149
DO  - 10.1007/s11042-022-13962-1
AN  - WOS:000869338900002
C6  - OCT 2022
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2022-10-24
ER  -

TY  - JOUR
AU  - Murtaza, Fiza
AU  - Yousaf, Muhammad Haroon
AU  - Velastin, Sergio A.
AU  - Qian, Yu
TI  - End-to-End Temporal Action Detection Using Bag of Discriminant Snippets
T2  - IEEE SIGNAL PROCESSING LETTERS
M3  - Article
AB  - Detecting human actions in long untrimmed videos is a challenging problem. Existing temporal-action detection methods have difficulties in finding the precise starting and ending times of the actions in untrimmed videos. In this letter, we propose a temporal-action detection framework that can detect multiple actions in an end-to-end manner, based on a Bag of Discriminant Snippets (BoDS). BoDS is based on the observation that multiple actions and the background classes have similar snippets, which cause incorrect classification of action regions and imprecise boundaries. We solve this issue by finding the key-snippets from the training data of each class and compute their discriminative power, which is used in BoDS encoding. During testing of an untrimmed video, we find the BoDS representation for multiple candidate proposals and find their class label based on a majority voting scheme. We test BoDS on the Thumos14 and ActivityNet datasets and obtain state-of-the-art results. For the sports subset of ActivityNet dataset, we obtain a mean Average Precision (mAP) value of 29% at 0.7 temporal Intersection over Union (tIoU) threshold. For the Thumos14 dataset, we obtain a significant gain in terms of mAP, i.e., improving from 20.8% to 31.6% at tIoU = 0.7.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1070-9908
SN  - 1558-2361
DA  - 2019 FEB
PY  - 2019
VL  - 26
IS  - 2
SP  - 272
EP  - 276
DO  - 10.1109/LSP.2018.2888758
AN  - WOS:000455730600001
AD  - Univ Engn & Technol Taxila, Dept Comp Engn, Taxila 47050, Pakistan
AD  - Cortex Vis Syst Ltd, London SE1 9LQ, England
AD  - Queen Mary Univ London, London E1 4NS, England
AD  - Univ Carlos III Madrid, Madrid 28903, Spain
M2  - Cortex Vis Syst Ltd
Y2  - 2019-01-25
ER  -

TY  - CPAPER
AU  - Helvaci, Halil Ismail
AU  - Chuah, Chen-Nee
AU  - Ozonoff, Sally
AU  - Cheung, Sen-Ching Samson
A1  - IEEE
TI  - LOCALIZING MOMENTS OF ACTIONS IN UNTRIMMED VIDEOS OF INFANTS WITH AUTISM SPECTRUM DISORDER
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP
M3  - Proceedings Paper
CP  - 2024 International Conference on Image Processing
CL  - Abu Dhabi, U ARAB EMIRATES
AB  - Autism Spectrum Disorder (ASD) presents significant challenges in early diagnosis and intervention, impacting children and their families. With prevalence rates rising, there is a critical need for accessible and efficient screening tools. Leveraging machine learning (ML) techniques, in particular Temporal Action Localization (TAL), holds promise for automating ASD screening. This paper introduces a self-attention based TAL model designed to identify ASD-related behaviors in infant videos. Unlike existing methods, our approach simplifies complex modeling and emphasizes efficiency, which is essential for practical deployment in real-world scenarios. Importantly, this work underscores the importance of developing computer vision methods capable of operating in naturilistic environments with little equipment control, addressing key challenges in ASD screening. This study is the first to conduct end-to-end temporal action localization in untrimmed videos of infants with ASD, offering promising avenues for early intervention and support. We report baseline results of behavior detection using our TAL model. We achieve 70% accuracy for look face, 79% accuracy for look object, 72% for smile and 65% for vocalization.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 979-8-3503-4940-5
SN  - 979-8-3503-4939-9
DA  - 2024 
PY  - 2024
SP  - 3841
EP  - 3847
DO  - 10.1109/ICIP51287.2024.10648046
AN  - WOS:001442947000564
AD  - Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506 USA
AD  - Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA USA
AD  - Univ Calif Davis, UC Davis MIND Inst, Davis, CA USA
Y2  - 2025-04-17
ER  -

TY  - JOUR
AU  - Luo, Wang
AU  - Zhang, Tianzhu
AU  - Yang, Wenfei
AU  - Liu, Jingen
AU  - Mei, Tao
AU  - Wu, Feng
AU  - Zhang, Yongdong
TI  - Action Unit Memory Network for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - &nbsp; &nbsp;Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without frame-level annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an action unit memory bank. In the proposed AUMN, two attention modules are designed to update the memory bank adaptively and learn action units specific classifiers. Furthermore, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the first work to explicitly model the action units with a memory network. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs favorably against state-of-the-art methods. Specifically, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly improved from 47.0% to 52.1%.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2104.14135
AN  - PPRN:11719279
AD  - Univ Sci & Technol China, Hebei, People R China
AD  - JD AI Res, Stanford 94305, CA, USA
M2  - Univ Sci & Technol China
M2  - JD AI Res
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Yu, Zikang
AU  - Cheng, Cheng
AU  - Wen, Wujun
AU  - Guo, Wenxuan
AU  - Feng, Lin
A1  - IEEE
TI  - Action Reinforcement and Indication Module for Single-frame Temporal Action Localization
T2  - 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - Broadbeach, AUSTRALIA
AB  - Single-frame temporal action localization aims to predict the start time, end time, and categories of action instances in untrimmed videos with only one timestamp label for each action instance. However, recent works have two challenges: incomplete location results caused by various same-class action snippets (or frames) and redundant predictions due to ambiguous background snippets. To tackle the above issues, we design a model consisting of an action reinforcement module (ARM) and an action indication module (AIM). Specifically, the ARM improves the model's generalization ability by augmenting local and global features, thus solving the challenges of incomplete predictions. Simultaneously, the AIM detects the frames that indicate the location of the action instances (called keyframes) in the training period. Then the AIM applies the detected keyframes to filter redundant prediction results in the testing period. Extensive experiments are performed on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3, demonstrating that the model could achieve state-of-the-art performance compared to some competitive approaches.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-1-6654-8867-9
DA  - 2023 
PY  - 2023
DO  - 10.1109/IJCNN54540.2023.10191287
AN  - WOS:001046198701120
AD  - Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian, Peoples R China
AD  - Dalian Univ Technol, Sch Innovat & Entrepreneurship, Dalian, Peoples R China
Y2  - 2023-09-30
ER  -

TY  - JOUR
AU  - Liu, Yangcen
AU  - Liu, Ziyi
AU  - Zhai, Yuanhao
AU  - Li, Wen
AU  - Doerman, David
AU  - Yuan, Junsong
TI  - STAT: Towards Generalizable Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization (WTAL) aims to recognize and localize action instances with only video -level labels. Despite the significant progress, existing methods suffer from severe performance degradation when transferring to different distributions and thus may hardly adapt to real -world scenarios. To address this problem, we propose the Generalizable Temporal Action Localization task (GTAL), which focuses on improving the generalizability of action localization methods. We observed that the performance decline can be primarily attributed to the lack of generalizability to different action scales. To address this problem, we propose STAT (Self -supervised Temporal Adaptive Teacher), which leverages a teacher -student structure for iterative refinement. Our STAT features a refinement module and an alignment module. The former iteratively refines the model’s output by leveraging contextual information and helps adapt to the target scale. The latter improves the refinement process by promoting a consensus between student and teacher models. We conduct extensive experiments on three datasets, THUMOS14, ActivityNet1.2, and HACS, and the results show that our method significantly improves the baseline methods under the crossdistribution evaluation setting, even approaching the same -distribution evaluation performance.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.13311
AN  - PPRN:88612613
AD  - Univ Elect Sci & Technol China, Chengdu, Peoples R China
AD  - Univ Sci & Technol Beijing, Beijing, Peoples R China
AD  - SUNY Buffalo, Buffalo, NY, USA
M2  - Univ Elect Sci & Technol China
M2  - Univ Sci & Technol Beijing
Y2  - 2024-05-01
ER  -

TY  - JOUR
AU  - He, Yuanpeng
AU  - Li, Lijian
AU  - Zhan, Tianxiang
AU  - Jiao, Wenpin
AU  - Pun, Chi-Man
TI  - Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head Attention for Weak-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization (WS-TAL) is a task of targeting at localizing complete action instances and categorizing them with video-level labels. Action-background ambiguity, primarily caused by background noise resulting from aggregation and intra-action variation, is a significant challenge for existing WS-TAL methods. In this paper, we introduce a hybrid multi-head attention (HMHA) module and generalized uncertainty-based evidential fusion (GUEF) module to address the problem. The proposed HMHA effectively enhances RGB and optical flow features by filtering redundant information and adjusting their feature distribution to better align with the WS-TAL task. Additionally, the proposed GUEF adaptively eliminates the interference of background noise by fusing snippet-level evidences to refine uncertainty measurement and select superior foreground feature information, which enables the model to concentrate on integral action instances to achieve better action localization and classification performance. Experimental results conducted on the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art methods. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2412.19418
AN  - PPRN:120256375
AD  - Peking Univ, Sch Comp Sci, Key Lab High Confidence Software Technol, Minist Educ, Beijing, Peoples R China
AD  - Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China
M2  - Univ Macau
Y2  - 2025-01-24
ER  -

TY  - CPAPER
AU  - Rizve, Mamshad Nayeem
AU  - Mittal, Gaurav
AU  - Yu, Ye
AU  - Hall, Matthew
AU  - Sajeev, Sandra
AU  - Shah, Mubarak
AU  - Chen, Mei
A1  - IEEE
TI  - PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Weakly-supervised Temporal Action Localization (WTAL) attempts to localize the actions in untrimmed videos using only video-level supervision. Most recent works approach WTAL from a localization-by-classification perspective where these methods try to classify each video frame followed by a manually-designed post-processing pipeline to aggregate these per-frame action predictions into action snippets. Due to this perspective, the model lacks any explicit understanding of action boundaries and tends to focus only on the most discriminative parts of the video resulting in incomplete action localization. To address this, we present PivoTAL, Prior-driven Supervision for Weakly-supervised Temporal Action Localization, to approach WTAL from a localization-by-localization perspective by learning to localize the action snippets directly. To this end, PivoTAL leverages the underlying spatio-temporal regularities in videos in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to supervise the localization-based training. PivoTAL shows significant improvement (of at least 3% avg mAP) over all existing methods on the benchmark datasets, THUMOS-14 and ActivitNet-v1.3.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 22992
EP  - 23002
DO  - 10.1109/CVPR52729.2023.02202
AN  - WOS:001062531307031
AD  - Microsoft, Redmond, WA USA
AD  - Univ Cent Florida, Orlando, FL 32816 USA
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Helvaci, Halil Ismail
AU  - Cheung, Sen-ching Samson
AU  - Chuah, Chen-Nee
AU  - Ozonoff, Sally
TI  - Localizing Moments of Actions in Untrimmed Videos of Infants with Autism Spectrum Disorder
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Autism Spectrum Disorder (ASD) presents significant challenges in early diagnosis and intervention, impacting children and their families. With prevalence rates rising, there is a critical need for accessible and efficient screening tools. Leveraging machine learning (ML) techniques, in particular Temporal Action Localization (TAL), holds promise for automating ASD screening. This paper introduces a self-attention based TAL model designed to identify ASD-related behaviors in infant videos. Unlike existing methods, our approach simplifies complex modeling and emphasizes efficiency, which is essential for practical deployment in real-world scenarios. Importantly, this work underscores the importance of developing computer vision methods capable of operating in naturilistic environments with little equipment control, addressing key challenges in ASD screening. This study is the first to conduct end-to-end temporal action localization in untrimmed videos of infants with ASD, offering promising avenues for early intervention and support. We report baseline results of behavior detection using our TAL model. We achieve 70% accuracy for look face, 79% accuracy for look object, 72% for smile and 65% for vocalization.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.05849
AN  - PPRN:88468589
AD  - Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506, USA
AD  - Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA, USA
AD  - Univ Calif Davis, UC Davis MIND Inst, Davis, CA, USA
M2  - Univ Calif Davis
M2  - Univ Calif Davis
Y2  - 2024-04-22
ER  -

TY  - JOUR
AU  - Fish, Edward
AU  - Weinbren, Jon
AU  - Gilbert, Andrew
TI  - PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization accuracy and robustness in few-shot settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100, highlighting the efficacy of our multi-prompt optimal transport approach in overcoming the challenges of conventional few-shot TAL methods.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2403.18915
AN  - PPRN:88332860
AD  - Univ Surrey, Guildford, England
M2  - Univ Surrey
Y2  - 2024-04-14
ER  -

TY  - CPAPER
AU  - Luo, Wang
AU  - Zhang, Tianzhu
AU  - Yang, Wenfei
AU  - Liu, Jingen
AU  - Mei, Tao
AU  - Wu, Feng
AU  - Zhang, Yongdong
A1  - IEEE COMP SOC
TI  - Action Unit Memory Network for Weakly Supervised Temporal Action Localization
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without framelevel annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an action unit memory bank. In the proposed AUMN, two attention modules are designed to update the memory bank adaptively and learn action units specific classifiers. Furthermore, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the first work to explicitly model the action units with a memory network. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs favorably against state-of-the-art methods. Specifically, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly improved from 47.0% to 52.1%.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 9964
EP  - 9974
DO  - 10.1109/CVPR46437.2021.00984
AN  - WOS:000742075008010
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - JD AI Res, Mountain View, CA USA
M2  - JD AI Res
Y2  - 2022-01-29
ER  -

TY  - CPAPER
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Li, Jie
AU  - Gao, Xinbo
ED  - Ma, H
ED  - Wang, L
ED  - Zhang, C
ED  - Wu, F
ED  - Tan, T
ED  - Wang, Y
ED  - Lai, J
ED  - Zhao, Y
TI  - Weakly Supervised Temporal Action Localization with Segment-Level Labels
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PT I
M3  - Proceedings Paper
CP  - 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Univ Sci & Technol Beijing, Zhuhai, PEOPLES R CHINA
AB  - Temporal action localization presents a trade-off between test performance and annotation-time cost. Fully supervised methods achieve good performance with time-consuming boundary annotations. Weakly supervised methods with cheaper video-level category label annotations result in worse performance. In this paper, we introduce a new segment-level supervision setting: segments are labeled when annotators observe actions happening here. We incorporate this segment-level supervision along with a novel localization module in the training. Specifically, we devise a partial segment loss regarded as a loss sampling to learn integral action parts from labeled segments. Since the labeled segments are only parts of actions, the model tends to overfit along with the training process. To tackle this problem, we first obtain a similarity matrix from discriminative features guided by a sphere loss. Then, a propagation loss is devised based on the matrix to act as a regularization term, allowing implicit unlabeled segments propagation during training. Experiments validate that our method can outperform the video-level supervision methods with almost same the annotation time.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-88004-0
SN  - 978-3-030-88003-3
DA  - 2021 
PY  - 2021
VL  - 13019
SP  - 42
EP  - 54
DO  - 10.1007/978-3-030-88004-0_4
AN  - WOS:000846859200004
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
Y2  - 2022-09-07
ER  -

TY  - CPAPER
AU  - Kim, Young Hwi
AU  - Kang, Hyolim
AU  - Kim, Seon Joo
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - A Sliding Window Scheme for Online Temporal Action Localization
T2  - COMPUTER VISION, ECCV 2022, PT XXXIV
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Most online video understanding tasks aim to immediately process each streaming frame and output predictions frame-by-frame. For extension to instance-level predictions of existing online video tasks, Online Temporal Action Localization (On-TAL) has been recently proposed. However, simple On-TAL approaches of grouping per-frame predictions have limitations due to the lack of instance-level context. To this end, we propose Online Anchor Transformer (OAT) to extend the anchor-based action localization model to the online setting. We also introduce an online-applicable post-processing method that suppresses repetitive action proposals. Evaluations of On-TAL on THUMOS'14, MUSES, and BBDB show significant improvements in terms of mAP, and our model shows comparable performance to the state-of-the-art offline TAL methods with a minor change of the post-processing method. In addition to mAP evaluation, we additionally present a new online-oriented metric of early detection for On-TAL, and measure the responsiveness of each On-TAL approach.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-19829-8
SN  - 978-3-031-19830-4
DA  - 2022 
PY  - 2022
VL  - 13694
SP  - 653
EP  - 669
DO  - 10.1007/978-3-031-19830-4_37
AN  - WOS:000903746100037
AD  - Yonsei Univ, 50 Yonsei Ro, Seoul, South Korea
Y2  - 2023-02-02
ER  -

TY  - CPAPER
AU  - Yang, Minglei
AU  - Song, Yan
AU  - Shu, Xiangbo
AU  - Tang, Jinhui
ED  - Kompatsiaris, I
ED  - Huet, B
ED  - Mezaris, V
ED  - Gurrin, C
ED  - Cheng, WH
ED  - Vrochidis, S
TI  - Temporal Action Localization Based on Temporal Evolution Model and Multiple Instance Learning
T2  - MULTIMEDIA MODELING, MMM 2019, PT II
M3  - Proceedings Paper
CP  - 25th International Conference on MultiMedia Modeling (MMM)
CL  - Thessaloniki, GREECE
AB  - Temporal action localization in untrimmed long videos is an important yet challenging problem. The temporal ambiguity and the intra-class variations of temporal structure of actions make existing methods far from being satisfactory. In this paper, we propose a novel framework which firstly models each action clip based on its temporal evolution, and then adopts a deep multiple instance learning (MIL) network for jointly classifying action clips and refining their temporal boundaries. The proposed network utilizes a MIL scheme to make clip-level decisions based on temporal-instance-level decisions. Besides, a temporal smoothness constraint is introduced into the multi-task loss. We evaluate our framework on THUMOS Challenge 2014 benchmark and the experimental results show that it achieves considerable improvements as compared to the state-of-the-art methods. The performance gain is especially remarkable under precise localization with high tIoU thresholds, e.g. mAP@tIoU=0.5 is improved from 31.0% to 35.0%.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-05716-9
SN  - 978-3-030-05715-2
DA  - 2019 
PY  - 2019
VL  - 11296
SP  - 341
EP  - 351
DO  - 10.1007/978-3-030-05716-9_28
AN  - WOS:000705944500028
AD  - Nanjing Univ Sci & Technol, Nanjing, Peoples R China
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Shao, Yuxiang
AU  - Zhang, Feifei
AU  - Xu, Changsheng
TI  - Snippet-to-Prototype Contrastive Consensus Network for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised temporal action localization aims to localize action instances from untrimmed videos with only video-level labels. Due to the lack of frame-wise annotations, most methods embrace a localization-by-classification paradigm. However, the large supervision gap between classification and localization hinders models from obtaining accurate snippet-wise classification sequences and action proposals. We propose a snippet-to-prototype contrastive consensus network (SPCC-Net) to simultaneously generate feature-level and label-level supervision information to narrow the supervision gap between classification and localization. Specifically, the network adopts a two-stream framework incorporating the optical flow and fusion streams to fully leverage the motion and complementary information from multiple modalities. Firstly, the snippet-to-prototype contrast module is executed within each stream to learn prototypes for all categories and contrast them with action snippets to guarantee intra-class compactness and inter-class separability of snippet features. Secondly, for generating accurate label-level supervision information through complementary information of multimodal features, the multi-modality consensus module ensures not only category consistency through knowledge distillation but also semantic consistency through contrastive learning. Finally, we introduce the auxiliary multiple instance learning (MIL) loss to alleviate the issue that existing MIL-based methods only localize sparse discriminative snippets. Extensive experiments are conducted on two public datasets, THUMOS-14 and ActivityNet-1.3, to demonstrate the superior performance of our method over state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 6717
EP  - 6729
DO  - 10.1109/TMM.2024.3355628
AN  - WOS:001200272600003
AD  - Tianjin Univ Technol, Tianjin 300382, Peoples R China
AD  - Tianjin Univ Technol, Sch Comp Sci & Engn, Tianjin 300384, Peoples R China
AD  - Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518066, Peoples R China
Y2  - 2024-06-11
ER  -

TY  - JOUR
AU  - Zhu, Zixin
AU  - Wang, Le
AU  - Tang, Wei
AU  - Zheng, Nanning
AU  - Hua, Gang
TI  - ContextLoc plus plus : A Unified Context Model for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching the local, global and multi-scale contexts in the popular two-stage temporal localization framework. Our proposed model, dubbed ContextLoc++, can be divided into three sub-networks: L-Net, G-Net, and M-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. Furthermore, the spatial and temporal snippet-level features, functioning as keys and values, are fused by temporal gating. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. M-Net further fuses the local and global contexts with multi-scale proposal features. Specially, proposal-level features from multi-scale video snippets can focus on different action characteristics. Short-term snippets with fewer frames pay attention to the action details while long-term snippets with more frames focus on the action variations. Experiments on the THUMOS14 and ActivityNet v1.3 datasets validate the efficacy of our method against existing state-of-the-art TAL algorithms.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 AUG
PY  - 2023
VL  - 45
IS  - 8
SP  - 9504
EP  - 9519
DO  - 10.1109/TPAMI.2023.3237597
AN  - WOS:001022958600015
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA
AD  - WithWormpex Res, Bellevue, WA 98004 USA
M2  - WithWormpex Res
Y2  - 2023-08-23
ER  -

TY  - CPAPER
AU  - Liberatori, Benedetta
AU  - Conti, Alessandro
AU  - Rota, Paolo
AU  - Wang, Yiming
AU  - Ricci, Elisa
A1  - IEEE
TI  - Test-Time Zero-Shot Temporal Action Localization
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18720
EP  - 18729
DO  - 10.1109/CVPR52733.2024.01771
AN  - WOS:001342515502006
AD  - Univ Trento, Trento, Italy
AD  - Fdn Bruno Kessler, Trento, Italy
Y2  - 2025-02-01
ER  -

TY  - CPAPER
AU  - Xu, Weijie
AU  - Tan, Jingwei
AU  - Wang, Shulin
AU  - Yang, Sheng
ED  - Huang, DS
ED  - Zhang, X
ED  - Zhang, Q
TI  - Temporal Relation-Aware Global Attention Network for Temporal Action Detection
T2  - ADVANCED INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS, PT I, ICIC 2024
M3  - Proceedings Paper
CP  - 20th International Conference on Intelligent Computing (ICIC)
CL  - Tianjin Univ Sci & Tech, Tianjin, PEOPLES R CHINA
AB  - Temporal Action Detection (TAD) is a crucial task in video understanding. Its primary objective is to accurately identify the semantic labels of each action instance in an untrimmed video, along with their temporal range. This paper constructs the Temporal Relation-aware Global Attention Network (TRGA-Net), which is a long-term temporal context modelling network. The model comprises video preprocessing, spatiotemporal feature extraction, temporal context modelling, and temporal action detection header. TRGA-Net introduces a temporal context modelling-based temporal channel global attention module to efficiently perform long-term temporal context modelling. Experiments were conducted on the ActivityNet and THUMOS14 datasets to evaluate the performance of TRGA-Net. The results demonstrate better mean average precision (mAP) metrics than the previously proposed temporal detection model, verifying the usefulness of TRGA-Net for temporal context modelling.
PU  - SPRINGER-VERLAG SINGAPORE PTE LTD
PI  - SINGAPORE
PA  - 152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-981-97-5662-9
SN  - 978-981-97-5663-6
DA  - 2024 
PY  - 2024
VL  - 14875
SP  - 257
EP  - 269
DO  - 10.1007/978-981-97-5663-6_22
AN  - WOS:001307366100022
AD  - Hunan Univ, Sch Informat Sci & Engn, Changsha 410000, Hunan, Peoples R China
Y2  - 2024-10-09
ER  -

TY  - JOUR
AU  - Yang, Ke
AU  - Shen, Xiaolong
AU  - Qiao, Peng
AU  - Li, Shijie
AU  - Li, Dongsheng
AU  - Dou, Yong
TI  - Exploring frame segmentation networks for temporal action localization
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - Temporal action localization is an important task of computer vision. Though many methods have been proposed, it still remains an open question how to predict the temporal location of action segments precisely. Most state-of-the-art works train action classifiers on video segments pre-determined by action proposal. However, recent work found that a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. In this paper, we propose a Frame Segmentation Network (FSN) that places a temporal CNN on top of the 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in spatial dimension while temporal CNN is responsible for introducing temporal context information and performing dense predictions. The proposed FSN can make dense predictions at frame-level for a video clip using both spatial and temporal context information. FSN is trained in an end-to-end manner, so the model can be optimized in spatial and temporal domain jointly. We also adapt FSN to use it in weakly supervised scenario (WFSN), where only video level labels are provided when training. Experiment results on public dataset show that FSN achieves superior performance in both frame-level action localization and temporal action localization. (C) 2019 Elsevier Inc. All rights reserved.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2019 MAY
PY  - 2019
VL  - 61
SP  - 296
EP  - 302
DO  - 10.1016/j.jvcir.2019.02.003
AN  - WOS:000468011100030
AD  - Natl Univ Def Technol, Coll Comp, Natl Lab Parallel & Distributed Proc, Changsha, Hunan, Peoples R China
Y2  - 2019-06-04
ER  -

TY  - JOUR
AU  - Wang, Binglu
AU  - Zhang, Xun
AU  - Zhao, Yongqiang
TI  - Exploring Sub-Action Granularity for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Modeling cross-video relationship is an important issue for the weakly supervised temporal action localization task. To this end, traditional methods operate at the action level and rely on complicated strategies to prepare triplet samples, which only mines the cross-video relationships among three videos from two categories. In this work, we observe that action instances from different categories could exhibit similar motion patterns, i.e. subaction, and propose to operate at the sub-action granularity to elaborately explore cross-video relationships. However, only given video-level category labels, the sub-actions are undefined and not annotated. To tackle this challenge, we represent video features via a group of sub-actions, i.e. the sub-action family. Specifically, the sub-action family contains multiple feature vectors, where each vector is in charge of representing a specific sub-action. The sub-action family is shared among all videos in the dataset, while all videos contribute to the learning of the sub-action family. Consequently, we can not only get rid of the complicated sampling strategy but also thoroughly mine cross-video relationships from all available videos in the dataset. To learn feature vectors within the sub-action family, we employ a bottom-up temporal action localization paradigm and introduce an extra top-down branch. The sub-action family is introduced into the top-down branch, and it learns feature vectors via representing raw video features. Moreover, we propose a consistency loss to guide the learning process and a diversity loss to mine distinct sub-actions. Extensive experiments are carried out on three benchmark datasets, i.e. THUMOS14, ActivityNet v1.2 and ActivityNet v1.3, and the proposed method builds new high performance.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2022 APR
PY  - 2022
VL  - 32
IS  - 4
SP  - 2186
EP  - 2198
DO  - 10.1109/TCSVT.2021.3089323
AN  - WOS:000778973700040
AD  - Northwestern Polytech Univ, Sch Automat Engn, Xian 710072, Peoples R China
AD  - Northwestern Polytech Univ, Res & Dev Inst, Shenzhen 518057, Peoples R China
Y2  - 2022-04-21
ER  -

TY  - JOUR
AU  - Keshvarikhojasteh, Hassan
AU  - Mohammadzade, Hoda
AU  - Behroozi, Hamid
TI  - Temporal action localization using gated recurrent units
T2  - VISUAL COMPUTER
M3  - Article
AB  - Temporal action localization (TAL) task which is to predict the start and end of each action in a video along with the class label of the action has numerous applications in the real world. But due to the complexity of this task, acceptable accuracy rates have not been achieved yet, whereas this is not the case regarding the action recognition task. In this paper, we propose a new network based on gated recurrent unit (GRU) and two novel post-processing methods for TAL task. Specifically, we propose a new design for the output layer of the conventionally GRU resulting in the so-called GRU-Split network. Moreover, linear interpolation is used to generate the action proposals with precise start and end times. Finally, to rank the generated proposals appropriately, we use a Learn to Rank approach. We evaluated the performance of the proposed method on Thumos14 and ActivityNet-1.3 datasets. Results show the superiority of the performance of the proposed method compared to state of the art. Specifically in the mean Average Precision metric at Intersection over Union of 0.7 on Thumos14, we get 27.52% accuracy which is 5.12% better than that of state-of-the-art methods.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0178-2789
SN  - 1432-2315
DA  - 2023 JUL
PY  - 2023
VL  - 39
IS  - 7
SP  - 2823
EP  - 2834
DO  - 10.1007/s00371-022-02495-1
AN  - WOS:000796320900001
C6  - MAY 2022
AD  - Sharif Univ Technol, Tehran, Iran
Y2  - 2022-05-26
ER  -

TY  - JOUR
AU  - Zhang, Wei
AU  - Wang, Binglu
AU  - Ma, Songhui
AU  - Zhang, Yani
AU  - Zhao, Yongqiang
TI  - I2Net: Mining intra-video and inter-video attention for temporal action localization
T2  - NEUROCOMPUTING
M3  - Article
AB  - This paper focuses on two challenges for temporal action localization community, i.e., lack of long-term relationship and action pattern uncertainty. The former prevents the cooperation among multiple action instances within a video, while the latter may cause incomplete localizations or false positives. The lack of long-term relationship challenge results from the limited receptive field. Instead of stacking multiple layers or using large convolution kernels, we propose the intra-video attention mechanism to bring global receptive field to each temporal point. As for the action pattern uncertainty challenge, although it is hard to precisely depict the desired action pattern, paired videos that share the same action category can provide complementary information about action pattern. Consequently, we propose an inter-video attention mechanism to assist learning accurate action patterns. Based on the intra-video attention and inter-video attention, we propose a unified framework, namely I2Net, to tackle the challenging temporal action localization task. Given two videos containing sharing action categories, I2Net adopts the widely used one-stage action localization paradigm to dispose of them in parallel. As for two neighboring layers within the same video, the intra-video attention brings global information to each temporal point and helps to learn representative features. As for two parallel layers between two videos, the inter-video attention introduces complementary information to each video and helps to learn accurate action patterns. With the cooperation of intra-video and inter-video attention mechanisms, I2Net shows obvious performance gains over the baseline and builds new state-of-the-art on two widely-used benchmarks, i.e., THUMOS14 and ActivityNet v1.3.(c) 2021 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2021 JUL 15
PY  - 2021
VL  - 444
SP  - 16
EP  - 29
DO  - 10.1016/j.neucom.2021.02.085
AN  - WOS:000648645900002
C6  - MAR 2021
AD  - Northwestern Polytech Univ, Natl Key Lab Sci & Technol UAV, Xian 710072, Peoples R China
AD  - Northwest Polytech Univ, Sch Automat Engn, Xian 710072, Peoples R China
Y2  - 2021-06-04
ER  -

TY  - CPAPER
AU  - Jin, Cece
AU  - Zhang, Tao
AU  - Kong, Weijie
AU  - Li, Thomas
AU  - Li, Ge
A1  - IEEE
TI  - REGRESSION BEFORE CLASSIFICATION FOR TEMPORAL ACTION DETECTION
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING
M3  - Proceedings Paper
CP  - IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
CL  - Barcelona, SPAIN
AB  - Action classification combined with location regression is a widely-utilized mechanism in existing temporal action detection methods. However, there exists an inconsistency problem between locations and categories of action instances in this mechanism. More specifically, while the location of the proposal has been refined by the regressor, the action classifier still uses input and loss corresponding to the outdated unrefined proposal to predict category. In this paper, we propose to eliminate this inconsistency by making two modifications to the action classifier: 1) redirecting the classification loss to the refined proposal, and 2) rearranging the location regressor before the action classifier so that the feature of the refined proposal is fed to the classifier. Extensive experiments show that eliminating the inconsistency problem can significantly promote the detection performance. Our method achieves state-of-the-art performance for temporal action detection on the challenging THUMOS'14 dataset.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-5090-6631-5
DA  - 2020 
PY  - 2020
SP  - 2318
EP  - 2322
DO  - 10.1109/icassp40776.2020.9053319
AN  - WOS:000615970402112
AD  - Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Peking Univ, Adv Inst Technol, Beijing, Peoples R China
Y2  - 2021-03-02
ER  -

TY  - JOUR
AU  - Ju, Chen
AU  - Zhao, Peisen
AU  - Chen, Siheng
AU  - Zhang, Ya
AU  - Zhang, Xiaoyun
AU  - Wang, Yanfeng
AU  - Tian, Qi
TI  - Adaptive Mutual Supervision for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised temporal action localization aims to localize actions from untrimmed long videos with only video-level category labels. Most previous methods ignore the incompleteness issue of Class Activation Sequences (CAS), suffering from trivial detection results. To tackle this issue, we propose a novel Adaptive Mutual Supervision (AMS) framework with two branches, where the base branch detects the most discriminative action regions, while the supplementary branch localizes the less discriminative action regions through an adaptive sampler. The sampler dynamically updates the inputs for the supplementary branch using a sampling weight sequence negatively correlated with the CAS from the base branch, thus encouraging the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between two branches, we further construct mutual location supervision. Each branch adopts the location pseudo-labels generated from the other branch as the localization supervision. By alternately optimizing two branches for multiple iterations, we progressively complete action regions. Extensive experiments on THUMOS14 and ActivityNet1.2 demonstrate that the proposed AMS method significantly outperforms state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 6688
EP  - 6701
DO  - 10.1109/TMM.2022.3213478
AN  - WOS:001098831500076
AD  - Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China
AD  - Huawei Cloud & AI, Shenzhen 518129, Guangdong, Peoples R China
Y2  - 2023-12-16
ER  -

TY  - JOUR
AU  - Li, Zhiheng
AU  - Zhong, Yujie
AU  - Song, Ran
AU  - Li, Tianjiao
AU  - Ma, Lin
AU  - Zhang, Wei
TI  - DeTAL: Open-Vocabulary Temporal Action Localization With Decoupled Networks
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Pre-trained visual-language (ViL) models have demonstrated good zero-shot capability in video understanding tasks, where they were usually adapted through fine-tuning or temporal modeling. However, in the task of open-vocabulary temporal action localization (OV-TAL), such adaption reduces the robustness of ViL models against different data distributions, leading to a misalignment between visual representations and text descriptions of unseen action categories. As a result, existing methods often strike a trade-off between action detection and classification. Aiming at this issue, this paper proposes DeTAL, a simple but effective two-stage approach for OV-TAL. DeTAL decouples action detection from action classification to avoid the compromise between them, and the state-of-the-art methods for close-set action localization can be handily adapted to OV-TAL, which significantly improves the performance. Meanwhile, DeTAL can easily tackle the scenario where action category annotations are unavailable in the training dataset. In the experiments, we propose a new cross-dataset setting to evaluate the zero-shot capability of different methods. And the results demonstrate that DeTAL outperforms the state-of-the-art methods for OV-TAL on both THUMOS14 and ActivityNet1.3.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2024 DEC
PY  - 2024
VL  - 46
IS  - 12
SP  - 7728
EP  - 7741
DO  - 10.1109/TPAMI.2024.3395778
AN  - WOS:001364431200018
AD  - Shandong Univ, Sch Control Sci & Engn, Jinan 250100, Peoples R China
AD  - Meituan, Beijing 100000, Peoples R China
AD  - Singapore Univ Technol & Design, Informat Syst Technol & Design, Singapore 487372, Singapore
M2  - Meituan
Y2  - 2024-12-09
ER  -

TY  - CPAPER
AU  - Ren, Hao
AU  - Ren, Haoran
AU  - Ran, Wu
AU  - Lu, Hong
AU  - Jin, Cheng
ED  - Khanna, S
ED  - Cao, J
ED  - Bai, Q
ED  - Xu, G
TI  - Weakly-Supervised Temporal Action Localization with Multi-Head Cross-Modal Attention
T2  - PRICAI 2022: TRENDS IN ARTIFICIAL INTELLIGENCE, PT III
M3  - Proceedings Paper
CP  - 19th Pacific Rim International Conference on Artificial Intelligence (PRICAI)
CL  - Shanghai, PEOPLES R CHINA
AB  - Weakly-supervised temporal action localization seeks to localize temporal boundaries of actions while concurrently identifying their categories using only video-level category labels during training. Among the existing methods, the modal cooperation methods have achieved great success by providing pseudo supervision signals to RGB and Flow features. However, most of these methods ignore the cross-correlation between modal characteristics which can help them learn better features. By considering the cross-correlation, we propose a novel multi-head cross-modal attention mechanism to explicitly model the cross-correlation of modal features. The proposed method collaboratively enhances RGB and Flow features through a cross-correlation matrix. In this way, the enhanced features for each modality encode the inter-modal information, while preserving the exclusive and meaningful intra-modal characteristics. Experimental results on three recent methods demonstrate that the proposed Multi-head Cross-modal Attention (MCA) mechanism can significantly improve the performance of these methods, and even achieve state-of-the-art results on the THUMOS14 and ActivityNet1.2 datasets.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20867-6
SN  - 978-3-031-20868-3
DA  - 2022 
PY  - 2022
VL  - 13631
SP  - 281
EP  - 295
DO  - 10.1007/978-3-031-20868-3_21
AN  - WOS:000899320200021
AD  - Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai, Peoples R China
Y2  - 2023-01-25
ER  -

TY  - JOUR
AU  - Zhou, Feixiang
AU  - Williams, Bryan
AU  - Rahmani, Hossein
TI  - Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised Temporal Action Localization (SS-TAL). Existing methods often filter pseudo labels based on strict conditions, but they typically assess classification and localization quality separately, leading to suboptimal pseudo-label ranking and selection. In particular, there might be inaccurate pseudo labels within selected positives, alongside reliable counterparts erroneously assigned to negatives. To tackle these problems, we propose a novel Adaptive Pseudo-label Learning (APL) framework to facilitate better pseudo-label selection. Specifically, to improve the ranking quality, Adaptive Label Quality Assessment (ALQA) is proposed to jointly learn classification confidence and localization reliability, followed by dynamically selecting pseudo labels based on the joint score. Additionally, we propose an Instance-level Consistency Discriminator (ICD) for eliminating ambiguous positives and mining potential positives simultaneously based on inter-instance intrinsic consistency, thereby leading to a more precise selection. We further introduce a general unsupervised Action-aware Contrastive Pre-training (ACP) to enhance the discrimination both within actions and between actions and backgrounds, which benefits SS-TAL. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our method achieves state-of-the-art performance under various semi-supervised settings.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.07673
AN  - PPRN:90762139
AD  - Lancaster Univ, Lancaster, England
Y2  - 2024-08-02
ER  -

TY  - CPAPER
AU  - Li, Qinyu
AU  - Chen, Lijun
AU  - Wang, Hanli
AU  - Liu, Xianhui
ED  - Ro, YM
ED  - Cheng, WH
ED  - Kim, J
ED  - Chu, WT
ED  - Cui, P
ED  - Choi, JW
ED  - Hu, MC
ED  - DeNeve, W
TI  - Wonderful Clips of Playing Basketball: A Database for Localizing Wonderful Actions
T2  - MULTIMEDIA MODELING (MMM 2020), PT I
M3  - Proceedings Paper
CP  - 26th International Conference on MultiMedia Modeling (MMM)
CL  - Daejeon, SOUTH KOREA
AB  - Video highlight detection, or wonderful clip localization, aims at automatically discovering interesting clips in untrimmed videos, which can be applied to a variety of scenarios in real world. With reference to its study, a video dataset of Wonderful Clips of Playing Basketball (WCPB) is developed in this work. The Segment-Convolutional Neural Network (S-CNN), a start-of-the-art model for temporal action localization, is adopted to localize wonderful clips and a two-stream S-CNN is designed which outperforms its former on WCPB. The WCPB dataset presents the specific meaning of wonderful clips and annotations in playing basketball and enables the measurement of performance and progress in other realistic scenarios.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-37731-1
SN  - 978-3-030-37730-4
DA  - 2020 
PY  - 2020
VL  - 11961
SP  - 445
EP  - 454
DO  - 10.1007/978-3-030-37731-1_36
AN  - WOS:000611565300036
AD  - Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China
AD  - Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai, Peoples R China
AD  - Lanzhou City Univ, Dept Comp Sci, Lanzhou, Peoples R China
Y2  - 2021-02-16
ER  -

TY  - JOUR
AU  - Zhang, Can
AU  - Cao, Meng
AU  - Yang, Dongming
AU  - Jiang, Ji
AU  - Zou, Yuexian
TI  - Synergic learning for noise-insensitive webly-supervised temporal action localization
T2  - IMAGE AND VISION COMPUTING
M3  - Article
AB  - Webly-supervised temporal action localization (WebTAL) leverages web videos to train localization models without requiring manual temporal annotations. WebTAL is extremely challenging since video-level labels on the web are always noisy, seriously damaging the overall performance. Most state-of-the-art methods filter out noise before training, which will inevitably reduce the training samples. In contrast, we propose a preprocessing-free WebTAL framework along with a new synergic learning paradigm to alleviate the noise interference. Specifically, we introduce a synergic task called Spatio-Temporal Order Prediction (STOP) for spatiotemporal representation learning. This task requires a network to arrange permuted spatial crops and temporal clips, thereby learning the inherent spatial semantics and temporal interactions in videos. Instead of pre extracting features with the well-trained STOP, we design a novel synergic learning paradigm called Warm-up Synergic Training (WST) to iteratively generate better spatio-temporal representations and improve action localization results. In this synergic fashion, experimental results show that the interference caused by label noise will be largely mitigated. We demonstrate that our method outperforms all other WebTAL methods on two public benchmarks, THUMOS'14 and ActivityNet v1.2. (c) 2021 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0262-8856
SN  - 1872-8138
DA  - 2021 SEP
PY  - 2021
VL  - 113
C7  - 104247
DO  - 10.1016/j.imavis.2021.104247
AN  - WOS:000685039900003
C6  - JUL 2021
AD  - Peking Univ, Sch ECE, ADSPLAB, Shenzhen, Peoples R China
AD  - Pengcheng Lab, Shenzhen, Peoples R China
M2  - Pengcheng Lab
Y2  - 2021-08-25
ER  -

TY  - CPAPER
AU  - Wu, Xiaoyao
AU  - Song, Yonghong
ED  - Luo, B
ED  - Cheng, L
ED  - Wu, ZG
ED  - Li, H
ED  - Li, C
TI  - SGLP-Net: Sparse Graph Label Propagation Network for Weakly-Supervised Temporal Action Localization
T2  - NEURAL INFORMATION PROCESSING, ICONIP 2023, PT V
M3  - Proceedings Paper
CP  - 30th International Conference on Neural Information Processing (ICONIP) of the Asia-Pacific-Neural-Network-Society (APNNS)
CL  - Changsha, PEOPLES R CHINA
AB  - The present weakly-supervised methods for Temporal Action Localization are primarily responsible for capturing the temporal context. However, these approaches have limitations in capturing semantic context, resulting in the risk of ignoring snippets that are far apart but sharing the same action categories. To address this issue, we propose an action label propagation network utilizing sparse graph networks to effectively explore both temporal and semantic information in videos. The proposed SGLP-Net comprises two key components. One is the multiscale temporal feature embedding module, a novel method that extracts both local and global temporal features of the videos during the initial stage using CNN and self-attention and serves as a generic module. The other is an action label propagation mechanism, which uses graph networks for feature aggregation and label propagation. To avoid the issue of excessive feature completeness, we optimize training using sparse graph convolutions. Extensive experiments are conducted on THUMOS14 and ActivityNet1.3 benchmarks, among which advanced results demonstrate the superiority of the proposed method. Code can be found at https://github.com/xyao-wu/SGLP-Net.
PU  - SPRINGER-VERLAG SINGAPORE PTE LTD
PI  - SINGAPORE
PA  - 152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-981-99-8072-7
SN  - 978-981-99-8073-4
DA  - 2024 
PY  - 2024
VL  - 14451
SP  - 149
EP  - 161
DO  - 10.1007/978-981-99-8073-4_12
AN  - WOS:001148055000012
AD  - Xi An Jiao Tong Univ, Fac Elect & Informat Engn, Xian, Peoples R China
Y2  - 2024-02-14
ER  -

TY  - JOUR
AU  - Yun, Wulian
AU  - Qi, Mengshi
AU  - Wang, Chuanming
AU  - Ma, Huadong
TI  - Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.12332
AN  - PPRN:46977538
AD  - Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software & Multimedia, Beijing, Peoples R China
M2  - Beijing Univ Posts & Telecommun
Y2  - 2024-01-04
ER  -

TY  - CPAPER
AU  - Hu, Junshan
AU  - Zhuang, Liansheng
AU  - Dong, Weisong
AU  - Ge, Shiming
AU  - Wang, Shafei
A1  - ACM
TI  - Learning Generalized Representations for Open-Set Temporal Action Localization
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - Open-set Temporal Action Localization (OSTAL) is a critical and challenging task that aims to recognize and temporally localize human actions in untrimmed videos in open word scenarios. The main challenge in this task is the knowledge transfer from known actions to unknown actions. However, existing methods utilize limited training data and overparameterized deep neural network, which have poor generalization. This paper proposes a novel Generalized OSTAL model (namely GOTAL) to learn generalized representations of actions. GOTAL utilizes a Transformer network to model actions and a open-set detection head to perform action localization and recognition. Benefitting from Transformer's temporal modeling capabilities, GOTAL facilitates the extraction of human motion information from videos to mitigate the effects of irrelevant background data. Furthermore, a sharpness minimization algorithm is used to learn the network parameters of GOTAL, which facilitates the convergence of network parameters towards flatter minima by simultaneously minimizing the training loss value and sharpness of the loss plane. The collaboration of the above components significantly enhances the generalization of the representation. Experimental results demonstrate that GOTAL achieves the state-of-the-art performance on THUMOS14 and ActivityNet1.3 benchmarks, confirming the effectiveness of our proposed method.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 1987
EP  - 1996
DO  - 10.1145/3581783.3612278
AN  - WOS:001199449102011
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China
Y2  - 2024-07-31
ER  -

TY  - JOUR
AU  - Gupta, Akshita
AU  - Arora, Aditya
AU  - Narayan, Sanath
AU  - Khan, Salman
AU  - Khan, Fahad Shahbaz
AU  - Taylor, Graham W.
TI  - Open-Vocabulary Temporal Action Localization using Multimodal Guidance
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Open-Vocabulary Temporal Action Localization (OVTAL) enables a model to recognize any desired action category in videos without the need to explicitly curate training data for all categories. However, this flexibility poses significant challenges, as the model must recognize not only the action categories seen during training but also novel categories specified at inference. Unlike standard temporal action localization, where training and test categories are predetermined, OVTAL requires understanding contextual cues that reveal the semantics of novel categories. To address these challenges, we introduce OVFormer, a novel open-vocabulary framework extending ActionFormer with three key contributions. First, we employ task-specific prompts as input to a large language model to obtain rich class-specific descriptions for action categories. Second, we introduce a cross-attention mechanism to learn the alignment between class representations and frame-level video features, facilitating the multimodal guided features. Third, we propose a two-stage training strategy which includes training with a larger vocabulary dataset and finetuning to downstream data to generalize to novel categories. OVFormer extends existing TAL methods to open-vocabulary settings. Comprehensive evaluations on the THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of our method. Code and pretrained models will be publicly released.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2406.15556
AN  - PPRN:89410662
AD  - Univ Guelph, Guelph, ON N1G 2W1, Canada
AD  - York Univ, Toronto, ON, Canada
AD  - Vector Inst, Toronto, ON, Canada
AD  - Technol Innovat Inst, Abu Dhabi, U Arab Emirates
AD  - Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates
M2  - Univ Guelph
M2  - York Univ
M2  - Vector Inst
M2  - Mohamed Bin Zayed Univ Artificial Intelligence
Y2  - 2024-07-12
ER  -

TY  - JOUR
AU  - Ju, Chen
AU  - Zhao, Peisen
AU  - Chen, Siheng
AU  - Zhang, Ya
AU  - Zhang, Xiaoyun
AU  - Tian, Qi
TI  - Adaptive Mutual Supervision for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level action category labels. Most of previous methods ignore the incompleteness issue of Class Activation Sequences (CAS), suffering from trivial localization results. To solve this issue, we introduce an adaptive mutual supervision framework (AMS) with two branches, where the base branch adopts CAS to localize the most discriminative action regions, while the supplementary branch localizes the less discriminative action regions through a novel adaptive sampler. The adaptive sampler dynamically updates the input of the supplementary branch with a sampling weight sequence negatively correlated with the CAS from the base branch, thereby prompting the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between these two branches, we construct mutual location supervision. Each branch leverages location pseudo-labels generated from the other branch as localization supervision. By alternately optimizing the two branches in multiple iterations, we progressively complete action regions. Extensive experiments on THUMOS14 and ActivityNet1.2 demonstrate that the proposed AMS method significantly outperforms the state-of-the-art methods.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2104.02357
AN  - PPRN:11806308
AD  - Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China
AD  - Huawei Noahs Ark Lab, Shenzhen 518129, Guangdong, Peoples R China
M2  - Huawei Noahs Ark Lab
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Shou, Zheng
AU  - Wang, Dongang
AU  - Chang, Shih-Fu
TI  - Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes on the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and therefore achieve high temporal localization accuracy. Only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014, when the overlap threshold for evaluation is set to 0.5.
PU  - CORNELL UNIV
DA  - 2016 
PY  - 2016
DO  - arXiv:1601.02129
AN  - PPRN:12119536
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Chen, Changjian
AU  - Chen, Jiashu
AU  - Yang, Weikai
AU  - Wang, Haoze
AU  - Knittel, Johannes
AU  - Zhao, Xibin
AU  - Koch, Steffen
AU  - Ertl, Thomas
AU  - Liu, Shixia
TI  - Enhancing Single-Frame Supervision for Better Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization aims to identify the boundaries and categories of actions in videos, such as scoring a goal in a football match. Single-frame supervision has emerged as a labor-efficient way to train action localizers as it requires only one annotated frame per action. However, it often suffers from poor performance due to the lack of precise boundary annotations. To address this issue, we propose a visual analysis method that aligns similar actions and then propagates a few user-provided annotations (e.g. , boundaries, category labels) to similar actions via the generated alignments. Our method models the alignment between actions as a heaviest path problem and the annotation propagation as a quadratic optimization problem. As the automatically generated alignments may not accurately match the associated actions and could produce inaccurate localization results, we develop a storyline visualization to explain the localization results of actions and their alignments. This visualization facilitates users in correcting wrong localization results and misalignments. The corrections are then used to improve the localization results of other actions. The effectiveness of our method in improving localization performance is demonstrated through quantitative evaluation and a case study.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2312.05178
AN  - PPRN:86517804
AD  - Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Peoples R China
AD  - Tsinghua Univ, Sch Software, BNRist, Beijing, Peoples R China
AD  - Univ Stuttgart, Stuttgart, Germany
M2  - Hunan Univ
M2  - Univ Stuttgart
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Hu, Bo
AU  - Cai, Jianfei
AU  - Cham, Tat-Jen
AU  - Yuan, Junsong
TI  - Progress Regression RNN for Online Spatial-Temporal Action Localization in Unconstrained Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Previous spatial-temporal action localization methods commonly follow the pipeline of object detection to estimate bounding boxes and labels of actions. However, the temporal relation of an action has not been fully explored. In this paper, we propose an end-to-end Progress Regression Recurrent Neural Network (PR-RNN) for online spatial-temporal action localization, which learns to infer the action by temporal progress regression. Two new action attributes, called progression and progress rate, are introduced to describe the temporal engagement and relative temporal position of an action. In our method, frame-level features are first extracted by a Fully Convolutional Network (FCN). Subsequently, detection results and action progress attributes are regressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the observed frames instead of a single frame or a short clip. Finally, a novel online linking method is designed to connect single-frame results to spatial-temporal tubes with the help of the estimated action progress attributes. Extensive experiments demonstrate that the progress attributes improve the localization accuracy by providing more precise temporal position of an action in unconstrained videos. Our proposed PR-RNN achieves the stateof-the-art performance for most of the IoU thresholds on two benchmark datasets.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1903.00304
AN  - PPRN:13619155
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Sun, Che
AU  - Song, Hao
AU  - Wu, Xinxiao
AU  - Jia, Yunde
ED  - Lin, Z
ED  - Wang, L
ED  - Yang, J
ED  - Shi, G
ED  - Tan, T
ED  - Zheng, N
ED  - Chen, X
ED  - Zhang, Y
TI  - Learning Weighted Video Segments for Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PT I, PRCV 2019
M3  - Proceedings Paper
CP  - 2nd Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - NW Polytechn Univ, Xian, PEOPLES R CHINA
AB  - This paper proposes a novel approach of learning weighted video segments via supervised temporal attention for action localization in untrimmed videos. The learned segment weights represent informativeness of video segments to recognize actions and benefit inferring the boundaries to temporally localize actions. We build a Supervised Temporal Attention Network (STAN) to dynamically learn the weights of video segments, and generate descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions, respectively. Extensive experiments are conducted on two public benchmarks THUMOS2014 and ActivityNet1.3. The results demonstrate that our approach achieves substantially better performance than the state-of-the-art methods, verifying the effectiveness of learning weighted video segments.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-31653-2
SN  - 978-3-030-31654-9
DA  - 2019 
PY  - 2019
VL  - 11857
SP  - 181
EP  - 192
DO  - 10.1007/978-3-030-31654-9_16
AN  - WOS:001312829200016
AD  - Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Zhang, Qiming
AU  - Hu, Zhengping
AU  - Wang, Yulu
AU  - Bi, Shuai
AU  - Zhang, Hehao
AU  - Di, Jirui
TI  - DmrNet: Dual-stream Mutual Information Contraction and Re-discrimination Network for Semi-supervised Temporal Action Detection
T2  - COGNITIVE COMPUTATION
M3  - Article
AB  - Semi-supervised temporal action detection only requires a small number of labeled samples from the dataset and utilizes the remaining unlabeled samples for model training, effectively alleviating the significant time and manpower costs associated with annotating large-scale temporal action detection datasets. However, previous semi-supervised temporal action detection methods relied on sequential action localization and classification, which leads to erroneous localization predictions that can easily affect subsequent classification predictions, resulting in error propagation problem. To overcome error propagation, we propose a dual-stream mutual information contraction and re-discrimination network (DmrNet). Specifically, the traditional two-step strategy of temporal action detection has been changed to a four-step parallel strategy by us. Firstly, this paper designs the first-step classification prediction and the second-step localization prediction as a parallel structure to prevent error propagation from localization to classification. Then, in the third step, the dual-stream mutual information contraction part maps the dual-stream features to a new vector space to ensure the cross-correlation between classification and action localization. Finally, the fourth step of classification re-discrimination part captures the consistency information of the dual-stream structure to enhance internal representation. Compared with existing methods, DmrNet achieved an average accuracy improvement of 10.7% on ActivityNet v1.3 and 5.2% on THUMOS14 using only 10% annotation data. The experimental results show that the proposed DmrNet not only achieves good detection performance in semi-supervised learning but also achieves performance comparable to state-of-the-art methods in fully supervised learning.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 1866-9956
SN  - 1866-9964
DA  - 2025 FEB
PY  - 2025
VL  - 17
IS  - 1
C7  - 15
DO  - 10.1007/s12559-024-10374-1
AN  - WOS:001365485300002
AD  - Yanshan Univ, Sch Informat & Engn, Qinhuangdao 066004, Hebei, Peoples R China
AD  - Yanshan Univ, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066004, Hebei, Peoples R China
Y2  - 2024-12-05
ER  -

TY  - JOUR
AU  - Liu, Yuanyuan
AU  - Zhou, Ning
AU  - Huang, Yuxuan
AU  - Liu, Shuyang
AU  - Liu, Leyuan
AU  - Zhou, Wujie
AU  - Tang, Chang
AU  - Wang, Ke
TI  - Beyond boundaries: Hierarchical-contrast unsupervised temporal action localization with high-coupling feature learning
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Current unsupervised temporal action localization (UTAL) methods mainly use clustering and localization with independent learning mechanisms. However, these individual mechanisms are low-coupled and struggle to finely localize action-background boundary information due to the lack of feature interactions in the clustering and localization process. To address this, we propose an end-to-end Hierarchical-Contrast UTAL (HC-UTAL) framework with high-coupling multi-task feature learning. HC-UTAL incorporates coarse-to-fine contrastive learning (CL) at three levels: video level, instance level and boundary level, thus obtaining adaptive interaction and robust performance. We first employ the video-level CL on video-level and cluster-level feature learning, generating video action pseudo-labels. Then, using the video action pseudo-labels, we further devise the instance-level CL on action-related feature learning for coarse localization and the boundary-level CL on ambiguous action-background boundary feature learning for finer localization, respectively. We conduct extensive experiments on THUMOS'14, ActivityNet v1.2, and ActivityNet v1.3 datasets. The results demonstrate that our method achieves state-of-the-art performance. The code and trained models are available at: https: //github.com/bugcat9/HC-UTAL.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2025 JUN
PY  - 2025
VL  - 162
C7  - 111421
DO  - 10.1016/j.patcog.2025.111421
AN  - WOS:001426264000001
C6  - FEB 2025
AD  - China Univ Geosci Wuhan, Sch Comp Sci, Wuhan 430074, Peoples R China
AD  - Cent China Normal Univ, Sch Natl Engn Res Ctr Elearning, Wuhan 430079, Peoples R China
AD  - Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310018, Peoples R China
Y2  - 2025-02-26
ER  -

TY  - JOUR
AU  - Qin, Xin
AU  - Zhao, Hanbin
AU  - Lin, Guangchen
AU  - Zeng, Hao
AU  - Xu, Songcen
AU  - Li, Xi
TI  - PcmNet: Position-Sensitive Context Modeling Network for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is an important and challenging task that aims to locate temporal regions in real-world untrimmed videos where actions occur and recognize their classes. It is widely acknowledged that video context is a critical cue for video understanding, and exploiting the context has become an important strategy to boost localization performance. However, previous state-of-the-art methods focus more on exploring semantic context which captures the feature similarity among frames or proposals, and neglect positional context which is vital for temporal localization. In this paper, we propose a temporal-position-sensitive context modeling approach to incorporate both positional and semantic information for more precise action localization. Specifically, we first augment feature representations with directed temporal positional encoding, and then conduct attention-based information propagation, in both frame-level and proposal-level. Consequently, the generated feature representations are significantly empowered with the discriminative capability of encoding the position-aware context information, and thus benefit boundary detection and proposal evaluation. We achieve state-of-the-art performance on both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the effectiveness and generalization ability of our method.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.05270
AN  - PPRN:12585014
AD  - Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China
AD  - Noahs Ark Lab, Huawei Technol, Shenzhen 518129, Peoples R China
AD  - Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China
AD  - Alibaba Zhejiang Univ, Joint Insititute Frontier Technol, Hangzhou 310027, Peoples R China
M2  - Noahs Ark Lab
M2  - Alibaba Zhejiang Univ
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Keisham, Kanchan
AU  - Jalali, Amin
AU  - Kim, Jonghong
AU  - Lee, Minho
TI  - Multi-level alignment for few-shot temporal action localization
T2  - INFORMATION SCIENCES
M3  - Article
AB  - Temporal action localization (TAL), which aims to localize actions in long untrimmed videos, requires a large number of annotated training data. However, it is expensive to obtain segment level annotations for large-scale datasets. To overcome this challenge, a new few-shot learning method is proposed that localizes temporal actions for unseen classes with only a few training samples. In this study, a new multi-level encoder cosine-similarity alignment module is adopted that exploits the alignment of visual information at each temporal location. The proposed method arranges the video snippets that contain similar foreground action instances, and it captures the intra-class variations more implicitly. In addition, it incorporates cosine similarity in Transformer encoder layers that supports the self-attention mechanism. This emphasizes more on refined features at the higher encoder layers. Towards this objective, an episodic-based training scheme is adopted to learn the alignment of similar video snippets with a few training examples. At the test time, the learned context information is then adapted to novel classes. Experimental results show that the proposed method outperforms the state-of-the-art methods for few-shot temporal action localization with single and multiple action instances on the ActivityNet-1.3 dataset and achieves competitive results on the THUMOS-14 and HACS datasets.
PU  - ELSEVIER SCIENCE INC
PI  - NEW YORK
PA  - STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN  - 0020-0255
SN  - 1872-6291
DA  - 2023 DEC
PY  - 2023
VL  - 650
C7  - 119618
DO  - 10.1016/j.ins.2023.119618
AN  - WOS:001082334100001
C6  - SEP 2023
AD  - Kyungpook Natl Univ, Grad Sch Artificial Intelligence, Daegu 41566, South Korea
AD  - Kyungpook Natl Univ, AI Inst Technol, KNU LG Elect Convergence Res Ctr, Daegu 41566, South Korea
AD  - Keimyung Univ, Dept Res, Dongsan Med Ctr, 402,Daegu Technopk Keimyung Univ Ctr 1035, Daegu 42601, South Korea
Y2  - 2023-10-26
ER  -

TY  - JOUR
AU  - Liu, Yuanyuan
AU  - Zhu, Hong
AU  - Ren, Haohao
AU  - Shi, Jing
AU  - Wang, Dong
TI  - Fusion detection network with discriminative enhancement for weakly-supervised temporal action localization
T2  - EXPERT SYSTEMS WITH APPLICATIONS
M3  - Article
AB  - Weakly-supervised temporal action localization aims to identify and localize action instances in untrimmed videos using only video-level action labels. Due to the lack of frame-level annotation information, correctly distinguishing foreground and background snippets in a video is crucial for temporal action localization. However, alongside foreground and background snippets, a large number of semantically similar snippets exist within the video. Such snippets share the same semantic information with foreground or background, leading to less fine-grained boundary localization of action instances. Inspired by the success of multimodal learning, we have extracted high-quality semantic features from multimodal inputs and constructed contrast loss to enhance the ability of the model to distinguish semantically similar snippets. In this paper, we propose a fusion detection network with discriminative enhancement(De-FDN). Specifically, we design a fusion detection model (FDM) that fully leverages the complementarity and correlation among multimodal features to extract high-quality semantic features from videos. We then construct multimodal class activation sequences to accomplish accurate identification and localization of action instances. Additionally, we design a discriminative enhancement mechanism (DEM), which increases the gap between semantically similar segments by calculating the semantic contrast loss. Extensive experiments on the THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets demonstrate the effectiveness of our method.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0957-4174
SN  - 1873-6793
DA  - 2024 MAR 15
PY  - 2024
VL  - 238
C7  - 122000
DO  - 10.1016/j.eswa.2023.122000
AN  - WOS:001098682000001
C6  - OCT 2023
AD  - Xian Univ Technol, Sch Automat & Informat Engn, 5 South Jinhua Rd, Xian 710048, Shaanxi, Peoples R China
Y2  - 2023-11-30
ER  -

TY  - JOUR
AU  - Zhang, Haiping
AU  - Lin, Haixiang
AU  - Wang, Dongjing
AU  - Xu, Dongyang
AU  - Zhou, Fuxing
AU  - Guan, Liming
AU  - Yu, Dongjing
AU  - Fang, Xujian
TI  - TSCANet: a two-stream context aggregation network for weakly-supervised temporal action localization
T2  - JOURNAL OF SUPERCOMPUTING
M3  - Article
AB  - Weakly supervised temporal action localization classifies and localizes actions in uncropped videos by using only video-level labels. Many current methods employ feature extractors initially intended for post-cropped video action classification. The accuracy of localization decreases when feature extractors of this type are used, because they may introduce redundant information into the action localization task. To overcome the aforementioned constraints, we propose a WSTAL technique based on the two-stream context aggregation network (TSCANet), which consists of two main modules: a multistage temporal feature aggregation module (MSTFA) and a feature alignment module (FA). The MSTFA enables TSCANet to rapidly expand the receptive field and acquire temporal dependencies between long-distance segments by stacking dilated convolutional layers. Therefore, MSTFA allows the model to better aggregate temporal information in optical flow features to reduce redundant information in the original features. To avoid inconsistencies between the enhanced optical flow and RGB flow features, this study designed an FA to calibrate RGB features using optimized optical flow features through a mutual learning approach. On THUMOS14 and ActivityNet datasets, many comparative tests are carried out, and an improved localization performance is attained. In particular, localization at low t-IoU thresholds outperforms many of the existing WSTAL methods.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-8542
SN  - 1573-0484
DA  - 2025 JAN
PY  - 2025
VL  - 81
IS  - 1
C7  - 311
DO  - 10.1007/s11227-024-06810-6
AN  - WOS:001380262400002
AD  - Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Informat Engn, Hangzhou 311305, Zhejiang, Peoples R China
Y2  - 2024-12-24
ER  -

TY  - JOUR
AU  - Chao, Yu-Wei
AU  - Vijayanarasimhan, Sudheendra
AU  - Seybold, Bryan
AU  - A. Ross, David
AU  - Deng, Jia
AU  - Sukthankar, Rahul
TI  - Rethinking the Faster R-CNN Architecture for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1804.07667
AN  - PPRN:12858189
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Ma, Chunfei
AU  - Choi, Joonhyang
AU  - Lee, Byeongwon
AU  - Yang, Seungji
TI  - Submission to ActivityNet Challenge 2019: Task B Spatio-temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report present an overview of our system proposed for the spatio-temporal action localization(SAL) task in ActivityNet Challenge 2019. Unlike previous two-streams-based works, we focus on exploring the end-to-end trainable architecture using only RGB sequential images. To this end, we employ a previously proposed simple yet effective two-branches network called SlowFast Networks which is capable of capturing both short- and long-term spatiotemporal features. Moreover, to handle the severe class imbalance and overfitting problems, we propose a correlation-preserving data augmentation method and a random label subsampling method which have been proven to be able to reduce overfitting and improve the performance.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1907.10837
AN  - PPRN:21667787
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Dou, Peng
AU  - Zhou, Wei
AU  - Liao, Zhongke
AU  - Hu, Haifeng
ED  - Ma, H
ED  - Wang, L
ED  - Zhang, C
ED  - Wu, F
ED  - Tan, T
ED  - Wang, Y
ED  - Lai, J
ED  - Zhao, Y
TI  - Feature Matching Network for Weakly-Supervised Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PT IV
M3  - Proceedings Paper
CP  - 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Univ Sci & Technol Beijing, Zhuhai, PEOPLES R CHINA
AB  - Weakly supervised temporal action localization needs to get the category of the action as well as the time period when the action of corresponding category occurs with only video level annotation during training. Currently, how to effectively localize the actions with weak discriminative information and distinguish background activities is the major challenge. To address these issues, we propose a feature matching network (FMNet) that can produce attention scores and perform different operations on them to represent different actions and background features. Moreover, we modify the cross-entropy loss to match different features. Experimental results on two standard datasets THUMOS14 and ActivityNet1.2 show the superiority of our methods.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-88013-2
SN  - 978-3-030-88012-5
DA  - 2021 
PY  - 2021
VL  - 13022
SP  - 459
EP  - 471
DO  - 10.1007/978-3-030-88013-2_38
AN  - WOS:000846899700038
AD  - Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China
Y2  - 2022-09-07
ER  -

TY  - JOUR
AU  - Cao, Meng
AU  - Zhang, Can
AU  - Chen, Long
AU  - Shou, Mike Zheng
AU  - Zou, Yuexian
TI  - Deep Motion Prior for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 5203
EP  - 5213
DO  - 10.1109/TIP.2022.3193752
AN  - WOS:000836645400004
AD  - Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China
AD  - Columbia Univ, Dept Elect Engn, New York, NY 10027 USA
AD  - Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore
AD  - Peng Cheng Lab, Shenzhen 518066, Peoples R China
Y2  - 2022-08-18
ER  -

TY  - JOUR
AU  - Hori, Shimon
AU  - Omi, Kazuki
AU  - Tamaki, Toru
TI  - Query matching for spatio-temporal action detection with query-based object detector
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we propose a method that extends the query-based object detection model, DETR, to spatio-temporal action detection, which requires maintaining temporal consistency in videos. Our proposed method applies DETR to each frame and uses feature shift to incorporate temporal information. However, DETR's object queries in each frame may correspond to different objects, making a simple feature shift ineffective. To overcome this issue, we propose query matching across different frames, ensuring that queries for the same object are matched and used for the feature shift. Experimental results show that performance on the JHMDB21 dataset improves significantly when query features are shifted using the proposed query matching.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2409.18408
AN  - PPRN:100701460
AD  - Nagoya Inst Technol, Nagoya, Japan
Y2  - 2024-10-09
ER  -

TY  - CPAPER
AU  - Wu, Wanghui
AU  - Lu, Tao
AU  - Wang, Jiaming
AU  - Tang, Pan
AU  - Gao, Fangqun
A1  - IEEE
TI  - Temporal Action Detection with Frequency Attention Mechanism
T2  - 2024 7TH INTERNATIONAL CONFERENCE ON MECHATRONICS, ROBOTICS AND AUTOMATION, ICMRA 2024
M3  - Proceedings Paper
CP  - 7th International Conference on Mechatronics, Robotics and Automation
CL  - Wuhan, PEOPLES R CHINA
AB  - Due to the variability of video length and action duration, the temporal action detection task faces the problem of blurred action boundaries that are difficult to capture accurately. To alleviate this problem, this paper proposes a Frequency Attention Mechanism (FAM) that adaptively models the frequency dependencies between video signal channels, enabling the model to better understand the frequency variations in the video and to handle the complexity of different action durations, thus enhancing the sensitivity and discriminative power of the action boundaries, and still providing powerful action recognition even in long video sequences Capabilities. Through comprehensive experimental validation on a series of representative benchmark datasets (e.g. THUMOS14 and ActivityNet1.3), our approach demonstrates significant performance improvement.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2996-3796
SN  - 979-8-3503-5248-1
SN  - 979-8-3503-5247-4
DA  - 2024 
PY  - 2024
SP  - 137
EP  - 141
DO  - 10.1109/ICMRA62519.2024.10809079
AN  - WOS:001419080500024
AD  - Wuhan Inst Technol, Hubei Key Lab Intelligent Robot, Wuhan, Peoples R China
AD  - Wuhan Inst Technol, Wuhan, Peoples R China
AD  - China Elect Power Res Inst, State Key Lab Power Grid Environm Protect, Wuhan, Peoples R China
Y2  - 2025-03-13
ER  -

TY  - CPAPER
AU  - Moniruzzaman, Md
AU  - Yin, Zhaozheng
AU  - He, Zhihai
AU  - Qin, Ruwen
AU  - Leu, Ming C.
A1  - ASSOC COMP MACHINERY
TI  - Action Completeness Modeling with Background Aware Networks for Weakly-Supervised Temporal Action Localization
T2  - MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA
M3  - Proceedings Paper
CP  - 28th ACM International Conference on Multimedia (MM)
CL  - ELECTR NETWORK
AB  - The state-of-the-art of fully-supervised methods for temporal action localization from untrimmed videos has achieved impressive results. Yet, it remains unsatisfactory for the weakly-supervised temporal action localization, where only video-level action labels are given without the timestamp annotation on when the actions occur. The main reason comes from that, the weakly-supervised networks only focus on the highly discriminative frames, but there are some ambiguous frames in both background and action classes. The ambiguous frames in background class are very similar to the real actions, which may be treated as target actions and result in false positives. On the other hand, the ambiguous frames in action class which possibly contain action instances, are prone to be false negatives by the weakly-supervised networks and result in a coarse localization. To solve these problems, we introduce a novelweakly-supervised Action Completeness Modeling with Background Aware Networks (ACM-BANets). Our Background Aware Network (BANet) contains a weight-sharing two-branch architecture, with an action guided Background aware Temporal Attention Module (B-TAM) and an asymmetrical training strategy, to suppress both highly discriminative and ambiguous background frames to remove the false positives. Our action completeness modeling contains multiple BANets, and the BANets are forced to discover different but complementary action instances to completely localize the action instances in both highly discriminative and ambiguous action frames. In the i-th iteration, the i-th BANet discovers the discriminative features, which are then erased from the feature map. The partially-erased feature map is fed into the (i + 1)-th BANet of the next iteration to force this BANet to discover discriminative features different from the i-th BANet. Evaluated on two challenging untrimmed video datasets, THUMOS14 and ActivityNet1.3, our approach outperforms all the current weakly-supervised methods for temporal action localization.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-7988-5
DA  - 2020 
PY  - 2020
SP  - 2166
EP  - 2174
DO  - 10.1145/3394171.3413687
AN  - WOS:000810735002027
AD  - SUNY Stony Brook, Stony Brook, NY 11794 USA
AD  - Univ Missouri, Columbia, MO USA
AD  - Missouri Univ Sci Technol, Rolla, MO USA
Y2  - 2020-01-01
ER  -

TY  - CPAPER
AU  - Wang, Lin
AU  - Song, Yan
AU  - Yan, Rui
AU  - Shu, Xiangbo
ED  - Jonsson, BP
ED  - Gurrin, C
ED  - Tran, MT
ED  - DangNguyen, DT
ED  - Hu, AMC
ED  - Thanh, BHT
ED  - Huet, B
TI  - Spatiotemporal Perturbation Based Dynamic Consistency for Semi-supervised Temporal Action Detection
T2  - MULTIMEDIA MODELING (MMM 2022), PT I
M3  - Proceedings Paper
CP  - 28th International Conference on MultiMedia Modeling (MMM)
CL  - Phu Quoc, VIETNAM
AB  - Temporal action detection usually relies on huge tagging costs to achieve significant performance. Semi-supervised learning, where only a small amount of data are annotated in the training set, can help reduce the burden of labeling. However, the existing action detection models will inevitably learn inductive bias from limited labeled data and hinder the effective use of unlabeled data in semi-supervised learning. To this end, we propose a generic end-to-end framework for Semi-Supervised Temporal Action Detection (SS-TAD). Specifically, the framework is based on the teacher-student structure that leverages the consistency between unlabeled data and their augmentations. To achieve this, we propose a dynamic consistency loss by employing an attention mechanism to alleviate the prediction bias of the model, so it can make full use of the unlabeled data. Besides, we design a concise yet valid spatiotemporal feature perturbation module to learn robust action representations. Experiments on THUMOS14 and ActivityNet v1.2 demonstrate that our method significantly outperforms the start-of-the-art semi-supervised methods and is even comparable to the fully-supervised methods.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-98358-1
SN  - 978-3-030-98357-4
DA  - 2022 
PY  - 2022
VL  - 13141
SP  - 178
EP  - 190
DO  - 10.1007/978-3-030-98358-1_15
AN  - WOS:000788273400015
AD  - Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Peoples R China
Y2  - 2022-05-06
ER  -

TY  - JOUR
AU  - Zhao, Mengyao
AU  - Hu, Zhengping
AU  - Li, Shufang
AU  - Bi, Shuai
AU  - Sun, Zhe
TI  - Two-stream graph convolutional neural network fusion for weakly supervised temporal action detection
T2  - SIGNAL IMAGE AND VIDEO PROCESSING
M3  - Article
AB  - Weakly supervised temporal action detection is an important and challenging task, which is to detect temporal intervals of actions and identify category with only video-level labels. Correctly identifying the transition state between action and background will improve the detection accuracy; therefore, this paper focuses on filtering the transition state and proposes two-stream graph convolutional neural network fusion for weakly supervised temporal action detection. Generally, the transition state changes prominently and lasts for a short time, but it is not the same as the characteristics of the action. The feature difference between two video segments with temporal interactions indicates whether this segment belongs to the transition state. Then, according to the feature similarity and temporal correlation of the segments, the semantic similarity weighted graph and the transition-aware temporal correlation graph are constructed. Finally, the temporal attention sequence of video segments is extracted according to the fused two-stream graph feature. Taking the attention-based feature expression as input for the linear classifier to generate the class activation sequence, and the temporal action detection is performed accordingly. Experimental results on shared datasets show that the proposed method can effectively improve the performance of action detection.
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 1863-1703
SN  - 1863-1711
DA  - 2022 JUN
PY  - 2022
VL  - 16
IS  - 4
SP  - 947
EP  - 954
DO  - 10.1007/s11760-021-02039-5
AN  - WOS:000706021000001
C6  - OCT 2021
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao, Hebei, Peoples R China
M2  - Hebei Key Lab Informat Transmiss & Signal Proc
Y2  - 2021-10-21
ER  -

TY  - JOUR
AU  - Cao, Congqi
AU  - Wang, Yizhe
AU  - Lu, Yue
AU  - Zhang, Xin
AU  - Zhang, Yanning
TI  - Co-Occurrence Matters: Learning Action Relation for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization (TAL) is a prevailing task due to its great application potential. Existing works in this field mainly suffer from two weaknesses: (1) They often neglect the multi-label case and only focus on temporal modeling. (2) They ignore the semantic information in class labels and only use the visual information. To solve these problems, we propose a novel Co-Occurrence Relation Module (CORM) that explicitly models the co-occurrence relationship between actions. Besides the visual information, it further utilizes the semantic embeddings of class labels to model the co-occurrence relationship. The CORM works in a plug-and-play manner and can be easily incorporated with the existing sequence models. By considering both visual and semantic co-occurrence, our method achieves high multi-label relationship modeling capacity. Meanwhile, existing datasets in TAL always focus on low-semantic atomic actions. Thus we construct a challenging multi-label dataset UCF-Crime-TAL that focuses on high-semantic actions by annotating the UCF-Crime dataset at frame level and considering the semantic overlap of different events. Extensive experiments on two commonly used TAL datasets, i.e., MultiTHUMOS and TSU, and our newly proposed UCF-Crime-TAL demenstrate the effectiveness of the proposed CORM, which achieves state-of-the-art performance on these datasets.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.08463
AN  - PPRN:46838797
AD  - Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big Data Applicat Technol, Xian 710129, Peoples R China
Y2  - 2023-03-25
ER  -

TY  - JOUR
AU  - Sheng, Jinrong
AU  - Yu, Jiaruo
AU  - Li, Ziqiang
AU  - Li, Ao
AU  - Ge, Yongxin
TI  - Self-supervised temporal adaptive learning for weakly-supervised temporal action localization
T2  - INFORMATION SCIENCES
M3  - Article
AB  - Weakly-supervised temporal action localization (WTAL) identifies and localizes actions in untrimmed videos with only video-level labels. Most methods prioritize discriminative snippets, often neglecting of hard action snippets while focusing on class-specific background. Although recent methods have tackled this issue through temporal modeling, they overlook the local temporal structure of actions. To model such temporal structure effectively, we propose a novel self-supervised temporal adaptive learning (STAL) framework, which is composed of two core parts, i.e. self-supervised temporal learning (STL) network and the adaptive learning unit (ALU). Specifically, STL constructs a self-supervised task by performing an erasure and reconstruction process. This pseudo-label-based method relies on a classification task to perceive continuous temporal information for action localization task. To avoid the disturbance of un-confident pseudo labels during self-supervised learning process, two adaptive learning strategies of ALU are designed from two perspectives. In detail, a task-adaptive learning strategy is used to train the proposed tasks to the best for more reliable pseudo labels. Meanwhile, a score-adaptive learning strategy is designed to balance class activation and attention scores. Experiments on two classical datasets, namely, THUMOS14 and ActivityNet datasets, verify the effectiveness of our method.
PU  - ELSEVIER SCIENCE INC
PI  - NEW YORK
PA  - STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN  - 0020-0255
SN  - 1872-6291
DA  - 2025 JUL
PY  - 2025
VL  - 705
C7  - 121986
DO  - 10.1016/j.ins.2025.121986
AN  - WOS:001429309500001
C6  - FEB 2025
AD  - Chongqing Univ, Sch Bigdata & Software Engn, Daxuecheng South Rd 55, Chognqing 400044, Peoples R China
Y2  - 2025-03-01
ER  -

TY  - JOUR
AU  - Wang, Xijun
AU  - Katsaggelos, Aggelos K.
TI  - Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Weakly-supervised temporal action localization aims to identify and localize the action instances in the untrimmed videos with only video-level action labels. When humans watch videos, we can adapt our abstract-level knowledge about actions in different video scenarios and detect whether some actions are occurring. In this paper, we mimic how humans do and bring a new perspective for locating and identifying multiple actions in a video. We propose a network named VQK-Net with a video-specific query-key attention modeling that learns a unique query for each action category of each input video. The learned queries not only contain the actions' knowledge features at the abstract level but also have the ability to fit this knowledge into the target video scenario, and they will be used to detect the presence of the corresponding action along the temporal dimension. To better learn these action category queries, we exploit not only the features of the current input video but also the correlation between different videos through a novel video-specific action category query learner worked with a query similarity loss. Finally, we conduct extensive experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and ActivityNet1.3) and achieve state-of-the-art performance.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2305.04186
AN  - PPRN:68472023
AD  - Northwestern Univ, Dept Comp Sci, Evanston, IL 60208, USA
AD  - Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL, USA
Y2  - 2024-01-04
ER  -

TY  - CPAPER
AU  - Li, Yixuan
AU  - Chen, Lei
AU  - He, Runyu
AU  - Wang, Zhenzhi
AU  - Wu, Gangshan
AU  - Wang, Limin
A1  - IEEE
TI  - MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our datasets are characterized with important properties of high diversity, dense annotation, and high quality. Our MultiSports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. To benchmark this, we adapt several baseline methods to our dataset and give an in-depth analysis on the action detection results in our dataset. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future. Our dataset website is at https://deeperaction.github.io/multisports/.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13516
EP  - 13525
DO  - 10.1109/ICCV48922.2021.01328
AN  - WOS:000798743203069
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2022-06-24
ER  -

TY  - JOUR
AU  - Feng, Qianhan
AU  - Li, Wenshuo
AU  - Lin, Tong
AU  - Chen, Xinghao
TI  - Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos using only video-level supervision. Latest WSTAL methods introduce pseudo label learning framework to bridge the gap between classification-based training and inferencing targets at localization, and achieve cutting-edge results. In these frameworks, a classification-based model is used to generate pseudo labels for a regression-based student model to learn from. However, the quality of pseudo labels in the framework, which is a key factor to the final result, is not carefully studied. In this paper, we propose a set of simple yet efficient pseudo label quality enhancement mechanisms to build our FuSTAL framework. FuSTAL enhances pseudo label quality at three stages: cross-video contrastive learning at proposal Generation-Stage, prior-based filtering at proposal Selection-Stage and EMA-based distillation at Training-Stage. These designs enhance pseudo label quality at different stages in the framework, and help produce more informative, less false and smoother action proposals. With the help of these comprehensive designs at all stages, FuSTAL achieves an average mAP of 50.8% on THUMOS'14, outperforming the previous best method by 1.2%, and becomes the first method to reach the milestone of 50%.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.08971
AN  - PPRN:90797180
AD  - Peking Univ, State Key Lab GAI, SIST, Beijing, Peoples R China
AD  - Huawei Noahs Ark Lab, Beijing, Peoples R China
M2  - Huawei Noahs Ark Lab
Y2  - 2024-07-23
ER  -

TY  - JOUR
AU  - Keshvarikhojasteh, Hassan
AU  - Mohammadzade, Hoda
AU  - Behroozi, Hamid
TI  - Temporal Action Localization Using Gated Recurrent Units
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) task which is to predict the start and end of each action in a video along with the class label of the action has numerous applications in the real world. But due to the complexity of this task, acceptable accuracy rates have not been achieved yet, whereas this is not the case regarding the action recognition task. In this paper, we propose a new network based on Gated Recurrent Unit (GRU) and two novel post-processing methods for TAL task. Specifically, we propose a new design for the output layer of the conventionally GRU resulting in the so-called GRU-Split network. Moreover, linear interpolation is used to generate the action proposals with precise start and end times. Finally, to rank the generated proposals appropriately, we use a Learn to Rank (LTR) approach. We evaluated the performance of the proposed method on Thumos14 and ActivityNet-1.3 datasets. Results show the superiority of the performance of the proposed method compared to state-of-the-art. Specifically in the mean Average Precision (mAP) metric at Intersection over Union (IoU) of 0.7 on Thumos14, we get 27.52% accuracy which is 5.12% better than that of state-of-the-art methods.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2108.03375
AN  - PPRN:12235321
AD  - Sharif Univ Technol, Tehran, Iran
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Agarwal, Nakul
AU  - Chen, Yi-Ting
AU  - Dariush, Behzad
AU  - Yang, Ming-Hsuan
TI  - Unsupervised Domain Adaptation for Spatio-Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Spatio-temporal action localization is an important problem in computer vision that involves detecting where and when activities occur, and therefore requires modeling of both spatial and temporal features. This problem is typically formulated in the context of supervised learning, where the learned classifiers operate on the premise that both training and test data are sampled from the same underlying distribution. However, this assumption does not hold when there is a significant domain shift, leading to poor generalization performance on the test data. To address this, we focus on the hard and novel task of generalizing training models to test samples without access to any labels from the latter for spatio-temporal action localization by proposing an end-to-end unsupervised domain adaptation algorithm. We extend the state-of-the-art object detection framework to localize and classify actions. In order to minimize the domain shift, three domain adaptation modules at image level (temporal and spatial) and instance level (temporal) are designed and integrated. We design a new experimental setup and evaluate the proposed method and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark datasets. We show that significant performance gain can be achieved when spatial and temporal features are adapted separately, or jointly for the most effective results.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2010.09211
AN  - PPRN:14893787
Y2  - 2023-03-22
ER  -

TY  - CPAPER
AU  - Zhou, Feixiang
AU  - Williams, Bryan
AU  - Rahmani, Hossein
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - Towards Adaptive Pseudo-Label Learning for Semi-Supervised Temporal Action Localization
T2  - COMPUTER VISION - ECCV 2024, PT LXII
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised Temporal Action Localization (SS-TAL). Existing methods often filter pseudo labels based on strict conditions, but they typically assess classification and localization quality separately, leading to suboptimal pseudo-label ranking and selection. In particular, there might be inaccurate pseudo labels within selected positives, alongside reliable counterparts erroneously assigned to negatives. To tackle these problems, we propose a novel Adaptive Pseudo-label Learning (APL) framework to facilitate better pseudo-label selection. Specifically, to improve the ranking quality, Adaptive Label Quality Assessment (ALQA) is proposed to jointly learn classification confidence and localization reliability, followed by dynamically selecting pseudo labels based on the joint score. Additionally, we propose an Instance-level Consistency Discriminator (ICD) for eliminating ambiguous positives and mining potential positives simultaneously based on inter-instance intrinsic consistency, thereby leading to a more precise selection. We further introduce a general unsupervised Action-aware Contrastive Pre-training (ACP) to enhance the discrimination both within actions and between actions and backgrounds, which benefits SS-TAL. Extensive experiments on THU-MOS14 and ActivityNet v1.3 demonstrate that our method achieves state-of-the-art performance under various semi-supervised settings.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-73032-0
SN  - 978-3-031-73033-7
DA  - 2025 
PY  - 2025
VL  - 15120
SP  - 320
EP  - 338
DO  - 10.1007/978-3-031-73033-7_18
AN  - WOS:001353685900018
AD  - Univ Lancaster, Lancaster, England
Y2  - 2024-12-03
ER  -

TY  - CPAPER
AU  - Yang, Cheng
AU  - Zhang, Weigang
A1  - IEEE
TI  - Weakly Supervised Temporal Action Localization Through Contrastive Learning
T2  - 2022 IEEE 5TH INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL, MIPR
M3  - Proceedings Paper
CP  - IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR)
CL  - ELECTR NETWORK
AB  - In recent years, weakly-supervised temporal action localization (WS-TAL) with only video-level annotations, which aims to learn whether each untrimmed video contains action frames gains more attention. Existing most WS-TAL methods especially rely on features learned for action localization. Therefore, it is important to improve the ability to separate the frames of action instances from the background frames. To address this challenge, this paper introduces a framework that learns two extra constraints, Action-Background Learning and Action-Foreground Learning. The former aims at maximizing the discrepancy inside the feature of action and background while the latter avoids the misjudgement of action instance. We evaluate the proposed model on two benchmark datasets, and the experimental results show that the method could gain comparable performance with current state-of-the-art WS-TAL methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2770-4327
SN  - 978-1-6654-9548-6
DA  - 2022 
PY  - 2022
SP  - 383
EP  - 386
DO  - 10.1109/MIPR54900.2022.00075
AN  - WOS:001345025800068
AD  - Harbin Inst Technol, Sch Comp Sci & Technol, Weihai, Peoples R China
Y2  - 2022-01-01
ER  -

TY  - CPAPER
AU  - Xiao, Xinyu
AU  - Hu, Yun
AU  - Liu, Eryun
A1  - IEEE
TI  - LOCAL-TO-GLOBAL SELF-CONSISTENCY LEARNING FOR TEMPORAL ACTION LOCALIZATION
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME 2024
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Niagra Falls, CANADA
AB  - The object of temporal action localization (TAL) is to predict the predefined action labels and the corresponding temporal boundary in a video. It can be found that TAL is a task of multi-modal modeling and highly dependent on the effect of temporal context representation. Inspired by this property, we propose an end-to-end local-to-global modeling architecture to learn the contextual consistency information in temporal sequence and cross-modal. Specifically, a local-toglobal encoding Transformer is applied to model the video sequence to obtain video representation of different time scales. To achieve a reasonable balance between the specificity and correlation of different modalities, a cross semantic alignment (CSA) module is proposed to re-weight the encoded multi-model features by whether attending to the semantic correlations or specificity in different modalities. Further, to learn the trans-modal consistency from local to global and the uni-modal consistency belonging to the same category, the self-consistency learning (SCL) is designed to train the network. The experimental results demonstrate the significance of our method in major improvements upon prior works. Our model achieves 68.3% and 37.1% average mAPs on THUMOS14 and ActivityNet 1.3, outperforming state-of-the-art multi-stage and one-stage models.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 979-8-3503-9015-5
SN  - 979-8-3503-9016-2
DA  - 2024 
PY  - 2024
DO  - 10.1109/ICME57554.2024.10688049
AN  - WOS:001364925202109
AD  - ANT Grp, Hangzhou, Peoples R China
AD  - ShanghaiTech Univ, Shanghai, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
Y2  - 2025-04-16
ER  -

TY  - JOUR
AU  - Wang, Binglu
AU  - Yang, Le
AU  - Zhao, Yongqiang
TI  - POLO: Learning Explicit Cross-Modality Fusion for Temporal Action Localization
T2  - IEEE SIGNAL PROCESSING LETTERS
M3  - Article
AB  - Temporal action localization aims at discovering action instances in untrimmed videos, where RGB and flow are two widely used feature modalities. Specifically, RGB chiefly reveals appearance and flow mainly depicts motion. Given RGB and flow features, previous methods employ the early fusion or late fusion paradigm to mine the complementarity between them. By concatenating raw RGB and flow features, the early fusion implicitly achieved complementarity by the network, but it partly discards the particularity of each modality. The late fusion independently maintains two branches to explore the particularity of each modality, but it only fuses the localization results, which is insufficient to mine the complementarity. In this work, we propose explicit cross-modality fusion (POLO) to effectively utilize the complementarity between two modalities and thoroughly explore the particularity of each modality. POLO performs cross-modality fusion via estimating the attention weight from RGB modality and employing it to flow modality (vice versa). This bridges the complementarity of one modality to supply the other. Assisted with the attention weight, POLO independently learns from RGB and flow features and explores the particularity of each modality. Extensive experiments on two benchmarks demonstrate the preferable performance of POLO.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1070-9908
SN  - 1558-2361
DA  - 2021 
PY  - 2021
VL  - 28
SP  - 503
EP  - 507
DO  - 10.1109/LSP.2021.3061289
AN  - WOS:000631199000003
AD  - Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China
Y2  - 2021-03-26
ER  -

TY  - CPAPER
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Li, Jie
AU  - Gao, Xinbo
ED  - Ma, H
ED  - Wang, L
ED  - Zhang, C
ED  - Wu, F
ED  - Tan, T
ED  - Wang, Y
ED  - Lai, J
ED  - Zhao, Y
TI  - CRNet: Centroid Radiation Network for Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PT I
M3  - Proceedings Paper
CP  - 4th Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Univ Sci & Technol Beijing, Zhuhai, PEOPLES R CHINA
AB  - Temporal action localization aims to localize segments in an untrimmed video that contains different actions. Since contexts at boundaries between action instances and backgrounds are similar, how to separate the action instances from their surrounding is a challenge to be solved. In fact, the similar or dissimilar contents in actions play an important role in accomplishing the task. Intuitively, the instances with the same class label are affinitive while those with different labels are divergent. In this paper, we propose a novel method to model the relations between pairs of frames and generate precise action boundaries based on the relations, namely Centroid Radiation Network (CRNet). Specifically, we propose a Relation Network (RelNet) to represent the relations between sampled pairs of frames by employing an affinity matrix. To generate action boundaries, we use an Offset Network (OffNet) to estimate centroids of each action segments and their corresponding class labels. Based on the assumption that a centroid and its propagating areas have the same action label, we obtain action boundaries by adopting random walk to propagate a centroid to its related areas. Our proposed method is an one-stage method and can be trained in an end-to-end fashion. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-88004-0
SN  - 978-3-030-88003-3
DA  - 2021 
PY  - 2021
VL  - 13019
SP  - 29
EP  - 41
DO  - 10.1007/978-3-030-88004-0_3
AN  - WOS:000846859200003
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
Y2  - 2022-09-07
ER  -

TY  - JOUR
AU  - Zhao, Yue
AU  - Xiong, Yuanjun
AU  - Wang, Limin
AU  - Wu, Zhirong
AU  - Tang, Xiaoou
AU  - Lin, Dahua
TI  - Temporal Action Detection with Structured Segment Networks
T2  - INTERNATIONAL JOURNAL OF COMPUTER VISION
M3  - Article
AB  - This paper addresses an important and challenging task, namely detecting the temporal intervals of actions in untrimmed videos. Specifically, we present a framework called structured segment network (SSN). It is built on temporal proposals of actions. SSN models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and precise localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end manner. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping is devised to generate high quality action proposals. We further study the importance of the decomposed discriminative model and discover a way to achieve similar accuracy using a single classifier, which is also complementary with the original SSN design. On two challenging benchmarks, THUMOS'14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-5691
SN  - 1573-1405
DA  - 2020 JAN
PY  - 2020
VL  - 128
IS  - 1
SP  - 74
EP  - 95
DO  - 10.1007/s11263-019-01211-2
AN  - WOS:000511490100004
AD  - Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China
AD  - Amazon Rekognit, Seattle, WA USA
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Microsoft Res Asia, Beijing, Peoples R China
M2  - Amazon Rekognit
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Cheng, Feng
AU  - Bertasius, Gedas
TI  - TALLFormer: Temporal Action Localization with a Long-memory Transformer
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Most modern approaches in temporal action localization divide this problem into two parts: (i) short-term feature extraction and (ii) long-range temporal boundary localization. Due to the high GPU memory cost caused by processing long untrimmed videos, many methods sacrifice the representational power of the short-term feature extractor by either freezing the backbone or using a small spatial video resolution. This issue becomes even worse with the recent video transformer models, many of which have quadratic memory complexity. To address these issues, we propose TALLFormer, a memory-efficient and end-to-end trainable Temporal Action Localization transformer with Long-term memory. Our long-term memory mechanism eliminates the need for processing hundreds of redundant video frames during each training iteration, thus, significantly reducing the GPU memory consumption and training time. These efficiency savings allow us (i) to use a powerful video transformer feature extractor without freezing the backbone or reducing the spatial video resolution, while (ii) also maintaining long-range temporal boundary localization capability. With only RGB frames as input and no external action recognition classifier, TALLFormer outperforms previous state-of-the-arts by a large margin, achieving an average mAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The code is public available1
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2204.01680
AN  - PPRN:11813887
AD  - Univ North Carolina Chapel Hill, Dept Comp Sci, Chapel Hill, NC 27599, USA
M2  - Univ North Carolina Chapel Hill
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Wang, Fengshun
AU  - Wang, Qiurui
AU  - Wang, Yuting
TI  - FMI-TAL: Few-shot Multiple Instances Temporal Action Localization by Probability Distribution Learning and Interval Cluster Refinement
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The present few-shot temporal action localization model can't handle the situation where videos contain multiple action instances. So the purpose of this paper is to achieve manifold action instances localization in a lengthy untrimmed query video using limited trimmed support videos. To address this challenging problem effectively, we proposed a novel solution involving a spatial-channel relation transformer with probability learning and cluster refinement. This method can accurately identify the start and end boundaries of actions in the query video, utilizing only a limited number of labeled videos. Our proposed method is adept at capturing both temporal and spatial contexts to effectively classify and precisely locate actions in videos, enabling a more comprehensive utilization of these crucial details. The selective cosine penalization algorithm is designed to suppress temporal boundaries that do not include action scene switches. The probability learning combined with the label generation algorithm alleviates the problem of action duration diversity and enhances the model's ability to handle fuzzy action boundaries. The interval cluster can help us get the final results with multiple instances situations in few-shot temporal action localization. Our model achieves competitive performance through meticulous experimentation utilizing the benchmark datasets ActivityNet1.3 and THUMOS14.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.13765
AN  - PPRN:91549566
AD  - Capital Univ Phys Educ & Sports, Beijing, Peoples R China
M2  - Capital Univ Phys Educ & Sports
Y2  - 2024-09-04
ER  -

TY  - JOUR
AU  - Zolfaghari, Mohammadreza
AU  - L. Oliveira, Gabriel
AU  - Sedaghat, Nima
AU  - Brox, Thomas
TI  - Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1704.00616
AN  - PPRN:22211536
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Yun, Wulian
AU  - Qi, Mengshi
AU  - Wang, Chuanming
AU  - Ma, Huadong
ED  - Wooldridge, M
ED  - Dy, J
ED  - Natarajan, S
TI  - Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature
T2  - THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 7
M3  - Proceedings Paper
CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
CL  - Vancouver, CANADA
AB  - Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods. Our source code is available at https://github.com/wuli55555/ISSF.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - *****************
DA  - 2024 
PY  - 2024
SP  - 6908
EP  - 6916
AN  - WOS:001239937300046
AD  - Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing, Peoples R China
Y2  - 2024-08-15
ER  -

TY  - JOUR
AU  - Hu, Kai
AU  - Shen, Chaowen
AU  - Wang, Tianyan
AU  - Xu, Keer
AU  - Xia, Qingfeng
AU  - Xia, Min
AU  - Cai, Chengxue
TI  - Overview of temporal action detection based on deep learning
T2  - ARTIFICIAL INTELLIGENCE REVIEW
M3  - Article
AB  - Temporal Action Detection (TAD) aims to accurately capture each action interval in an untrimmed video and to understand human actions. This paper comprehensively surveys the state-of-the-art techniques and models used for TAD task. Firstly, it conducts comprehensive research on this field through Citespace and comprehensively introduce relevant dataset. Secondly, it summarizes three types of methods, i.e., anchor-based, boundary-based, and query-based, from the design method level. Thirdly, it summarizes three types of supervised learning methods from the level of learning methods, i.e., fully supervised, weakly supervised, and unsupervised. Finally, this paper explores the current problems, and proposes prospects in TAD task.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0269-2821
SN  - 1573-7462
DA  - 2024 FEB 1
PY  - 2024
VL  - 57
IS  - 2
C7  - 26
DO  - 10.1007/s10462-023-10650-w
AN  - WOS:001160866600001
AD  - Nanjing Univ Informat Sci & Technol, Sch Automat, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China
AD  - Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China
AD  - Wuxi Univ, Sch Automat, 333 Xishan Rd, Wuxi 214105, Jiangsu, Peoples R China
AD  - Nanjing Univ, Sch Management & Engn, 22 Hankou Rd, Nanjing 210093, Jiangsu, Peoples R China
Y2  - 2024-02-21
ER  -

TY  - JOUR
AU  - Zhang, Dejun
AU  - He, Linchao
AU  - Tu, Zhigang
AU  - Zhang, Shifu
AU  - Han, Fei
AU  - Yang, Boxiong
TI  - Learning motion representation for real-time spatio-temporal action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - The current deep learning based spatio-temporal action localization methods that using motion information (predominated is optical flow) obtain the state-of-the-art performance. However, since the optical flow is pre-computed, leading to these methods face two problems - the computational efficiency is low and the whole network is not end-to-end trainable. We propose a novel spatio-temporal action localization approach with an integrated optical flow sub-network to address these two issues. Specifically, our designed flow subnet can estimate optical flow efficiently and accurately by using multiple consecutive RGB frames rather than two adjacent frames in a deep network, simultaneously, action localization is implemented in the same network interactive with flow computation end-to-end. To faster the speed, we exploit a neural network based feature fusion method in a pyramid hierarchical manner. It fuses spatial and temporal features at different granularities via combination function (Le. concatenation) and point-wise convolution to obtain multiscale spatio-temporal action features. Experimental results on three publicly available datasets, e.g. UCF101-24, JHMDB and AVA show that with both RGB appearance and optical flow cues, the proposed method gets the state-of-the-art performance in both efficiency and accuracy. Noticeably, it gets a significant improvement on efficiency. Compared to the currently most efficient method, it is 1.9 times faster in the running speed and 1.3% video-mAP more accurate on the UCF101-24. Our proposed method reaches real-time computation for the first time (up to 38 FPS). (C) 2020 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2020 JUL
PY  - 2020
VL  - 103
C7  - 107312
DO  - 10.1016/j.patcog.2020.107312
AN  - WOS:000530845000043
AD  - China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China
AD  - Sichuan Agr Univ, Coll Informat & Engn, Yaan 625014, Peoples R China
AD  - Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China
AD  - Shenzhen Infinova Ltd Co, Infinova Bldg,Guanlan Hitech Ind Pk, Shenzhen 518100, Peoples R China
AD  - Univ Sanya, Sch Informat & Intelligence Engn, Sanya 572022, Peoples R China
M2  - Shenzhen Infinova Ltd Co
Y2  - 2020-05-19
ER  -

TY  - CPAPER
AU  - Zhang, Yong
AU  - Yu, Chunan
AU  - Fu, Chenglong
AU  - Hu, Yuanqi
AU  - Zang, Ying
A1  - IEEE
TI  - SPATIO-TEMPORAL ACTION DETECTION WITH A MOTION SENSE AND SEMANTIC CORRECTION FRAMEWORK
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING, ICASSP 2024
M3  - Proceedings Paper
CP  - 49th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
CL  - Seoul, SOUTH KOREA
AB  - Accurately distinguishing between action-related features and non-action-related features is crucial in spatio-temporal action detection tasks. Additionally, the calibration and fusion of information across different modalities remain challenging. This paper proposes a novel Motion Sense and Semantic Correction framework (MS-SC) to address these issues. The MS-SC framework achieves accurate detection by fusing features from images (spatial dimension) and videos (spatio-temporal dimension). A Motion Sense Module (MSM) is proposed to significantly increase the feature distance between action and non-action features in the semantic space, enhancing feature discriminability. Considering the complementary nature of information across different modalities, an efficient Semantic Correction Fusion Module (SFM) is introduced to facilitate interaction between features of distinct modalities and maximize their complementary information integration. To evaluate the performance of the MS-SC framework, extensive experiments were conducted on two challenging datasets, UCF101-24 and AVA. The results demonstrate the effectiveness of the MS-SC framework in handling spatio-temporal action detection tasks.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 979-8-3503-4486-8
SN  - 979-8-3503-4485-1
DA  - 2024 
PY  - 2024
SP  - 3645
EP  - 3649
DO  - 10.1109/ICASSP48485.2024.10447413
AN  - WOS:001285850003180
AD  - Huzhou Univ, Sch Informat Engn, Huzhou, Peoples R China
AD  - Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian, Peoples R China
Y2  - 2025-03-02
ER  -

TY  - CPAPER
AU  - Cheng, Yi
AU  - Sun, Ying
AU  - Lin, Dongyun
AU  - Lim, Joo-Hwee
A1  - IEEE
TI  - ACTION RELATIONAL GRAPH FOR WEAKLY-SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - IEEE International Conference on Image Processing (ICIP)
CL  - ELECTR NETWORK
AB  - The task of weakly-supervised temporal action localization (WTAL) is to recognize plentiful unstructured actions in untrimmed videos with only video-level class labels. As various actions may occur in an untrimmed video, it is desirable to capture the correlation among different actions to effectively identify the target actions. In this paper, we propose a novel Action Relational Graph Network (ARG-Net) to model the correlation between action labels. Specifically, we build a co-occurrence graph using Graph Convolutional Network (GCN), where the graph nodes and edges are represented by word embedding of action labels and relations between two labels, respectively. Then we apply the GCNs to project the action label embeddings into a set of correlated action classifiers which are multiplied with the learned video representations for video-level classification. To facilitate discriminative video representation learning, we employ the attention mechanism to model the probability of a frame containing action instances. A new Action Normalization Loss (ANL) is proposed to further alleviate the confusion from irrelevant background frames (i.e., frames containing no actions). Experimental results on THUMOS14 and ActivityNet1.2 datasets demonstrate that our ARG-Net outperforms the state-of-the-art methods.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-6654-4115-5
DA  - 2021 
PY  - 2021
SP  - 2563
EP  - 2567
DO  - 10.1109/ICIP42928.2021.9506622
AN  - WOS:000819455102137
AD  - ASTAR, Inst Infocomm Res, Singapore, Singapore
AD  - Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore
Y2  - 2022-08-11
ER  -

TY  - JOUR
AU  - Wang, Yu
AU  - Zhao, Shengjie
AU  - Chen, Shiwei
TI  - SQL-Net: Semantic Query Learning for Point-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Point-supervised Temporal Action Localization (PS-TAL) detects temporal intervals of actions in untrimmed videos with a label-efficient paradigm. However, most existing methods fail to learn action completeness without instance-level annotations, resulting in fragmentary region predictions. In fact, the semantic information of snippets is crucial for detecting complete actions, meaning that snippets with similar representations should be considered as the same action category. To address this issue, we propose a novel representation refinement framework with a semantic query mechanism to enhance the discriminability of snippet-level features. Concretely, we set a group of learnable queries, each representing a specific action category, and dynamically update them based on the video context. With the assistance of these queries, we expect to search for the optimal action sequence that agrees with their semantics. Besides, we leverage some reliable proposals as pseudo labels and design a refinement and completeness module to refine temporal boundaries further, so that the completeness of action instances is captured. Finally, we demonstrate the superiority of the proposed method over existing state-of-the-art approaches on THUMOS14 and ActivityNet13 benchmarks. Notably, thanks to completeness learning, our algorithm achieves significant improvements under more stringent evaluation metrics.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2025 
PY  - 2025
VL  - 27
SP  - 84
EP  - 94
DO  - 10.1109/TMM.2024.3521799
AN  - WOS:001398670600015
AD  - Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China
AD  - Minist Educ, Engn Res Ctr, Key Software Technol Smart City Percept & Planning, Shanghai 201804, Peoples R China
AD  - Minist Educ, Serv Comp, Key Lab Embedded Syst, Shanghai 201804, Peoples R China
AD  - Microsoft Asia Pacific Technol Co Ltd, Dept R&D Data, Shanghai 200241, Peoples R China
M2  - Microsoft Asia Pacific Technol Co Ltd
Y2  - 2025-01-27
ER  -

TY  - JOUR
AU  - Xu, Zhe
AU  - Wei, Kun
AU  - Yang, Erkun
AU  - Deng, Cheng
AU  - Liu, Wei
TI  - Bilateral Relation Distillation for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly supervised temporal action localization (WSTAL), which aims to locate the time interval of actions in an untrimmed video with only video-level action labels, has attracted increasing research interest in the past few years. However, a model trained with such labels will tend to focus on segments that contributions most to the video-level classification, leading to inaccurate and incomplete localization results. In this paper, we tackle the problem from a novel perspective of relation modeling and propose a method dubbed Bilateral Relation Distillation (BRD). The core of our method involves learning representations by jointly modeling the relation at the category and sequence levels. Specifically, category-wise latent segment representations are first obtained by different embedding networks, one for each category. We then distill knowledge obtained from a pre-trained language model to capture the category-level relations, which is achieved by performing correlation alignment and category-aware contrast in an intra- and inter-video manner. To model the relations among segments at the sequence-level, we elaborate a gradient-based feature augmentation method and encourage the learned latent representation of the augmented feature to be consistent with that of the original one. Extensive experiments illustrate that our approach achieves state-of-the-art results on THUMOS14 and ActivityNet1.3 datasets.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 OCT
PY  - 2023
VL  - 45
IS  - 10
SP  - 11458
EP  - 11471
DO  - 10.1109/TPAMI.2023.3284853
AN  - WOS:001068816800004
AD  - Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China
AD  - Tencent AI Lab, Shenzhen 518057, Peoples R China
Y2  - 2023-10-18
ER  -

TY  - JOUR
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Gao, Xinbo
AU  - Li, Jie
AU  - Wang, Xiaoyu
AU  - Liu, Tongliang
TI  - KFC: An Efficient Framework for Semi-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - In temporal action localization (TAL), semi-supervised learning is a promising technique to mitigate the cost of precise boundary annotations. Semi-supervised approaches employing consistency regularization (CR), encouraging models to be robust to the perturbed inputs, have achieved great success in image classification problems. The success of CR is largely depended on the perturbations, where instances are perturbed to train a robust model without altering their semantic information. However, the perturbations for image or video classification tasks are not fit to apply to TAL. Since videos in TAL are too long to train the model with raw videos in an end-to-end manner. In this paper, we devise a method named K-farthest crossover to construct perturbations based on video features and apply it to TAL. Motivated by the observation that features in the same action instance become more and more similar during the training process while those in different action instances or backgrounds become more and more divergent, we add perturbations to each feature along temporal axis and adopt CR to encourage the model to retain this observation. Specifically, for a feature, we first find the top-k dissimilar features and average them to form a perturbation. Then, similar to chromosomal crossover, we select a large part of the feature and a small part of the perturbation to recombine a perturbed feature, which preserves the feature semantics yet enough discrepancy.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 6869
EP  - 6878
DO  - 10.1109/TIP.2021.3099407
AN  - WOS:000681133600004
AD  - Xidian Univ, State Key Lab Integrated Serv Networks, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
AD  - Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China
AD  - Univ Sydney, Fac Engn, Sch Comp Sci, Trustworthy Machine Learning Lab, Sydney, NSW 2006, Australia
Y2  - 2021-08-11
ER  -

TY  - JOUR
AU  - Zhao, Yibo
AU  - Zhang, Hua
AU  - Gao, Zan
AU  - Guan, Weili
AU  - Wang, Meng
AU  - Chen, Shengyong
TI  - A Snippets Relation and Hard-Snippets Mask Network for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly-supervised temporal action localization (WTAL) is a problem learning an action localization model with only video-level labels available. In recent years, many WTAL methods have developed. However, hard-to-predict snippets near action boundaries are often not considered in these existing approaches, causing action incompleteness and action over-complete issues. To solve these issues, in this work, an end-to-end snippets relation and hard-snippets mask network (SRHN) is proposed. Specifically, a hard-snippets mask module is applied to mask the hard-to-predict snippets adaptively, and in this way, the trained model focuses more on those snippets with low uncertainty. Then, a snippets relation module is designed to capture the relationship among snippets and can make hard-to-predict snippets easy to predict by aggregating the information of multiple temporal receptive fields. Finally, a snippet enhancement loss is further developed to reduce the action probabilities that are not present in videos for hard-to-predict snippets and other snippets, enlarging the action probabilities that exist in videos. Extensive experiments on THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets demonstrate the effectiveness of the SRHN method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 AUG
PY  - 2024
VL  - 34
IS  - 8
SP  - 7202
EP  - 7215
DO  - 10.1109/TCSVT.2024.3374870
AN  - WOS:001327614800008
AD  - Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China
AD  - Qilu Univ Technol, Shandong Acad Sci, Shandong Artificial Intelligence Inst, Jinan 250014, Peoples R China
AD  - Monash Univ, Fac Informat Technol, Clayton Campus, Clayton, Vic 3800, Australia
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China
Y2  - 2024-11-15
ER  -

TY  - CPAPER
AU  - Lee, Jun-Tae
AU  - Yun, Sungrack
AU  - Jain, Mihir
A1  - IEEE Comp Soc
TI  - Leaky Gated Cross-Attention for Weakly Supervised Multi-Modal Temporal Action Localization
T2  - 2022 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV 2022)
M3  - Proceedings Paper
CP  - 22nd IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - As multiple modalities sometimes have a weak complementary relationship, multi-modal fusion is not always beneficial for weakly supervised action localization. Hence, to attain the adaptive multi-modal fusion, we propose a leaky gated cross-attention mechanism. In our work, we take the multi-stage cross-attention as the baseline fusion module to obtain multi-modal features. Then, for the stages of each modality, we design gates to decide the dependency on the other modality. For each input frame, if two modalities have a strong complementary relationship, the gate selects the cross-attended feature, otherwise the non-attended feature. Also, the proposed gate allows the non-selected feature to escape through it with a small intensity, we call it leaky gate. This leaky feature makes effective regularization of the selected major feature. Therefore, our leaky gating makes cross-attention more adaptable and robust even when the modalities have a weak complementary relationship. The proposed leaky gated cross-attention provides a modality fusion module that is generally compatible with various temporal action localization methods. To show its effectiveness, we do extensive experimental analysis and apply the proposed method to boost the performance of the state-of-the-art methods on two benchmark datasets (ActivityNet1.2 and THUMOS14).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-6654-0915-5
DA  - 2022 
PY  - 2022
SP  - 817
EP  - 826
DO  - 10.1109/WACV51458.2022.00089
AN  - WOS:000800471200082
AD  - Qualcomm AI Res, San Diego, CA 92121 USA
M2  - Qualcomm AI Res
Y2  - 2022-07-06
ER  -

TY  - JOUR
AU  - Sun, Jianing
AU  - Wu, Xuan
AU  - Xiao, Yubin
AU  - Wu, Chunguo
AU  - Liang, Yanchun
AU  - Liang, Yi
AU  - Wang, Liupu
AU  - Zhou, You
TI  - DANet: Temporal Action Localization with Double Attention
T2  - APPLIED SCIENCES-BASEL
M3  - Article
AB  - Temporal action localization (TAL) aims to predict action instance categories in videos and identify their start and end times. However, existing Transformer-based backbones focus only on global or local features, resulting in the loss of information. In addition, both global and local self-attention mechanisms tend to average embeddings, thereby reducing the preservation of critical features. To solve these two problems better, we propose two kinds of attention mechanisms, namely multi-headed local self-attention (MLSA) and max-average pooling attention (MA) to extract simultaneously local and global features. In MA, max-pooling is used to select the most critical information from local clip embeddings instead of averaging embeddings, and average-pooling is used to aggregate global features. We use MLSA for modeling local temporal context. In addition, to enhance collaboration between MA and MLSA, we propose the double attention block (DABlock), comprising MA and MLSA. Finally, we propose the final network double attention network (DANet), composed of DABlocks and other advanced blocks. To evaluate DANet's performance, we conduct extensive experiments for the TAL task. Experimental results demonstrate that DANet outperforms the other state-of-the-art models on all datasets. Finally, ablation studies demonstrate the effectiveness of the proposed MLSA and MA. Compared with structures using backbone with convolution and global Transformer, DABlock consisting of MLSA and MA has a superior performance, achieving an 8% and 0.5% improvement on overall average mAP, respectively.
PU  - MDPI
PI  - BASEL
PA  - MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
DA  - 2023 JUN
PY  - 2023
VL  - 13
IS  - 12
C7  - 7176
DO  - 10.3390/app13127176
AN  - WOS:001016926600001
AD  - Jilin Univ, Coll Comp Sci & Technol, Lab Symbol Computat & Knowledge Engn, Minist Educ, Changchun 130012, Peoples R China
AD  - Zhuhai Coll Sci & Technol, Sch Comp Sci, Zhuhai 519041, Peoples R China
AD  - Jilin Univ, Coll Business & Adm, Changchun 130012, Peoples R China
M2  - Zhuhai Coll Sci & Technol
Y2  - 2023-07-07
ER  -

TY  - JOUR
AU  - Wang, Bin
AU  - Song, Yan
AU  - Wang, Fanming
AU  - Zhao, Yang
AU  - Shu, Xiangbo
AU  - Rui, Yan
TI  - Dilation-erosion for single-frame supervised temporal action localization
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
M3  - Early Access
AB  - To balance the annotation labor and the granularity of supervision, single-frame annotation has been introduced in temporal action localization. It provides a rough temporal location for an action but implicitly overstates the supervision from the annotated-frame during training, leading to the confusion between actions and backgrounds, i.e., action incompleteness and background false positives. To tackle the two challenges, in this work, we present the Snippet Classification model and the Dilation-Erosion module. In the Dilation-Erosion module, we expand the potential action segments with a loose criterion to alleviate the problem of action incompleteness and then remove the background from the potential action segments to alleviate the problem of action incompleteness. Relying on the single-frame annotation and the output of the snippet classification, the Dilation-Erosion module mines pseudo snippet-level ground-truth, hard backgrounds and evident backgrounds, which in turn further trains the Snippet Classification model. It forms a cyclic dependency. Furthermore, we propose a new embedding loss to aggregate the features of action instances with the same label and separate the features of actions from backgrounds. Experiments on THUMOS14 and ActivityNet 1.2 validate the effectiveness of the proposed method. Code has been made publicly available (https://github.com/LingJun123/single-frame-TAL).
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2023 MAY 16
PY  - 2023
DO  - 10.1007/s11042-023-15196-1
AN  - WOS:000989408100008
C6  - MAY 2023
AD  - Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China
AD  - Nanjing Xidao Culture Commun Ltd, Nanjing 210094, Jiangsu, Peoples R China
M2  - Nanjing Xidao Culture Commun Ltd
Y2  - 2023-06-28
ER  -

TY  - JOUR
AU  - Zhao, Tao
AU  - Han, Junwei
AU  - Yang, Le
AU  - Wang, Binglu
AU  - Zhang, Dingwen
TI  - SODA: Weakly Supervised Temporal Action Localization Based on Astute Background Response and Self-Distillation Learning
T2  - INTERNATIONAL JOURNAL OF COMPUTER VISION
M3  - Article
AB  - Weakly supervised temporal action localization is a practical yet challenging task. Although great efforts have been made in recent years, the existing methods still have limited capacity in dealing with the challenges of over-localization, joint-localization, and under-localization. Based on our investigation, the first two challenges arise from insufficient ability to suppress background response, while the third challenge is due to the lack of discovering action frames. To better address these challenges, we first propose the astute background response strategy. By enforcing the classification target of the background category to be zero, such a strategy can endow the conductive effect between video-level classification and frame-level classification, thus guiding the action category to suppress responses at background frames astutely and helping address the over-localization and joint-localization challenges. For alleviating the under-localization challenge, we introduce the self-distillation learning strategy. It simultaneously learns one master network and multiple auxiliary networks, where the auxiliary networks enhance the master network to discover complete action frames. Experimental results on three benchmarks demonstrate the favorable performance of the proposed method against previous counterparts, and its efficacy to tackle the existing three challenges.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-5691
SN  - 1573-1405
DA  - 2021 AUG
PY  - 2021
VL  - 129
IS  - 8
SP  - 2474
EP  - 2498
DO  - 10.1007/s11263-021-01473-9
AN  - WOS:000656515600001
C6  - MAY 2021
AD  - Northwestern Polytech Univ, Sch Automat, Xian, Peoples R China
Y2  - 2021-05-31
ER  -

TY  - CPAPER
AU  - Liao, Yiguan
AU  - Qiu, Changzhen
AU  - Zhang, Zhiyong
AU  - Wang, Luping
AU  - Wang, Liang
A1  - ACM
TI  - GCRNet: Global Context Relation Network for Weakly-Supervised Temporal Action Localization Identify the target actions in a long untrimmed video and find the corresponding action start point and end point
T2  - 2021 THE 5TH INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING, ICVIP 2021
M3  - Proceedings Paper
CP  - 5th International Conference on Video and Image Processing (ICVIP)
CL  - ELECTR NETWORK
AB  - Weakly-Supervised Temporal Action Localization is a very challenging task of classifying and locating all actions in an untrimmed video because the frame-wise label is not given during the training stage while the only label is action class. Due to the complexity of video structure, previous methods do not take the advantage of the context information between long-term action-related frames. In this paper, we propose a Global Context Relation Network which introduces the self-attention mechanism. The first part uses the context relation module to encode features according to the relationship of the global context and merge them into the original features, which allows the network to better capture the long-term dependence on the video. Then the local feature encoding is performed by convolution to obtain a more accurate class activation sequence. Extensive experiments on two benchmark datasets THUMOS14 and ActivityNet1.3 and demonstrate our method outperforms existing state-of-the-art results on THUMOS14 and achieves very comparable performance on ActivityNet1.3.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8589-3
DA  - 2021 
PY  - 2021
SP  - 184
EP  - 190
DO  - 10.1145/3511176.3511204
AN  - WOS:001086016700028
AD  - Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Chen, Tianquan
AU  - Lie, Bairong
AU  - Tao, Yusheng
AU  - Wang, Yuqing
AU  - Zhu, Yuesheng
ED  - Tanveer, M
ED  - Agarwal, S
ED  - Ozawa, S
ED  - Ekbal, A
ED  - Jatowt, A
TI  - Class-Incremental Learning with Multiscale Distillation for Weakly Supervised Temporal Action Localization
T2  - NEURAL INFORMATION PROCESSING, PT I, ICONIP 2022
M3  - Proceedings Paper
CP  - 29th International Conference on Neural Information Processing
CL  - Indore, INDIA
AB  - Despite recent works having made great progress in weakly supervised temporal action localization (WTAL), they still suffer from catastrophic forgetting. When only new-class videos can be utilized to update these models, their performance in old classes diminishes drastically. Even while some class-incremental learning methods are presented to assist models in continuously learning new-class knowledge, most of them focus on image classification but pay little attention to WTAL. To fill this gap, we propose a novel class-incremental learning method with multiscale distillation, which mines two separate scales of old-class information in incoming videos for updating the model. Precisely, we calculate class activation sequences (CAS) with frame-level spatio-temporal information to provide fine-grained old-class labels for the updated model. Moreover, since the high activation segments contain rich action information, we select them and construct video-level logits to constrain the updated model for maintaining the old-class knowledge further. The experimental results under various incremental learning settings on the THUMOS'14 and ActivityNet 1.3 datasets reveal that our method effectively alleviates the catastrophic forgetting problem in WTAL.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-30104-9
SN  - 978-3-031-30105-6
DA  - 2023 
PY  - 2023
VL  - 13623
SP  - 367
EP  - 378
DO  - 10.1007/978-3-031-30105-6_31
AN  - WOS:001417117600031
AD  - Peking Univ, Shenzhen Grad Sch, Beijing, Peoples R China
Y2  - 2023-01-01
ER  -

TY  - JOUR
AU  - Gao, Zan
AU  - Cui, Xinglei
AU  - Zhuo, Tao
AU  - Cheng, Zhiyong
AU  - Liu, An-An
AU  - Wang, Meng
AU  - Chen, Shenyong
TI  - A Multitemporal Scale and Spatial-Temporal Transformer Network for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS
M3  - Article
AB  - Temporal action localization plays an important role in video analysis, which aims to localize and classify actions in untrimmed videos. Previous methods often predict actions on a feature space of a single temporal scale. However, the temporal features of a low-level scale lack sufficient semantics for action classification, while a high-level scale cannot provide the rich details of the action boundaries. In addition, the long-range dependencies of video frames are often ignored. To address these issues, a novel multitemporal-scale spatial-temporal transformer (MSST) network is proposed for temporal action localization, which predicts actions on a feature space of multiple temporal scales. Specifically, we first use refined feature pyramids of different scales to pass semantics from high-level scales to low-level scales. Second, to establish the long temporal scale of the entire video, we use a spatial-temporal transformer encoder to capture the long-range dependencies of video frames. Then, the refined features with long-range dependencies are fed into a classifier for coarse action prediction. Finally, to further improve the prediction accuracy, we propose a frame-level self-attention module to refine the classification and boundaries of each action instance. Most importantly, these three modules are jointly explored in a unified framework, and MSST has an anchor-free and end-to-end architecture. Extensive experiments show that the proposed method can outperform state-of-the-art approaches on the THUMOS14 dataset and achieve comparable performance on the ActivityNet1.3 dataset. Compared with A2Net (TIP20, Avg{0.3:0.7}), Sub-Action (CSVT2022, Avg{0.1:0.5}), and AFSD (CVPR21, Avg{0.3:0.7}) on the THUMOS14 dataset, the proposed method can achieve improvements of 12.6%, 17.4%, and 2.2%, respectively.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2168-2291
SN  - 2168-2305
DA  - 2023 JUN
PY  - 2023
VL  - 53
IS  - 3
SP  - 569
EP  - 580
DO  - 10.1109/THMS.2023.3266037
AN  - WOS:000988540500001
C6  - MAY 2023
AD  - Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China
AD  - Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China
Y2  - 2023-05-31
ER  -

TY  - JOUR
AU  - Wang, Qingyun
AU  - Song, Yan
AU  - Zou, Rong
AU  - Shu, Xiangbo
TI  - Progressive enhancement network with pseudo labels for weakly supervised temporal action localization
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - Weakly supervised temporal action localization (WSTAL) is crucial for real world applications, as it relieves the huge burden of frame-level annotations for fully supervised action detection. Most existing WSTAL methods focused on classifying video snippets, or detecting action boundaries. However, the predictions from these well-designed models have not been fully utilized. Accordingly, we propose a weakly-supervised framework called the progressive enhancement network (PEN), which takes full advantages of the predictions generated by the preceding models to enhance the subsequent models. Specifically, snippet-level pseudo labels are generated from the preceding predictions by considering the similarity and temporal distance between action snippets. Then subsequent models are progressively enhanced by using pseudo labels as a supervision, and utilizing their underlying semantics to make the feature representation more qualified for the temporal localization task. Extensive experiments which are carried out on two popular benchmarks, THUMOS'14 and ActivityNet v1.2, demonstrate the effectiveness of our method.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2022 AUG
PY  - 2022
VL  - 87
C7  - 103590
DO  - 10.1016/j.jvcir.2022.103590
AN  - WOS:000858655700003
C6  - AUG 2022
AD  - Nanjing Univ Sci & Technol, 200 Xiaoling Wei St, Nanjing 210094, Peoples R China
Y2  - 2022-10-08
ER  -

TY  - JOUR
AU  - Kim, Ho-Joong
AU  - Lee, Seong-Whan
TI  - Ensuring spatial scalability with temporal-wise spatial attentive pooling for temporal action detection
T2  - NEURAL NETWORKS
M3  - Article
AB  - Recent temporal action detection models have focused on end-to-end trainable approaches to utilize the representational power of backbone networks. Despite the advantages of end-to-end trainable methods, these models still employ a small spatial resolution (e.g., 96 x 96) due to the inefficient trade-off between computational cost and spatial resolution. In this study, we argue that a simple pooling method (e.g., adaptive average pooling) acts as a bottleneck at the spatial aggregation part, restricting representational power. To address this issue, we propose a temporal-wise spatial attentive pooling (TSAP), which alleviates the bottleneck between the backbone and the detection head using a temporal-wise attention mechanism. Our approach mitigates the inefficient trade-off between spatial resolution and computational cost, thereby enhancing spatial scalability in temporal action detection. Moreover, TSAP is adaptable to previous end-to-end approaches by simply replacing the spatial pooling part. Our experiments demonstrated the essential role of spatial aggregation, and consistent improvements are observed by incorporating TSAP into previous end-to-end methods.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0893-6080
SN  - 1879-2782
DA  - 2024 AUG
PY  - 2024
VL  - 176
C7  - 106321
DO  - 10.1016/j.neunet.2024.106321
AN  - WOS:001349048100001
C6  - APR 2024
AD  - Korea Univ, Dept Artificial Intelligence, Seoul 02841, South Korea
Y2  - 2024-11-12
ER  -

TY  - CPAPER
AU  - Kang, Hyolim
AU  - Kim, Kyungmin
AU  - Ko, Yumin
AU  - Kim, Seon Joo
A1  - IEEE
TI  - CAG-QIL: Context-Aware Actionness Grouping via Q Imitation Learning for Online Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Temporal action localization has been one of the most popular tasks in video understanding, due to the importance of detecting action instances in videos. However, not much progress has been made on extending it to work in an online fashion, although many video related tasks can benefit by going online with the growing video streaming services. To this end, we introduce a new task called Online Temporal Action Localization (On-TAL), in which the goal is to immediately detect action instances from an untrimmed streaming video. The online setting makes the new task very challenging as the actionness decision for every frame has to be made without access to future frames and also because post-processing methods cannot be used to modify past action proposals. We propose a novel framework, Context-Aware Actionness Grouping (CAG) as a solution for On-TAL and train it with the imitation learning algorithm, which allows us to avoid sophisticated reward engineering. Evaluation of our work on THUMOS14 and Activitynet1.3 shows significant improvement over non-naive baselines, demonstrating the effectiveness of our approach. As a by-product, our method can also be used for the Online Detection of Action Start (ODAS), in which our method also outperforms previous state-of-the-art models.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13709
EP  - 13718
DO  - 10.1109/ICCV48922.2021.01347
AN  - WOS:000798743203088
AD  - Yonsei Univ, Seoul, South Korea
Y2  - 2022-06-24
ER  -

TY  - CPAPER
AU  - Huang, Yupan
AU  - Dai, Qi
AU  - Lu, Yutong
A1  - IEEE
TI  - DECOUPLING LOCALIZATION AND CLASSIFICATION IN SINGLE SHOT TEMPORAL ACTION DETECTION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Shanghai, PEOPLES R CHINA
AB  - Video temporal action detection aims to temporally localize and recognize the action in untrimmed videos. Existing one-stage approaches mostly focus on unifying two subtasks, i.e., localization of action proposals and classification of each proposal through a fully shared backbone. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. In this paper, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling the localization and classification in a one-stage scheme. Particularly, two separate branches are designed in parallel to enable each component to own representations privately for accurate localization or classification. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the feature representations. We conduct extensive experiments on THUMOS14 dataset and demonstrate superior performance over state-of-the-art methods. Our code is available online(1).
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-5386-9552-4
DA  - 2019 
PY  - 2019
SP  - 1288
EP  - 1293
DO  - 10.1109/ICME.2019.00224
AN  - WOS:000501820600216
AD  - Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China
AD  - Microsoft Res Asia, Beijing, Peoples R China
Y2  - 2019-12-27
ER  -

TY  - CPAPER
AU  - Zhang, Tao
AU  - Liu, Shan
AU  - Li, Thomas
AU  - Li, Ge
A1  - IEEE
TI  - BOUNDARY INFORMATION MATTERS MORE: ACCURATE TEMPORAL ACTION DETECTION WITH TEMPORAL BOUNDARY NETWORK
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)
M3  - Proceedings Paper
CP  - 44th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - Brighton, ENGLAND
AB  - Temporal action detection in untrimmed videos is an important yet challenging task. How to locate complex actions accurately is still an open question due to the ambiguous boundaries between action instances and the background. Recently a newly proposed work exploits Structured Segment Networks (SSN) for temporal action detection, which models temporal structure of action instances via structured temporal pyramids, and comprises two classifiers, respectively for classifying actions and determining proposal completeness. In this paper we attempt to delve the temporal boundary information when modeling temporal structure of action instance, by introducing to SSN the structured temporal boundary attention pyramid. On top of the pyramid, we add another set of classifiers for unit-wise completeness evaluation, which enables proposal recycling for efficient action detection. Experimental results on two challenging benchmarks, THUMOS' 14 and ActivityNet, indicate that our Temporal Boundary Network shows a significant performance improvement compared with SSN, and achieves a competitive performance compared with state-of-the-arts.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-4799-8131-1
DA  - 2019 
PY  - 2019
SP  - 1642
EP  - 1646
DO  - 10.1109/icassp.2019.8682261
AN  - WOS:000482554001174
AD  - Peking Univ, Sch Elect & Comp Engn, Shenzhen, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Tencent, Media Lab, Shenzhen, Peoples R China
AD  - Peking Univ, Adv Inst Informat Technol, Hangzhou, Zhejiang, Peoples R China
Y2  - 2019-09-30
ER  -

TY  - JOUR
AU  - Xie, Ting-Ting
AU  - Tzelepis, Christos
AU  - Patras, Ioannis
TI  - Boundary Uncertainty in a Single-Stage Temporal Action Localization Network
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we address the problem of temporal action localization with a single stage neural network. In the proposed architecture we model the boundary predictions as uni-variate Gaussian distributions in order to model their uncertainties, which is the first in this area to the best of our knowledge. We use two uncertainty-aware boundary regression losses: first, the Kullback-Leibler divergence between the ground truth location of the boundary and the Gaussian modeling the prediction of the boundary and second, the expectation of the $\ell_1$ loss under the same Gaussian. We show that with both uncertainty modeling approaches improve the detection performance by more than $1.5\%$ in mAP@tIoU=0.5 and that the proposed simple one-stage network performs closely to more complex one and two stage networks.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.11170
AN  - PPRN:13130961
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Huang, Linjiang
AU  - Huang, Yan
AU  - Ouyang, Wanli
AU  - Wang, Liang
A1  - Assoc Advancement Artificial Intelligence
TI  - Relational Prototypical Network for Weakly Supervised Temporal Action Localization
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
CL  - New York, NY
AB  - In this paper, we propose a weakly supervised temporal action localization method on untrimmed videos based on prototypical networks. We observe two challenges posed by weakly supervision, namely action-background separation and action relation construction. Unlike the previous method, we propose to achieve action-background separation only by the original videos. To achieve this, a clustering loss is adopted to separate actions from backgrounds and learn intra-compact features, which helps in detecting complete action instances. Besides, a similarity weighting module is devised to further separate actions from backgrounds. To effectively identify actions, we propose to construct relations among actions for prototype learning. A GCN-based prototype embedding module is introduced to generate relational prototypes. Experiments on THUMOS14 and ActivityNet1.2 datasets show that our method outperforms the state-of-the-art methods.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
DA  - 2020 
PY  - 2020
VL  - 34
SP  - 11053
EP  - 11060
AN  - WOS:000668126803062
AD  - Natl Lab Pattern Recognit NLPR, Ctr Res Intelligent Percept & Comp CRIPAC, Sydney, NSW, Australia
AD  - Chinese Acad Sci CASIA, Ctr Excellence Brain Sci & Intelligence Technol C, Inst Automat, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci UCAS, Beijing, Peoples R China
AD  - Univ Sydney, Sydney, NSW, Australia
M2  - Natl Lab Pattern Recognit NLPR
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - He, Yilong
AU  - Han, Xiao
AU  - Zhong, Yong
AU  - Wang, Lishun
TI  - Non-Local Temporal Difference Network for Temporal Action Detection
T2  - SENSORS
M3  - Article
AB  - As an important part of video understanding, temporal action detection (TAD) has wide application scenarios. It aims to simultaneously predict the boundary position and class label of every action instance in an untrimmed video. Most of the existing temporal action detection methods adopt a stacked convolutional block strategy to model long temporal structures. However, most of the information between adjacent frames is redundant, and distant information is weakened after multiple convolution operations. In addition, the durations of action instances vary widely, making it difficult for single-scale modeling to fit complex video structures. To address this issue, we propose a non-local temporal difference network (NTD), including a chunk convolution (CC) module, a multiple temporal coordination (MTC) module, and a temporal difference (TD) module. The TD module adaptively enhances the motion information and boundary features with temporal attention weights. The CC module evenly divides the input sequence into N chunks, using multiple independent convolution blocks to simultaneously extract features from neighboring chunks. Therefore, it realizes the information delivered from distant frames while avoiding trapping into the local convolution. The MTC module designs a cascade residual architecture, which realizes the multiscale temporal feature aggregation without introducing additional parameters. The NTD achieves a state-of-the-art performance on two large-scale datasets, 36.2% mAP@avg and 71.6% mAP@0.5 on ActivityNet-v1.3 and THUMOS-14, respectively.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
DA  - 2022 NOV
PY  - 2022
VL  - 22
IS  - 21
C7  - 8396
DO  - 10.3390/s22218396
AN  - WOS:000883636200001
AD  - Chinese Acad Sci, Chengdu Inst Comp Applicat, Chengdu 610081, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China
Y2  - 2022-11-25
ER  -

TY  - CPAPER
AU  - Ren, Yifan
AU  - Xu, Xing
AU  - Shen, Fumin
AU  - Wang, Zheng
AU  - Yang, Yang
AU  - Shen, Heng Tao
A1  - ACM
TI  - Multi-scale Dynamic Network for Temporal Action Detection
T2  - PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21)
M3  - Proceedings Paper
CP  - 11th International Conference on Multimedia Retrieval (ICMR)
CL  - ELECTR NETWORK
AB  - In recent years, as the fundamental task in video understanding, Temporal Action Detection is attracting extensive attention. Most existing approaches use the same model parameters to process all input videos, which are not adaptive to the input video during the inference stage. In this paper, we propose a novel model termed Multi-scale Dynamic Network (MDN) to tackle this problem. The proposed MDN model incorporates multiple Multi-scale Dynamic Modules (MDMs). Each MDM can generate video-specific and segment-specific convolution kernels based on video content from different scales and adaptively capture rich semantic information for the prediction. Besides, we also design a new Edge Suppression Loss (ESL) function for MDN to pay more attention to hard examples. Extensive experiments conducted on two popular benchmarks ActivityNet-1.3 and THUMOS-14 show that the proposed MDN model achieves the state-of-the-art performance.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8463-6
DA  - 2021 
PY  - 2021
SP  - 267
EP  - 275
DO  - 10.1145/3460426.3463613
AN  - WOS:000723651900030
AD  - Univ Elect Sci & Technol China, Ctr Future Media, Chengdu, Peoples R China
AD  - Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China
Y2  - 2021-12-15
ER  -

TY  - JOUR
AU  - Liberatori, Benedetta
AU  - Conti, Alessandro
AU  - Rota, Paolo
AU  - Wang, Yiming
AU  - Ricci, Elisa
TI  - Test-Time Zero-Shot Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.05426
AN  - PPRN:88466877
AD  - Univ Trento, Trento, Italy
AD  - Fdn Bruno Kessler, Trento, Italy
M2  - Univ Trento
Y2  - 2024-04-24
ER  -

TY  - CPAPER
AU  - Chen, Keke
AU  - Tu, Zhewei
AU  - Shu, Xiangbo
A1  - IEEE
TI  - Leveraging Multimodal Knowledge for Spatio-temporal Action Localization 
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO WORKSHOPS, ICMEW 2024
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Niagra Falls, CANADA
AB  - Locating persons and recognizing their actions in chaotic scenes is a more challenging task in video understanding. Contrary to the quotidian conduct observed amongst humans, the actions exhibited during chaotic incidents markedly diverge in execution and the resultant influence on surrounding individuals, engendering a heightened complexity. Conventional spatio-temporal action localization methods that rely solely on a single visual modality fall short in complex scenarios. This paper explores STAL from a multimodal perspective by leveraging large language models (LLMs) and vision-language (VL) foundation models. We analyze the inherent feature aggregation phase of visual STAL and introduce a knowledge aggregation approach tailored for VL foundation models, termed Multimodal Foundation Knowledge Integration (MFKI). MFKI includes a generic decoder that facilitates the association of knowledge from VL foundation models with action features, as well as a specific decoder for relational reasoning in visual STAL. MFKI combines generic visual representations with specific video features to meet the demands of complex STAL tasks. Additionally, we utilize LLMs (i.e. GPT) and prompting to enrich label augmentation, fostering a more comprehensive linguistic understanding of complex actions. Experiments on the Chaotic World dataset have proven the effectiveness of the method proposed in this paper. The code is available at https://github.com/CKK-coder/Chaoticorld/tree/master. 
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2330-7927
SN  - 979-8-3503-7982-2
SN  - 979-8-3503-7981-5
DA  - 2024 
PY  - 2024
DO  - 10.1109/ICMEW63481.2024.10645431
AN  - WOS:001308272300048
AD  - Nanjing Univ Sci & Technol, Nanjing, Peoples R China
Y2  - 2024-12-05
ER  -

TY  - CPAPER
AU  - Liu, Qinying
AU  - Wang, Zilei
AU  - Chen, Ruoxi
AU  - Li, Zhilin
A1  - IEEE
TI  - Unleashing the Potential of Adjacent Snippets for Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Brisbane, AUSTRALIA
AB  - Weakly-supervised temporal action localization (WTAL) intends to detect action instances with only weak supervision, e.g., video-level labels. The current de facto pipeline locates action instances by thresholding and grouping continuous high-score regions on temporal class activation sequences. In this route, the capacity of the model to recognize the relationships between adjacent snippets is of vital importance which determines the quality of the action boundaries. However, it is error-prone since the variations between adjacent snippets are typically subtle, and unfortunately this is overlooked in the literature. To tackle the issue, we propose a novel WTAL approach named Convex Combination Consistency between Neighbors ((CBN)-B-3). (CBN)-B-3 consists of two key ingredients: a micro data augmentation strategy that increases the diversity in-between adjacent snippets by convex combination of adjacent snippets, and a macro-micro consistency regularization that enforces the model to be invariant to the transformations w.r.t. video semantics, snippet predictions, and snippet representations. Consequently, fine-grained patterns in-between adjacent snippets are enforced to be explored, thereby resulting in a more robust action boundary localization. Experimental results demonstrate the effectiveness of (CBN)-B-3 on top of various baselines for WTAL with video-level and point-level supervision. Code is at: https://github.com/canbaoburen/C3BN.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-6654-6891-6
DA  - 2023 
PY  - 2023
SP  - 1032
EP  - 1037
DO  - 10.1109/ICME55011.2023.00181
AN  - WOS:001062707300167
AD  - Univ Sci & Technol China, Hefei, Peoples R China
Y2  - 2023-11-05
ER  -

TY  - JOUR
AU  - Li, Ziqiang
AU  - Ge, Yongxin
AU  - Yu, Jiaruo
AU  - Chen, Zhongming
AU  - Chen, Zhongming
TI  - Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - With video-level labels, weakly supervised temporal action localization (WTAL) applies a localization-by-classification paradigm to detect and classify the action in untrimmed videos. Due to the characteristic of classification, class-specific background snippets are inevitably mis-activated to improve the discriminability of the classifier in WTAL. To alleviate the disturbance of background, existing methods try to enlarge the discrepancy between action and background through modeling background snippets with pseudo-snippet-level annotations, which largely rely on artificial hypotheticals. Distinct from the previous works, we present an adversarial learning strategy to break the limitation of mining pseudo background snippets. Concretely, the background classification loss forces the whole video to be regarded as the background by a background gradient reinforcement strategy, confusing the recognition model. Reversely, the foreground(action) loss guides the model to focus on action snippets under such conditions. As a result, competition between the two classification losses drives the model to boost its ability for action modeling. Simultaneously, a novel temporal enhancement network is designed to facilitate the model to construct temporal relation of affinity snippets based on the proposed strategy, for further improving the performance of action localization. Finally, extensive experiments conducted on THUMOS14 and ActivityNet1.2 demonstrate the effectiveness of the proposed method.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.06659
AN  - PPRN:10627553
AD  - Chongqing Univ, Chongqing, Peoples R China
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Li, Ding
AU  - Yang, Xuebing
AU  - Tang, Yongqiang
AU  - Zhang, Chenyang
AU  - Zhang, Wensheng
TI  - Active Learning with Effective Scoring Functions for Semi-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) aims to predict both action category and temporal boundary of action instances in untrimmed videos, i.e., start and end time. Fully-supervised solutions are usually adopted in most existing works, and proven to be effective. One of the practical bottlenecks in these solutions is the large amount of labeled training data required. To reduce expensive human label cost, this paper focuses on a rarely investigated yet practical task named semi-supervised TAL and proposes an effective active learning method, named AL-STAL. We leverage four steps for actively selecting video samples with high informativeness and training the localization model, named Train, Query, Annotate, Append. Two scoring functions that consider the uncertainty of localization model are equipped in AL-STAL, thus facilitating the video sample rank and selection. One takes entropy of predicted label distribution as measure of uncertainty, named Temporal Proposal Entropy (TPE). And the other introduces a new metric based on mutual information between adjacent action proposals and evaluates the informativeness of video samples, named Temporal Context Inconsistency (TCI). To validate the effectiveness of proposed method, we conduct extensive experiments on two benchmark datasets THUMOS&rsquo;14 and ActivityNet 1.3. Experiment results show that AL-STAL outperforms the existing competitors and achieves satisfying performance compared with fully-supervised learning.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.14856
AN  - PPRN:12844752
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Res Ctr Precis Sensing & Control, Beijing 100190, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Res Ctr Precis Sensing & Control, Beijing 100190, Peoples R China
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Kang, Tae-Kyung
AU  - Lee, Gun-Hee
AU  - Jin, Kyung-Min
AU  - Lee, Seong-Whan
A1  - IEEE
TI  - Action-aware Masking Network with Group-based Attention for Temporal Action Localization
T2  - 2023 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - 23rd IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Temporal Action Localization (TAL) is a significant and challenging task that searches for subtle human activities in an untrimmed video. To extract snippet-level video features, existing TAL methods commonly use video encoders pre-trained on short-video classification datasets. However, the snippet-level features can incur ambiguity between consecutive frames due to short and poor temporal information, disrupting the precise prediction of action instances. Several methods incorporating temporal relations have been proposed to mitigate this problem; however, they still suffer from poor video features. To address this issue, we propose a novel temporal action localization framework called an Action-aware Masking Network (AMNet). Our method simultaneously refines video features using action-aware attention and considers inherent temporal relations using self-attention and cross-attention mechanisms. First, we present an Action Masking Encoder (AME) that generates an action-aware mask to represent positive characteristics, which is then used to refine snippet-level features to be more salient around actions. Second, we design a Group Attention Module (GAM), which models relations of temporal information and exchanges mutual information by dividing the features into two groups, i.e., long and short-groups. Extensive experiments and ablation studies on two primary benchmark datasets demonstrate the effectiveness of AMNet, and our method achieves state-of-the-art performances on THUMOS-14 and ActivityNet1.3.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-6654-9346-8
DA  - 2023 
PY  - 2023
SP  - 6047
EP  - 6056
DO  - 10.1109/WACV56688.2023.00600
AN  - WOS:000971500206018
AD  - Korea Univ, Dept Artificial Intelligence, Seoul, South Korea
AD  - Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea
Y2  - 2023-07-22
ER  -

TY  - JOUR
AU  - Xia, Kun
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Hua, Gang
AU  - Tang, Wei
TI  - Dual relation network for temporal action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Temporal action localization is a challenging task for video understanding. Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization. To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities. Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods. (C) 2022 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2022 SEP
PY  - 2022
VL  - 129
C7  - 108725
DO  - 10.1016/j.patcog.2022.108725
AN  - WOS:000832702600008
C6  - APR 2022
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA
M2  - Wormpex AI Res
Y2  - 2022-08-10
ER  -

TY  - JOUR
AU  - Chéron, Guilhem
AU  - Osokin, Anton
AU  - Laptev, Ivan
AU  - Schmid, Cordelia
TI  - Modeling Spatio-Temporal Human Track Structure for Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper addresses spatio-temporal localization of human actions in video. In order to localize actions in time, we propose a recurrent localization network (RecLNet) designed to model the temporal structure of actions on the level of person tracks. Our model is trained to simultaneously recognize and localize action classes in time and is based on two layer gated recurrent units (GRU) applied separately to two streams, i.e. appearance and optical flow streams. When used together with state-of-the-art person detection and tracking, our model is shown to improve substantially spatio-temporal action localization in videos. The gain is shown to be mainly due to improved temporal localization. We evaluate our method on two recent datasets for spatio-temporal action localization, UCF101-24 and DALY, demonstrating a significant improvement of the state of the art.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1806.11008
AN  - PPRN:49384495
Y2  - 2023-03-30
ER  -

TY  - JOUR
AU  - Fish, Edward
AU  - Weinbren, Jon
AU  - Gilbert, Andrew
TI  - Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal Action Localization (TAL) aims to identify actions’ start, end, and class labels in untrimmed videos. While recent advancements using transformer net-works and Feature Pyramid Networks (FPN) have enhanced visual feature recogni-tion in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal resolutions. Central to our approach is a hierarchical gated cross-attention mechanism, which discerningly weighs the importance of audio information at diverse temporal scales. Such a technique not only refines the precision of regression boundaries but also bolsters classification confidence. Importantly, MRAV-FF is versatile, making it compatible with existing FPN TAL architectures and offering a significant enhancement in performance when audio data is available.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2310.03456
AN  - PPRN:85430728
AD  - Univ Surrey, Guildford, England
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Tong, Haoran
AU  - Cui, Xu
AU  - Qing, Laiyun
A1  - IEEE COMPUTER SOC
TI  - Single-frame Supervised Action Temporal Localization Based on Multi-view Contrastive Learning
T2  - 2024 IEEE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL, MIPR 2024
M3  - Proceedings Paper
CP  - 7th IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR)
CL  - San Jose, CA
AB  - This paper focuses on temporal action localization with single-frame supervision, aiming to determine the categories and temporal boundaries of action instances within video sequences with only a single annotated frame per action instance. This task presents considerable challenges in both action recognition and localization due to sparse annotations. To address these challenges, we propose a novel temporal action localization framework by multi-view contrastive learning. The graph enhancement module based on multi-head graph attention mechanism captures the complex relationships among frames through graph structure, and the contrastive learning module is designed to enhance the discriminative power of the features by introducing more reliable positive pairs. Finally, the segment generation module refines the temporal boundaries of the instances by eliminating the noisy mistaken classifications. Experiments conducted on multiple public datasets verify the effectiveness of the proposed method.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2770-4327
SN  - 979-8-3503-5143-9
SN  - 979-8-3503-5142-2
DA  - 2024 
PY  - 2024
SP  - 383
EP  - 389
DO  - 10.1109/MIPR62202.2024.00067
AN  - WOS:001343060900056
AD  - Univ Chinese Acad Sci, Beijing, Peoples R China
Y2  - 2025-01-09
ER  -

TY  - JOUR
AU  - Weinzaepfel, Philippe
AU  - Harchaoui, Zaid
AU  - Schmid, Cordelia
TI  - Learning to track for spatio-temporal action localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.
PU  - CORNELL UNIV
DA  - 2015 
PY  - 2015
DO  - arXiv:1506.01929
AN  - PPRN:22717181
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Jo, Hyejeong
AU  - Gwon, Huiwon
AU  - Jo, Sunhee
AU  - Chanho, Jung
TI  - A Study on Kernel Size Variations in 1D Convolutional Layer for Single-Frame supervised Temporal Action Localization
TI  - 단일 프레임 지도 시간적 행동 지역화에서1D 합성곱 층의 커널 사이즈 변화 연구
T2  - Journal of IKEEE
T2  - 전기전자학회논문지
M3  - research-article
AB  - In this paper, we propose variations in the kernel size of 1D convolutional layers for single-frame supervisedtemporal action localization. Building upon the existing method, which utilizes two 1D convolutional layers withkernel sizes of 3 and 1, we introduce an approach that adjusts the kernel sizes of each 1D convolutional layer. Tovalidate the efficiency of our proposed approach, we conducted comparative experiments using the THUMOS’14dataset. Additionally, we use overall video classification accuracy, mAP (mean Average Precision), and AveragemAP as performance metrics for evaluation. According to the experimental results, our proposed approachdemonstrates higher accuracy in terms of mAP and Average mAP compared to the existing method. The methodwith variations in kernel size of 7 and 1 further demonstrates an 8.0% improvement in overall video classificationaccuracy.
PU  - Institute of Korean Electrical and Electronics Engineers
SN  - 1226-7244
DA  - 2024 
PY  - 2024
VL  - 28
IS  - 2
SP  - 199
EP  - 203
AN  - KJD:ART003097624
Y2  - 2024-07-26
ER  -

TY  - JOUR
AU  - Li, Zhilin
AU  - Wang, Zilei
AU  - Dong, Cerui
TI  - Multilevel semantic and adaptive actionness learning for weakly supervised temporal action localization
T2  - NEURAL NETWORKS
M3  - Article
AB  - Weakly supervised temporal action localization aims to identify and localize action instances in untrimmed videos with only video-level labels. Typically, most methods are based on a multiple instance learning framework that uses a top-K strategy to select salient segments to represent the entire video. Therefore fine-grained video information cannot be learned, resulting in poor action classification and localization performance. In this paper, we propose a Multilevel S emantic and Adaptive A ctionness L earning Network (SAL), which is mainly composed of multilevel semantic learning (MSL) branch and adaptive actionness learning (AAL) branch. The MSL branch introduces second-order video semantics, which can capture finegrained information in videos and improve video-level classification performance. Furthermore, we propagate second-order semantics to action segments to enhance the difference between different actions. The AAL branch uses pseudo labels to learn class-agnostic action information. It introduces a video segments mix-up strategy to enhance foreground generalization ability and adds an adaptive actionness mask to balance the quality and quantity of pseudo labels, thereby improving the stability of training. Extensive experiments show that SAL achieves state-of-the-art results on three benchmarks. Code: https://github.com/lizhilin-ustc/SAL
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0893-6080
SN  - 1879-2782
DA  - 2025 FEB
PY  - 2025
VL  - 182
C7  - 106905
DO  - 10.1016/j.neunet.2024.106905
AN  - WOS:001364721500001
C6  - NOV 2024
AD  - Univ Sci & Technol China, Natl Engn Lab Brain inspired Intelligence Technol, Hefei 230026, Peoples R China
Y2  - 2024-12-05
ER  -

TY  - JOUR
AU  - Ilin, Semyon
AU  - Borodacheva, Julia
AU  - Shamsiev, Ildar
AU  - Bondar, Igor
AU  - Shichkina, Yulia
TI  - Temporal action localisation in video data containing rabbit behavioural patterns
T2  - SCIENTIFIC REPORTS
M3  - Article
AB  - In this paper we present the results of a research on artificial intelligence based approaches to temporal action localisation in video recordings of rabbit behavioural patterns. When using the artificial intelligence, special attention should be paid to quality and quantity of data collected for the research. Conducting the experiments in science may take long time and involve expensive preparatory work. Artificial intelligence based approaches can be applied to different kinds of actors in the video including animals, humans, intelligent agents, etc. The peculiarities of using these approaches in specific research conditions can be of particular importance for project cost reduction. In this paper we analyze the peculiarities of using the frame-by-frame classification based approach to temporal localisation of rabbit actions in video data and propose a metric for evaluating its consistency. The analysis of existing approaches described in the literature indicates that the aforementioned approach has high accuracy (up to 99%) and F1 score of temporal action localisation (up to 0.97) thus fulfilling conditions for substantial reduction or total exclusion of manual data labeling from the process of studying actor behaviour patterns in video data collected in experimental setting. We conducted further investigation in order to determine the optimal number of manually labeled frames required to achieve 99% accuracy of automatic labeling and studied the dependence of labeling accuracy on the number of actors presented in the training data.
PU  - NATURE PORTFOLIO
PI  - BERLIN
PA  - HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN  - 2045-2322
DA  - 2025 FEB 17
PY  - 2025
VL  - 15
IS  - 1
C7  - 5710
DO  - 10.1038/s41598-025-89687-6
AN  - WOS:001424376600022
AD  - St Petersburg Electrotech Univ LETI, Fac Comp Sci & Technol, St Petersburg 197022, Russia
AD  - RAS, Inst Higher Nervous Act & Neurophysiol, Moscow 117485, Russia
Y2  - 2025-02-26
ER  -

TY  - CPAPER
AU  - Cheng, Feng
AU  - Bertasius, Gedas
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - TALLFormer: Temporal Action Localization with a Long-Memory Transformer
T2  - COMPUTER VISION, ECCV 2022, PT XXXIV
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Most modern approaches in temporal action localization divide this problem into two parts: (i) short-term feature extraction and (ii) long-range temporal boundary localization. Due to the high GPU memory cost caused by processing long untrimmed videos, many methods sacrifice the representational power of the short-term feature extractor by either freezing the backbone or using a small spatial video resolution. This issue becomes even worse with the recent video transformer models, many of which have quadratic memory complexity. To address these issues, we propose TallFormer, a memory-efficient and end-to-end trainable Temporal Action Localization transformer with Longterm memory. Our long-term memory mechanism eliminates the need for processing hundreds of redundant video frames during each training iteration, thus, significantly reducing the GPU memory consumption and training time. These efficiency savings allow us (i) to use a powerful video transformer feature extractor without freezing the backbone or reducing the spatial video resolution, while (ii) also maintaining long-range temporal boundary localization capability. With only RGB frames as input and no external action recognition classifier, TallFormer outperforms previous state-of-the-arts by a large margin, achieving an average mAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The code is public available (https://github.com/klauscc/TALLFormer.)
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-19829-8
SN  - 978-3-031-19830-4
DA  - 2022 
PY  - 2022
VL  - 13694
SP  - 503
EP  - 521
DO  - 10.1007/978-3-031-19830-4_29
AN  - WOS:000903746100029
AD  - Univ North Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA
Y2  - 2023-02-02
ER  -

TY  - JOUR
AU  - Su, Shao wen
AU  - Zhang, Yan
AU  - Gan, Minggang
TI  - Proposal Semantic Relationship Graph Network for Temporal Action Detection
T2  - ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY
M3  - Article
AB  - Temporal action detection, a critical task in video activity understanding, is typically divided into two stages: proposal generation and classification. However, most existing methods overlook the importance of information transfer among proposals during classification, often treating each proposal in isolation, which hampers accurate label prediction. In this article, we propose a novel method for inferring semantic relationships both within and between action proposals, guiding the fusion of action proposal features accordingly. Building on this approach, we introduce the Proposal Semantic Relationship Graph Network (PSRGN), an end-to-end model that leverages intra-proposal semantic relationship graphs to extract cross-scale temporal context and an interproposal semantic relationship graph to incorporate complementary neighboring information, significantly improving proposal feature quality and overall detection performance. This is the first method to apply graph structure learning in temporal action detection, adaptively constructing the inter-proposal semantic graph. Extensive experiments on two datasets demonstrate the effectiveness of our approach, achieving state-of-the-art (SOTA). Code and results are available at http://github.com/Riiick2011/PSRGN.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 2157-6904
SN  - 2157-6912
DA  - 2024 DEC
PY  - 2024
VL  - 15
IS  - 6
C7  - 135
DO  - 10.1145/3702233
AN  - WOS:001387763600002
AD  - Beijing Inst Technol, Beijing, Peoples R China
Y2  - 2025-01-07
ER  -

TY  - JOUR
AU  - Wang, Ke
AU  - Li, Xuejing
AU  - Yang, Jianhua
AU  - Wu, Jun
AU  - Li, Ruifeng
TI  - Temporal action detection based on two-stream You Only Look Once network for elderly care service robot
T2  - INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS
M3  - Article
AB  - Human action segmentation and recognition from the continuous untrimmed sensor data stream is a challenging issue known as temporal action detection. This article provides a two-stream You Only Look Once-based network method, which fuses video and skeleton streams captured by a Kinect sensor, and our data encoding method is used to turn the spatiotemporal temporal action detection into a one-dimensional object detection problem in constantly augmented feature space. The proposed approach extracts spatial-temporal three-dimensional convolutional neural network features from video stream and view-invariant features from skeleton stream, respectively. Furthermore, these two streams are encoded into three-dimensional feature spaces, which are represented as red, green, and blue images for subsequent network input. We proposed the two-stream You Only Look Once-based networks which are capable of fusing video and skeleton information by using the processing pipeline to provide two fusion strategies, boxes-fusion or layers-fusion. We test the temporal action detection performance of two-stream You Only Look Once network based on our data set High-Speed Interplanetary Tug/Cocoon Vehicles-v1, which contains seven activities in the home environment and achieve a particularly high mean average precision. We also test our model on the public data set PKU-MMD that contains 51 activities, and our method also has a good performance on this data set. To prove that our method can work efficiently on robots, we transplanted it to the robotic platform and an online fall down detection experiment.
PU  - SAGE PUBLICATIONS INC
PI  - THOUSAND OAKS
PA  - 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN  - 1729-8814
DA  - 2021 JUL
PY  - 2021
VL  - 18
IS  - 4
C7  - 17298814211038342
DO  - 10.1177/17298814211038342
AN  - WOS:000697941100001
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Heilongjiang, Peoples R China
Y2  - 2021-10-02
ER  -

TY  - CPAPER
AU  - Chen, Xiaoqiu
AU  - Li, Mengge
AU  - Ma, Miao
A1  - IEEE
TI  - Two-Stream Completeness Modeling for Weakly Supervised Temporal Action Detection
T2  - IEEE TALE2021: IEEE INTERNATIONAL CONFERENCE ON ENGINEERING, TECHNOLOGY AND EDUCATION
M3  - Proceedings Paper
CP  - IEEE International Conference on Engineering, Technology and Education (IEEE TALE)
CL  - Cent China Normal Univ, Wuhan, PEOPLES R CHINA
AB  - In the process of dataset construction, the cost is expensive to make frame-wise temporal annotations for videos. Therefore, weakly supervised temporal action detection methods which only leverage video-level action categories annotations during training have become an important research branch. At present, most of the weakly supervised methods adopt feature early fusion, and have two inevitable problems, namely action completeness and background frame interference. Therefore, this paper proposes a method based on the Two-Stream Completeness Modeling (TSCM) network. First, this method separately inputs spatial flow features and temporal flow features into the network to make full use of the characteristics of the two modal features. Second, it employs the modeling of the multi-branch complementary completeness to generate as complete action instances as possible. Finally, Angular Center Loss with a Pair of Triplets (ACL-PT) is designed to suppress the interference from background frames. In particular, this paper constructs a temporal action detection dataset (STAD), which is based on learning scenes to explore the effectiveness of our method in real applications. Experimental results show that the proposed TSCM method not only is superior to most mainstream methods in terms of mean Average Precision (mAP) on the THUMOS14 dataset and the ActivityNet1.2 dataset, but also achieves good detection accuracy on the STAD dataset.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-3687-8
DA  - 2021 
PY  - 2021
SP  - 823
EP  - 828
DO  - 10.1109/TALE52509.2021.9678609
AN  - WOS:000810176600122
AD  - Shaanxi Normal Univ, Sch Comp Sci, Xian, Peoples R China
Y2  - 2022-07-01
ER  -

TY  - CPAPER
AU  - Li, Ziqiang
AU  - Ge, Yongxin
AU  - Yu, Jiaruo
AU  - Chen, Zhongming
ED  - ACM
TI  - Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization
T2  - PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022
M3  - Proceedings Paper
CP  - 30th ACM International Conference on Multimedia (MM)
CL  - Lisboa, PORTUGAL
AB  - With video-level labels, weakly supervised temporal action localization (WTAL) applies a localization-by-classification paradigm to detect and classify the action in untrimmed videos. Due to the characteristic of classification, class-specific background snippets are inevitably mis-activated to improve the discriminability of the classifier in WTAL. To alleviate the disturbance of background, existing methods try to enlarge the discrepancy between action and background through modeling background snippets with pseudos-nippet-level annotations, which largely rely on artificial hypotheticals. Distinct from the previous works, we present an adversarial learning strategy to break the limitation of mining pseudo background snippets. Concretely, the background classification loss forces the whole video to be regarded as the background by a background gradient reinforcement strategy, confusing the recognition model. Reversely, the foreground(action) loss guides the model to focus on action snippets under such conditions. As a result, competition between the two classification losses drives the model to boost its ability for action modeling. Simultaneously, a novel temporal enhancement network is designed to facilitate the model to construct temporal relation of affinity snippets based on the proposed strategy, for further improving the performance of action localization. Finally, extensive experiments conducted on THUMOS14 and ActivityNet1.2 demonstrate the effectiveness of the proposed method.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9203-7
DA  - 2022 
PY  - 2022
SP  - 5371
EP  - 5379
DO  - 10.1145/3503161.3548300
AN  - WOS:001150372705044
AD  - Chongqing Univ, Chongqing, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - CPAPER
AU  - Chen, Bo
AU  - Nahrstedt, Klara
A1  - ASSOC COMP MACHINERY
TI  - EScALation: A Framework for Efficient and Scalable Spatio-temporal Action Localization
T2  - MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE
M3  - Proceedings Paper
CP  - 12th ACM Multimedia Systems Conference (MMSys)
CL  - Istanbul, TURKEY
AB  - Spatio-temporal action localization aims to detect the spatial location and the start/end time of the action in a video. The stateof-the-art approach uses convolutional neural networks to extract possible bounding boxes for the action in each frame and then link bounding boxes into action tubes based on the location and the class-specific score of each bounding box. Though this approach has been successful at achieving a good localization accuracy, it is computation-intensive. High-end GPUs are usually demanded for it to achieve real-time performance. In addition, this approach does not scale well on a large number of action classes. In this work, we present a framework, EScALation, for making spatiotemporal action localization efficient and scalable. Our framework involves two main strategies. One is the frame sampling technique that utilizes the temporal correlation between frames and selects key frame(s) from a temporally correlated set of frames to perform bounding box detection. The other is the class filtering technique that exploits bounding box information to predict the action class prior to linking bounding boxes. We compare EScALation with the state-of-the-art approach on UCF101-24 and J-HMDB-21 datasets. One of our experiments shows EScALation is able to save 72.2% of the time with only 6.1% loss of mAP. In addition, we show that EScALation scales better to a large number of action classes than the state-of-the-art approach.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8434-6
DA  - 2021 
PY  - 2021
SP  - 146
EP  - 158
DO  - 10.1145/3458305.3459598
AN  - WOS:000723649200012
AD  - Univ Illinois, Urbana, IL 61801 USA
Y2  - 2021-12-10
ER  -

TY  - JOUR
AU  - Zhang, Shiwei
AU  - Song, Lin
AU  - Gao, Changxin
AU  - Sang, Nong
TI  - GLNet: Global Local Network for Weakly Supervised Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - In this paper, we address the challenging problem of weakly supervised spatio-temporal action localization for which only video-level action labels are available during training. To solve this problem, we propose an end-to-end Global Local Network (GLNet) to predict the probability distribution simultaneously in both spatial and temporal space. The proposed GLNet model includes two key components: a local spatial module and a global temporal module. The local spatial module aims to predict the frame-level spatial distribution by encoding short-term temporal information. In particular, we propose a Region Actionness Network (RAN) to select the target region boxes from the precomputed exhaustive proposals. The global temporal module can predict temporal distribution by a long-term temporal structuremodelling. Specifically, we design a temporal fusion-and-excitation architecture on the top of several clips, and trained by a sparse loss function. Therefore, the proposed GLNet model can perform spatio-temporal action localization in an end-to-end manner. We evaluate the performance of GLNet on the J-HMDB and UCF101-24 datasets. The experimental results demonstrate GLNet achieves a significant margin against other state-of-the-art weakly supervised methods and even some fully supervised methods in terms of frame mean Average Precision (mAP) and the video mAP (called frame-mAP and video-mAP, respectively).
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2020 OCT
PY  - 2020
VL  - 22
IS  - 10
SP  - 2610
EP  - 2622
DO  - 10.1109/TMM.2019.2959425
AN  - WOS:000572628000008
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Minist Educ Image Proc & Intelligent Cont, Wuhan 430074, Peoples R China
AD  - Xi An Jiao Tong Univ, Sch Artificial Intelligence & Automat, Xian 710049, Peoples R China
Y2  - 2020-10-08
ER  -

TY  - JOUR
AU  - Zhang, Xiao-Yu
AU  - Zhang, Yaru
AU  - Shi, Haichao
AU  - Dong, Jing
TI  - SAPS: Self-Attentive Pathway Search for weakly-supervised action localization with-action
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - Weakly supervised temporal action localization is a challenging computer vision task, which aims to derive frame-level action identifier based on video-level supervision. Attention mechanism is a widely used paradigm for action recognition and localization in most recent methods. However, existing attention-based methods mostly focus on capturing the global dependency of the frame sequence regardless of the local inter-frame distances. Moreover, during background modeling, different background contents are typically classified into one category, which inevitably jeopardizes the discriminative ability of classifiers and brings about irrelevant noise. In this paper, we present a novel self-attentive pathway search framework, namely SAPS, to address the above challenges. To achieve comprehensive representation with discriminative attention weights, we design a NAS-based attentive module with a path-level searching process, and construct a competitive attention structure revealing both local and global dependency. Furthermore, we propose the action-related background modeling for robust background-action augmentation, where knowledge derived from background can provide informative clues for action recognition. An ensemble T-CAM operation is subsequently designed to incorporate background information to further refine the temporal action localization results. Extensive experiments on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.2) have clearly corroborated the efficacy of our method.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2021 SEP
PY  - 2021
VL  - 210
C7  - 103256
DO  - 10.1016/j.cviu.2021.103256
AN  - WOS:000691812700004
C6  - AUG 2021
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
Y2  - 2021-09-08
ER  -

TY  - CPAPER
AU  - Chen, Mengyuan
AU  - Gao, Junyu
AU  - Yang, Shicai
AU  - Xu, Changsheng
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - Dual-Evidential Learning for Weakly-supervised Temporal Action Localization
T2  - COMPUTER VISION - ECCV 2022, PT IV
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Weakly-supervised temporal action localization (WS-TAL) aims to localize the action instances and recognize their categories with only video-level labels. Despite great progress, existing methods suffer from severe action-background ambiguity, which mainly comes from background noise introduced by aggregation operations and large intra-action variations caused by the task gap between classification and localization. To address this issue, we propose a generalized evidential deep learning (EDL) framework for WS-TAL, called Dual-Evidential Learning for Uncertainty modeling (DELU), which extends the traditional paradigm of EDL to adapt to the weakly-supervised multi-label classification goal. Specifically, targeting at adaptively excluding the undesirable background snippets, we utilize the video-level uncertainty to measure the interference of background noise to video-level prediction. Then, the snippet-level uncertainty is further deduced for progressive learning, which gradually focuses on the entire action instances in an "easy-to-hard" manner. Extensive experiments show that DELU achieves state-of-the-art performance on THUMOS14 and ActivityNetl.2 benchmarks. Our code is available in github.com/MengyuanChen21/ECCV2022-DELU.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-19771-0
SN  - 978-3-031-19772-7
DA  - 2022 
PY  - 2022
VL  - 13664
SP  - 192
EP  - 208
DO  - 10.1007/978-3-031-19772-7_12
AN  - WOS:000898297000012
AD  - Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China
AD  - Hikvis Res Inst, Hangzhou, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
M2  - Univ Chinese Acad Sci UCAS
M2  - Hikvis Res Inst
Y2  - 2023-01-25
ER  -

TY  - CPAPER
AU  - Liul, Mengnan
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Xia, Kun
AU  - Wu, Qi
AU  - Zhang, Qilin
AU  - Hua, Gang
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - Stepwise Multi-grained Boundary Detector for Point-Supervised Temporal Action Localization
T2  - COMPUTER VISION-ECCV 2024, PT VII
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Point-supervised temporal action localization pursues high-accuracy action detection under low-cost data annotation. Despite recent advances, a significant challenge remains: sparse labeling of individual frames leads to semantic ambiguity in determining action boundaries due to the lack of continuity in the highly sparse point-supervision scheme. We propose a Stepwise Multi-grained Boundary Detector (SMBD), which is comprised of a Background Anchor Generator (BAG) and a Dual Boundary Detector (DBD) to provide fine-grained supervision. Specifically, for each epoch in the training process, BAG computes the optimal background snippet between each pair of adjacent action labels, which we term Background Anchor. Subsequently, DBD leverages the background anchor and the action labels to locate the action boundaries from the perspectives of detecting action changes and scene changes. Then, the corresponding labels can be assigned to each side of the boundaries, with the boundaries continuously updated throughout the training process. Consequently, the proposed SMBD could ensure that more snippets contribute to the training process. Extensive experiments on the THUMOS'14, GTEA and BEOID datasets demonstrate that the proposed method outperforms existing state-of-the-art methods.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-72666-8
SN  - 978-3-031-72667-5
DA  - 2025 
PY  - 2025
VL  - 15065
SP  - 333
EP  - 349
DO  - 10.1007/978-3-031-72667-5_19
AN  - WOS:001346380800019
AD  - Xi An Jiao Tong Univ, Natl Engn Res Ctr Visual Informat & Applicat, Inst Artificial Intelligence & Robot, Natl Key Lab Human Machine Hybrid Augmented Intel, Xian, Peoples R China
AD  - DeepNight Ai, Hayward, CA USA
AD  - Dolby Labs, Multimodal Experiences Res Lab, San Francisco, CA USA
M2  - DeepNight Ai
Y2  - 2024-12-03
ER  -

TY  - JOUR
AU  - Wu, Yanchun
AU  - Yin, Jianqin
AU  - Wang, Lei
AU  - Liu, Huaping
AU  - Dang, Qi
AU  - Li, Zhiming
AU  - Yin, Yilong
TI  - Temporal Action Detection Based on Action Temporal Semantic Continuity
T2  - IEEE ACCESS
M3  - Article
AB  - This research proposes a method for optimizing extracted candidate proposals based on the action temporal semantic continuity rule to accurately detect the category and start and end time in the temporal action detection of long untrimmed videos. First, sliding windows of the same scale and different scales are integrated according to the rule of action temporal semantic continuity. Subsequently, we reacquire the classification confidence score and relocate the integration results. Finally, inaccurate detections are eliminated by non-maximum value suppression. In contrast to the specified scale of the detection results obtained by sliding window, this method can produce the action temporal segments of any length and suppress the redundant detection. Therefore, the detection results are more consistent with the expectation of an individual. Experimental results show that the mean average precision increases from 19.0% to 20.6% when the intersection-over-union threshold is set to 0.5 on THUMOS 2014 data set.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2018 
PY  - 2018
VL  - 6
SP  - 31677
EP  - 31684
DO  - 10.1109/ACCESS.2018.2842428
AN  - WOS:000437871500001
AD  - Univ Jinan, Sch Informat Sci & Engn, Shandong Prov Key Lab Network Based Intelligent C, Jinan 250022, Shandong, Peoples R China
AD  - Beijing Univ Posts & Telecommun, Automat Sch, Beijing 100876, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Lab Human Machine Control, Shenzhen 518055, Peoples R China
AD  - Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China
AD  - Shandong Univ, Sch Software Engn, Jinan 250101, Shandong, Peoples R China
Y2  - 2018-07-24
ER  -

TY  - JOUR
AU  - Hamdi, Maher
AU  - Wen, Shiping
AU  - Yang, Yin
TI  - BTM: Boundary Trimming Module for Temporal Action Detection
T2  - ELECTRONICS
M3  - Article
AB  - Temporal action detection (TAD) aims to recognize actions as well as their corresponding time spans from an input video. While techniques exist that accurately recognize actions from manually trimmed videos, current TAD solutions often struggle to identify the precise temporal boundaries of each action, which are required in many real applications. This paper addresses this problem with a novel Boundary Trimming Module (BTM), a post-processing method that adjusts the temporal boundaries of the detected actions from existing TAD solutions. Specifically, BTM operates based on the classification of frames in the input video, aiming to detect the action more accurately by adjusting the surrounding frames of the start and end frames of the original detection results. Experimental results on the THUMOS14 benchmark data set demonstrate that the BTM significantly improves the performance of several existing TAD methods. Meanwhile, we establish a new state of the art for temporal action detection through the combination of BTM and the previous best TAD solution.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2022 NOV
PY  - 2022
VL  - 11
IS  - 21
C7  - 3520
DO  - 10.3390/electronics11213520
AN  - WOS:000886116000001
AD  - George Washington Univ, Sch Business, Washington, DC 20052 USA
AD  - Univ Technol Sydney, Australia AI Inst, Ultimo 2007, Australia
AD  - Hamad Bin Khalifa Univ HBKU, Coll Sci & Engn, Doha 5825, Qatar
Y2  - 2022-12-02
ER  -

TY  - CPAPER
AU  - Reza, Sakib
AU  - Zhang, Yuexi
AU  - Moghaddam, Mohsen
AU  - Camps, Octavia
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization
T2  - COMPUTER VISION - ECCV 2024, PT XXI
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Online video understanding often relies on individual frames, leading to frame-by-frame predictions. Recent advancements such as Online Temporal Action Localization (OnTAL), extend this approach to instance-level predictions. However, existing methods mainly focus on short-term context, neglecting historical information. To address this, we introduce the History-Augmented Anchor Transformer (HAT) Framework for OnTAL. By integrating historical context, our framework enhances the synergy between long-term and short-term information, improving the quality of anchor features crucial for classification and localization. We evaluate our model on both procedural egocentric (PREGO) datasets (EGTEA and EPIC) and standard non-PREGO OnTAL datasets (THUMOS and MUSES). Results show that our model outperforms state-of-the-art approaches significantly on PREGO datasets and achieves comparable or slightly superior performance on non-PREGO datasets, underscoring the importance of leveraging long-term history, especially in procedural and egocentric action scenarios. Code is available at: https://github.com/ sakibreza/ECCV24-HAT/.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-72663-7
SN  - 978-3-031-72664-4
DA  - 2025 
PY  - 2025
VL  - 15079
SP  - 205
EP  - 222
DO  - 10.1007/978-3-031-72664-4_12
AN  - WOS:001352788000012
AD  - Northeastern Univ, Boston, MA 02115 USA
Y2  - 2024-12-05
ER  -

TY  - JOUR
AU  - Su, Rui
AU  - Xu, Dong
AU  - Zhou, Luping
AU  - Ouyang, Wanli
TI  - Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-Resolution Information in Temporal Domain
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly supervised temporal action localization is a challenging task as only the video-level annotation is available during the training process. To address this problem, we propose a two-stage approach to generate high-quality frame-level pseudo labels by fully exploiting multi-resolution information in the temporal domain and complementary information between the appearance (i.e., RGB) and motion (i.e., optical flow) streams. In the first stage, we propose an Initial Label Generation (ILG) module to generate reliable initial frame-level pseudo labels. Specifically, in this newly proposed module, we exploit temporal multi-resolution consistency and cross-stream consistency to generate high quality class activation sequences (CASs), which consist of a number of sequences with each sequence measuring how likely each video frame belongs to one specific action class. In the second stage, we propose a Progressive Temporal Label Refinement (PTLR) framework to iteratively refine the pseudo labels, in which we use a set of selected frames with highly confident pseudo labels to progressively train two networks and better predict action class scores at each frame. Specifically, in our newly proposed PTLR framework, two networks called Network-OTS and Network-RTS, which are respectively used to generate CASs for the original temporal scale and the reduced temporal scales, are used as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo labels in turn. By this way, multi-resolution information in the temporal domain is exchanged at the pseudo label level, and our work can help improve each network/stream by exploiting the refined pseudo labels from another network/stream. Comprehensive experiments on two benchmark datasets THUMOS14 and ActivityNet v1.3 demonstrate the effectiveness of our newly proposed method for weakly supervised temporal action localization.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 6659
EP  - 6672
DO  - 10.1109/TIP.2021.3089355
AN  - WOS:000679941200004
AD  - Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia
Y2  - 2021-08-09
ER  -

TY  - CPAPER
AU  - Chen, Bo
AU  - Ali-Eldin, Ahmed
AU  - Shenoy, Prashant
AU  - Nahrsted, Klara
A1  - IEEE
TI  - Real-time Spatio-Temporal Action Localization in 360 Videos
T2  - 2020 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2020)
M3  - Proceedings Paper
CP  - 22nd IEEE International Symposium on Multimedia (IEEE ISM)
CL  - ELECTR NETWORK
AB  - Spatio-temporal action localization of human actions in a video has been a popular topic over the past few years. It tries to localize the bounding boxes, the time span and the class of one action, which summarizes information in the video and helps humans understand it. Though many approaches have been proposed to solve this problem, these efforts have only focused on perspective videos. Unfortunately, perspective videos only cover a small field-of-view (FOV), which limits the capability of action localization. In this paper, we develop a comprehensive approach to real-time spatio-temporal localization that can be used to detect actions in 360 videos. We create two datasets named UCF-10124-360 and JHMDB-21-360 for our evaluation. Our experiments show that our method consistently outperforms other competing approaches and achieves a real-time processing speed of 15fps for 360 videos.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-8697-9
DA  - 2020 
PY  - 2020
SP  - 73
EP  - 76
DO  - 10.1109/ISM.2020.00018
AN  - WOS:000654273000012
AD  - Univ Illinois, Champaign, IL 61820 USA
AD  - Univ Massachusetts, Amherst, MA 01003 USA
Y2  - 2021-06-19
ER  -

TY  - JOUR
AU  - Kim, Jinah
AU  - Cho, Jungchan
TI  - Background-Aware Robust Context Learning for Weakly-Supervised Temporal Action Localization
T2  - IEEE ACCESS
M3  - Article
AB  - Weakly supervised temporal action localization (WTAL) aims to localize temporal intervals of actions in an untrimmed video using only video-level action labels. Although the learning of the background is an important issue in WTAL, most previous studies have not utilized an effective background. In this study, we propose a novel method for robustly separating contexts, e.g., action-like background, from the foreground to more accurately localize the action intervals. First, we detect background segments based on their probabilities to minimize the impact of background estimation errors. Second, we define the entropy boundary of the foreground and the positive distance between the boundary and background entropy. The background probability and entropy boundary allow the segment-level classifier to robustly learn the background. Third, we improve the performance of the overall actionness model based on a consensus of the RGB and flow features. The results of extensive experiments demonstrate that the proposed method learns the context separately from the action, consequently achieving new state-of-the-art results on the THUMOS-14 and ActivityNet-1.2 benchmarks. We also confirm that using feature adaptation helps overcome the limitation of a pretrained feature extractor on datasets that contain many backgrounds, such as THUMOS-14.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2022 
PY  - 2022
VL  - 10
SP  - 65315
EP  - 65325
DO  - 10.1109/ACCESS.2022.3183789
AN  - WOS:000815511400001
AD  - Gachon Univ, Coll Informat Technol, Seongnam Si 13120, Gyeonggi Do, South Korea
Y2  - 2022-07-01
ER  -

TY  - JOUR
AU  - Cao, Congqi
AU  - Wang, Yizhe
AU  - Zhang, Yueran
AU  - Lu, Yue
AU  - Zhang, Xin
AU  - Zhang, Yanning
TI  - Co-Occurrence Matters: Learning Action Relation for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Temporal action localization (TAL) is a prevailing task due to its great application potential. Existing works in this field mainly suffer from two weaknesses: (1) They often neglect the multi-label case and only focus on temporal modeling. (2) They ignore the semantic information in class labels and only use the visual information. To solve these problems, we propose a novel Co-Occurrence Relation Module (CORM) that explicitly models the co-occurrence relationship between actions. Besides the visual information, it further utilizes the semantic embeddings of class labels to model the co-occurrence relationship. The CORM works in a plug-and-play manner and can be easily incorporated with the existing sequence models. By considering both visual and semantic co-occurrence, our method achieves high multi-label relationship modeling capacity. Meanwhile, existing datasets in TAL always focus on low-semantic atomic actions. Thus we construct a challenging multi-label dataset UCF-Crime-TAL that focuses on high-semantic actions by annotating the UCF-Crime dataset at frame level and considering the semantic overlap of different events. Extensive experiments on two commonly used TAL datasets, i.e., MultiTHUMOS and TSU, and our newly proposed UCF-Crime-TAL demenstrate the effectiveness of the proposed CORM, which achieves state-of-the-art performance on these datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 MAY
PY  - 2024
VL  - 34
IS  - 5
SP  - 3327
EP  - 3339
DO  - 10.1109/TCSVT.2023.3321508
AN  - WOS:001221132000041
AD  - Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Xian 710129, Peoples R China
Y2  - 2024-06-15
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Han, Junwei
AU  - Zhao, Tao
AU  - Lin, Tianwei
AU  - Zhang, Dingwen
AU  - Chen, Jianxin
TI  - Background-Click Supervision for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly supervised temporal action localization aims at learning the instance-level action pattern from the video-level labels, where a significant challenge is action-context confusion. To overcome this challenge, one recent work builds an action-click supervision framework. It requires similar annotation costs but can steadily improve the localization performance when compared to the conventional weakly supervised methods. In this paper, by revealing that the performance bottleneck of the existing approaches mainly comes from the background errors, we find that a stronger action localizer can be trained with labels on the background video frames rather than those on the action frames. To this end, we convert the action-click supervision to the background-click supervision and develop a novel method, called BackTAL. Specifically, BackTAL implements two-fold modeling on the background video frames, i.e., the position modeling and the feature modeling. In position modeling, we not only conduct supervised learning on the annotated video frames but also design a score separation module to enlarge the score differences between the potential action frames and backgrounds. In feature modeling, we propose an affinity module to measure frame-specific similarities among neighboring frames and dynamically attend to informative neighbors when calculating temporal convolution. Extensive experiments on three benchmarks are conducted, which demonstrate the high performance of the established BackTAL and the rationality of the proposed background-click supervision.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2022 DEC 1
PY  - 2022
VL  - 44
IS  - 12
SP  - 9814
EP  - 9829
DO  - 10.1109/TPAMI.2021.3132058
AN  - WOS:000880661400090
AD  - Northwestern Polytech Univ, BRAIN Lab, Xian 710060, Shaanxi, Peoples R China
AD  - Baidu VIS, Beijing 100085, Peoples R China
AD  - Beijing Univ Chinese Med, Beijing 100029, Peoples R China
M2  - Baidu VIS
Y2  - 2022-11-25
ER  -

TY  - JOUR
AU  - Zhai, Yuanhao
AU  - Wang, Le
AU  - Tang, Wei
AU  - Zhang, Qilin
AU  - Zheng, Nanning
AU  - Hua, Gang
TI  - Action Coherence Network for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised Temporal Action Localization (W-TAL) aims at simultaneously classifying and locating all action instances with only video-level supervision. However, current W-TAL methods have two limitations. First, they ignore the difference in video representations between an action instance and its surrounding background when generating and scoring action proposals. Second, the unique characteristics of the RGB frames and optical flow are largely ignored when fusing these two modalities. To address these problems, an Action Coherence Network (ACN) is proposed in this paper. Its core is a new coherence loss which exploits both classification predictions and video content representations to supervise action boundary regression and thus leads to more accurate action localization results. Besides, the proposed ACN explicitly takes into account the specific characteristics of RGB frames and optical flow by training two separate sub-networks, each of which is able to generate modality-specific action proposals independently. Finally, to take advantage of the complementary action proposals generated by two streams, a novel fusion module is introduced to reconcile them and obtain the final action localization results. Experiments on the THUMOS14 and ActivityNet datasets show that our ACN outperforms the state-of-the-art W-TAL methods, and is even comparable to some recent fully-supervised methods. Particularly, ACN achieves a mean average precision of 26.4% on the THUMOS14 dataset under the IoU threshold 0.5.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2022 
PY  - 2022
VL  - 24
SP  - 1857
EP  - 1870
DO  - 10.1109/TMM.2021.3073235
AN  - WOS:000778959200007
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA
AD  - ABB Corp Res Ctr, Raleigh, NC 27606 USA
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
M2  - Wormpex AI Res
Y2  - 2022-04-26
ER  -

TY  - JOUR
AU  - Xia, Huifen
AU  - Zhan, Yongzhao
AU  - Cheng, Keyang
TI  - Spatial-temporal correlations learning and action-background jointed attention for weakly-supervised temporal action localization
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - Weakly supervised temporal action localization (W-TAL) is designed to detect and classify all the action instances in an untrimmed video with only video-level labels. Due to the lack of frame-level annotations, the correlations learning between action snippets and the separation between action and background are the two key issues for accurate action localization. To mine the intrinsic correlations of space and time embodied in the occurrences of action in a video and identify the action and background in the snippets, a novel method based on spatial-temporal correlations learning and action-background jointed attention for W-TAL is proposed. In this method, the graph convolution and 1-D temporal convolution networks are constructed to learn the spatial and temporal features of the video, respectively, then fused to learn a fruitful spatial-temporal correlative feature map. This ensures more completed features representation for action localization. Next, different from the other methods, action-background jointed attention mechanism is presented to explicitly modelled background as well as action in a three-branch classification network. This classification network can distinguish action and background and realize the separation of action and background better, so as to promote more accurate action localization. Experiments conducted on Thumos14 and ActivityNet1.3 show that our method outperforms state-of-the-art methods, especially at some high t-IoU thresholds, which further validates the effectiveness of our method.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2022 AUG
PY  - 2022
VL  - 28
IS  - 4
SP  - 1529
EP  - 1541
DO  - 10.1007/s00530-022-00912-y
AN  - WOS:000770200700001
C6  - MAR 2022
AD  - Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China
AD  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept, Zhenjiang 212013, Jiangsu, Peoples R China
AD  - Changzhou Vocat Inst Mechatron Technol, Changzhou 213164, Jiangsu, Peoples R China
M2  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept
Y2  - 2022-03-26
ER  -

TY  - JOUR
AU  - Gu, Jialiang
AU  - Yi, Yang
AU  - Wang, Min
TI  - Advancing Temporal Action Localization with a Boundary Awareness Network
T2  - ELECTRONICS
M3  - Article
AB  - Temporal action localization (TAL) is crucial in video analysis, yet presents notable challenges. This process focuses on the precise identification and categorization of action instances within lengthy, raw videos. A key difficulty in TAL lies in determining the exact start and end points of actions, owing to the often unclear boundaries of these actions in real-world footage. Existing methods tend to take insufficient account of changes in action boundary features. To tackle these issues, we propose a boundary awareness network (BAN) for TAL. Specifically, the BAN mainly consists of a feature encoding network, coarse pyramidal detection to obtain preliminary proposals and action categories, and fine-grained detection with a Gaussian boundary module (GBM) to get more valuable boundary information. The GBM contains a novel Gaussian boundary pooling, which serves to aggregate the relevant features of the action boundaries and to capture discriminative boundary and actionness features. Furthermore, we introduce a novel approach named Boundary Differentiated Learning (BDL) to ensure our model's capability in accurately identifying action boundaries across diverse proposals. Comprehensive experiments on both the THUMOS14 and ActivityNet v1.3 datasets, where our BAN model achieved an increase in mean Average Precision (mAP) by 1.6% and 0.2%, respectively, over existing state-of-the-art methods, illustrate that our approach not only improves upon the current state of the art but also achieves outstanding performance.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2024 MAR
PY  - 2024
VL  - 13
IS  - 6
C7  - 1099
DO  - 10.3390/electronics13061099
AN  - WOS:001191748700001
AD  - Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China
Y2  - 2024-03-31
ER  -

TY  - JOUR
AU  - Chen, Mengyuan
AU  - Gao, Junyu
AU  - Xu, Changsheng
TI  - Uncertainty-Aware Dual-Evidential Learning for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly-supervised temporal action localization (WTAL) aims to localize the action instances and recognize their categories with only video-level labels. Despite great progress, existing methods suffer from severe action-background ambiguity, which mainly arises from background noise and neglect of non-salient action snippets. To address this issue, we propose a generalized evidential deep learning (EDL) framework for WTAL, called Uncertainty-aware Dual-Evidential Learning (UDEL), which extends the traditional paradigm of EDL to adapt to the weakly-supervised multi-label classification goal with the guidance of epistemic and aleatoric uncertainties, of which the former comes from models lacking knowledge, while the latter comes from the inherent properties of samples themselves. Specifically, targeting excluding the undesirable background snippets, we fuse the video-level epistemic and aleatoric uncertainties to measure the interference of background noise to video-level prediction. Then, the snippet-level aleatoric uncertainty is further deduced for progressive mutual learning, which gradually focuses on the entire action instances in an "easy-to-hard" manner and encourages the snippet-level epistemic uncertainty to be complementary with the foreground attention scores. Extensive experiments show that UDEL achieves state-of-the-art performance on four public benchmarks. Our code is available in github/mengyuanchen2021/UDEL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 DEC
PY  - 2023
VL  - 45
IS  - 12
SP  - 15896
EP  - 15911
DO  - 10.1109/TPAMI.2023.3308571
AN  - WOS:001104973300114
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Peoples R China
Y2  - 2024-03-15
ER  -

TY  - JOUR
AU  - Li, Qiang
AU  - Zu, Guang
AU  - Xu, Hui
AU  - Kong, Jun
AU  - Zhang, Yanni
AU  - Wang, Jianzhong
TI  - An Adaptive Dual Selective Transformer for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization (TAL), which aims to identify and localize actions in long untrimmed videos, is a challenging task in video understanding. Recent studies have shown that the Transformer and its variants are effective at improving the performance of TAL. The success of the Transformer can be attributed to the use of multi-head self-attention (MHSA) as a token mixer to capture long-term temporal dependencies within the video sequence. However, in the existing Transformer architecture, the features obtained by multiple token mixing (i.e., self-attention) heads are treated equally, which neglects the distinct characteristics of different heads and hampers the exploitation of discriminative information. To this end, we present a new method called the adaptive dual selective Transformer (ADSFormer) for TAL in this paper. The key component in ADSFormer is the dual selective multi-head token mixer (DSMHTM), which integrates multiple feature representations from different token mixing heads by adaptively selecting important features across both the head and channel dimensions. Moreover, we also incorporate our ADSFormer into a pyramid structure so that the multi-scale features obtained can be effectively combined to improve TAL performance. Benefiting from the dual selective multi-head token mixer (DSMHTM) and pyramid feature combination, ADSFormer outperforms several state-of-the-art methods on four challenging benchmark datasets: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100 and ActivityNet-1.3.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 7398
EP  - 7412
DO  - 10.1109/TMM.2024.3367599
AN  - WOS:001209811000063
AD  - Northeast Normal Univ, Sch Informat Sci & Technol, Changchun 130117, Peoples R China
AD  - Northeast Normal Univ, Key Lab Appl Stat MOE, Changchun 130117, Peoples R China
Y2  - 2024-05-16
ER  -

TY  - JOUR
AU  - Gao, Junyu
AU  - Chen, Mengyuan
AU  - Xu, Changsheng
TI  - Vectorized Evidential Learning for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - With the explosive growth of videos, weakly-supervised temporal action localization (WS-TAL) task has become a promising research direction in pattern analysis and machine learning. WS-TAL aims to detect and localize action instances with only video-level labels during training. Modern approaches have achieved impressive progress via powerful deep neural networks. However, robust and reliable WS-TAL remains challenging and underexplored due to considerable uncertainty caused by weak supervision, noisy evaluation environment, and unknown categories in the open world. To this end, we propose a new paradigm, named vectorized evidential learning (VEL), to explore local-to-global evidence collection for facilitating model performance. Specifically, a series of learnable meta-action units (MAUs) are automatically constructed, which serve as fundamental elements constituting diverse action categories. Since the same meta-action unit can manifest as distinct action components within different action categories, we leverage MAUs and category representations to dynamically and adaptively learn action components and action-component relations. After performing uncertainty estimation at both category-level and unit-level, the local evidence from action components is accumulated and optimized under the Subject Logic theory. Extensive experiments on the regular, noisy, and open-set settings of three popular benchmarks show that VEL consistently obtains more robust and reliable action localization performance than state-of-the-arts.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 DEC
PY  - 2023
VL  - 45
IS  - 12
SP  - 15949
EP  - 15963
DO  - 10.1109/TPAMI.2023.3311447
AN  - WOS:001104973300117
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Peoples R China
Y2  - 2024-03-15
ER  -

TY  - CPAPER
AU  - Ji, Yuan
AU  - Jia, Xu
AU  - Lu, Huchuan
AU  - Ruan, Xiang
A1  - ACM
TI  - Weakly-Supervised Temporal Action Localization via Cross-Stream Collaborative Learning
T2  - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021
M3  - Proceedings Paper
CP  - 29th ACM International Conference on Multimedia (MM)
CL  - ELECTR NETWORK
AB  - Weakly supervised temporal action localization (WTAL) is a challenging task as only video-level category labels are available during training stage. Without precise temporal annotations, most approaches rely on complementary RGB and optical flow features to predict the start and end frame of each action category in a video. However, existing approaches simply resort to either concatenation or weighted sum to learn how to take advantages of these two modalities for accurate action localization, which ignore the substantial variance between such two modalities. In this paper, we present Cross-Stream Collaborative Learning (CSCL) to address these issues. The proposed CSCL introduce a cross-stream weighting module to identify which modality is more robust during training and take advantage of the robust modality to guide the weaker one. Furthermore, we suppress the snippets which has high action-ness scores in both modalities to further exploiting the complementary property between two modalities. In addition, we bring the concept of co-training for WTAL and take both modalities into account for pseudo label generation to help training a stronger model. Extensive experiments conducted on THUMOS14 and ActivityNet dataset demonstrate that CSCL achieves a favorable performance against state-of-the-arts methods.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8651-7
DA  - 2021 
PY  - 2021
SP  - 853
EP  - 861
DO  - 10.1145/3474085.3475261
AN  - WOS:001147786900094
AD  - Dalian Univ Technol, Dalian, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Tiwaki Co Ltd, Kusatsu, Japan
M2  - Tiwaki Co Ltd
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Huang, Linjiang
AU  - Huang, Yan
AU  - Ouyang, Wanli
AU  - Wang, Liang
TI  - Modeling Sub-Actions for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - As a challenging task of high-level video understanding, weakly supervised temporal action localization has attracted more attention recently. Due to the usage of video-level category labels, this task is usually formulated as the task of classification, which always suffers from the contradiction between classification and detection. In this paper, we describe a novel approach to alleviate the contradiction for detecting more complete action instances by explicitly modeling sub-actions. Our method makes use of three innovations to model the latent sub-actions. First, our framework uses prototypes to represent sub-actions, which can be automatically learned in an end-to-end way. Second, we regard the relations among sub-actions as a graph, and construct the correspondences between sub-actions and actions by the graph pooling operation. Doing so not only makes the sub-actions inter-dependent to facilitate the multi-label setting, but also naturally use the video-level labels as weak supervision. Third, we devise three complementary loss functions, namely, representation loss, balance loss and relation loss to ensure the learned sub-actions are diverse and have clear semantic meanings. Experimental results on THUMOS14 and ActivityNet1.3 datasets demonstrate the effectiveness of our method and superior performance over state-of-the-art approaches.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 5154
EP  - 5167
DO  - 10.1109/TIP.2021.3078324
AN  - WOS:000655246800003
AD  - Chinese Acad Sci CASIA, Ctr Res Intelligent Percept & Comp CRIPAC, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci UCAS, Beijing 100864, Peoples R China
AD  - Chinese Acad Sci CASIA, Ctr Res Intelligent Percept & Comp CRIPAC, Ctr Excellence Brain Sci & Intelligence Technol C, Inst Automat,Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100864, Peoples R China
AD  - Chinese Acad Sci CAS AIR, Artificial Intelligence Res, Beijing 100190, Peoples R China
AD  - Univ Sydney, SenseTime Comp Vis Res Grp, Sydney, NSW 2006, Australia
M2  - Univ Chinese Acad Sci UCAS
M2  - Univ Chinese Acad Sci UCAS
M2  - Chinese Acad Sci CAS AIR
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Zhai, Yuanhao
AU  - Wang, Le
AU  - Tang, Wei
AU  - Zhang, Qilin
AU  - Zheng, Nanning
AU  - Doermann, David
AU  - Yuan, Junsong
AU  - Hua, Gang
TI  - Adaptive Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly-supervised temporal action localization (W-TAL) aims to classify and localize all action instances in untrimmed videos under only video-level supervision. Without frame-level annotations, it is challenging for W-TAL methods to clearly distinguish actions and background, which severely degrades the action boundary localization and action proposal scoring. In this paper, we present an adaptive two-stream consensus network (A-TSCN) to address this problem. Our A-TSCN features an iterative refinement training scheme: a frame-level pseudo ground truth is generated and iteratively updated from a late-fusion activation sequence, and used to provide frame-level supervision for improved model training. Besides, we introduce an adaptive attention normalization loss, which adaptively selects action and background snippets according to video attention distribution. By differentiating the attention values of the selected action snippets and background snippets, it forces the predicted attention to act as a binary selection and promotes the precise localization of action boundaries. Furthermore, we propose a video-level and a snippet-level uncertainty estimator, and they can mitigate the adverse effect caused by learning from noisy pseudo ground truth. Experiments conducted on the THUMOS14, ActivityNet v1.2, ActivityNet v1.3, and HACS datasets show that our A-TSCN outperforms current state-of-the-art methods, and even achieves comparable performance with several fully-supervised methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 APR 1
PY  - 2023
VL  - 45
IS  - 4
SP  - 4136
EP  - 4151
DO  - 10.1109/TPAMI.2022.3189662
AN  - WOS:000947840300010
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - Univ Buffalo State Univ New York, Comp Sci & Engn Dept, Buffalo, NY 14260 USA
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL USA
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
M2  - Wormpex AI Res
Y2  - 2023-03-31
ER  -

TY  - JOUR
AU  - Tang, Yiping
AU  - Zheng, Yang
AU  - Wei, Chen
AU  - Guo, Kaitai
AU  - Hu, Haihong
AU  - Liang, Jimin
TI  - Video representation learning for temporal action detection using global-local attention
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Video representation is of significant importance for temporal action detection. The two sub-tasks of temporal action detection, i.e., action classification and action localization, have different requirements for video representation. Specifically, action classification requires video representations to be highly dis-criminative, so that action features and background features are as dissimilar as possible. For action local-ization, it is crucial to obtain information about the action itself and the surrounding context for accurate prediction of action boundaries. However, the previous methods failed to extract the optimal representa-tions for the two sub-tasks, whose representations for both sub-tasks are obtained in a similar way. In this paper, a Global-Local Attention (GLA) mechanism is proposed to produce a more powerful video rep-resentation for temporal action detection without introducing additional parameters. The global attention mechanism predicts each action category by integrating features in the entire video that are similar to the action while suppressing other features, thus enhancing the discriminability of video representation during the training process. The local attention mechanism uses a Gaussian weighting function to inte-grate each action and its surrounding contextual information, thereby enabling precise localization of the action. The effectiveness of GLA is demonstrated on THUMOS'14 and ActivityNet-1.3 with a simple one -stage action detection network, achieving state-of-the-art performance among the methods using only RGB images as input. The inference speed of the proposed model reaches 1373 FPS on a single Nvidia Titan Xp GPU. The generalizability of GLA to other detection architectures is verified using R-C3D and Decouple-SSAD, both of which achieve consistent improvements. The experimental results demonstrate that designing representations with different properties for the two sub-tasks leads to better performance for temporal action detection compared to the representations obtained in a similar way.(c) 2022 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2022 FEB
PY  - 2022
VL  - 134
C7  - 109135
DO  - 10.1016/j.patcog.2022.109135
AN  - WOS:000882982300011
AD  - Xidian Univ, Sch Elect Engn, Xian, Peoples R China
Y2  - 2022-11-29
ER  -

TY  - JOUR
AU  - Li, Bairong
AU  - Liu, Ruixin
AU  - Chen, Tianquan
AU  - Zhu, Yuesheng
TI  - Weakly Supervised Temporal Action Detection With Temporal Dependency Learning
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly supervised temporal action detection aims at localizing temporal positions of action instances in untrimmed videos with only action class labels. In general, previous methods individually classify each frame based on the appearance information and the short-term motion information, and then integrate consecutive high-response action frames into entities which serve as detected action instances. However, the long-range temporal dependencies between action frames are not fully utilized, and the detection results are more likely to be trapped in the most discriminative action segments. To alleviate this issue, we propose a novel two-branch (i.e., the coarse detection branch and the refining detection branch) detection framework with learning the long-range temporal dependencies for obtaining more accurate detection results, where only action class labels are required. The coarse detection branch is used to localize the most discriminative segments of action instances based on a typical multi-instance learning paradigm under the supervision of action class labels, whereas the refining detection branch is expected to localize the less discriminative segments of action instances via learning the long-range temporal dependencies between frames based on the proposed Transformer-style architecture and learning strategies. This collaboration mechanism takes full advantage of complementary information from the provided action class labels and the natural temporal dependencies between action frames, forming a more comprehensive solution. Consequently, our method obtains more precise detection results. Expectedly, the proposed method outperforms recent weakly supervised temporal action detection methods on dataset THUMOS14 and ActivityNet measured by mAP@tIoU and AR@AN.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2022 JUL
PY  - 2022
VL  - 32
IS  - 7
SP  - 4473
EP  - 4485
DO  - 10.1109/TCSVT.2021.3125701
AN  - WOS:000819817700031
AD  - Peking Univ, Shenzhen Grad Sch, Shenzhen 518000, Peoples R China
Y2  - 2022-07-10
ER  -

TY  - JOUR
AU  - Ahn, Dasom
AU  - Lee, Jong-Ha
AU  - Ko, Byoung Chul
TI  - Interpretable Information Visualization for Enhanced Temporal Action Detection in Videos
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action detection (TAD) is one of the most active research areas in computer vision. TAD is the task of detecting actions in untrimmed videos and predicting the start and end times of the actions. TAD is a challenging task and requires a variety of temporal cues. In this paper, we present a one-stage transformer-based temporal action detection model using enhanced long- and short-term attention. Recognizing multiple actions in a video sequence requires an understanding of various temporal continuities. These temporal continuities encompass both long- and short-term temporal dependencies. To learn these long- and short-term temporal dependencies, our model leverages long- and short-term temporal attention based on transformers. In short-term temporal attention, we consider long-term memory to learn short-term temporal features and use compact long-term memory to efficiently learn long-term memory. Long-term temporal attention uses deformable attention to dynamically select the required features from long-term memory and efficiently learn the long-term features. Furthermore, our model offers interpretability for TAD by providing visualizations of class-specific probability changes for temporal action variations. This allows for a deeper understanding of the model's decision-making process and facilitates further analysis of TAD. Based on the results of experiments conducted on the THUMOS14 and ActivityNet-1.3 datasets, our proposed model achieves an improved performance compared to previous state-of-the-art models. Our code is available at https://github.com/tommy-ahn/LSTA.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2024 
PY  - 2024
VL  - 12
SP  - 107385
EP  - 107393
DO  - 10.1109/ACCESS.2024.3438546
AN  - WOS:001288389200001
AD  - Keimyung Univ, Dept Comp Engn, Daegu 1095, South Korea
AD  - Keimyung Univ, Dept Biomed Engn, Daegu 1095, South Korea
Y2  - 2024-08-15
ER  -

TY  - JOUR
AU  - Li, Chenhao
AU  - He, Chen
AU  - Zhang, Hui
AU  - Yao, Jiacheng
AU  - Zhang, Jing
AU  - Zhuo, Li
TI  - Streamer temporal action detection in live video by co-attention boundary matching
T2  - INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS
M3  - Article
AB  - With the advent of the we-media era, live video is being sought after by more and more web users. How to effectively identify and supervise the streamer activities in the live video is of great significance to promote the high-quality development of the live video industry. The streamer activity can be characterized by the temporal composition of a series of actions. To improve the accuracy of streamer temporal action detection, it is a promising path to utilize the temporal action location and co-attention mechanism to overcome the problem of blurring action boundary. Therefore, a streamer temporal action detection method by co-attention boundary matching in live video is proposed. (1) The global spatiotemporal features and action template features of live video are extracted by using two-stream convolutional network and action spatiotemporal attention network respectively. (2) The probability sequences are generated from the global spatiotemporal features through temporal action evaluation, and the boundary matching confidence maps are produced by confidence evaluation of global spatiotemporal features and action template features under the co-attention mechanism. (3) The streamer temporal actions are detected based on the action proposals generated by probability sequences and boundary matching maps. We establish a real-world streamer action BJUT-SAD dataset and conduct extensive experiments to verify that our method can boost the accuracy of streamer temporal action detection in live video. In particular, our temporal action proposal generation and streamer action detection task produce competitive results to prior methods, demonstrating the effectiveness of our method.
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN  - 1868-8071
SN  - 1868-808X
DA  - 2022 OCT
PY  - 2022
VL  - 13
IS  - 10
SP  - 3071
EP  - 3088
DO  - 10.1007/s13042-022-01581-z
AN  - WOS:000809523900001
C6  - JUN 2022
AD  - Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China
AD  - Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China
Y2  - 2022-06-20
ER  -

TY  - CPAPER
AU  - Jin, Xiaodong
AU  - Zhang, Taiping
A1  - ACM
TI  - MTSN: Multiscale Temporal Similarity Network for Temporal Action Localization
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - Temporal Action Localization (TAL) aims to predict the categories and temporal segments of all action instances in untrimmed videos, which is a critical and challenging task in the video understanding field. The performances of existing TAL methods remain unsatisfactory, due to the lack of highly effective temporal modeling and refined action proposal decoding. In this paper, we propose Multiscale Temporal Similarity Network (MTSN), a novel one-stage method for TAL, which mainly benefits from dynamic complementary modeling and temporal similarity decoding. Specifically, we first design Dynamic Complementary Context Aggregation (DCCA), a Transformer-based encoder. DCCA performs both long-range and short-range temporal modeling through different interaction range types of attention heads at each feature pyramid level, while higher-level semantic representations are effectively complemented with more short-range detail information in a dynamic fashion. Moreover, Temporal Similarity Mask (TSM) is designed to generate masks through an optimized globally-aware decoding process, including similarity cross-modeling, region-aware optimization and multiscale aggregated residual, which leads to high-quality action proposals. We conduct extensive experiments on two major TAL benchmarks: THUMOS14 and ActivityNet-1.3, where our method establishes a new state-of-the-art and significantly out-performs the previous best methods. Without bells and whistles, on THUMOS14, MTSN achieves an average mAP of 72.1% (+5.3%). On ActivityNet-1.3, MTSN reaches an average mAP of 40.7% (+3.1%), which crosses the 40% average mAP for the first time.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 2573
EP  - 2581
DO  - 10.1145/3581783.3612455
AN  - WOS:001199449102070
AD  - Chongqing Univ, Coll Comp Sci, Chongqing, Peoples R China
Y2  - 2024-07-31
ER  -

TY  - JOUR
AU  - Xia, Kun
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Hua, Gang
AU  - Tang, Wei
TI  - Boosting Semi-Supervised Temporal Action Localization by Learning from Non-Target Classes
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The crux of semi-supervised temporal action localization (SS-TAL) lies in excavating valuable information from abundant unlabeled videos. However, current approaches predominantly focus on building models that are robust to the error-prone target class (i.e, the predicted class with the highest confidence) while ignoring informative semantics within non-target classes. This paper approaches SS-TAL from a novel perspective by advocating for learning from non-target classes, transcending the conventional focus solely on the target class. The proposed approach involves partitioning the label space of the predicted class distribution into distinct subspaces: target class, positive classes, negative classes, and ambiguous classes, aiming to mine both positive and negative semantics that are absent in the target class, while excluding ambiguous classes. To this end, we first devise innovative strategies to adaptively select high-quality positive and negative classes from the label space, by modeling both the confidence and rank of a class in relation to those of the target class. Then, we introduce novel positive and negative losses designed to guide the learning process, pushing predictions closer to positive classes and away from negative classes. Finally, the positive and negative processes are integrated into a hybrid positive-negative learning framework, facilitating the utilization of non-target classes in both labeled and unlabeled videos. Experimental results on THUMOS14 and ActivityNet v1.3 demonstrate the superiority of the proposed method over prior state-of-the-art approaches.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2403.11189
AN  - PPRN:88197589
AD  - Xian Jiaotong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Wormpex AI Res, Seattle, WA, USA
AD  - Univ Illinois Chicago, Chicago, IL, USA
M2  - Wormpex AI Res
M2  - Univ Illinois Chicago
Y2  - 2024-04-11
ER  -

TY  - JOUR
AU  - Du, Jia-Run
AU  - Lin, Kun-Yu
AU  - Meng, Jingke
AU  - Zheng, Wei-Shi
TI  - Towards Completeness: A Generalizable Action Proposal Generator for Zero-Shot Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - To address the zero-shot temporal action localization (ZSTAL) task, existing works develop models that are generalizable to detect and classify actions from unseen categories. They typically develop a category-agnostic action detector and combine it with the Contrastive Language-Image Pre-training (CLIP) model to solve ZSTAL. However, these methods suffer from incomplete action proposals generated for unseen categories, since they follow a frame-level prediction paradigm and require hand-crafted post-processing to generate action proposals. To address this problem, in this work, we propose a novel model named Generalizable Action Proposal generator (GAP), which can interface seamlessly with CLIP and generate action proposals in a holistic way. Our GAP is built in a query-based architecture and trained with a proposallevel objective, enabling it to estimate proposal completeness and eliminate the hand-crafted post-pro cessing. Based on this architecture, we propose an Action-aware Discrimination loss to enhance the categoryagnostic dynamic information of actions. Besides, we introduce a StaticDynamic Rectifying module that incorporates the generalizable static information from CLIP to refine the predicted proposals, which improves proposal completeness in a generalizable manner. Our experiments show that our GAP achieves state-of-the-art performance on two challenging ZSTAL benchmarks, i.e., Thumos14 and ActivityNet1.3. Specifically, our model obtains significant performance improvement over previous works on the two benchmarks, i.e., +3.2% and +3.4% average mAP, respectively. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.13777
AN  - PPRN:91542103
AD  - Sun Yat sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China
AD  - Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou, Peoples R China
AD  - Sun Yat sen Univ, Guangdong Prov Key Lab Informat Secur Technol, Guangzhou, Peoples R China
M2  - Minist Educ
Y2  - 2024-09-04
ER  -

TY  - CPAPER
AU  - Bu, Aojie
AU  - Zhang, Han
AU  - Li, Jun
AU  - Shi, Zhiping
A1  - IEEE COMPUTER SOC
TI  - A Two-stream Network with Spatial Long-range Modeling for Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE INTERNATIONAL CONFERENCES ON INTERNET OF THINGS, ITHINGS IEEE GREEN COMPUTING AND COMMUNICATIONS, GREENCOM IEEE CYBER, PHYSICAL AND SOCIAL COMPUTING, CPSCOM IEEE SMART DATA, SMARTDATA AND IEEE CONGRESS ON CYBERMATICS,CYBERMATICS
M3  - Proceedings Paper
CP  - International Conferences on Internet of Things (iThings) / Green Computing and Communications (GreenCom) / Cyber, Physical and Social Computing (CPSCom) / Smart Data, SmartData and Congress on Cybermatics (Cybermatics)
CL  - Danzhou, PEOPLES R CHINA
AB  - This paper focuses on weakly supervised temporal action localization, a critical issue within the field of Artificial Intelligence (AI) in the context of the Internet of Things (IoT). The objective is to identify and locate action segments within untrimmed videos, which are solely trained using video-level action labels. Current visual perception and analysis techniques in the IoT face substantial challenges, including a significant amount of noise in videos, the complexity of backgrounds, and the absence of clear motion boundaries in lengthy video sequences. In this paper, we fully recognize that each frame space contains a wealth of information that contributes to classification and localization tasks. Therefore, we propose a method based on modeling features using spatial information within long video frames, aiming to more accurately locate action boundaries and achieve perception of complex data within IoT environments. Additionally, we employ traditional features within our model to alleviate noise issues typically encountered in lengthy videos. Traditional and deep features are processed separately, constituting a two-stream network within the entire model. We conducted comprehensive experiments on the widely used THUMOS14 dataset, demonstrating significant improvements compared to state-of-the-art methods. Furthermore, we conducted experiments on the ActivityNet v1.2 dataset, where traditional features were omitted. The experiments revealed that even without traditional features, our module and two-stream network strategy remained effective within IoT environments.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2836-3698
SN  - 979-8-3503-0946-1
DA  - 2024 
PY  - 2024
SP  - 115
EP  - 120
DO  - 10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00043
AN  - WOS:001228341300037
AD  - Capital Normal Univ, Informat Engn Coll, Beijing, Peoples R China
Y2  - 2024-07-27
ER  -

TY  - JOUR
AU  - Qu, Sanqing
AU  - Chen, Guang
AU  - Li, Zhijun
AU  - Zhang, Lijun
AU  - Lu, Fan
AU  - Knoll, Alois
TI  - ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to localize action instances temporal boundary and identify the corresponding action category with only video-level labels. Traditional methods mainly focus on foreground and background frames separation with only a single attention branch and class activation sequence. However, we argue that apart from the distinctive foreground and background frames there are plenty of semantically ambiguous action context frames. It does not make sense to group those context frames to the same background class since they are semantically related to a specific action category. Consequently, it is challenging to suppress action context frames with only a single class activation sequence. To address this issue, in this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. Then based on the obtained three-branch attention values, we construct three-branch class activation sequences to represent the action instances, contexts, and non-action backgrounds, individually. To evaluate the effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method can outperform current state-of-the-art methods, and even achieve comparable performance with fully-supervised methods. Code can be found at https://github.com/ispc-lab/ACM-Net
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2104.02967
AN  - PPRN:11587784
AD  - Tongji Univ, Dept Automot Engn, Shanghai, Peoples R China
AD  - Tech Univ Munich, Robot Artificial Intelligence & Embedded Syst, Munich, Germany
AD  - Univ Sci & Technol China, Dept Automation, Hefei, Anhui, Peoples R China
M2  - Tech Univ Munich
M2  - Univ Sci & Technol China
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Bock, Marius
AU  - Moeller, Michael
AU  - Van Laerhoven, Kristof
TI  - Temporal Action Localization for Inertial-based Human Activity Recognition
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - As of today, state-of-the-art activity recognition from wearable sensors relies on algorithms being trained to classify fixed windows of data. In contrast, video-based Human Activity Recognition, known as Temporal Action Localization (TAL), has followed a segment-based prediction approach, localizing activity segments in a timeline of arbitrary length. This paper is the first to systematically demonstrate the applicability of state-of-the-art TAL models for both offline and near-online Human Activity Recognition (HAR) using raw inertial data as well as pre-extracted latent features as input. Offline prediction results show that TAL models are able to outperform popular inertial models on a multitude of HAR benchmark datasets, with improvements reaching as much as 26% in F1-score. We show that by analyzing timelines as a whole, TAL models can produce more coherent segments and achieve higher NULL-class accuracy across all datasets. We demonstrate that TAL is less suited for the immediate classification of small-sized windows of data, yet offers an interesting perspective on inertial-based HAR – alleviating the need for fixed-size windows and enabling algorithms to recognize activities of arbitrary length. With design choices and training concepts yet to be explored, we argue that TAL architectures could be of significant value to the inertial-based HAR community. The code and data download to reproduce experiments is publicly available via github.com/mariusbock/tal_for_har.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2311.15831
AN  - PPRN:86294404
AD  - Univ Siegen, Ubiquitous Comp, Comp Vis, Siegen, Germany
AD  - Univ Siegen, Comp Vis, Siegen, Germany
AD  - Univ Siegen, Ubiquitous Comp, Siegen, Germany
Y2  - 2024-11-06
ER  -

TY  - JOUR
AU  - Wang, Peng
AU  - Zeng, Fanwei
AU  - Qian, Yuntao
TI  - A Survey on Deep Learning-based Spatio-temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatio-temporal action detection (STAD) aims to classify the actions present in a video and localize them in space and time. It has become a particularly active area of research in computer vision because of its explosively emerging real-world applications, such as autonomous driving, visual surveillance, entertainment, etc. Many efforts have been devoted in recent years to building a robust and effective framework for STAD. This paper provides a comprehensive review of the state-of-the-art deep learning-based methods for STAD. Firstly, a taxonomy is developed to organize these methods. Next, the linking algorithms, which aim to associate the frame- or clip-level detection results together to form action tubes, are reviewed. Then, the commonly used benchmark datasets and evaluation metrics are introduced, and the performance of state-of-the-art models is compared. At last, this paper is concluded, and a set of potential research directions of STAD are discussed.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2308.01618
AN  - PPRN:74244254
AD  - Zhejiang Univ, Coll Comp Sci, Hangzhou 310007, Zhejiang, Peoples R China
AD  - Ant Grp, Hangzhou 310007, Zhejiang, Peoples R China
M2  - Ant Grp
Y2  - 2023-08-19
ER  -

TY  - CPAPER
AU  - Li, Zhi
AU  - He, Lu
AU  - Xu, Huijuan
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions
T2  - COMPUTER VISION, ECCV 2022, PT X
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weakly-supervised models for general action detection cannot perform well in the fine-grained setting. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20079-3
SN  - 978-3-031-20080-9
DA  - 2022 
PY  - 2022
VL  - 13670
SP  - 567
EP  - 584
DO  - 10.1007/978-3-031-20080-9_33
AN  - WOS:000897089200033
AD  - Univ Calif Berkeley, Berkeley, CA 94720 USA
AD  - Tencent Amer, Palo Alto, CA USA
AD  - Penn State Univ, University Pk, PA USA
M2  - Tencent Amer
Y2  - 2023-01-21
ER  -

TY  - JOUR
AU  - Huang, Wei-Jhe
AU  - Yeh, Jheng-Hsien
AU  - Chen, Min-Hung
AU  - Faure, Gueter Josmy
AU  - Lai, Shang-Hong
TI  - Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achieving excellent accuracy for zero-shot spatio-temporal action detection.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.04688
AN  - PPRN:58040042
AD  - Natl Tsing Hua Univ, Hsinchu, Taiwan
AD  - NVIDIA, Santa Clara, CA, USA
AD  - Natl Taiwan Univ, Taipei, Taiwan
M2  - NVIDIA
M2  - Natl Taiwan Univ
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Gao, Jiyang
AU  - Yang, Zhenheng
AU  - Nevatia, Ram
TI  - Cascaded Boundary Regression for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1705.01180
AN  - PPRN:22212993
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Fang, Zhenying
AU  - Yu, Jun
AU  - Hong, Richang
TI  - Boundary Discretization and Reliable Classification Network for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 4
AB  - Temporal action detection aims to recognize the action category and determine each action instance's starting and ending time in untrimmed videos. The mixed methods have achieved remarkable performance by seamlessly merging anchor-based and anchor-free approaches. Nonetheless, there are still two crucial issues within the mixed framework: (1) Brute-force merging and handcrafted anchor design hinder the substantial potential and practicality of the mixed methods. (2) Within-category predictions show a significant abundance of false positives. In this paper, we propose a novel Boundary Discretization and Reliable Classification Network (BDRC-Net) that addresses the issues above by introducing boundary discretization and reliable classification modules. Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, eliminating the need for the traditional handcrafted anchor design. Furthermore, the reliable classification module (RCM) predicts reliable global action categories to reduce false positives. Extensive experiments conducted on different benchmarks demonstrate that our proposed method achieves competitive detection performance. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2310.06403
AN  - PPRN:85521410
AD  - Hefei Univ Technol, Sch Comp & Informat Engn, Hefei 188065, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China
M2  - Hangzhou Dianzi Univ
Y2  - 2024-11-20
ER  -

TY  - JOUR
AU  - Zhou, Cheng
AU  - Wang, Yuxiang
AU  - You, Ke
AU  - Wang, Rubin
TI  - In-vehicle vision-based automatic identification of bulldozer operation cycles with temporal action detection
T2  - ADVANCED ENGINEERING INFORMATICS
M3  - Article
AB  - Automated monitoring of bulldozer operation cycles is crucial for efficient productivity assessment and precise construction management. Harsh earthwork environments and complex, variable operation processes present challenges for identifying these cycles. To address this issue, we developed a multiscale temporal feature fusion and dual attention mechanism-based temporal action detection model (FDA-AFSD) for the automatic identification of bulldozer operation cycles from in to vehicle vision. This model enhances long-term sequence modeling, key temporal information learning, and precise action boundary identification through its multiscale temporal feature fusion structure, dual attention mechanism module, and scalable granularity perception (SGP) layer. In tests for earth levelling and mine edge dumping operations, the average detection accuracy (mAP) for bulldozer operation cycles reached 90.9%. Furthermore, under various adverse weather conditions and diverse construction processes, the model maintained stable and excellent detection capabilities, demonstrating its feasibility and practical application value.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 1474-0346
SN  - 1873-5320
DA  - 2024 OCT
PY  - 2024
VL  - 62
C7  - 102899
DO  - 10.1016/j.aei.2024.102899
AN  - WOS:001347979500001
C6  - OCT 2024
AD  - Huazhong Univ Sci & Technol, Natl Ctr Technol Innovat Digital Construct, Wuhan, Hubei, Peoples R China
AD  - Huazhong Univ Sci & Technol, Sch Civil & Hydraul Engn, Wuhan, Hubei, Peoples R China
AD  - Huazhong Univ Sci & Technol, Inst Artificial Intelligence, Wuhan, Hubei, Peoples R China
Y2  - 2024-11-11
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Zheng, Ziwei
AU  - Han, Yizeng
AU  - Cheng, Hao
AU  - Song, Shiji
AU  - Huang, Gao
AU  - Li, Fan
TI  - DyFADet: Dynamic Feature Aggregation for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recent proposed neural network-based Temporal Action Detection (TAD) models are inherently limited to extracting the discriminative representations and modeling action instances with various lengths from complex scenes by shared-weights detection heads. Inspired by the successes in dynamic neural networks, in this paper, we build a novel dynamic feature aggregation (DFA) module that can simultaneously adapt kernel weights and receptive fields at different timestamps. Based on DFA, the proposed dynamic encoder layer aggregates the temporal features within the action time ranges and guarantees the discriminability of the extracted representations. Moreover, using DFA helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the multi-scale features with adjusted parameters and learned receptive fields better to detect the action instances with diverse ranges from videos. With the proposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves promising performance on a series of challenging TAD benchmarks, including HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment QueriesV1.0, and FineAction.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.03197
AN  - PPRN:90691804
AD  - Xian Jiaotong Univ, Xi an, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - HKUST GZ, Guangzhou, Peoples R China
AD  - Tsinghua Univ, Beijing, Peoples R China
M2  - Xian Jiaotong Univ
M2  - HKUST GZ
M2  - Tsinghua Univ
Y2  - 2024-12-06
ER  -

TY  - JOUR
AU  - Wang, Peng
AU  - Zeng, Fanwei
AU  - Qian, Yuntao
TI  - A survey on deep learning-based spatio-temporal action detection
T2  - INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION PROCESSING
M3  - Article
AB  - Spatio-temporal action detection (STAD) aims to classify the actions present in a video and localize them in space and time. It has become a particularly active area of research in computer vision because of its explosively emerging real-world applications, such as autonomous driving, visual surveillance and entertainment. Many efforts have been devoted in recent years to build a robust and effective framework for STAD. This paper provides a comprehensive review of the state-of-the-art deep learning-based methods for STAD. First, a taxonomy is developed to organize these methods. Next, the linking algorithms, which aim to associate the frame- or clip-level detection results together to form action tubes, are reviewed. Then, the commonly used benchmark datasets and evaluation metrics are introduced, and the performance of state-of-the-art models is compared. At last, this paper is concluded, and a set of potential research directions of STAD are discussed.
PU  - WORLD SCIENTIFIC PUBL CO PTE LTD
PI  - SINGAPORE
PA  - 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN  - 0219-6913
SN  - 1793-690X
DA  - 2024 JUL
PY  - 2024
VL  - 22
IS  - 04
DO  - 10.1142/S0219691323500662
AN  - WOS:001162174400002
C6  - FEB 2024
AD  - Zhejiang Univ, Coll Comp Sci, Hangzhou 310007, Zhejiang, Peoples R China
AD  - Ant Grp, Hangzhou 310007, Zhejiang, Peoples R China
Y2  - 2024-02-27
ER  -

TY  - JOUR
AU  - Lee, Sangyoun
AU  - Jung, Juho
AU  - Oh, Changdae
AU  - Yun, Sunghee
TI  - Enhancing Temporal Action Localization: Advanced S6 Modeling with Recurrent Mechanism
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) is a critical task in video analysis, identifying precise start and end times of actions. Existing methods like CNNs, RNNs, GCNs, and Transformers have limitations in capturing long-range dependencies and temporal causality. To address these challenges, we propose a novel TAL architecture leveraging the Selective State Space Model (S6). Our approach integrates the Feature Aggregated Bi-S6 block, Dual Bi-S6 structure, and a recurrent mechanism to enhance temporal and channel-wise dependency modeling without increasing parameter complexity. Extensive experiments on benchmark datasets demonstrate state-of-the-art results with mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Ablation studies validate our method's effectiveness, showing that the Dual structure in the Stem module and the recurrent mechanism outperform traditional approaches. Our findings demonstrate the potential of S6-based models in TAL tasks, paving the way for future research.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.13078
AN  - PPRN:90881293
AD  - Sogang Univ, Seoul, South Korea
AD  - Sungkyunkwan Univ, Seoul, South Korea
AD  - Univ Wisconsin Madison, Madison, WI, USA
AD  - Erudio Bio, Palo Alto, CA, USA
M2  - Sogang Univ
M2  - Univ Wisconsin Madison
M2  - Erudio Bio
Y2  - 2024-07-26
ER  -

TY  - JOUR
AU  - Chen, Yaosen
AU  - Guo, Bing
AU  - Shen, Yan
AU  - Wang, Wei
AU  - Lu, Weichen
AU  - Suo, Xinhua
TI  - Capsule Boundary Network With 3D Convolutional Dynamic Routing for Temporal Action Detection
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Temporal action detection is a challenging task in video understanding, due to the complexity of the background and rich action content impacting high-quality temporal proposals generation in untrimmed videos. Capsule networks can avoid some limitations of the invariance caused by pooling and inability from convolutional neural networks, which can better understand the temporal relations for temporal action detection. However, because of the extremely computationally expensive procedure, capsule network is difficult to be applied to the task of temporal action detection. To address this issue, this paper proposes a novel U-shaped capsule network framework with a k-Nearest Neighbor (k-NN) mechanism of 3D convolutional dynamic routing, which we named U-BlockConvCaps. Furthermore, we build a Capsules Boundary Network (CapsBoundNet) based on U-BlockConvCaps for dense temporal action proposal generation. Specifically, the first module is one 1D convolutional layer for fusing the two-stream with RGB and optical flow video features. The sampling module further processes the fused features to generate the 2D start-end action proposal feature maps. Then, the multi-scale U-Block convolutional capsule module with 3D convolutional dynamic routing is used to process the proposal feature map. Finally, the feature maps generated from the CapsBoundNet are used to predict starting, ending, action classification, and action regression score maps, which help to capture the boundary and intersection over union features. Our work innovatively improves the dynamic routing algorithm of capsule networks and extends the use of capsule networks to the temporal action detection task for the first time in the literature. The experimental results on benchmarks THUMOS14 show that the performance of CapsBoundNet is obviously beyond the state-of-the-art methods, e.g., the mAP@tIoU = 0.3, 0.4, 0.5 on THUMOS14 are improved from 63.6% to 70.0%, 57.8% to 63.1%, 51.3% to 52.9%, respectively. We also got competitive results on the action detection dataset of ActivityNet1.3.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2022 MAY
PY  - 2022
VL  - 32
IS  - 5
SP  - 2962
EP  - 2975
DO  - 10.1109/TCSVT.2021.3104226
AN  - WOS:000790830300039
AD  - Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China
AD  - Chengdu Sobey Digital Technol Co Ltd, Chengdu 610041, Sichuan, Peoples R China
AD  - Chengdu Univ Informat Technol, Sch Comp Sci, Chengdu 610225, Sichuan, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Peoples R China
M2  - Chengdu Sobey Digital Technol Co Ltd
Y2  - 2022-05-15
ER  -

TY  - JOUR
AU  - Wan, Herun
AU  - Luo, Minnan
AU  - Li, Zhihui
AU  - Wang, Yang
TI  - TEST: Temporal-spatial separated transformer for temporal action localization
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal action localization is a fundamental task in video understanding. Existing methods fall into three categories: anchor-based, actionness-guided, and anchor-free. Anchor-based and actionness-guided models need huge computation resources to process redundant proposals or enumerate every possible proposal. Anchor- free models with lighter parameters become a more attractive option as temporal actions become more complex. However, they typically struggle to achieve high performance due to the need to aggregate global temporal-spatial features at every time step. To overcome this limitation, we design three efficient transformer- based architectures, bringing two advantages: (i) the global receptive field of transformers enables models to aggregate spatial and temporal at each time step, and (ii) the transformers could capture the moment-level feature, enhancing localization performance. Our designed architectures are adapted to any framework, thus we propose a simple but effective anchor-free framework named TEST. Compared to strong baselines, TEST achieves 0.96% to 3.20% improvement on two real-world datasets. Meanwhile, it improves time efficiency by 1.36 times and space efficiency by 1.08 times. Further experiments prove the effectiveness of TEST's modules. Implementation of our work is available at https://github.com/whr000001/TeST.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2025 JAN 21
PY  - 2025
VL  - 614
C7  - 128688
DO  - 10.1016/j.neucom.2024.128688
AN  - WOS:001358329300001
C6  - NOV 2024
AD  - Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China
AD  - Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China
AD  - Xi An Jiao Tong Univ, Shaanxi Prov Key Lab Big Data Knowledge Engn, Xian 710049, Peoples R China
AD  - Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China
AD  - Xi An Jiao Tong Univ, Sch Continuing Educ, Xian 710049, Peoples R China
Y2  - 2024-11-24
ER  -

TY  - CPAPER
AU  - Ren, Haoran
AU  - Ren, Hao
AU  - Lu, Hong
AU  - Jin, Cheng
ED  - Dang-Nguyen, DT
ED  - Gurrin, C
ED  - Larson, M
ED  - Smeaton, AF
ED  - Rudinac, S
ED  - Dao, MS
ED  - Trattner, C
ED  - Chen, P
TI  - Weakly-Supervised Temporal Action Localization with Regional Similarity Consistency
T2  - MULTIMEDIA MODELING, MMM 2023, PT I
M3  - Proceedings Paper
CP  - 29th International Conference on MultiMedia Modeling (MMM)
CL  - Bergen, NORWAY
AB  - The weakly-supervised temporal action localization task aims to train a model that can accurately locate each action instance in the video using only video-level class labels. The existing methods take into account the information of different modalities (primarily RGB and Flow), and present numerous multi-modal complementary methods. RGB features are obtained by calculating appearance information, which are easy to be disrupted by the background. On the contrary, Flow features are obtained by calculating motion information, which are usually less disrupted by the background. Based on this phenomenon, we propose a Regional Similarity Consistency (RSC) constraint between these two modalities to suppress the disturbance of background in RGB features. Specifically, we calculate the regional similarity matrices of RGB and Flow features, and impose the consistency constraint through L-2 loss. To verify the effectiveness of our method, we integrate the proposed RSC constraint into three recent methods. The comprehensive experimental results show that the proposed RSC constraint can boost the performance of these methods, and achieve the state-of-the-art results on the widely-used THUMOS14 and ActivityNet1.2 datasets.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-27076-5
SN  - 978-3-031-27077-2
DA  - 2023 
PY  - 2023
VL  - 13833
SP  - 69
EP  - 81
DO  - 10.1007/978-3-031-27077-2_6
AN  - WOS:000996563000006
AD  - Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai, Peoples R China
Y2  - 2023-06-14
ER  -

TY  - CPAPER
AU  - He, Yuanpeng
AU  - Li, Lijian
AU  - Zhan, Tianxiang
AU  - Jiao, Wenpin
AU  - Pun, Chi-Man
A1  - IEEE
TI  - GENERALIZED UNCERTAINTY-BASED EVIDENTIAL FUSION WITH HYBRID MULTI-HEAD ATTENTION FOR WEAK-SUPERVISED TEMPORAL ACTION LOCALIZATION
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING, ICASSP 2024
M3  - Proceedings Paper
CP  - 49th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
CL  - Seoul, SOUTH KOREA
AB  - Weakly supervised temporal action localization (WS-TAL) is a task of targeting at localizing complete action instances and categorizing them with video-level labels. Action-background ambiguity, primarily caused by background noise resulting from aggregation and intra-action variation, is a significant challenge for existing WS-TAL methods. In this paper, we introduce a hybrid multi-head attention (HMHA) module and generalized uncertainty-based evidential fusion (GUEF) module to address the problem. The proposed HMHA effectively enhances RGB and optical flow features by filtering redundant information and adjusting their feature distribution to better align with the WS-TAL task. Additionally, the proposed GUEF adaptively eliminates the interference of background noise by fusing snippet-level evidences to refine uncertainty measurement and select superior foreground feature information, which enables the model to concentrate on integral action instances to achieve better action localization and classification performance. Experimental results conducted on the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art methods. Our code is available in https://github. com/heyuanpengpku/GUEF/tree/main.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 979-8-3503-4486-8
SN  - 979-8-3503-4485-1
DA  - 2024 
PY  - 2024
SP  - 3855
EP  - 3859
DO  - 10.1109/ICASSP48485.2024.10446799
AN  - WOS:001285850004023
AD  - Peking Univ, Key Lab High Confidence Software Technol, Minist Educ, Beijing 100871, Peoples R China
AD  - Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China
AD  - Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China
Y2  - 2025-03-02
ER  -

TY  - JOUR
AU  - Chen, Peihao
AU  - Gan, Chuang
AU  - Shen, Guangyao
AU  - Huang, Wenbing
AU  - Zeng, Runhao
AU  - Tan, Mingkui
TI  - Relation Attention for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Temporal action localization aims to accurately localize and recognize all possible action instances from an untrimmed video automatically. Most existing methods perform this task by first generating a set of proposals and then recognizing each independently. However, due to the complex structures and large content variations in action instances, recognizing them individually can be difficult. Fortunately, some proposals often share information regarding one specific action. Such information, which is ignored in existing methods, can be used to boost recognition performance. In this paper, we propose a novel mechanism, called relation attention, to exploit informative relations among proposals based on their appearance or optical flowfeatures. Specifically, we propose a relation attention module to enhance representation power by capturing useful information from other proposals. This module does not change the dimensions of the original input and output and does not rely on any specific proposal generation methods or feature extraction backbone networks. Experimental results show that the proposed relation attention mechanism improves performance significantly on both Thumos14 and ActivityNet1.3 datasets compared to existing architectures. For example, relying on Structured Segment Networks (SSN), the proposed relation attention module helps to increase the mAP from 41.4 to 43.7 on the Thumos14 dataset and outperforms the state-of-the-art results.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2020 OCT
PY  - 2020
VL  - 22
IS  - 10
SP  - 2723
EP  - 2733
DO  - 10.1109/TMM.2019.2959977
AN  - WOS:000572628000016
AD  - South China Univ Technol, Sch Software Engn, Guangzhou 510630, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518000, Peoples R China
AD  - MIT, IBM Watson AI Lab, Cambridge, MA 02142 USA
AD  - Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China
Y2  - 2020-10-08
ER  -

TY  - JOUR
AU  - Liu, Liang
AU  - Liu, Yong
AU  - Zhang, Jiangning
TI  - Learning-Based Hand Motion Capture and Understanding in Assembly Process
T2  - IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS
M3  - Article
AB  - Manual assembly is still an essential part in modern manufacturing. Understanding the actual state of the assembly process can not only improve quality control of products, but also collect comprehensive data for production planning and proficiency assessments. Addressing the rising complexity led by the uncertainty in manual assembly, this paper presents an efficient approach to automatically capture and analyze hand operations in the assembly process. In this paper, a detection-based tracking method is introduced to capture trajectories of hand movement from the camera installed in each workstation. Then, the actions in hand trajectories are identified with a novel temporal action localization model. The experimental results have proved that our method reached the application level with high accuracy and a low computational cost. The proposed system is lightweight enough to be quickly set up on an embedded computing device for real-time online inference and on a cloud server for offline analysis as well.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0278-0046
SN  - 1557-9948
DA  - 2019 DEC
PY  - 2019
VL  - 66
IS  - 12
SP  - 9703
EP  - 9712
DO  - 10.1109/TIE.2018.2884206
AN  - WOS:000480309400060
AD  - Zhejiang Univ, Inst Cyber Syst & Control, Hangzhou 310027, Zhejiang, Peoples R China
AD  - Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Zhejiang, Peoples R China
Y2  - 2020-09-22
ER  -

TY  - JOUR
AU  - Zhang, Songchun
AU  - Zhao, Chunhui
TI  - Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2308.12609
AN  - PPRN:83906908
AD  - Zhejiang Univ, Coll Control Sci & Engn, Hangzhou 310027, Peoples R China
Y2  - 2023-12-21
ER  -

TY  - JOUR
AU  - Zeng, Runhao
AU  - Huang, Wenbing
AU  - Tan, Mingkui
AU  - Rong, Yu
AU  - Zhao, Peilin
AU  - Huang, Junzhou
AU  - Gan, Chuang
TI  - Graph Convolutional Module for Temporal Action Localization in Videos
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Temporal action localization, which requires a machine to recognize the location as well as the category of action instances in videos, has long been researched in computer vision. The main challenge of temporal action localization lies in that videos are usually long and untrimmed with diverse action contents involved. Existing state-of-the-art action localization methods divide each video into multiple action units (i.e., proposals in two-stage methods and segments in one-stage methods) and then perform action recognition/regression on each of them individually, without explicitly exploiting their relations during learning. In this paper, we claim that the relations between action units play an important role in action localization, and a more powerful action detector should not only capture the local content of each action unit but also allow a wider field of view on the context related to it. To this end, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. To be specific, we first construct a graph, where each action unit is represented as a node and their relations between two action units as an edge. Here, we use two types of relations, one for capturing the temporal connections between different action units, and the other one for characterizing their semantic relationship. Particularly for the temporal connections in two-stage methods, we further explore two different kinds of edges, one connecting the overlapping action units and the other one connecting surrounding but disjointed units. Upon the graph we built, we then apply graph convolutional networks (GCNs) to model the relations among different action units, which is able to learn more informative representations to enhance action localization. Experimental results show that our GCM consistently improves the performance of existing action localization methods, including two-stage methods (e.g., CBR [15] and R-C3D [47]) and one-stage methods (e.g., D-SSAD [22]), verifying the generality and effectiveness of our GCM. Moreover, with the aid of GCM, our approach significantly outperforms the state-of-the-art on THUMOS14 (50.9 percent versus 42.8 percent). Augmentation experiments on ActivityNet also verify the efficacy of modeling the relationships between action units. The source code and the pre-trained models are available at https://github.com/Alvin-Zeng/GCM.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2022 OCT 1
PY  - 2022
VL  - 44
IS  - 10
SP  - 6209
EP  - 6223
DO  - 10.1109/TPAMI.2021.3090167
AN  - WOS:000853875300028
AD  - South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China
AD  - South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Peoples R China
AD  - Pazhou Lab, Guangzhou 510335, Guangdong, Peoples R China
AD  - Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China
AD  - Tsinghua Natl Lab Informat Sci & Technol TNList, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China
AD  - Tencent AI Lab, Shenzhen, Peoples R China
AD  - MIT IBM Watson AI Lab, Cambridge, MA 02142 USA
M2  - MIT IBM Watson AI Lab
Y2  - 2022-09-26
ER  -

TY  - CPAPER
AU  - Gao, Zan
AU  - Cui, Xinglei
AU  - Zhao, Yibo
AU  - Zhuo, Tao
AU  - Guan, Weili
AU  - Wang, Meng
A1  - ACM
TI  - A Novel Temporal Channel Enhancement and Contextual Excavation Network for Temporal Action Localization
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - The temporal action localization (TAL) task aims to locate and classify action instances in untrimmed videos. Most previous methods use classifiers and locators to act on the same feature; thus, the classification and localization processes are relatively independent. Therefore, if the classification results and localization results are fused, there will be a problem that the classification results are correct while the localization results are wrong, resulting in inaccurate final results, and vice versa. To solve this problem, we propose a novel temporal channel enhancement and contextual excavation network (TCN) for the TAL task, which generates robust classification and localization features and refines the final localization results. Specifically, a temporal channel enhancement module is designed to enhance the temporal and channel information of the feature sequence. Then, the temporal semantic contextual excavation module is developed to establish relationships between similar frames. Finally, the features with enhanced contextual information are transferred to a classifier. While executing the classification process, we obtain powerful classification features. Most importantly, with the robust classification features, the final localization features are produced by the refine localization module, which is applied to obtain the final localization results. Extensive experiments show that TCN can outperform all the SOTA methods on the THUMOS14 dataset, and achieves a comparable performance on the ActivityNet1.3 dataset. Compared with ActionFormer (ECCV 2022) and BREM (MM 2022) on the THUMOS14 dataset, the proposed TCN can achieve improvements of 1.8% and 5.0%, respectively.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 6724
EP  - 6733
DO  - 10.1145/3581783.3612167
AN  - WOS:001199449106073
AD  - Qilu Univ Technol, Shandong Artificial Intelligence Inst, Jinan, Peoples R China
AD  - Tianjin Univ Technol, Sch Comp Sci & Technol, Tianjin, Peoples R China
AD  - Monash Univ, Sch Informat Technol, Melbourne, Vic, Australia
Y2  - 2024-07-31
ER  -

TY  - JOUR
AU  - Gao, Zan
AU  - Cui, Xinglei
AU  - Zhuo, Tao
AU  - Cheng, Zhiyong
AU  - Liu, An-An
AU  - Wang, Meng
AU  - Chen, Shenyong
AU  - Chen, Shenyong
TI  - Temporal Action Localization with Multi-temporal Scales
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization plays an important role in video analysis, which aims to localize and classify actions in untrimmed videos. The previous methods often predict actions on a feature space of a single-temporal scale. However, the temporal features of a low-level scale lack enough semantics for action classification while a high-level scale cannot provide rich details of the action boundaries. To address this issue, we propose to predict actions on a feature space of multi-temporal scales. Specifically, we use refined feature pyramids of different scales to pass semantics from high-level scales to low-level scales. Besides, to establish the long temporal scale of the entire video, we use a spatial-temporal transformer encoder to capture the long-range dependencies of video frames. Then the refined features with long-range dependencies are fed into a classifier for the coarse action prediction. Finally, to further improve the prediction accuracy, we propose to use a frame-level self attention module to refine the classification and boundaries of each action instance. Extensive experiments show that the proposed method can outperform state-of-the-art approaches on the THUMOS14 dataset and achieves comparable performance on the ActivityNet1.3 dataset. Compared with A2Net (TIP20, Avg\{0.3:0.7\}), Sub-Action (CSVT2022, Avg\{0.1:0.5\}), and AFSD (CVPR21, Avg\{0.3:0.7\}) on the THUMOS14 dataset, the proposed method can achieve improvements of 12.6\%, 17.4\% and 2.2\%, respectively
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.07493
AN  - PPRN:12216281
AD  - Qilu Univ Technol, Shandong Acad Sci, Shandong Artificial Intelligence Inst, Jinan 250014, Peoples R China
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China
AD  - Tianjin Univ Technol, Minist Educ, Key Lab Comp Vis & Syst, Tianjin 300384, Peoples R China
M2  - Qilu Univ Technol
M2  - Tianjin Univ
M2  - Tianjin Univ Technol
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Pan, Xiaoying
AU  - Zhang, Nijuan
AU  - Xie, Hewei
AU  - Li, Shoukun
AU  - Feng, Tong
TI  - MBGNet:Multi-branch boundary generation network with temporal context aggregation for temporal action detection
T2  - APPLIED INTELLIGENCE
M3  - Article
AB  - Temporal action detection is an important and fundamental video understanding task that aims to locate the temporal regions where human actions or events may occur and to identify the classes of actions in untrimmed videos. The main challenge of temporal action detection is that videos are usually of different durations and untrimmed. Although existing methods have achieved better results in recent years, there are still some challenges, such as a lack of full utilisation of video context features, insufficient accuracy of generated action boundaries and failure to consider the relationship between proposals. To address the above issues, this paper proposes a Multi-branch Boundary Generation Network (MBGNet) with temporal context aggregation. It improves the performance of temporal action proposal generation by exploiting rich temporal context features and complementary boundary generators.First, we propose a multi-path temporal context feature aggregation (MTCA) module to exploit "local and global" contextual temporal features for the generation of temporal action proposals. Second, in order to generate accurate action boundaries, we design a multi-branch temporal boundary detector (MBG) to optimise the prediction results by exploiting the complementary relationship between the two boundary detectors.In addition, to accurately predict the confidence of densely distributed proposals, we design a proposal relation-aware module (PRAM) that exploits global correlation for proposal relationship modelling. Experiments on the popular datasets ActivityNet1.3, THUMOS14, and HACS demonstrate the effectiveness of the method proposed in this paper on the task of temporal action proposal generation, which can generate action proposals with high precision and recall. Moreover, combining with existing action classifiers can also achieve better performance in temporal action detection.These results demonstrate the effectiveness of the method in this paper in improving the accuracy of temporal action proposal generation and detection.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-669X
SN  - 1573-7497
DA  - 2024 OCT
PY  - 2024
VL  - 54
IS  - 19
SP  - 9045
EP  - 9066
DO  - 10.1007/s10489-024-05664-y
AN  - WOS:001268477300002
C6  - JUL 2024
AD  - Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Shaanxi, Peoples R China
AD  - Shaanxi Key Lab Network Data Anal & Intelligent Pr, Xian 710121, Shaanxi, Peoples R China
M2  - Shaanxi Key Lab Network Data Anal & Intelligent Pr
Y2  - 2024-07-22
ER  -

TY  - JOUR
AU  - Lin, Chenxiang
AU  - Ma, Teng
AU  - Wu, Fei
AU  - Qian, Jian
AU  - Liao, Feilong
AU  - Huang, Jinaye
TI  - Application of Temporal Action Detection Technology in Abnormal Event Detection of Surveillance Video
T2  - IEEE ACCESS
M3  - Article
AB  - By detecting abnormal violation event in surveillance videos, the safety management capabilities in high-risk power operations can be improved. This research constructs an intelligent abnormal event detection technology using deep learning algorithms, aiming to improve the detection accuracy of anomaly event. This research improves the parameter setting method and fully connected layer of three-dimensional convolutional networks to enhance their ability to recognize three-dimensional features. An improved algorithm is adopted as the basic structure of temporal action detection technology. Frame interpolation is applied to improve the accuracy of temporal action detection. A monitoring video anomaly event detection model based on the improved temporal action detection technology is established. The experiment outcomes show that the improved three-dimensional convolutional network achieves convergence after 32 iterations, with an accuracy of 99.15% and a recall rate of 98.3%. The average accuracy of the three datasets tested is better than other algorithms. The average precision of the research model for detecting throwing objects from high altitude, crossing fences, smoking, and checking electricity without gloves are 89.1%, 88.9%, 96.6%, and 96.2%, respectively. The accuracy for abnormal event detection of different time periods is superior to other models. The average recall value of the research model is 94.3%, which is higher than other models. The results indicate that the research model has the capacity to accurately recognize abnormal events in massive, diverse, and complex surveillance videos. The abnormal event detection model proposed in the study can be applied to the intelligent management platform of the power industry, thereby improving the safety management capability in power operations.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2025 
PY  - 2025
VL  - 13
SP  - 26958
EP  - 26972
DO  - 10.1109/ACCESS.2025.3533552
AN  - WOS:001422025700017
AD  - State Grid Fujian Elect Power Co Ltd, Elect Power Sci Res Inst, Fuzhou 350000, Fujian, Peoples R China
Y2  - 2025-02-21
ER  -

TY  - JOUR
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Li, Hongsheng
TI  - Multi-Modality Self-Distillation for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - As a challenging task of high-level video understanding, Weakly-supervised Temporal Action Localization (WTAL) has attracted increasing attention in recent years. However, due to the weak supervisions of whole-video classification labels, it is challenging to accurately determine action instance boundaries. To address this issue, pseudo-label-based methods [Alwassel et al. (2019), Luo et al. (2020), and Zhai et al. (2020)] were proposed to generate snippet-level pseudo labels from classification results. In spite of the promising performance, these methods hardly take full advantages of multiple modalities, i.e., RGB and optical flow sequences, to generate high quality pseudo labels. Most of them ignored how to mitigate the label noise, which hinders the capability of the network on learning discriminative feature representations. To address these challenges, we propose a Multi-Modality Self-Distillation (MMSD) framework, which contains two single-modal streams and a fused-modal stream to perform multi-modality knowledge distillation and multi-modality self-voting. On the one hand, multi-modality knowledge distillation improves snippet-level classification performance by transferring knowledge between single-modal streams and a fused-modal stream. On the other hand, multi-modality self-voting mitigates the label noise in a modality voting manner according to the reliability and complementarity of the streams. Experimental results on THUMOS14 and ActivityNet1.3 datasets demonstrate the effectiveness of our method and superior performance over state-of-the-art approaches. Our code is available at https://github.com/LeonHLJ/MMSD.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 1504
EP  - 1519
DO  - 10.1109/TIP.2021.3137649
AN  - WOS:000748370500006
AD  - Ctr Perceptual & Interact Intelligence CPII, Hong Kong, Peoples R China
AD  - Chinese Univ Hong Kong, Multimedia Lab, Hong Kong, Peoples R China
AD  - Chinese Acad Sci CASIA, Inst Automat, Beijing 100190, Peoples R China
M2  - Ctr Perceptual & Interact Intelligence CPII
Y2  - 2022-02-06
ER  -

TY  - JOUR
AU  - Li, Guozhang
AU  - Cheng, De
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Li, Jie
AU  - Gao, Xinbo
TI  - Weakly Supervised Temporal Action Localization With Bidirectional Semantic Consistency Constraint
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
M3  - Article
AB  - Weakly supervised temporal action localization (WTAL) aims to classify and localize temporal boundaries of actions for the video, given only video-level category labels in the training datasets. Due to the lack of boundary information during training, existing approaches formulate WTAL as a classification problem, i.e., generating the temporal class activation map (T-CAM) for localization. However, with only classification loss, the model would be suboptimized, i.e., the action-related scenes are enough to distinguish different class labels. Regarding other actions in the action-related scene (i.e., the scene same as positive actions) as co-scene actions, this suboptimized model would misclassify the co-scene actions as positive actions. To address this misclassification, we propose a simple yet efficient method, named bidirectional semantic consistency constraint (Bi-SCC), to discriminate the positive actions from co-scene actions. The proposed Bi-SCC first adopts a temporal context augmentation to generate an augmented video that breaks the correlation between positive actions and their co-scene actions in the inter-video. Then, a semantic consistency constraint (SCC) is used to enforce the predictions of the original video and augmented video to be consistent, hence suppressing the co-scene actions. However, we find that this augmented video would destroy the original temporal context. Simply applying the consistency constraint would affect the completeness of localized positive actions. Hence, we boost the SCC in a bidirectional way to suppress co-scene actions while ensuring the integrity of positive actions, by cross-supervising the original and augmented videos. Finally, our proposed Bi-SCC can be applied to current WTAL approaches and improve their performance. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet. The code is available at https://github.com/lgzlIlIlI/BiSCC.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2162-237X
SN  - 2162-2388
DA  - 2024 SEP
PY  - 2024
VL  - 35
IS  - 9
SP  - 13032
EP  - 13045
DO  - 10.1109/TNNLS.2023.3266062
AN  - WOS:000982504500001
C6  - MAY 2023
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Hong Kong Univ Sci & Technol, Sch Engn, Hong Kong, Peoples R China
AD  - Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
Y2  - 2023-05-24
ER  -

TY  - CPAPER
AU  - Wang, Haoying
AU  - Wei, Ping
AU  - Liu, Meiqin
AU  - Zheng, Nanning
ED  - Iliadis, L
ED  - Papaleonidas, A
ED  - Angelov, P
ED  - Jayne, C
TI  - Temporal Deformable Transformer for Action Localization
T2  - ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2023, PT VI
M3  - Proceedings Paper
CP  - 32nd International Conference on Artificial Neural Networks (ICANN)
CL  - Heraklion, GREECE
AB  - Temporal action localization (TAL) is a challenging task that has received significant attention in video understanding. Recently, Transformer-based models have demonstrated their effectiveness in capturing contextual information and achieved outstanding performance on various TAL benchmarks. However, these methods still face challenges in computational efficiency and contextual modeling rigidity. In this paper, we propose a method to address those problems in Transformer-based models. Our model introduces a temporal deformable Transformer module and the corresponding time normalization, enabling flexible aggregation of temporal context information in videos, leading to enhanced video representations. To demonstrate the effectiveness of the proposed method, we construct a Transformer-based anchor-free model with a simple prediction head, which yields superior performance on widely used benchmarks. Specifically, it achieves an average mAP of 67.4% on THUMOS14 and an average mAP of 36.8% on ActivityNet-v1.3.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-44222-3
SN  - 978-3-031-44223-0
DA  - 2023 
PY  - 2023
VL  - 14259
SP  - 563
EP  - 575
DO  - 10.1007/978-3-031-44223-0_45
AN  - WOS:001156951000045
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
Y2  - 2024-03-06
ER  -

TY  - JOUR
AU  - Su, Rui
AU  - Xu, Dong
AU  - Zhou, Luping
AU  - Ouyang, Wanli
TI  - Progressive Cross-Stream Cooperation in Spatial and Temporal Domain for Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Spatio-temporal action localization consists of three levels of tasks: spatial localization, action classification, and temporal localization. In this work, we propose a new progressive cross-stream cooperation (PCSC) framework that improves all three tasks above. The basic idea is to utilize both spatial region (resp., temporal segment proposals) and features from one stream (i.e., the Flow/RGB stream) to help another stream (i.e., the RGB/Flow stream) to iteratively generate better bounding boxes in the spatial domain (resp., temporal segments in the temporal domain). In this way, not only the actions could be more accurately localized both spatially and temporally, but also the action classes could be predicted more precisely. Specifically, we first combine the latest region proposals (for spatial detection) or segment proposals (for temporal localization) from both streams to form a larger set of labelled training samples to help learn better action detection or segment detection models. Second, to learn better representations, we also propose a new message passing approach to pass information from one stream to another stream, which also leads to better action detection and segment detection models. By first using our newly proposed PCSC framework for spatial localization at the frame-level and then applying our temporal PCSC framework for temporal localization at the tube-level, the action localization results are progressively improved at both the frame level and the video level. Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB demonstrate the effectiveness of our newly proposed approaches for spatio-temporal action localization in realistic scenarios.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2021 DEC 1
PY  - 2021
VL  - 43
IS  - 12
SP  - 4477
EP  - 4490
DO  - 10.1109/TPAMI.2020.2997860
AN  - WOS:000714203900024
AD  - Univ Sydney, Sch Elect & Informat Engn, Camperdown, NSW 2006, Australia
Y2  - 2021-11-13
ER  -

TY  - JOUR
AU  - Wu, Kewei
AU  - Luo, Wenjie
AU  - Xie, Zhao
AU  - Guo, Dan
AU  - Zhang, Zhao
AU  - Hong, Richang
TI  - Ensemble Prototype Network For Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
M3  - Article
AB  - Weakly supervised temporal action localization (TAL) aims to localize the action instances in untrimmed videos using only video-level action labels. Without snippet-level labels, this task should be hard to distinguish all snippets with accurate action/background categories. The main difficulties are the large variations brought by the unconstraint background snippets and multiple subactions in action snippets. The existing prototype model focuses on describing snippets by covering them with clusters (defined as prototypes). In this work, we argue that the clustered prototype covering snippets with simple variations still suffers from the misclassification of the snippets with large variations. We propose an ensemble prototype network (EPNet), which ensembles prototypes learned with consensus-aware clustering. The network stacks a consensus prototype learning (CPL) module and an ensemble snippet weight learning (ESWL) module as one stage and extends one stage to multiple stages in an ensemble learning way. The CPL module learns the consensus matrix by estimating the similarity of clustering labels between two successive clustering generations. The consensus matrix optimizes the clustering to learn consensus prototypes, which can predict the snippets with consensus labels. The ESWL module estimates the weights of the misclassified snippets using the snippet-level loss. The weights update the posterior probabilities of the snippets in the clustering to learn prototypes in the next stage. We use multiple stages to learn multiple prototypes, which can cover the snippets with large variations for accurate snippet classification. Extensive experiments show that our method achieves the state-of-the-art weakly supervised TAL methods on two benchmark datasets, that is, THUMOS'14, ActivityNet v1.2, and ActivityNet v1.3 datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2162-237X
SN  - 2162-2388
DA  - 2025 MAR
PY  - 2025
VL  - 36
IS  - 3
SP  - 4560
EP  - 4574
DO  - 10.1109/TNNLS.2024.3377468
AN  - WOS:001194014200001
C6  - MAR 2024
AD  - Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China
AD  - Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230039, Peoples R China
AD  - Anhui Zhonghuitong Technol Co Ltd, Hefei 230088, Peoples R China
M2  - Hefei Comprehens Natl Sci Ctr
M2  - Anhui Zhonghuitong Technol Co Ltd
Y2  - 2024-04-04
ER  -

TY  - JOUR
AU  - Dang, Yuanjie
AU  - Huang, Chunxia
AU  - Chen, Peng
AU  - Zhao, Dongdong
AU  - Gao, Nan
AU  - Liang, Ronghua
AU  - Huan, Ruohong
TI  - Discriminative Action Snippet Propagation Network for Weakly Supervised Temporal Action Localization
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Weakly supervised temporal action localization (WTAL) aims to classify and localize actions in untrimmed videos with only video-level labels. Recent studies have attempted to obtain more accurate temporal boundaries by exploiting latent action instances in ambiguous snippets or propagating representative action features. However, empirically handcrafted ambiguous snippet extraction and the imprecise alignment of representative snippet propagation lead to challenges in modeling the completeness of actions for these methods. In this article, we propose a Discriminative Action Snippet Propagation Network (DASP-Net) to accurately discover ambiguous snippets in videos and propagate discriminative instance-level features throughout the video for improving action completeness. Specifically, we introduce a novel discriminative feature propagation module for capturing the global contextual attention and propagating the action concept across the whole video by perceiving the discriminative action snippets with instance information from the same video. Simultaneously, we incorporate denoised pseudo-labels as supervision, where we correct the controversial prediction based on the feature space distribution during training, thereby alleviating false detection caused by noise background features. Furthermore, we design an ambiguous feature mining module, which maximizes the feature affinity information of action and background in ambiguous snippets to generate more accurate latent action and background snippets and learns more precise action instance boundaries through contrastive learning of action and background snippets. Extensive experiments show that DASP-Net achieves state-of-the-art results on THUMOS14 and ActivityNet1.2 datasets.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2024 JUN
PY  - 2024
VL  - 20
IS  - 6
C7  - 180
DO  - 10.1145/3643815
AN  - WOS:001208681800030
AD  - Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China
Y2  - 2024-05-16
ER  -

TY  - CPAPER
AU  - Huang, Wei-Jhe
AU  - Yeh, Jheng-Hsien
AU  - Chen, Min-Hung
AU  - Faure, Gueter Josmy
AU  - Lai, Shang-Hong
A1  - IEEE
TI  - Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS, ICCVW
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text feature. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achieving excellent accuracy for zero-shot spatio-temporal action detection. The code will be available at https://github.com/webber2933/iCLIP.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2473-9936
SN  - 979-8-3503-0744-3
DA  - 2023 
PY  - 2023
SP  - 284
EP  - 293
DO  - 10.1109/ICCVW60793.2023.00036
AN  - WOS:001156680300030
AD  - Natl Tsing Hua Univ, Hsinchu, Taiwan
AD  - NVIDIA, Santa Clara, CA USA
AD  - Natl Taiwan Univ, Taipei, Taiwan
Y2  - 2024-03-21
ER  -

TY  - JOUR
AU  - Song, Lin
AU  - Zhang, Shiwei
AU  - Yu, Gang
AU  - Sun, Hongbin
TI  - TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Current state-of-the-art approaches for spatio-temporal action detection have achieved impressive results but remain unsatisfactory for temporal extent detection. The main reason comes from that, there are some ambiguous states similar to the real actions which may be treated as target actions even by a well-trained network. In this paper, we define these ambiguous samples as "transitional states", and propose a Transition-Aware Context Network (TACNet) to distinguish transitional states. The proposed TACNet includes two main components, i.e., temporal context detector and transition-aware classifier. The temporal context detector can extract long-term context information with constant time complexity by constructing a recurrent network. The transition-aware classifier can further distinguish transitional states by classifying action and transitional states simultaneously. Therefore, the proposed TACNet can substantially improve the performance of spatio-temporal action detection. We extensively evaluate the proposed TACNet on UCF101-24 and J-HMDB datasets. The experimental results demonstrate that TACNet obtains competitive performance on JHMDB and significantly outperforms the state-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP and video-mAP.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1905.13417
AN  - PPRN:12962306
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Li, Guozhang
AU  - Li, Jie
AU  - Wang, Nannan
AU  - Ding, Xinpeng
AU  - Li, Zhifeng
AU  - Gao, Xinbo
TI  - Multi-Hierarchical Category Supervision for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly Supervised Temporal Action Localization (WTAL) aims to localize action segments in untrimmed videos with only video-level category labels in the training phase. In WTAL, an action generally consists of a series of sub-actions, and different categories of actions may share the common sub-actions. However, to distinguish different categories of actions with only video-level class labels, current WTAL models tend to focus on discriminative sub-actions of the action, while ignoring those common sub-actions shared with different categories of actions. This negligence of common sub-actions would lead to the located action segments incomplete, i.e., only containing discriminative sub-actions. Different from current approaches of designing complex network architectures to explore more complete actions, in this paper, we introduce a novel supervision method named multi-hierarchical category supervision (MHCS) to find more sub-actions rather than only the discriminative ones. Specifically, action categories sharing similar sub-actions will be constructed as super-classes through hierarchical clustering. Hence, training with the new generated super-classes would encourage the model to pay more attention to the common sub-actions, which are ignored training with the original classes. Furthermore, our proposed MHCS is model-agnostic and non-intrusive, which can be directly applied to existing methods without changing their structures. Through extensive experiments, we verify that our supervision method can improve the performance of four state-of-the-art WTAL methods on three public datasets: THUMOS14, ActivityNet1.2, and ActivityNet1.3.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 9332
EP  - 9344
DO  - 10.1109/TIP.2021.3124671
AN  - WOS:000717767800007
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrat Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrat Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Hong Kong Univ Sci & Technol, Sch Engn, Hong Kong, Peoples R China
AD  - Tencent, Shenzhen 518057, Peoples R China
AD  - Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
Y2  - 2021-11-24
ER  -

TY  - JOUR
AU  - Li, Qiang
AU  - Liu, Di
AU  - Zu, Guang
AU  - Li, Sen
AU  - Sun, Hui
AU  - Wang, Jianzhong
TI  - Multigranularity Feature Aggregation and Cross-level Boundary Modeling for Temporal Action Detection
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - This article presents a Temporal Action Detection (TAD) method with Multigranularity (MG) feature aggregation and Cross-level Boundary Modeling (CBM). Compared with other methods, our proposed approach has the following advantages. First, different from most existing works which only consider the local temporal context, a simple and computationally efficient MG module is proposed to comprehensively extract video features in instant, local, and global temporal granularities. Second, unlike the methods that only employ the information from single feature pyramid level for action boundary regression, a CBM strategy that integrates the relative information from both the same and higher level features is designed to improve the accuracy of boundary prediction. At lastfere, benefiting from the MG module and CBM strategy, our method outperforms other state-of-the-art approaches on five challenging TAD datasets: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100, ActivityNet-1.3, and HACS. We make our code and pre-trained model publicly available CCS Concepts: center dot Computing methodologies -> Artificial intelligence; Computer vision tasks; Activity recognition and understanding
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2025 MAR
PY  - 2025
VL  - 21
IS  - 3
C7  - 90
DO  - 10.1145/3712598
AN  - WOS:001459164900001
AD  - Northeast Normal Univ, Sch Informat Sci & Technol, Changchun, Peoples R China
AD  - Changchun Humanities & Sci Coll, Changchun, Peoples R China
AD  - Northeast Elect Power Univ, Jilin, Peoples R China
AD  - Jilin Univ, Sch Artificial Intelligence, Changchun, Peoples R China
M2  - Changchun Humanities & Sci Coll
Y2  - 2025-04-09
ER  -

TY  - JOUR
AU  - Nishioka, Ken
AU  - Noguchi, Wataru
AU  - Izuka, Hiroyuki
AU  - Yamamoto, Masahito
TI  - Detecting Eating Behavior of Elephants in a Zoo Using Temporal Action Localization
T2  - SENSORS AND MATERIALS
M3  - Article
AB  - The behavioral observation of animals in zoos is indispensable for their health management and the improvement of their breeding environment. However, the day-to-day recording of animal behaviors is time-consuming for zookeepers. Hence, we aim to automatically generate animal behavioral observation reports, called "ethograms", in cooperation with the Sapporo Maruyama Zoo. While studies using contact sensors [e.g., accelerometers, global positioning system (GPS) , and radio frequency identification (RFID)] have had some success in zoos, noncontact sensors (e.g., cameras and microphones) tend to be avoided because of frequent occlusion and the need for nighttime detection. However, noncontact sensors are preferable to contact sensors owing to animal welfare concerns. Here, we propose a method for automatic elephant behavior recognition based on elephant tracking information using video from surveillance cameras. In particular, we focus only on "eating", which is difficult to detect accurately because it requires relatively long-term observation. Therefore, we solve the problem by using a method based on temporal action localization (TAL), which is a task of predicting when and where a target action is performed over a relatively lengthy period. The TAL method has been applied mainly to humans and less to animals. In our experiments, the average precision of eating behavior detection using TAL was 0.853. The results show that TAL is also effective in animal behavior recognition.
PU  - MYU, SCIENTIFIC PUBLISHING DIVISION
PI  - TOKYO
PA  - 1-23-3-303 SENDAGI, TOKYO, 113-0022, JAPAN
SN  - 0914-4935
DA  - 2023 
PY  - 2023
VL  - 35
IS  - 11
SP  - 3927
EP  - 3945
DO  - 10.18494/SAM4501
AN  - WOS:001115028700001
AD  - Hokkaido Univ, Grad Sch Informat Sci & Technol, Kita 14,Nishi 9,Kita ku, Sapporo, Hokkaido 0600814, Japan
AD  - Res Inst Syst Planning Inc, 23-23 Sakuragaoka cho, Tokyo 1500031, Japan
AD  - Hokkaido Univ, Educ & Res Ctr Math & Data Sci, Kita 12,Nishi 7,Kita ku, Sapporo, Hokkaido 0600812, Japan
AD  - Hokkaido Univ, Ctr Human Nat Artificial Intelligence & Neurosci, Kita 12,Nishi 7,Kita ku, Sapporo, Hokkaido 0600812, Japan
AD  - Hokkaido Univ, Fac Informat Sci & Technol, Kita 14,Nishi 9,Kita ku, Sapporo, Hokkaido 0600814, Japan
M2  - Res Inst Syst Planning Inc
Y2  - 2023-12-19
ER  -

TY  - JOUR
AU  - Han, Chaolei
AU  - Wang, Hongsong
AU  - Kuang, Jidong
AU  - Zhang, Lei
AU  - Gui, Jie
TI  - Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing zero-shot temporal action detection (ZSTAD) methods predominantly use fully supervised or unsupervised strategies to recognize unseen activities. However, these training-based methods are prone to domain shifts and require high computational costs, which hinder their practical applicability in real-world scenarios. In this paper, unlike previous works, we propose a training-Free Zero-shot temporal Action Detection (FreeZAD) method, leveraging existing vision-language (ViL) models to directly classify and localize unseen activities within untrimmed videos without any additional fine-tuning or adaptation. We mitigate the need for explicit temporal modeling and reliance on pseudo-label quality by designing the LOGarithmic decay weighted Outer-Inner-Contrastive Score (LogOIC) and frequency-based Actionness Calibration. Furthermore, we introduce a test-time adaptation (TTA) strategy using Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling ViL models to adapt more effectively for ZSTAD. Extensive experiments on the THUMOS14 and ActivityNet-1.3 datasets demonstrate that our training-free method outperforms state-of-the-art unsupervised methods while requiring only 1/13 of the runtime. When equipped with TTA, the enhanced method further narrows the gap with fully supervised methods.
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2501.13795
AN  - PPRN:120926043
AD  - Southeast Univ, Sch Cyber Sci & Engn, Dhaka, Bangladesh
AD  - Nanjing Normal Univ, Sch Elect Engn & Automat, Nanjing, Peoples R China
M2  - Southeast Univ
Y2  - 2025-03-15
ER  -

TY  - JOUR
AU  - Zhang, Songchun
AU  - Zhao, Chunhui
TI  - Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 JUN
PY  - 2024
VL  - 34
IS  - 6
SP  - 4568
EP  - 4580
DO  - 10.1109/TCSVT.2023.3341881
AN  - WOS:001241605300038
AD  - Zhejiang Univ, Coll Control Sci & Engn, Hangzhou 310027, Peoples R China
Y2  - 2024-08-30
ER  -

TY  - JOUR
AU  - Qin, Xiaolei
AU  - Ge, Yongxin
AU  - Yu, Hui
AU  - Chen, Feiyu
AU  - Yang, Dan
TI  - Spatial Enhancement and Temporal Constraint for Weakly Supervised Action Localization
T2  - IEEE SIGNAL PROCESSING LETTERS
M3  - Article
AB  - Weakly supervised temporal action localization (WSTAL) is a practical but challenging issue in video understanding. However, most existing methods have to activate background snippets or deactivate action snippets in cases of no boundary annotations, which inevitably affects the localization of action instances. In this letter, we propose a spatial enhancement and temporal constraint (SETC) model to address this problem from three aspects. Specifically, we first propose a spatial enhancement module to enhance the discrimination of the extracted features. Then we leverage the instance sparse constraint to restrain the drastic fluctuation class activation sequence (CAS). Finally, we use the confidence connectivity enhancement to connect the snippets that are broken up by mistake. Experiments on THUMOS'14 and ActivityNet datasets validate the efficacy of SETC against existing state-of-the-art WSTAL algorithms.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1070-9908
SN  - 1558-2361
DA  - 2020 
PY  - 2020
VL  - 27
SP  - 1520
EP  - 1524
DO  - 10.1109/LSP.2020.3018914
AN  - WOS:000568665100004
AD  - Chongqing Univ, Sch Big Data & Software Engn, Chongqing 401331, Peoples R China
AD  - Univ Portsmouth, Portsmouth PO1 2DJ, Hants, England
Y2  - 2020-09-25
ER  -

TY  - CPAPER
AU  - Ren, Yifan
AU  - Xu, Xing
AU  - Shen, Fumin
AU  - Yao, Yazhou
AU  - Lu, Huimin
A1  - ACM
TI  - CAA: Candidate-Aware Aggregation for Temporal Action Detection
T2  - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021
M3  - Proceedings Paper
CP  - 29th ACM International Conference on Multimedia (MM)
CL  - ELECTR NETWORK
AB  - Temporal action detection aims to locate specific segments of action instances in an untrimmed video. Most existing approaches commonly extract the features of all candidate video segments and then classify them separately. However, they may neglect the underlying relationship among candidates unconsciously. In this paper, we propose a novel model termed Candidate-Aware Aggregation (CAA) to tackle this problem. In CAA, we design the Global Awareness (GA) module to exploit long-range relations among all candidates from a global perspective, which enhances the features of action instances. The GA module is then embedded into a multi-level hierarchical network named FENet, to aggregate local features in adjacent candidates to suppress background noise. As a result, the relationship among candidates is explicitly captured from both local and global perspectives, which ensures more accurate prediction results for the candidates. Extensive experiments conducted on two popular benchmarks ActivityNet-1.3 and THUMOS-14 demonstrate the superiority of CAA comparing to the state-of-the-art methods.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8651-7
DA  - 2021 
PY  - 2021
SP  - 4930
EP  - 4938
DO  - 10.1145/3474085.3475616
AN  - WOS:001147786904119
AD  - Univ Elect Sci & Technol China, Ctr Future Media, Hefei, Peoples R China
AD  - Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Hefei, Peoples R China
AD  - Nanjing Univ Sci & Technol, Nanjing, Peoples R China
AD  - Kyushu Inst Technol, Kitakyushu, Fukuoka, Japan
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Su, Shaowen
AU  - Gan, Minggang
TI  - Online spatio-temporal action detection with adaptive sampling and hierarchical modulation
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - Online spatio-temporal action detection (OSTAD) is a crucial task in video understanding, responsible for identifying and categorizing action instances in video streams in an online manner. This paper presents a novel approach that employs adaptive sampling and hierarchical modulation to enhance OSTAD capabilities. Traditional methods, often constrained by fixed sampling rates, may lead to redundancy in scenarios with slower action speeds and overlook essential details in faster-moving sequences. Our innovative dynamic sampling strategy, informed by speed estimation, adaptively adjusts sampling intervals based on speed attention and visual differential features, thereby optimizing the informational content of each sampled video clip. Additionally, our method incorporates a hierarchical modulation mechanism that synergizes high-level semantic and low-level spatial information, significantly enhancing action localization and classification accuracy. The adaptive sampling network with hierarchical modulation, underpinned by these advancements, demonstrates substantial improvements on benchmark datasets such as JHMDB21 and UCF24, proving our methods' efficacy in handling diverse and dynamic action sequences in an online setting.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2024 DEC
PY  - 2024
VL  - 30
IS  - 6
C7  - 349
DO  - 10.1007/s00530-024-01543-1
AN  - WOS:001359815600001
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2024-11-30
ER  -

TY  - CPAPER
AU  - Xu, Ronghai
AU  - Liu, Changhong
AU  - Chen, Yong
AU  - Lei, Zhenchun
A1  - IEEE
TI  - Snippet-level Supervised Contrastive Learning-based Transformer for Temporal Action Detection
T2  - 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - Broadbeach, AUSTRALIA
AB  - Anchor-free temporal action detection methods have recently achieved many good results in solving the problem of flexible boundaries and different duration of actions. But the anchor-free methods use local features to predict the action boundaries so that it is sensitive to noises and prone to generate incomplete action proposals. Moreover, there exist long-term temporal dependencies between actions and temporal semantic consistency between action primitives in the same classes of actions. Therefore, we propose a snippet-level supervised contrastive learning-based transformer (SSCL-T) model for temporal action detection, which can learn semantically local and global temporal relationships in actions. This model learns the local temporal dynamic features of actions through local temporal coding and uses the transformer to model the global semantic dependencies between long-term actions. In addition, we utilize the action class information to learn the high-level semantic features of actions by designing a snippet-level supervised contrastive learning, forcing the temporal dynamic features of the same class of actions to be as close as possible and the features of different classes of actions to be as far away as possible, thus effectively realizing accurate prediction of action boundaries. Our model has been verified on two benchmark datasets ActivityNet-v1.3 and THUMOS14. The experimental results demonstrate that the proposed model has significantly improved on both datasets. Compared with the benchmark method BMN, the average mAP value has increased by 2.91% and 8.4% on ActivityNet-v1.3 and THUMOS14, respectively.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-1-6654-8867-9
DA  - 2023 
PY  - 2023
DO  - 10.1109/IJCNN54540.2023.10191802
AN  - WOS:001046198705124
AD  - Jiangxi Normal Univ, Sch Comp & Informat Engn, Nanchang, Jiangxi, Peoples R China
AD  - Nanchang Inst Technol, Sch Business Adm, Nanchang, Jiangxi, Peoples R China
Y2  - 2023-09-30
ER  -

TY  - CPAPER
AU  - Gao, Zikai
AU  - Qiao, Peng
AU  - Dou, Yong
A1  - ACM
TI  - HAAN: Human Action Aware Network for Multi-label Temporal Action Detection
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - The task of multi-label temporal action detection aims to accurately detect dense action instances in untrimmed videos. Previous methods focused on modeling the appearance features of RGB images have struggled to capture the fine details and subtle variations in human actions, resulting in three critical issues: overlapping action confusion, intra-class appearance diversity, and background interferences. These issues have significantly undermined the accuracy and generalization of detection models. To tackle these issues, we propose incorporating the human skeleton into the feature design of the detection model. By utilizing multi-person skeletons, our proposed method can accurately represent various human actions in the scene, balance the salience of overlapping actions, and reduce the impact of changes in human appearance and background interferences on action features. Overall, we propose a novel two-stream human action aware network (HAAN) for multi-label temporal action detection based on the original RGB frames and the estimated skeleton frames. To leverage the complementary advantages of RGB features and skeleton features, we design a cross-modality fusion module that allows the two features to guide each other and enhance their representation of human actions. On the popular benchmarks MultiTHUMOS and Charades, our HAAN achieves state-of-the-art performance with 56.9% (+5.4%) and 32.1% (+3.3%) mean average precision (mAP) compared to the best available methods. Importantly, HAAN shows superior improvements of +6.83%, +22.35%, and +2.56% on the challenging sample subsets of the three critical issues.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 5059
EP  - 5069
DO  - 10.1145/3581783.3612097
AN  - WOS:001199449105010
AD  - Natl Univ Def Technol, Changsha, Hunan, Peoples R China
Y2  - 2024-07-31
ER  -

TY  - JOUR
AU  - Su, Shaowen
AU  - Zhang, Yan
TI  - Online Hierarchical Linking of Action Tubes for Spatio-Temporal Action Detection Based on Multiple Clues
T2  - IEEE ACCESS
M3  - Article
AB  - The spatio-temporal action detection task requires the output of the temporal and spatial positions as well as the action category of the target action instances in the form of action tubes. However, the current definition of video-level metrics in spatio-temporal action detection tasks is not sufficiently clear and unified to fully describe the ability of network models to perform spatio-temporal detection. Furthermore, existing tube linking methods are not only heavily dependent on the quality of the detection stage but also lack reliable linking criteria, resulting in poor tube linking performance. To address these issues, this study proposes a hierarchical linking method based on multiple clues. This method first dynamically utilizes various correlation clues at two levels, including appearance features, spatial overlap, motion prediction, category scores, tube length, and tube confidence status, to reduce the negative impact of unreliable information on the correlation. Then, it employs inter-class correlation to handle the mutual influence between different categories, followed by joint probability data association to address the mutual influence between correlated objects, ultimately achieving robust and accurate online linking of action tubes. The method is experimentally compared with other correlation methods on the untrimmed UCF24 and MultiSports datasets, demonstrating state-of-the-art tube link performance. We also conducted ablation experiments to explore the impact of the different modules and stages in the proposed tube-linking method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2024 
PY  - 2024
VL  - 12
SP  - 54661
EP  - 54672
DO  - 10.1109/ACCESS.2024.3388532
AN  - WOS:001208850600001
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2024-05-03
ER  -

TY  - JOUR
AU  - Shen, Quanping
AU  - Ye, Songzhong
TI  - Research on Human Motion Analysis in Moving Scene Based on Timing Detection and Video Description Algorithm (Retracted Article)
T2  - DISCRETE DYNAMICS IN NATURE AND SOCIETY
M3  - Article
M3  - Retracted Publication
AB  - Technical movement analysis requires specialized domain knowledge and processing a large amount of data, and the advantages of AI in processing data can improve the efficiency of data analysis. In this paper, we propose a feature pyramid network-based temporal action detection (FPN-TAD) algorithm, which is used to solve the problem that the action proposal module has a low recall rate for small-scale temporal target action regions in the current video temporal action detection algorithm research. This paper is divided into three parts. The first part is an overview of the algorithm; the second part elaborates the network structure and the working principle of the FPN-TAD algorithm; and the third part gives the experimental results and analysis of the algorithm.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 1026-0226
SN  - 1607-887X
DA  - 2021 DEC 16
PY  - 2021
VL  - 2021
C7  - 4320846
DO  - 10.1155/2021/4320846
AN  - WOS:000798626100001
AD  - Minjiang Normal Coll, Fuzhou 350018, Fujian, Peoples R China
AD  - Fujian Jiangxia Univ, Sports Ind Dev Res Ctr, Fuzhou 350108, Fujian, Peoples R China
M2  - Minjiang Normal Coll
Y2  - 2022-06-02
ER  -

TY  - JOUR
AU  - Zhao, Mengyao
AU  - Hu, Zhengping
AU  - Li, Shufang
AU  - Bi, Shuai
AU  - Sun, Zhe
TI  - Mask attention-guided graph convolution layer for weakly supervised temporal action detection
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Graph convolutional network has been applied for temporal action detection. However, it is performed on the temporal context from the local view of segments containing incomplete actions, which weakens its ability to learn global representation. To alleviate this problem, a novel mask attention-guided graph convolution layer with the cooperation of local and global views is introduced. The graph weighted by feature similarity between segments is regarded as local view, while the global view takes the overall action distribution of the video as a guide to establish the relevance of all actions in a video. Via adding such a global guide, graph convolutional network can learn more discriminative spatio-temporal correlation representation, therefore, we propose mask attention-guided graph convolution layer for weakly supervised temporal action detection. Taking the segments features as the graph nodes, the mask attention-guided foreground-background graph and the transition-aware temporal mask graph are constructed, and then the segments association features and temporal context features are obtained, and finally cascaded and used for action detection. Experiments on the Thumos14 and Activitynet1.2 datasets achieve a mean average precision of 29.7% and 32.7% under the tIoU threshold 0.5, respectively. The results show that the proposed approach can effectively improve the performance of action detection.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2022 JAN
PY  - 2022
VL  - 81
IS  - 3
SP  - 4323
EP  - 4340
DO  - 10.1007/s11042-021-11768-1
AN  - WOS:000727165900002
C6  - DEC 2021
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao, Hebei, Peoples R China
AD  - Hebei Univ Environm Engn, Dept Informat Engn, Qinhuangdao, Hebei, Peoples R China
M2  - Hebei Key Lab Informat Transmiss & Signal Proc
Y2  - 2021-12-12
ER  -

TY  - JOUR
AU  - Vahdani, Elahe
AU  - Tian, Yingli
TI  - Deep Learning-Based Action Detection in Untrimmed Videos: A Survey
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Understanding human behavior and activity facilitates advancement of numerous real-world applications, and is critical for video analysis. Despite the progress of action recognition algorithms in trimmed videos, the majority of real-world videos are lengthy and untrimmed with sparse segments of interest. The task of temporal activity detection in untrimmed videos aims to localize the temporal boundary of actions and classify the action categories. Temporal activity detection task has been investigated in full and limited supervision settings depending on the availability of action annotations. This article provides an extensive overview of deep learning-based algorithms to tackle temporal action detection in untrimmed videos with different supervision levels including fully-supervised, weakly-supervised, unsupervised, self-supervised, and semi-supervised. In addition, this article reviews advances in spatio-temporal action detection where actions are localized in both temporal and spatial dimensions. Action detection in online setting is also reviewed where the goal is to detect actions in each frame without considering any future context in a live video stream. Moreover, the commonly used action detection benchmark datasets and evaluation metrics are described, and the performance of the state-of-the-art methods are compared. Finally, real-world applications of temporal action detection in untrimmed videos and a set of future directions are discussed.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 APR 1
PY  - 2023
VL  - 45
IS  - 4
SP  - 4302
EP  - 4320
DO  - 10.1109/TPAMI.2022.3193611
AN  - WOS:000947840300021
AD  - CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10016 USA
AD  - CUNY, City Coll, Grad Ctr, Dept Elect Engn, New York, NY 10031 USA
AD  - CUNY, Grad Ctr, Dept Comp Sci, New York, NY 10031 USA
Y2  - 2023-03-31
ER  -

TY  - CPAPER
AU  - Huang, Jing
AU  - Kong, Ming
AU  - Chen, Luyuan
AU  - Liang, Tian
AU  - Zhu, Qiang
ED  - Yanikoglu, B
ED  - Buntine, W
TI  - Temporal RPN Learning for Weakly-Supervised Temporal Action Localization
T2  - ASIAN CONFERENCE ON MACHINE LEARNING, VOL 222
M3  - Proceedings Paper
CP  - 15th Asian Conference on Machine Learning (ACML)
CL  - Istanbul, TURKEY
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to train an action instance localization model from untrimmed videos with only video-level labels, similar to the Object Detection (OD) task. Existing Top-k MIL-based WSTAL methods cannot flexibly define the learning space, which limits the model's learning efficiency and performance. Faster R-CNN is a classic two-stage object detection architecture with an efficient Region Proposal Network. This paper successfully migrates the Faster R-CNN liked two-stage architecture to the WSTAL task: first to build a T-RPN and integrate it with the traditional WSTAL framework; and then to propose a pseudo label generation mechanism to enable the T-RPN learning without temporal annotations. Our new framework has achieved breakthrough performances on THUMOS-14 and ActivityNet-v1.2 datasets, and comprehensive ablation experiments have verified the effectiveness of the innovations. Code will be available at: https://github.com/ZJUHJ/TRPN.
PU  - JMLR-JOURNAL MACHINE LEARNING RESEARCH
PI  - SAN DIEGO
PA  - 1269 LAW ST, SAN DIEGO, CA, UNITED STATES
SN  - 2640-3498
SN  - *****************
DA  - 2023 
PY  - 2023
VL  - 222
AN  - WOS:001221095300031
AD  - Zhejiang Univ, Hangzhou 310058, Peoples R China
AD  - Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China
AD  - Hikvis Res Inst, Hangzhou 310051, Peoples R China
AD  - Beijing Informat Sci & Technol Univ, Beijing 100101, Peoples R China
M2  - Hikvis Res Inst
Y2  - 2024-08-13
ER  -

TY  - CPAPER
AU  - Gong, Guoqiang
AU  - Zheng, Liangfeng
AU  - Mu, Yadong
A1  - IEEE
TI  - SCALE MATTERS: TEMPORAL SCALE AGGREGATION NETWORK FOR PRECISE ACTION LOCALIZATION IN UNTRIMMED VIDEOS
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - ELECTR NETWORK
AB  - Temporal action localization is a recently-emerging task, aiming to localize video segments from untrimmed videos which contain specific actions. This work proposes a novel integrated temporal scale aggregation network (TSA-Net). Our main insight is that ensembling convolution filters with different dilation rates can effectively enlarge the receptive field with low computational cost, which inspires us to devise multi-dilation temporal convolution (MDC) block. Furthermore, to tackle video action instances with different durations, TSA-Net consists of multiple branches of sub-networks. Each of them adopts stacked MDC blocks with different dilation parameters, accomplishing a temporal receptive field specially optimized for specific-duration actions. We follow the formulation of boundary point detection, novelly detecting three kinds of critical points (i.e., starting / mid-point / ending) and pairing them for proposal generation. Comprehensive evaluations are conducted on THUMOS14. Our proposed TSA-Net demonstrates clear and consistent better performances and recalibrates new state-of-the-art on THUMOS14 benchmark.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-7281-1331-9
DA  - 2020 
PY  - 2020
DO  - 10.1109/icme46284.2020.9102850
AN  - WOS:000612843900118
AD  - Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China
AD  - Peking Univ, Ctr Data Sci, Beijing, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Tang, Xiaojun
AU  - Fan, Junsong
AU  - Luo, Chuanchen
AU  - Zhang, Zhaoxiang
AU  - Zhang, Man
AU  - Yang, Zongyuan
TI  - DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization 
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them neglect the adverse effects of ambiguous information, which would reduce the discriminability of others. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2307.16415
AN  - PPRN:74186460
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Beijing, Peoples R China
AD  - HKISI CAS, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China
AD  - Univ Chinese Acad Sci, Beijing, Peoples R China
M2  - Beijing Univ Posts & Telecommun
M2  - HKISI CAS
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Gao, Xinbo
AU  - Li, Jie
AU  - Wang, Xiaoyu
AU  - Liu, Tongliang
TI  - Weakly Supervised Temporal Action Localization with Segment-Level Labels
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization presents a trade-off between test performance and annotation-time cost. Fully supervised methods achieve good performance with time-consuming boundary annotations. Weakly supervised methods with cheaper video-level category label annotations result in worse performance. In this paper, we introduce a new segment-level supervision setting: segments are labeled when annotators observe actions happening here. We incorporate this segment-level supervision along with a novel localization module in the training. Specifically, we devise a partial segment loss regarded as a loss sampling to learn integral action parts from labeled segments. Since the labeled segments are only parts of actions, the model tends to overfit along with the training process. To tackle this problem, we first obtain a similarity matrix from discriminative features guided by a sphere loss. Then, a propagation loss is devised based on the matrix to act as a regularization term, allowing implicit unlabeled segments propagation during training. Experiments validate that our method can outperform the video-level supervision methods with almost same the annotation time.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2007.01598
AN  - PPRN:13658929
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Shao, Jiayi
AU  - Wang, Xiaohan
AU  - Yang, Yi
TI  - ReLER@ZJU Submission to the Ego4D Moment Queries Challenge 2022
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - In this report, we present the ReLER@ZJU1 submission to the Ego4D Moment Queries Challenge in ECCV 2022. In this task, the goal is to retrieve and localize all instances of possible activities in egocentric videos. Ego4D dataset is challenging for the temporal action localization task as the temporal duration of the videos is quite long and each video contains multiple action instances with fine-grained action classes. To address these problems, we utilize a multi-scale transformer to classify different action categories and predict the boundary of each instance. Moreover, in order to better capture the long-term temporal dependencies in the long videos, we propose a segment-level recurrence mechanism. Compared with directly feeding all video features to the transformer encoder, the proposed segment-level recurrence mechanism alleviates the optimization difficulties and achieves better performance. The final submission achieved Recall@1,tIoU=0.5 score of 37.24, average mAP score of 17.67 and took 3-rd place on the leaderboard.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2211.09558
AN  - PPRN:22996274
AD  - Zhejiang Univ, ReLER Lab, CCAI, Hangzhou, Peoples R China
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Gong, Guoqiang
AU  - Zheng, Liangfeng
AU  - Jiang, Wenhao
AU  - Mu, Yadong
ED  - Zhou, ZH
TI  - Self-Supervised Video Action Localization with Adversarial Temporal Transforms
T2  - PROCEEDINGS OF THE THIRTIETH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2021
M3  - Proceedings Paper
CP  - 30th International Joint Conference on Artificial Intelligence (IJCAI)
CL  - ELECTR NETWORK
AB  - Weakly-supervised temporal action localization aims to locate intervals of action instances with only video-level action labels for training. However, the localization results generated from video classification networks are often not accurate due to the lack of temporal boundary annotation of actions. Our motivating insight is that the temporal boundary of action should be stably predicted under various temporal transforms. This inspires a self-supervised equivariant transform consistency constraint. We design a set of temporal transform operations, including naive temporal down-sampling to learnable attention-piloted time warping. In our model, a localization network aims to perform well under all transforms, and another policy network is designed to choose a temporal transform at each iteration that adversarially brings localization result inconsistent with the localization network's. Additionally, we devise a self-refine module to enhance the completeness of action intervals harnessing temporal and semantic contexts. Experimental results on THUMOS14 and ActivityNet demonstrate that our model consistently outperforms the state-of-the-art weakly-supervised temporal action localization methods.
PU  - IJCAI-INT JOINT CONF ARTIF INTELL
PI  - FREIBURG
PA  - ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY
SN  - 978-0-9992411-9-6
DA  - 2021 
PY  - 2021
SP  - 693
EP  - 699
AN  - WOS:001202335500096
AD  - Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China
AD  - Tencent AI Lab, Shanghai, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - CPAPER
AU  - Jiang, Yudong
AU  - Cui, Kaixu
AU  - Chen, Leilei
AU  - Wang, Canjin
AU  - Xu, Changliang
A1  - ACM
TI  - SoccerDB: A Large-Scale Database for Comprehensive Video Understanding
T2  - PROCEEDINGS OF THE 3RD INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS, MMSPORTS 2020
M3  - Proceedings Paper
CP  - 3rd International Workshop on Multimedia Content Analysis in Sports
CL  - Washington, DC
AB  - Soccer videos can serve as a perfect research object for video understanding because soccer games are played under well-defined rules while complex and intriguing enough for researchers to study. In this paper, we propose a new soccer video database named SoccerDB, comprising 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. To our knowledge, it is the largest database for comprehensive sports video understanding on various aspects. We further survey a collection of strong baselines on SoccerDB, which have demonstrated state-of-the-art performances on independent tasks. Our evaluation suggests that we can benefit significantly when jointly considering the inner correlations among those tasks. We believe the release of SoccerDB will tremendously advance researches around comprehensive video understanding. Our dataset and code published on https://github.com/newsdata/SoccerDB.y
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-8149-9
DA  - 2020 
PY  - 2020
SP  - 1
EP  - 8
DO  - 10.1145/3422844.3423051
AN  - WOS:001428722900001
AD  - State Key Lab Media Convergence Prod Technol & Sy, Beijing, Peoples R China
AD  - Xinhua Zhiyun Technol Co Ltd, Guilin, Peoples R China
AD  - Tencent Inc, Shenzhen, Peoples R China
M2  - State Key Lab Media Convergence Prod Technol & Sy
M2  - Xinhua Zhiyun Technol Co Ltd
Y2  - 2020-01-01
ER  -

TY  - CPAPER
AU  - Lee, Pilhyeon
AU  - Uh, Youngjung
AU  - Byun, Hyeran
A1  - Assoc Advancement Artificial Intelligence
TI  - Background Suppression Network for Weakly-Supervised Temporal Action Localization
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
CL  - New York, NY
AB  - Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaS-Net) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at https://github.com/Pilhyeon/BaSNet-pytorch.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
DA  - 2020 
PY  - 2020
VL  - 34
SP  - 11320
EP  - 11327
AN  - WOS:000668126803095
AD  - Yonsei Univ, Seoul, South Korea
AD  - NAVER Corp, Clova AI Res, Seongnam, South Korea
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Lee, Pilhyeon
AU  - Uh, Youngjung
AU  - Byun, Hyeran
TI  - Background Suppression Network for Weakly-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaS-Net) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at https://github.com/Pilhyeon/BaSNet-pytorch.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1911.09963
AN  - PPRN:22625078
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Bagchi, Anurag
AU  - Mahmood, Jazib
AU  - Fernandes, Dolton
AU  - Sarvadevabhatla, Ravi Kiran
TI  - Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2106.14118
AN  - PPRN:11947044
AD  - IIIT Hyderabad, CVIT, Hyderabad, India
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Kang, Hyolim
AU  - Kim, Hanjung
AU  - An, Joungbin
AU  - Cho, Minsu
AU  - Kim, Seon Joo
TI  - Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) methods typically operate on top of feature sequences from a frozen snippet encoder that is pretrained with the Trimmed Action Classification (TAC) tasks, resulting in a task discrepancy problem. While existing TAL methods mitigate this issue either by retraining the encoder with a pretext task or by end-to-end fine-tuning, they commonly require an overload of high memory and computation. In this work, we introduce Soft-Landing (SoLa) strategy, an efficient yet effective framework to bridge the transferability gap between the pretrained encoder and the downstream tasks by incorporating a light-weight neural network, i.e., a SoLa module, on top of the frozen encoder. We also propose an unsupervised training scheme for the SoLa module; it learns with inter-frame Similarity Matching that uses the frame interval as its supervisory signal, eliminating the need for temporal annotations. Experimental evaluation on various benchmarks for downstream TAL tasks shows that our method effectively alleviates the task discrepancy problem with remarkable computational efficiency.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2211.06023
AN  - PPRN:22913209
AD  - Yonsei Univ, Seoul, South Korea
AD  - POSTECH, Pohang, South Korea
M2  - POSTECH
Y2  - 2023-11-24
ER  -

TY  - JOUR
AU  - Zhai, Yuanhao
AU  - Wang, Le
AU  - Tang, Wei
AU  - Zhang, Qilin
AU  - Yuan, Junsong
AU  - Hua, Gang
TI  - Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2010.11594
AN  - PPRN:22522352
Y2  - 2023-03-27
ER  -

TY  - JOUR
AU  - Li, Yuxi
AU  - Lin, Weiyao
AU  - See, John
AU  - Xu, Ning
AU  - Xu, Shugong
AU  - Yan, Ke
AU  - Yang, Cong
TI  - CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Most current pipelines for spatio-temporal action localization connect frame-wise or clip-wise detection results to generate action proposals, where only local information is exploited and the efficiency is hindered by dense per-frame localization. In this paper, we propose Coarse-to-Fine Action Detector (CFAD),an original end-to-end trainable framework for efficient spatio-temporal action localization. The CFAD introduces a new paradigm that first estimates coarse spatio-temporal action tubes from video streams, and then refines the tubes' location based on key timestamps. This concept is implemented by two key components, the Coarse and Refine Modules in our framework. The parameterized modeling of long temporal information in the Coarse Module helps obtain accurate initial tube estimation, while the Refine Module selectively adjusts the tube location under the guidance of key timestamps. Against other methods, theproposed CFAD achieves competitive results on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21 with inference speed that is 3.3x faster than the nearest competitors.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.08332
AN  - PPRN:13123647
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Jin, Cece
AU  - Chen, Yuanqi
AU  - Li, Ge
AU  - Zhang, Tao
AU  - Li, Thomas
TI  - Low Pass Filter for Anti-aliasing in Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In temporal action localization methods, temporal down sampling operations are widely used to extract proposal features, but they often lead to the aliasing problem, due to lacking consideration of sampling rates. This paper aims to verify the existence of aliasing in TAL methods and investigate utilizing low pass filters to solve this problem by inhibiting the high-frequency band. However, the high frequency band usually contains large amounts of specific information, which is important for model inference. Therefore, it is necessary to make a tradeoff between anti-aliasing and reserving high-frequency information. To acquire optimal performance, this paper learns different cutoff frequencies for different instances dynamically. This design can be plugged into most existing temporal modeling programs requiring only one additional cutoff frequency parameter. Integrating low pass filters to the downsampling operations significantly improves the detection performance and achieves comparable results on THUMOS&rsquo;14, ActivityNet 1.3, and Charades datasets. Experiments demonstrate that anti-aliasing with low pass filters in TAL is advantageous and efficient.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2104.11403
AN  - PPRN:13046840
AD  - Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China
AD  - Peng Cheng Lab, Beijing, Peoples R China
AD  - Peking Univ, Adv Inst Informat Technol, Beijing, Peoples R China
M2  - Peng Cheng Lab
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Kang, Hyolim
AU  - Kim, Hanjung
AU  - An, Joungbin
AU  - Cho, Minsu
AU  - Kim, Seon Joo
A1  - IEEE
TI  - Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Temporal Action Localization (TAL) methods typically operate on top of feature sequences from a frozen snippet encoder that is pretrained with the Trimmed Action Classification (TAC) tasks, resulting in a task discrepancy problem. While existing TAL methods mitigate this issue either by retraining the encoder with a pretext task or by end-to-end fine-tuning, they commonly require an overload of high memory and computation. In this work, we introduce Soft-Landing (SoLa) strategy, an efficient yet effective framework to bridge the transferability gap between the pretrained encoder and the downstream tasks by incorporating a light-weight neural network, i.e., a SoLa module, on top of the frozen encoder. We also propose an unsupervised training scheme for the SoLa module; it learns with inter-frame Similarity Matching that uses the frame interval as its supervisory signal, eliminating the need for temporal annotations. Experimental evaluation on various benchmarks for downstream TAL tasks shows that our method effectively alleviates the task discrepancy problem with remarkable computational efficiency.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 6514
EP  - 6523
DO  - 10.1109/CVPR52729.2023.00630
AN  - WOS:001058542606083
AD  - Yonsei Univ, Seoul, South Korea
AD  - POSTECH, Pohang, South Korea
Y2  - 2023-11-08
ER  -

TY  - JOUR
AU  - Ouyang, Yizheng
AU  - Zhang, Tianjin
AU  - Gu, Weibo
AU  - Wang, Hongfa
AU  - Wang, Hongfa
TI  - Adaptive Perception Transformer for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization aims to predict the boundary and category of each action instance in untrimmed long videos. Most of previous methods based on anchors or proposals neglect the global-local context interaction in entire video sequences. Besides, their multi-stage designs cannot generate action boundaries and categories straightforwardly. To address the above issues, this paper proposes a end-to-end model, called Adaptive Perception transformer (AdaPerFormer for short). Specifically, AdaPerFormer explores a dual-branch attention mechanism. One branch takes care of the global perception attention, which can model entire video sequences and aggregate global relevant contexts. While the other branch concentrates on the local convolutional shift to aggregate intra-frame and inter-frame information through our bidirectional shift operation. The end-to-end nature produces the boundaries and categories of video actions without extra steps. Extensive experiments together with ablation studies are provided to reveal the effectiveness of our design. Our method obtains competitive performance on the THUMOS14 and ActivityNet-1.3 dataset.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.11908
AN  - PPRN:14787085
AD  - Tianjin Univ, Tianjin, Peoples R China
AD  - Sichuan Univ, Chengdu, Peoples R China
AD  - Huazhong Univ Sci & Technol, Wuhan, Peoples R China
AD  - Chinese Acad Sci, Beijing, Peoples R China
M2  - Sichuan Univ
M2  - Huazhong Univ Sci & Technol
M2  - Chinese Acad Sci
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Liu, Yushu
AU  - Zhang, Weigang
AU  - Li, Guorong
AU  - Su, Li
AU  - Huang, Qingming
A1  - IEEE
TI  - One-Shot Example Videos Localization Network for Weakly-Supervised Temporal Action Localization
T2  - 2021 IEEE 4TH INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL, MIPR
M3  - Proceedings Paper
CP  - IEEE 4th International Conference on Multimedia Information Processing and Retrieval (IEEE MIPR)
CL  - ELECTR NETWORK
AB  - This paper tackles the problem of example-driven weakly-supervised temporal action localization. We propose the One-shot Example Videos Localization Network (OSEVLNet) for precisely localizing the action instances in untrimmed videos with only one trimmed example video. Since the frame-level ground truth is unavailable under weakly-supervised settings, our approach automatically trains a self-attention module with reconstruction and feature discrepancy restriction. Specifically, the reconstruction restriction minimizes the discrepancy between the original input features and the reconstructed features of a Variational AutoEncoder (VAE) module. The feature discrepancy restriction maximizes the distance of weighted features between highly-responsive regions and slightly-responsive regions. Our approach achieves comparable or better results on THUMOS'14 dataset than other weakly-supervised methods while it is trained with much less videos. Moreover, our approach is especially suitable for the expansion of newly emerging action categories to meet the requirements of different occasions.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 978-1-6654-1865-2
DA  - 2021 
PY  - 2021
SP  - 125
EP  - 130
DO  - 10.1109/MIPR51284.2021.00026
AN  - WOS:001345027300019
AD  - Harbin Inst Technol, Sch Comp Sci & Technol, Weihai, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Wang, Binglu
AU  - Zhao, Yongqiang
AU  - Zhang, Yani
TI  - PFWNet: Pretraining neural network via feature jigsaw puzzle for weakly-supervised temporal action localization
T2  - NEUROCOMPUTING
M3  - Article
AB  - Weakly supervised temporal action localization is a challenging yet interesting task. Existing methods usually apply a few temporal convolutional layers or linear layers to predict classification scores, where the model capacity is limited. Inspired by counterpart researches, increasing model capacity is the potential to improve the localization performance. However, under the weakly supervised paradigm, the video level classification label is insufficient to learn large-capacity models. The essential reason lies in that most of the inputs to action localization networks are high-level features extracted by video recognition models. In lack of off-the-shelf initialization weights, the action localization networks have to train from scratch and can only explore low-capacity models. In this work, we are inspired by the self-supervised learning paradigm and propose to learn high-quality representative models via solving the feature jigsaw puzzle task. The proposed self-supervised pretraining process can explore networks with large kernel size and deeper layers, which can provide valuable initialization to action localization networks. In the implementation, we first discover potential action scopes via calculating motion intensity. Then, we cut features into snippets and permute them into out-of-order status. We randomly discard frames for boundaries between two snippets to guide the network learning high-level representations and prevent information leakage. Moreover, because the potential permutation number factorially rises with the increase of snippet number, we select a fixed number of permutation operations via the maximum hamming distance criterion, which eases the learning process. Comprehensive experiments on two benchmarks demonstrate the efficiency of pretraining to weakly supervised action localization task, and the proposed method builds new state-of-the-art performance.(c) 2021 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2021 JUL 5
PY  - 2021
VL  - 443
SP  - 162
EP  - 173
DO  - 10.1016/j.neucom.2021.02.086
AN  - WOS:000647022600014
C6  - MAR 2021
AD  - Northwestern Polytech Univ Shenzhen, Res & Dev Inst, Shenzhen 518057, Peoples R China
AD  - Northwestern Polytech Univ, Sch Automat Engn, Xian 710072, Peoples R China
Y2  - 2021-06-01
ER  -

TY  - CPAPER
AU  - Zhao, Chen
AU  - Thabet, Ali
AU  - Ghanem, Bernard
A1  - IEEE
TI  - Video Self-Stitching Graph Network for Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy a major proportion in the datasets, but tend to have the lowest performance. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3. VSGN code is available at https://github.com/coolbay/VSGN.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13638
EP  - 13647
DO  - 10.1109/ICCV48922.2021.01340
AN  - WOS:000798743203081
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
Y2  - 2022-06-24
ER  -

TY  - JOUR
AU  - Zhang, Min
AU  - Hu, Haiyang
AU  - Li, Zhongjin
TI  - Temporal Action Localization With Coarse-to-Fine Network
T2  - IEEE ACCESS
M3  - Article
AB  - Precisely localizing temporal intervals for each action segment in long raw videos is essential challenge in practical video content analysis (e.g., activity detection or video caption generation). Most of previous works often neglect the hierarchical action granularity and eventually fail to identify precise action boundaries. (e.g., embracing approaching or turning a screw in mechanical maintenance). In this paper, we introduce a simple yet efficient coarse-to-fine network (CFNet) to solve the challenging issue of temporal action localization by progressively refining action boundary at multiple action granularities. The proposed CFNet is mainly composed of three components: a coarse proposal module (CPM) to generate coarse action candidates, a fusion block (FB) to enhance feature representation by fusing the coarse candidate features and corresponding features of raw input frames, and a boundary transformer module (BTM) to further refine action boundaries. Specifically, CPM exploits framewise, matching and gated actionness curves to complement each other for coarse candidate generation at different levels, while FB is devised to enrich feature representation by fusing the last feature map of CPM and corresponding raw frame input. Finally, BTM learns long-term temporal dependency with a transformer structure to further refine action boundaries at a finer granularity. Thus, the fine-grained action intervals can be incrementally obtained. Compared with previous state-of-the-art techniques, the proposed coarse-to-fine network can asymptotically approach fine-grained action boundary. Comprehensive experiments are conducted on both publicly available THUMOS14 and ActivityNet-v1.3 datasets, and show the outstanding improvements of our method when compared with the prior methods on various video action parsing tasks.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2022 
PY  - 2022
VL  - 10
SP  - 96378
EP  - 96387
DO  - 10.1109/ACCESS.2022.3205594
AN  - WOS:000859386100001
AD  - Zhejiang Ind Polytech Coll, Dept Design & Art, Shaoxing 312000, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China
M2  - Zhejiang Ind Polytech Coll
Y2  - 2022-10-03
ER  -

TY  - JOUR
AU  - Li, Guozhang
AU  - Cheng, De
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Li, Jie
AU  - Gao, Xinbo
TI  - Weakly-Supervised Temporal Action Localization with Bidirectional Semantic Consistency Constraint
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly Supervised Temporal Action Localization (WTAL) aims to classify and localize temporal boundaries of actions for the video, given only video-level category labels in the training datasets. Due to the lack of boundary information during training, existing approaches formulate WTAL as a classificationproblem, i.e., generating the temporal class activation map (T-CAM) for localization. However, with only classification loss, the model would be sub-optimized, i.e., the action-related scenes are enough to distinguish different class labels. Regarding other actions in the action-related scene ( i.e., the scene same as positive actions) as co-scene actions, this sub-optimized model would misclassify the co-scene actions as positive actions. To address this misclassification, we propose a simple yet efficient method, named bidirectional semantic consistency constraint (Bi-SCC), to discriminate the positive actions from co-scene actions. The proposed Bi-SCC firstly adopts a temporal context augmentation to generate an augmented video that breaks the correlation between positive actions and their co-scene actions in the inter-video; Then, a semantic consistency constraint (SCC) is used to enforce the predictions of the original video and augmented video to be consistent, hence suppressing the co-scene actions. However, we find that this augmented video would destroy the original temporal context. Simply applying the consistency constraint would affect the completeness of localized positive actions. Hence, we boost the SCC in a bidirectional way to suppress co-scene actions while ensuring the integrity of positive actions, by cross-supervising the original and augmented videos. Finally, our proposed Bi-SCC can be applied to current WTAL approaches, and improve their performance. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.12616
AN  - PPRN:65324502
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Hong Kong Univ Sci & Technol, Sch Engn, Hong Kong, Peoples R China
AD  - Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
M2  - Hong Kong Univ Sci & Technol
M2  - Chongqing Univ Posts & Telecommun
Y2  - 2023-08-21
ER  -

TY  - JOUR
AU  - Wang, Yu
AU  - Zhao, Shengjie
AU  - Chen, Shiwei
TI  - Action-Semantic Consistent Knowledge for Weakly-Supervised Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised temporal action localization aims to detect temporal intervals of actions in arbitrarily long untrimmed videos with only video-level annotations. Owing to label sparsity, learning action consistency is intractable. In this paper, we assume that frames with similar representations in a given video should be considered as the same action. To this end, we develop a query-based contrastive learning paradigm to ensure action-semantic consistency. This mechanism encourages normalized embeddings with the same class to be pulled closer together, while embeddings from different classes are repelled apart. Besides, we design a two-branch framework, consisting of a class-aware branch and a class-agnostic branch, to learn salient features and fine-grained clues respectively. To further guarantee the action-semantic consistency of the two branches, unlike previous methods that handle each branch independently, we model the relationship between the two branches to avoid unreasonable predictions. Finally, the proposed model demonstrates superior performance over existing methods on the publicly available THUMOS-14 and ActivityNet-1.3 datasets. Substantial experiments and ablation studies also demonstrate the effectiveness of our model.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 10279
EP  - 10289
DO  - 10.1109/TMM.2024.3405710
AN  - WOS:001338399700034
AD  - Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China
AD  - Minist Educ, Engn Res Ctr Key Software Technol Smart City Perce, Shanghai 201804, Peoples R China
AD  - Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 201804, Peoples R China
AD  - Microsoft Asia Pacific Technol Co Ltd, Dept R&D Data, Shanghai 200241, Peoples R China
M2  - Microsoft Asia Pacific Technol Co Ltd
Y2  - 2024-11-09
ER  -

TY  - CPAPER
AU  - Bao, Wentao
AU  - Yu, Qi
AU  - Kong, Yu
A1  - IEEE COMP SOC
TI  - OpenTAL: Towards Open Set Temporal Action Localization
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Temporal Action Localization (TAL) has experienced remarkable success under the supervised learning paradigm. However, existing TAL methods are rooted in the closed set assumption, which cannot handle the inevitable unknown actions in open-world scenarios. In this paper, we, for the first time, step toward the Open Set TAL (OSTAL) problem and propose a general framework Open TAL based on Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of uncertainty-aware action classification, actionness prediction, and temporal location regression. With the proposed importance-balanced EDL method, classification uncertainty is learned by collecting categorical evidence majorly from important samples. To distinguish the unknown actions from background video frames, the actionness is learned by the positive-unlabeled learning. The classification uncertainty is further calibrated by leveraging the guidance from the temporal localization quality. The OpenTAL is general to enable existing TAL models for open set scenarios, and experimental results on THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method. The code and pre-trained models are released at https://www.rit.edu/actionlab/opental.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 2969
EP  - 2979
DO  - 10.1109/CVPR52688.2022.00299
AN  - WOS:000867754203023
AD  - Rochester Inst Technol, Rochester, NY 14623 USA
Y2  - 2022-12-17
ER  -

TY  - JOUR
AU  - Zhao, Zixuan
AU  - Wang, Dongqi
AU  - Zhao, Xu
TI  - M 3 Net : Movement Enhancement with Multi-Relation toward Multi-Scale video representation for Temporal Action Detection
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Locating boundary is very important for Temporal Action Detection (TAD) and is a key factor affecting the performance of TAD. However, two factors lead to inaccurate boundary localization: the movement feature submergence and the existence of multi-scale actions. In this work, to address the submergence of movement feature, we design the Movement Enhance Module (MEM), in which the Movement Feature Extractor (MFE) and Multi-Relation Module (MRM) are used to highlight short-term and long-term movement information respectively. To address the characteristic of multi-scale actions, we propose a Scale Feature Pyramid Network (SFPN) to detect multi-scale actions and design a two-stage training strategy that makes each layer focus on a specific scale action. These tow modules are integrated as M 3 Net , and extensive experiments demonstrate its effectiveness. M 3 Net outperforms other representative TAD methods on ActivityNet-1.3 and THUMOS-14.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2024 NOV
PY  - 2024
VL  - 155
C7  - 110702
DO  - 10.1016/j.patcog.2024.110702
AN  - WOS:001266234000001
C6  - JUL 2024
AD  - Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China
Y2  - 2024-07-16
ER  -

TY  - CPAPER
AU  - Song, Lin
AU  - Zhang, Shiwei
AU  - Yu, Gang
AU  - Sun, Hongbin
A1  - IEEE Comp Soc
TI  - TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection
T2  - 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Long Beach, CA
AB  - Current state-of-the-art approaches for spatio-temporal action detection have achieved impressive results but remain unsatisfactoryfor temporal extent detection. The main reason comes from that, there are some ambiguous states similar to the real actions which may be treated as target actions even by a well-trained network. In this paper; we define these ambiguous samples as "transitional states", and propose a Transition -Aware Context Network (TACNet) to distinguish transitional states. The proposed TACNet includes two main components, i.e., temporal context detectorand transition -aware classifier The temporal context detector can extract long-term context information with constant time complexity by constructing a recurrent network. The transition-aware classifier can further- distinguish transitional states by classifying action and transitional states simultaneously. Therefore, the proposed TACNet can substantially improve the performance of spatio-temporal action detection. We extensively evaluate the proposed TACNet on UCF101-24 and J-HMDB datasets. The experimental results demonstrate that TACNet obtains competitive performance on JHMDB and significantly outperforms the state-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP and video-mAP.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1063-6919
SN  - 978-1-7281-3293-8
DA  - 2019 
PY  - 2019
SP  - 11979
EP  - 11987
DO  - 10.1109/CVPR.2019.01226
AN  - WOS:000542649305060
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Huazhong Univ Sci & Technol, Artificial Intelligence & Automat, Wuhan, Peoples R China
AD  - Megvii Inc Face, Beijing, Peoples R China
M2  - Megvii Inc Face
Y2  - 2020-07-16
ER  -

TY  - JOUR
AU  - Wang, Peng
AU  - Lu, Shoupeng
AU  - Dai, Cheng
AU  - Dai, Shengxin
AU  - Guo, Bing
TI  - Temporal action localization with State-Sensitive Mamba and centroid sequences enhancement
T2  - NEUROCOMPUTING
M3  - Article
AB  - The temporal action localization task aims to identify and localize human behaviors in unedited videos. However, most previous studies have employed sampling processing and local information aggregation to reduce the computational burden, which results insignificant information loss and a weak understanding of contextual information. In this paper, we improve localization accuracy by extending the sequence length and employing global information aggregation, which enhances the model's ability to capture long-range dependencies and contextual information. To this end, we first propose a novel State-Sensitive Mamba module to replace the self-attention mechanism. Based on its linear computational cost and high sensitivity to state boundaries, we extend the sequence length to five times that of the previous model, which helps to preserve information integrity and leads to amore comprehensive information capture. Secondly, in order to overcome the noise and boundary ambiguity introduced by the extended sequences, we propose a centroid sequence enhancement strategy. This strategy optimizes boundary localization by using central features that are closer to the motion semantics. Finally, extensive experimental results on the THUMOS'14, ActivityNet1.3 and FineAction datasets demonstrate the superior performance of the proposed method.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2025 MAR 1
PY  - 2025
VL  - 620
C7  - 129246
DO  - 10.1016/j.neucom.2024.129246
AN  - WOS:001412452100001
C6  - DEC 2024
AD  - Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China
Y2  - 2025-02-09
ER  -

TY  - CPAPER
AU  - Liu, Minghao
AU  - Liu, Haiyi
AU  - Zhao, Sirui
AU  - Ma, Fei
AU  - Li, Minglei
AU  - Dai, Zonghong
AU  - Wang, Hao
AU  - Xu, Tong
AU  - Chen, Enhong
A1  - ACM
TI  - STAN: Spatial-Temporal Awareness Network for Temporal Action Detection
T2  - PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS, MMSPORTS 2023
M3  - Proceedings Paper
CP  - 6th ACM International Workshop on Multimedia Content Analysis in Sports (MMSports)
CL  - Ottawa, CANADA
AB  - In recent years, there have been significant advancements in the field of temporal action detection. However, few studies have focused on detecting actions in sporting events. In this context, the MMSports 2023 cricket bowl release challenge aims to identify the bowl release action by segmenting untrimmed videos. To achieve this, we propose a novel cricket bowl release detection framework based on Spatial-Temporal Awareness Network (STAN) which mainly consists of three modules: the spatial feature extraction module (SFEM), the temporal feature extraction module (TFEM), and the classification module (CM). Specifically, we first adopt ResNet to extract the spatial features from videos in SFEM. Then, the TFEM is designed to aggregate temporal features using Bi-LSTM to obtain spatial-temporal features. Afterward, the CM converts the spatial-temporal features into action category probabilities to localize the action segments. Besides, we introduce the weighted binary cross entropy loss to solve the data imbalance problem in cricket bowl release detection. Finally, the experiments show that our proposed STAN achieves competitive performance in 1st place with a PQ score of 0.643 on the cricket bowl release challenge. The code is also publicly available at https://github.com/lmhr/STAN.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0269-3
DA  - 2023 
PY  - 2023
SP  - 161
EP  - 165
DO  - 10.1145/3606038.3616169
AN  - WOS:001148097700021
AD  - Univ Sci & Technol China USTC, Hefei, Anhui, Peoples R China
AD  - Huawei Cloud Comp Technol Ltd, Shenzhen, Guangdong, Peoples R China
M2  - Huawei Cloud Comp Technol Ltd
Y2  - 2024-02-10
ER  -

TY  - JOUR
AU  - Wang, Tian
AU  - Lei, Shiye
AU  - Jiang, Youyou
AU  - Chang, Choi
AU  - Snoussi, Hichem
AU  - Shan, Guangcun
AU  - Fu, Yao
TI  - Accelerating temporal action proposal generation via high performance computing
T2  - FRONTIERS OF COMPUTER SCIENCE
M3  - Article
AB  - Temporal action proposal generation aims to output the starting and ending times of each potential action for long videos and often suffers from high computation cost. To address the issue, we propose a new temporal convolution network called Multipath Temporal ConvNet (MTCN). In our work, one novel high performance ring parallel architecture based is further introduced into temporal action proposal generation in order to respond to the requirements of large memory occupation and a large number of videos. Remarkably, the total data transmission is reduced by adding a connection between multiple-computing load in the newly developed architecture. Compared to the traditional Parameter Server architecture, our parallel architecture has higher efficiency on temporal action detection tasks with multiple GPUs. We conduct experiments on ActivityNet-1.3 and THUMOS14, where our method outperforms-other state-of-art temporal action detection methods with high recall and high temporal precision. In addition, a time metric is further proposed here to evaluate the speed performancein the distributed training process.
PU  - HIGHER EDUCATION PRESS
PI  - BEIJING
PA  - CHAOYANG DIST, 4, HUIXINDONGJIE, FUSHENG BLDG, BEIJING 100029, PEOPLES R CHINA
SN  - 2095-2228
SN  - 2095-2236
DA  - 2022 AUG
PY  - 2022
VL  - 16
IS  - 4
C7  - 164317
DO  - 10.1007/s11704-021-0173-7
AN  - WOS:000720649100002
AD  - Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China
AD  - Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China
AD  - Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China
AD  - Gachon Univ, Dept Comp Engn, Seongnam 13120, South Korea
AD  - Univ Technol Troyes, Inst Charles Delaunay LM2S FRE CNRS 2019, F-10010 Troyes, France
AD  - Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Beijing 100191, Peoples R China
AD  - Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China
Y2  - 2021-11-27
ER  -

TY  - JOUR
AU  - Kim, Ho-Joong
AU  - Hong, Jung-Ho
AU  - Kong, Heejo
AU  - Lee, Seong-Whan
TI  - TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on handcrafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end -to -end framework in object detection, query-based detectors have been limited in achieving full end -to -end modeling in TAD. To address this issue, we propose TE-TAD, a full end -to -end temporal action detection transformer that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to stateof-the-art methods on popular benchmark datasets. Code is available at: https://github.com/Dotori-HJ/TE-TAD
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.02405
AN  - PPRN:88405579
AD  - Korea Univ, Dept Artificial Intelligence, Seoul, South Korea
AD  - Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea
Y2  - 2024-04-19
ER  -

TY  - JOUR
AU  - Huang, Yupan
AU  - Dai, Qi
AU  - Lu, Yutong
TI  - Decoupling Localization and Classification in Single Shot Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video temporal action detection aims to temporally localize and recognize the action in untrimmed videos. Existing one-stage approaches mostly focus on unifying two subtasks, i.e., localization of action proposals and classification of each proposal through a fully shared backbone. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. In this paper, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling the localization and classification in a one-stage scheme. Particularly, two separate branches are designed in parallel to enable each component to own representations privately for accurate localization or classification. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. Each branch produces a set of feature maps by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the feature representations. We conduct extensive experiments on THUMOS14 dataset and demonstrate superior performance over state-of-the-art methods. Our code is available online.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1904.07442
AN  - PPRN:12966786
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Feng, Yue
AU  - Zhang, Zhengye
AU  - Quan, Rong
AU  - Wang, Limin
AU  - Qin, Jie
A1  - ACM
TI  - RefineTAD: Learning Proposal-free Refinement for Temporal Action Detection
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - Temporal action detection (TAD) aims to localize the start and end frames of actions in untrimmed videos, which is a challenging task due to the similarity of adjacent frames and the ambiguity of action boundaries. Previous methods often generate coarse proposals first and then perform proposal-based refinement, which is coupled with prior action detectors and leads to proposal-oriented offsets. However, this paradigm increases the training difficulty of the TAD model and is heavily influenced by the quantity and quality of the proposals. To address the above issues, we decouple the refinement process from conventional TAD methods and propose a learnable, proposal-free refinement method for fine boundary localization, named RefineTAD. We first propose a multi-level refinement module to generate multi-scale boundary offsets, score offsets and boundary-aware probability at each time point based on the feature pyramid. Then, we propose an offset focusing strategy to progressively refine the predicted results of TAD models in a coarse-to-fine manner with our multi-scale offsets. We perform extensive experiments on three challenging datasets and demonstrate that our RefineTAD significantly improves the state-of-the-art TAD methods with minimal computational overhead.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 135
EP  - 143
DO  - 10.1145/3581783.3611872
AN  - WOS:001199449100016
AD  - Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China
AD  - Nanjing Univ, Nanjing, Peoples R China
Y2  - 2024-07-31
ER  -

TY  - CPAPER
AU  - Wei, Jiangchuan
AU  - Wang, Hanli
AU  - Yi, Yun
AU  - Li, Qinyu
AU  - Huang, Deshuang
A1  - IEEE
TI  - P3D-CTN: PSEUDO-3D CONVOLUTIONAL TUBE NETWORK FOR SPATIO-TEMPORAL ACTION DETECTION IN VIDEOS
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - The spatial independence and temporal continuity of video data as a whole are not fully investigated for video action detection. To tackle this issue, a deep network architecture is proposed, named Pseudo-3D Convolutional Tube Network (P3D-CTN). In particular, the proposed P3D-CTN integrates the frame-based two-dimensional convolutional module with the P3D convolutional module to balance the spatial and temporal information, and generates deeper features about human actions. Evaluations on two benchmark datasets (i.e., UCF-Sports and J-HMDB) demonstrate that the proposed P3D-CTN has superior performances in the task of action label prediction and yields state-of-the-art results for spatio-temporal action detection.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 300
EP  - 304
DO  - 10.1109/icip.2019.8802979
AN  - WOS:000521828600060
AD  - Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China
AD  - Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai, Peoples R China
Y2  - 2020-04-15
ER  -

TY  - CPAPER
AU  - Yan, Tingfa
A1  - IEEE
TI  - Hybrid Self-Supervised and Semi-Supervised Framework for Robust Spatio-Temporal Action Detection
T2  - 2024 IEEE 7TH INTERNATIONAL CONFERENCE ON AUTOMATION, ELECTRONICS AND ELECTRICAL ENGINEERING, AUTEEE
M3  - Proceedings Paper
CP  - 7th International Conference on Automation Electronics and Electrical Engineering
CL  - Shenyang, PEOPLES R CHINA
AB  - This paper presents a novel Hybrid Self-Supervised and Semi-Supervised Framework for Robust Spatio-Temporal Action Detection, which integrates the advantages of self-supervised transformation classification and semi-supervised temporal action proposal generation. The proposed approach addresses critical challenges in video action detection, specifically the requirement for spatially robust features and temporally consistent action proposals in contexts with limited labeled data. The self-supervised transformation classification branch enhances spatial generalization by training the model on a diverse set of transformations, while the semi-supervised temporal proposal branch employs a mean teacher framework with temporal perturbations to improve action proposal accuracy. The framework is evaluated on standard benchmarks, including UCF101, HMDB51, THUMOS14, and ActivityNet, where it consistently demonstrates superior performance compared to baseline models, achieving higher accuracy and mean Average Precision (mAP) across varying Intersection over Union (IoU) thresholds. An ablation study confirms the significance of each component, emphasizing the benefits of a hybrid approach in learning robust spatio-temporal representations. This research contributes an efficient, scalable solution for action detection, with potential applications in various video understanding tasks and domains.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 979-8-3503-7704-0
SN  - 979-8-3503-7703-3
DA  - 2024 
PY  - 2024
SP  - 289
EP  - 294
DO  - 10.1109/AUTEEE62881.2024.10869674
AN  - WOS:001443032700052
AD  - Yantai Vocat Coll, Yantai, Peoples R China
M2  - Yantai Vocat Coll
Y2  - 2025-04-11
ER  -

TY  - JOUR
AU  - Yang, Jianhua
AU  - Wang, Ke
AU  - Zhao, Lijun
AU  - Jiang, Zhiqiang
AU  - Li, Ruifeng
TI  - ActionMixer: Temporal action detection with Optimal Action Segment Assignment and mixers
T2  - EXPERT SYSTEMS WITH APPLICATIONS
M3  - Article
AB  - In this paper, we propose a novel method for dynamic label assignment in temporal action detection (TAD) called Optimal Action Segment Assignment (OASA). The OASA method converts label assignment into an optimal transportation problem by computing the cost matrix between predicted temporal action segments and groundtruths. The unit transportation cost between a predicted temporal segment and a groundtruth pair is defined as the weighted summation of action classification loss and temporal localization loss. Additionally, we deploy Adaptive Estimation of Candidate Segment Number (AE-CSN) to adaptively determine the number of positive samples for each groundtruth. After formulation, the label assignment problem is converted to find a global optimal assignment plan by minimizing the cost. Therefore, OASA eliminates the need for manually designed prior parameters, which exist in fixed label assignment methods, and improves the generalization of the algorithm between different datasets. To evaluate OASA, we also introduce a simple anchor-free temporal action detector called ActionMixer. It consists of two components: Temporal Mixer and Channel Mixer. The Temporal Mixer employs depth-wise convolution layers with large kernels to capture temporal information, while the Channel Mixer mixes and extracts features across the channel dimension. Extensive experiments conducted on the THUMOS-14, ActivityNet-1.3, and EPIC-Kitchens-100 datasets show that ActionMixer equipped with OASA achieves state-of-the-art performance, surpassing other advanced temporal action detection methods.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0957-4174
SN  - 1873-6793
DA  - 2024 MAR 1
PY  - 2024
VL  - 237
C7  - 121330
DO  - 10.1016/j.eswa.2023.121330
AN  - WOS:001079337900001
C6  - SEP 2023
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China
AD  - Harbin Inst Technol, Wuhu Robot Ind Technol Res Inst, Wuhu 241000, Peoples R China
Y2  - 2023-10-19
ER  -

TY  - JOUR
AU  - Moniruzzaman, Md.
AU  - Yin, Zhaozheng
TI  - Collaborative Foreground, Background, and Action Modeling Network for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - In this paper, we explore the problem of Weakly supervised Temporal Action Localization (W-TAL), where the task is to localize the temporal boundaries of all action instances in an untrimmed video with only video-level supervision. The existing W-TAL methods achieve a good action localization performance by separating the discriminative action and background frames. However, there is still a large performance gap between the weakly and fully supervised methods. The main reason comes from that there are plenty of ambiguous action and background frames in addition to the discriminative action and background frames. Due to the lack of temporal annotations in W-TAL, the ambiguous background frames may be localized as foreground and the ambiguous action frames may be suppressed as background, which result in false positives and false negatives, respectively. In this paper, we introduce a novel collaborative Foreground, Background, and Action Modeling Network (FBA Net) to suppress the background (i.e., both the discriminative and ambiguous background) frames, and localize the actual action-related (i.e., both the discriminative and ambiguous action) frames as foreground, for the precise temporal action localization. We design our FBA-Net with three branches: the foreground modeling (FM) branch, the background modeling (BM) branch, and the class-specific action and background modeling (CM) branch. The CM branch learns to highlight the video frames related to C action classes, and separate the action-related frames of C action classes from the (C + 1)th background class. The collaboration between FM and CM regularizes the consistency between the FM and the C action classes of CM, which reduces the false negative rate by localizing different actual-action-related (i.e., both the discriminative and ambiguous action) frames in a video as foreground. On the other hand, the collaboration between BM and CM regularizes the consistency between the BM and the (C + 1)th background class of CM, which reduces the false positive rate by suppressing both the discriminative and ambiguous background frames. Furthermore, the collaboration between FM and BM enforces more effective foreground background separation. To evaluate the effectiveness of our FBA-Net, we perform extensive experiments on two challenging datasets, THUMOS14 and ActivityNet1.3. The experiments show that our FBA-Net attains superior results.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2023 NOV
PY  - 2023
VL  - 33
IS  - 11
SP  - 6939
EP  - 6951
DO  - 10.1109/TCSVT.2023.3272891
AN  - WOS:001093434100050
AD  - SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA
AD  - SUNY Stony Brook, Dept Comp Sci, Dept Biomed Informat, Stony Brook, NY 11794 USA
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Yang, Jin
AU  - Wei, Ping
AU  - Zheng, Nanning
TI  - Cross Time-Frequency Transformer for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Most modern approaches in temporal action localization (TAL) mainly focus on time domain information, while neglecting the advantages of information from other domains. How to effectively utilize information from different domains and their interactions in a reasonable manner has been an attractive yet challenging issue in TAL. In this paper, we propose a novel cross time-frequency Transformer model (TFFormer) for TAL. A dual-branch network architecture is designed to capture the time and frequency features at multiple scales, using the multi-scale transformer in the time branch and the DB1 Discrete Wavelet Transform (DWT) in the frequency branch. To fuse these features from different domains, we propose a cross time-frequency attention mechanism that includes a time pathway and a frequency pathway, enhancing the interaction between the temporal and frequency features. Furthermore, a gated control mechanism is designed to aggregate features from different scales, characterizing the respective contributions of features at different scales. We also design a new regression loss function for locating the time boundaries. Extensive experiments were carried out on four challenging benchmark datasets, including two third-person datasets and two first-person datasets. The proposed method achieves impressive results on these datasets. Specifically, TFFormer achieves an average mAP of 23.2% on Ego4D and 25.6% on EPIC-Kitchens 100, which outperform previous state-of-the-arts by a large margin. It also obtains competitive results on ActivityNet v1.3 and THUMOS14, with an average mAP of 36.2% and 67.8%. We also conducted extensive ablation studies to validate the effectiveness of each component in the proposed method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 JUN
PY  - 2024
VL  - 34
IS  - 6
SP  - 4625
EP  - 4638
DO  - 10.1109/TCSVT.2023.3326692
AN  - WOS:001241605300035
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intell, Xian 710049, Shaanxi, Peoples R China
Y2  - 2024-08-30
ER  -

TY  - CPAPER
AU  - Ju, Chen
AU  - Zhao, Peisen
AU  - Chen, Siheng
AU  - Zhang, Ya
AU  - Wang, Yanfeng
AU  - Tian, Qi
A1  - IEEE
TI  - Divide and Conquer for Single-frame Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Single-frame temporal action localization (STAL) aims to localize actions in untrimmed videos with only one time-stamp annotation for each action instance. Existing methods adopt the one-stage framework but couple the counting goal and the localization goal. This paper proposes a novel two-stage framework for the STAL task with the spirit of divide and conquer. The instance counting stage leverages the location supervision to determine the number of action instances and divide a whole video into multiple video clips, so that each video clip contains only one complete action instance; and the location estimation stage leverages the category supervision to localize the action instance in each video clip. To efficiently represent the action instance in each video clip, we introduce the proposal-based representation, and design a novel differentiable mask generator to enable the end-to-end training supervised by category labels. On THUMOS14, GTEA, and BEOID datasets, our method outperforms state-of-the-art methods by 3.5%, 2.7%, 4.8% mAP on average. And extensive experiments verify the effectiveness of our method.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13435
EP  - 13444
DO  - 10.1109/ICCV48922.2021.01320
AN  - WOS:000798743203061
AD  - Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China
AD  - Huawei Cloud & AI, Kuala Lumpur, Malaysia
M2  - Huawei Cloud & AI
Y2  - 2022-06-24
ER  -

TY  - CPAPER
AU  - Deng, Bowen
AU  - Zhao, Shuangliang
AU  - Liu, Dongchang
ED  - Sheng, B
ED  - Bi, L
ED  - Kim, J
ED  - Magnenat-Thalmann, N
ED  - Thalmann, D
TI  - TadML: A Fast Temporal Action Detection with Mechanics-MLP
T2  - ADVANCES IN COMPUTER GRAPHICS, CGI 2023, PT I
M3  - Proceedings Paper
CP  - 40th Computer Graphics International Conference (CGI)
CL  - Shanghai, PEOPLES R CHINA
AB  - Temporal Action Detection (TAD) involves identifying action categories and their respective start and end frames in lengthy untrimmed videos, with current models utilizing both RGB and optical flow streams that require manual intervention, add computational complexity, and consume time. Moreover, two-stage approaches prioritizing proposal generation in the ini-tial stage result in a substantial reduction in inference speed. To address this, we propose a single-stage anchor-free method that solely utilizes the RGB stream and incorporates a novel Newtonian Mechanics-MLP architecture. Our model achieves comparable accuracy to existing state-of-the-art models but with significantly faster inference speeds, clocking in at an av-erage of 4.44 videos per second on THUMOS14. Our approach showcases the potential of MLP in downstream tasks like TAD. The source code is available at https://github.com/BonedDeng/TadML.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-50068-8
SN  - 978-3-031-50069-5
DA  - 2024 
PY  - 2024
VL  - 14495
SP  - 28
EP  - 40
DO  - 10.1007/978-3-031-50069-5_4
AN  - WOS:001206986300003
AD  - Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China
AD  - Guangxi Univ, Nanning 69121, Peoples R China
Y2  - 2024-05-08
ER  -

TY  - CPAPER
AU  - Narayan, Sanath
AU  - Cholakkal, Hisham
AU  - Khan, Fahad Shahbaz
AU  - Shao, Ling
A1  - IEEE
TI  - 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art [16]. Source code is available at https://github.com/naraysa/3c-net.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1550-5499
SN  - 978-1-7281-4803-8
DA  - 2019 
PY  - 2019
SP  - 8678
EP  - 8686
DO  - 10.1109/ICCV.2019.00877
AN  - WOS:000548549203080
AD  - Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates
M2  - Incept Inst Artificial Intelligence
Y2  - 2019-01-01
ER  -

TY  - CPAPER
AU  - Yu, Jun
AU  - Zheng, Yingshuai
AU  - Ruan, Shulan
AU  - Liu, Qi
AU  - Cheng, Zhiyuan
AU  - Wu, Jinze
ED  - Elkind, E
TI  - Actor-Multi-Scale Context Bidirectional Higher Order Interactive Relation Network for Spatial-Temporal Action Localization
T2  - PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023
M3  - Proceedings Paper
CP  - 32nd International Joint Conference on Artificial Intelligence (IJCAI)
CL  - Macao, PEOPLES R CHINA
AB  - The key to video action detection lies in the understanding of interaction between persons and background objects in a video. Current methods usually employ object detectors to extract objects directly or use grid features to represent objects in the environment, which underestimate the great potential of multi-scale context information (e.g., objects and scenes of different sizes). How to exactly represent the multi-scale context and make full utilization of it still remains an unresolved challenge for spatial-temporal action localization. In this paper, we propose a novel Actor-Multi-Scale Context Bidirectional Higher Order Interactive Relation Network (AMCRNet) that extracts multi-scale context through multiple pooling layers with different sizes. Specifically, we develop an Interactive Relation Extraction Module to model the higher-order relation between the target person and the context (e.g., other persons and objects). Along this line, we further propose a History Feature Bank and Interaction Module to achieve better performance by modeling such relation across continuing video clips. Extensive experimental results on AVA2.2 and UCF101-24 demonstrate the superiority and rationality of our proposed AMCRNet.
PU  - IJCAI-INT JOINT CONF ARTIF INTELL
PI  - FREIBURG
PA  - ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY
SN  - 978-1-956792-03-4
DA  - 2023 
PY  - 2023
SP  - 1676
EP  - 1685
AN  - WOS:001202344201085
AD  - Univ Sci & Technol China, Anhui Prov Key Lab Big Data Anal & Applicat, Langfang, Peoples R China
AD  - State Key Lab Cognit Intelligence, Hefei, Peoples R China
AD  - IFLYTEK Co Ltd, Hefei, Peoples R China
M2  - State Key Lab Cognit Intelligence
M2  - IFLYTEK Co Ltd
Y2  - 2024-07-27
ER  -

TY  - JOUR
AU  - Lu, Chongkai
AU  - Mak, Man-Wai
TI  - DITA: DETR with improved queries for end-to-end temporal action detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - The DEtection TRansformer (DETR), with its elegant architecture and set prediction, has revolutionized object detection. However, DETR-like models have yet to achieve comparable success in temporal action detection (TAD). To address this gap, we introduce a series of improvements to the original DETR, proposing a new DETR-based model for TAD that achieves competitive performance relative to conventional TAD methods. Specifically, we adapt advanced techniques from DETR variants used in object detection, including deformable attention, denoising training, and selective query recollection. Furthermore, we propose several new techniques aimed at enhancing detection precision and model convergence speed, such as geographic query grouping and learnable proposals. Leveraging these innovations, we introduce a new model called D ETR with I mproved queries for T emporal A ction Detection (DITA). DITA not only adheres to DETR's elegant design philosophy but is also competitive to state-of-the-art action detection models. Remarkably, it is the first TAD model to achieve an mAP over 70% on THUMOS14, outperforming the previous best DETR variant by 13.5 percentage points.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2024 SEP 1
PY  - 2024
VL  - 596
C7  - 127914
DO  - 10.1016/j.neucom.2024.127914
AN  - WOS:001254469300001
C6  - JUN 2024
AD  - Hong Kong Polytech Univ, Dept Elect & Elect Engn, Hong Kong, Peoples R China
Y2  - 2024-07-02
ER  -

TY  - JOUR
AU  - Liu, Meng
AU  - Nie, Liqiang
AU  - Wang, Yunxiao
AU  - Wang, Meng
AU  - Rui, Yong
TI  - A Survey on Video Moment Localization
T2  - ACM COMPUTING SURVEYS
M3  - Article
AB  - Video moment localization, also known as video moment retrieval, aims to search a target segment within a video described by a given natural language query. Beyond the task of temporal action localization whereby the target actions are pre-defined, video moment retrieval can query arbitrary complex activities. In this survey paper, we aim to present a comprehensive review of existing video moment localization techniques, including supervised, weakly supervised, and unsupervised ones. We also review the datasets available for video moment localization and group results of related work. In addition, we discuss promising future directions for this field, in particular large-scale datasets and interpretable video moment localization models.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 0360-0300
SN  - 1557-7341
DA  - 2023 SEP
PY  - 2023
VL  - 55
IS  - 9
C7  - 188
DO  - 10.1145/3556537
AN  - WOS:000924882300014
AD  - Shandong Jianzhu Univ, Fengming Rd, Jinan 250101, Shandong, Peoples R China
AD  - Harbin Inst Technol Shenzhen, Taoyuan Rd, Shenzhen 518055, Guangdong, Peoples R China
AD  - Shandong Univ, Binhai Rd, Qingdao 266237, Shandong, Peoples R China
AD  - Hefei Univ Technol, Tunxi Rd, Hefei 230002, Anhui, Peoples R China
AD  - Lenovo Co Ltd, Xibeiwang East Rd, Beijing 100094, Peoples R China
M2  - Lenovo Co Ltd
Y2  - 2023-03-01
ER  -

TY  - JOUR
AU  - Xia, Kun
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Zheng, Nanning
AU  - Tang, Wei
TI  - Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The main challenge of Temporal Action Localization is to retrieve subtle human actions from various co-occurring ingredients, e.g., context and background, in an untrimmed video. While prior approaches have achieved substantial progress through devising advanced action detectors, they still suffer from these co-occurring ingredients which often dominate the actual action content in videos. In this paper, we explore two orthogonal but complementary aspects of a video snippet, i.e., the action features and the co-occurrence features. Especially, we develop a novel auxiliary task by decoupling these two types of features within a video snippet and recombining them to generate a new feature representation with more salient action information for accurate action localization. We term our method RefactorNet, which first explicitly factorizes the action content and regularizes its co-occurrence features, and then synthesizes a new action-dominated video representation. Extensive experimental results and ablation studies on THUMOS14 and ActivityNet v1.3 demonstrate that our new representation, combined with a simple action detector, can significantly improve the action localization performance.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2206.11493
AN  - PPRN:12187190
AD  - Xian Jiaotong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples  R China
AD  - Univ Illinois Chicago, Chicago, IL 60607, USA
M2  - Univ Illinois Chicago
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Bagchi, Anurag
AU  - Mahmood, Jazib
AU  - Fernandes, Dolton
AU  - Sarvadevabhatla, Ravi Kiran
ED  - Farinella, GM
ED  - Radeva, P
ED  - Bouatouch, K
TI  - Hear Me out: Fusional Approaches for Audio Augmented Temporal Action Localization
T2  - PROCEEDINGS OF THE 17TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5
M3  - Proceedings Paper
CP  - 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP) / 17th International Conference on Computer Vision Theory and Applications (VISAPP)
CL  - ELECTR NETWORK
AB  - State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality unexploited. Audio fusion has been explored for the related but an arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for the state-of-the-art video-only TAL approaches. Specifically, they help achieve a new state-of-the-art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@O.5). Our experiments include ablations involving multiple fusion schemes, modality combinations, and TAL architectures. Our code, models, and associated data are available at https://github.com/skelemoa/tal-hmo.
PU  - SCITEPRESS
PI  - SETUBAL
PA  - AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
SN  - 2184-4321
SN  - 978-989-758-555-5
DA  - 2022 
PY  - 2022
SP  - 144
EP  - 154
DO  - 10.5220/0010832700003124
AN  - WOS:000777505000013
AD  - Int Inst Informat Technol, Hyderabad, India
AD  - IIIT Hyderabad, Ctr Visual Informat Technol CVIT, Hyderabad, India
Y2  - 2022-04-22
ER  -

TY  - CPAPER
AU  - Liu, Ziyi
AU  - Wang, Le
AU  - Zhang, Qilin
AU  - Gao, Zhanning
AU  - Niu, Zhenxing
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - IEEE
TI  - Weakly Supervised Temporal Action Localization through Contrast based Evaluation Networks
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Weakly-supervised temporal action localization (WS-TAL) is a promising but challenging task with only video-level action categorical labels available during training. Without requiring temporal action boundary annotations in training data, WS-TAL could possibly exploit automatically retrieved video tags as video-level labels. However, such coarse video-level supervision inevitably incurs confusions, especially in untrimmed videos containing multiple action instances. To address this challenge, we propose the Contrast-based Localization EvaluAtioN Network (CleanNet) with our new action proposal evaluator, which provides pseudo-supervision by leveraging the temporal contrast in snippet-level action classification predictions. Essentially, the new action proposal evaluator enforces an additional temporal contrast constraint so that high-evaluation-score action proposals are more likely to coincide with true action instances. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Experiments on THUMOS14 and ActivityNet datasets validate the efficacy of CleanNet against existing state-of-the-art WS-TAL algorithms.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 978-1-7281-4803-8
DA  - 2019 
PY  - 2019
SP  - 3898
EP  - 3907
DO  - 10.1109/ICCV.2019.00400
AN  - WOS:000531438104005
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - HERE Technol, Amsterdam, Netherlands
AD  - Alibaba Grp, DAMO Acad, Hangzhou, Peoples R China
AD  - Alibaba Grp, Machine Intelligence Israel Lab, Hangzhou, Peoples R China
AD  - Wormpex AI Res, Bellevue, WA USA
M2  - HERE Technol
M2  - Wormpex AI Res
Y2  - 2020-08-05
ER  -

TY  - CPAPER
AU  - Wang, Yu
AU  - Zhao, Shengjie
A1  - IEEE
TI  - WEAKLY-SUPERVISED ACTION LOCALIZATION BY HIERARCHICAL ATTENTION MECHANISM WITH MULTI-SCALE FUSION STRATEGIES
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME 2024
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Niagra Falls, CANADA
AB  - Weakly-supervised temporal action localization focuses on locating action intervals when merely video-level supervised signals are available. Conventional methods mostly rely on the attention framework, which generates a set of scores indicating the confidence that the video snippet belongs to the foreground, the background, and the context, respectively. However, such methods fail to consider the structural properties of snippet-level features when generating attention scores, and these structural properties are critical for capturing contextual information in temporal tasks. To this end, we propose a hierarchical attention generation mechanism with multi-scale fusion strategies to model such structural information. Besides, to resolve action-context confusion issues that are quite intractable in weakly-supervised action localization tasks, metric learning is further introduced into our framework to suppress context features from approaching action features, while encouraging them to be close to background features. Finally, our model is evaluated on THUMOS14 and ActivityNet1.3 benchmarks, and the results demonstrate that the proposed approach achieves desirable performance.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 979-8-3503-9015-5
SN  - 979-8-3503-9016-2
DA  - 2024 
PY  - 2024
DO  - 10.1109/ICME57554.2024.10688175
AN  - WOS:001364925203027
AD  - Tongji Univ, Coll Software Engn, Shanghai, Peoples R China
Y2  - 2025-04-16
ER  -

TY  - JOUR
AU  - Kalogeiton, Vicky
AU  - Weinzaepfel, Philippe
AU  - Ferrari, Vittorio
AU  - Schmid, Cordelia
TI  - Action Tubelet Detector for Spatio-Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level that are then linked or tracked across time. In this paper, we leverage the temporal continuity of videos instead of operating at the frame level. We propose the ACtion Tubelet detector (ACT-detector) that takes as input a sequence of frames and outputs tubelets, i.e., sequences of bounding boxes with associated scores. The same way state-of-the-art object detectors rely on anchor boxes, our ACT-detector is based on anchor cuboids. We build upon the SSD framework. Convolutional features are extracted for each frame, while scores and regressions are based on the temporal stacking of these features, thus exploiting information from a sequence. Our experimental results show that leveraging sequences of frames significantly improves detection performance over using individual frames. The gain of our tubelet detector can be explained by both more accurate scores and more precise localization. Our ACT-detector outperforms the state-of-the-art methods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in particular at high overlap thresholds.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1705.01861
AN  - PPRN:12717184
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Pujol-Perich, David
AU  - Clapes, Albert
AU  - Escalera, Sergio
TI  - SADA: Semantic adversarial unsupervised domain adaptation for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - Temporal Action Localization (TAL) is a complex task that poses relevant challenges, particularly when attempting to generalize on new – unseen – domains in real- world applications. These scenarios, despite realistic, are often neglected in the literature, exposing these solutions to important performance degradation. In this work, we tackle this issue by introducing, for the first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse TAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation (SADA). Our contributions are threefold: (1) we pioneer the development of a domain adaptation model that operates on realistic sparse action detection benchmarks; (2) we tackle the limitations of global-distribution alignment techniques by introducing a novel adversarial loss that is sensitive to local class distributions, ensuring finer-grained adaptation; and (3) we present a novel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate multiple domain shifts in a comprehensive manner. Our experiments indicate that SADA improves the adaptation across domains when compared to fully supervised state-of-the-art and alternative UDA methods, attaining a performance boost of up to 6.14% mAP. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2312.13377
AN  - PPRN:86787065
AD  - Univ Barcelona, Barcelona, Spain
AD  - Comp Vis Ctr, Barcelona, Spain
M2  - Univ Barcelona
Y2  - 2025-01-08
ER  -

TY  - JOUR
AU  - Tang, Tuan N.
AU  - Kim, Kwonyoung
AU  - Sohn, Kwanghoon
TI  - TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) is a challenging task in video understanding that aims to identify and localize actions within a video sequence. Recent studies have emphasized the importance of applying long-term temporal context modeling (TCM) blocks to the extracted video clip features such as employing complex self-attention mechanisms. In this paper, we present the simplest method ever to address this task and argue that the extracted video clip features are already informative to achieve outstanding performance without sophisticated architectures. To this end, we introduce TemporalMaxer, which minimizes long-term temporal context modeling while maximizing information from the extracted video clip features with a basic, parameter-free, and local region operating max-pooling block. Picking out only the most critical information for adjacent and local clip embeddings, this block results in a more efficient TAL model. We demonstrate that TemporalMaxer outperforms other state-of-the-art methods that utilize long-term TCM such as self-attention on various TAL datasets while requiring significantly fewer parameters and computational resources. The code for our approach is publicly available at https://github.com/TuanTNG/TemporalMaxer
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.09055
AN  - PPRN:46905735
AD  - Yonsel Univ, Sch Elect & Elect Engn, Seoul, South Korea
M2  - Yonsel Univ
Y2  - 2023-03-27
ER  -

TY  - CPAPER
AU  - Chen, Mengyuan
AU  - Gao, Junyu
AU  - Xu, Changsheng
A1  - IEEE
TI  - Cascade Evidential Learning for Open-world Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Targeting at recognizing and localizing action instances with only video-level labels during training, Weakly-supervised Temporal Action Localization (WTAL) has achieved significant progress in recent years. However, living in the dynamically changing open world where unknown actions constantly spring up, the closed-set assumption of existing WTAL methods is invalid. Compared with traditional open-set recognition tasks, Open-world WTAL (OWTAL) is challenging since not only are the annotations of unknown samples unavailable, but also the fine-grained annotations of known action instances can only be inferred ambiguously from the video category labels. To address this problem, we propose a Cascade Evidential Learning framework at an evidence level, which targets at OWTAL for the first time. Our method jointly leverages multi-scale temporal contexts and knowledge-guided prototype information to progressively collect cascade and enhanced evidence for known action, unknown action, and background separation. Extensive experiments conducted on THUMOS-14 and ActivityNet-v1.3 verify the effectiveness of our method. Besides the classification metrics adopted by previous openset recognition methods, we also evaluate our method on localization metrics which are more reasonable for OWTAL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 14741
EP  - 14750
DO  - 10.1109/CVPR52729.2023.01416
AN  - WOS:001062522107007
AD  - Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing, Peoples R China
AD  - UCAS, Sch Artificial Intelligence, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
M2  - UCAS
Y2  - 2023-11-15
ER  -

TY  - CPAPER
AU  - Zhou, Jianxiong
AU  - Wu, Ying
A1  - IEEE
TI  - Temporal Feature Enhancement Dilated Convolution Network for Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - 23rd IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Weakly-supervised Temporal Action Localization (WTAL) aims to classify and localize action instances in untrimmed videos with only video-level labels. Existing methods typically use snippet-level RGB and optical flow features extracted from pre-trained extractors directly. Because of two limitations: the short temporal span of snippets and the inappropriate initial features, these WTAL methods suffer from the lack of effective use of temporal information and have limited performance. In this paper, we propose the Temporal Feature Enhancement Dilated Convolution Network (TFE-DCN) to address these two limitations. The proposed TFE-DCN has an enlarged receptive field that covers a long temporal span to observe the full dynamics of action instances, which makes it powerful to capture temporal dependencies between snippets. Furthermore, we propose the Modality Enhancement Module that can enhance RGB features with the help of enhanced optical flow features, making the overall features appropriate for the WTAL task. Experiments conducted on THUMOS'14 and ActivityNet v1.3 datasets show that our proposed approach far outperforms state-of-the-art WTAL methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-6654-9346-8
DA  - 2023 
PY  - 2023
SP  - 6017
EP  - 6026
DO  - 10.1109/WACV56688.2023.00597
AN  - WOS:000971500206015
AD  - Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL 60208 USA
Y2  - 2023-07-22
ER  -

TY  - JOUR
AU  - Hyun, Jeongseok
AU  - Han, Su Ho
AU  - Kang, Hyolim
AU  - Lee, Joon-Young
AU  - Kim, Seon Joo
TI  - Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.07024
AN  - PPRN:90749699
AD  - Yonsei Univ, Seoul, South Korea
AD  - Adobe Res, San Francisco, CA 94107, USA
M2  - Adobe Res
Y2  - 2025-01-26
ER  -

TY  - JOUR
AU  - Zhang, Chenlin
AU  - Wu, Jianxin
AU  - Li, Yin
TI  - ActionFormer: Localizing Moments of Actions with Transformers
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at http://github.com/happyharrycn/actionformer_release.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2202.07925
AN  - PPRN:12824231
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - 4Paradigm Inc, Beijing, Peoples R China
AD  - Univ Wisconsin Madison, Madison, WI 53706, USA
M2  - 4Paradigm Inc
M2  - Univ Wisconsin Madison
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Zhang, Quan
AU  - Qi, Yuxin
TI  - Can MLLMs Guide Weakly-Supervised Temporal Action Localization Tasks?
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2411.08466
AN  - PPRN:119218012
AD  - Tsinghua Univ, Beijing, Peoples R China
AD  - Shanghai Jiao Tong Univ, Shanghai, Peoples R China
M2  - Shanghai Jiao Tong Univ
Y2  - 2024-12-21
ER  -

TY  - JOUR
AU  - Li, Qiang
AU  - Liu, Di
AU  - Kong, Jun
AU  - Li, Sen
AU  - Xu, Hui
AU  - Wang, Jianzhong
TI  - Temporal Action Localization with Cross Layer Task Decoupling and Refinement
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action localization (TAL) involves dual tasks to classify and localize actions within untrimmed videos. However, the two tasks often have conflicting requirements for features. Existing methods typically employ separate heads for classification and localization tasks but share the same input feature, leading to suboptimal performance. To address this issue, we propose a novel TAL method with Cross Layer Task Decoupling and Refinement (CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates semantically strong features from higher pyramid layers and detailed boundary-aware boundary features from lower pyramid layers to effectively disentangle the action classification and localization tasks. Moreover, the multiple features from cross layers are also employed to refine and align the disentangled classification and regression results. At last, a lightweight Gated Multi-Granularity (GMG) module is proposed to comprehensively extract and aggregate video features at instant, local, and global temporal granularities. Benefiting from the CLTDR and GMG modules, our method achieves state-of-the-art performance on five challenging benchmarks: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100, ActivityNet-1.3, and HACS. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2412.09202
AN  - PPRN:119900324
AD  - Northeast Normal Univ, Changchun, Peoples R China
AD  - Northeast Elect Power Univ, Jilin, Peoples R China
AD  - MOE, KLAS, Beijing, Peoples R China
AD  - Changchun Humanities & Sci Coll, Changchun, Peoples R China
M2  - Northeast Normal Univ
M2  - MOE
M2  - Changchun Humanities & Sci Coll
Y2  - 2025-01-21
ER  -

TY  - JOUR
AU  - Shou, Zheng
AU  - Gao, Hang
AU  - Zhang, Lei
AU  - Miyazawa, Kazuyuki
AU  - Chang, Shih-Fu
TI  - AutoLoc: Weakly-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Localization (TAL) in untrimmed video is important for many applications. But it is very expensive to annotate the segment-level ground truth (action class and temporal boundary). This raises the interest of addressing TAL with weak supervision, namely only video-level annotations are available during training). However, the state-of-the-art weakly-supervised TAL methods only focus on generating good Class Activation Sequence (CAS) over time but conduct simple thresholding on CAS to localize actions. In this paper, we first develop a novel weakly-supervised TAL framework called AutoLoc to directly predict the temporal boundary of each action instance. We propose a novel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed segment-level supervision for training such a boundary predictor. Our method achieves dramatically improved performance: under the IoU threshold 0.5, our method improves mAP on THUMOS'14 from 13.7% to 21.2% and mAP on ActivityNet from 7.4% to 27.3%. It is also very encouraging to see that our weakly-supervised method achieves comparable results with some fully-supervised methods.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.08333
AN  - PPRN:13503302
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Zhang, Zejian
AU  - Palmero, Cristina
AU  - Escalera, Sergio
A1  - IEEE
TI  - DualH: A Dual Hierarchical Model for Temporal Action Localization
T2  - 2024 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2024
M3  - Proceedings Paper
CP  - 18th International Conference on Automatic Face and Gesture Recognition (FG)
CL  - Istanbul, TURKEY
AB  - Temporal action localization aims to detect action boundaries and classify action labels in untrimmed videos. Recent efforts have focused on utilizing Transformers to encode extracted features into a bottom-up pyramid feature map and localizing actions from all levels of the pyramid while only considering features from those specific levels. A limitation of this bottom-up encoding is that the lower-level features lack broader contexts, while the upper-level features lose local boundary information. Consequently, the performance of the model may be hindered. In this work, we propose a dual hierarchical model to mitigate this issue. The first hierarchy operates on the full temporal sequence to encode features at multiple scales. These features are fused to ensure all temporal locations consider both local boundary information and broader contexts. Next, the fused feature is downsampled to a pyramid representation for localizing actions at multiple resolutions. Experimental results on THUMOS14, ActivityNet-1.3, and EPIC-KITCHENS-100 demonstrate that our dual hierarchical design improves the performance with respect to the conventional bottom-up pyramid Transformer-based models.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2326-5396
SN  - 979-8-3503-9494-8
SN  - 979-8-3503-9495-5
DA  - 2024 
PY  - 2024
DO  - 10.1109/FG59268.2024.10581917
AN  - WOS:001270976600035
AD  - Univ Barcelona, Barcelona, Spain
AD  - Comp Vis Ctr, Barcelona, Spain
Y2  - 2024-10-16
ER  -

TY  - JOUR
AU  - Narayan, Sanath
AU  - Cholakkal, Hisham
AU  - Shahbaz Khan, Fahad
AU  - Shao, Ling
TI  - 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1908.08216
AN  - PPRN:21808229
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Shi, Haichao
AU  - Zhang, Xiao-Yu
AU  - Li, Changsheng
TI  - StochasticFormer: Stochastic Modeling for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly supervised temporal action localization (WS-TAL) aims to identify the time intervals corresponding to actions of interest in untrimmed videos with video-level weak supervision. For most existing WS-TAL methods, two commonly encountered challenges are under-localization and over-localization, which inevitably bring about severe performance deterioration. To address the issues, this paper proposes a transformer-structured stochastic process modeling framework, namely StochasticFormer, to fully investigate finer-grained interactions among the intermediate predictions to achieve further refined localization. StochasticFormer is built on a standard attention-based pipeline to derive preliminary frame/snippet-level predictions. Then, the pseudo localization module generates variable-length pseudo action instances with the corresponding pseudo labels. Using the pseudo "action instance - action category " pairs as fine-grained pseudo supervision, the stochastic modeler aims to learn the underlying interaction among the intermediate predictions with an encoder-decoder network. The encoder consists of the deterministic and latent path to capture the local and global information, which are subsequently integrated by the decoder to obtain reliable predictions. The framework is optimized with three carefully designed losses, i.e. the video-level classification loss, the frame-level semantic coherence loss, and the ELBO loss. Extensive experiments on two benchmarks, i.e., THUMOS14 and ActivityNet1.2, have shown the efficacy of StochasticFormer compared with the state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2023 
PY  - 2023
VL  - 32
SP  - 1379
EP  - 1389
DO  - 10.1109/TIP.2023.3244411
AN  - WOS:000940169100007
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing 100193, Peoples R China
AD  - Beijing Inst Technol, Beijing 100081, Peoples R China
Y2  - 2023-03-20
ER  -

TY  - CPAPER
AU  - Xia, Kun
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Hua, Gang
AU  - Tang, Wei
A1  - IEEE
TI  - Learning from Noisy Pseudo Labels for Semi-Supervised Temporal Action Localization
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Semi-Supervised Temporal Action Localization (SS-TAL) aims to improve the generalization ability of action detectors with large-scale unlabeled videos. Albeit the recent advancement, one of the major challenges still remains: noisy pseudo labels hinder efficient learning on abundant unlabeled videos, embodied as location biases and category errors. In this paper, we dive deep into such an important but understudied dilemma. To this end, we propose a unified framework, termed Noisy Pseudo-Label Learning, to handle both location biases and category errors. Specifically, our method is featured with (1) Noisy Label Ranking to rank pseudo labels based on the semantic confidence and boundary reliability, (2) Noisy Label Filtering to address the class-imbalance problem of pseudo labels caused by category errors, (3) Noisy Label Learning to penalize inconsistent boundary predictions to achieve noise-tolerant learning for heavy location biases. As a result, our method could effectively handle the label noise problem and improve the utilization of a large amount of unlabeled videos. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate the effectiveness of our method. The code is available at github.com/kunnxia/NPL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 10126
EP  - 10135
DO  - 10.1109/ICCV51070.2023.00932
AN  - WOS:001169499002053
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Natl Engn Res Ctr Visual Informat & Applicat, Natl Key Lab Human Machine Hybrid Augmented Intel, Xian, Peoples R China
AD  - Wormpex AI Res, Bellevue, WA USA
AD  - Univ Illinois, Chicago, IL USA
M2  - Wormpex AI Res
Y2  - 2024-04-06
ER  -

TY  - JOUR
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Li, Hongsheng
TI  - Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization aims to localize temporal boundaries of actions and simultaneously identify their categories with only video-level category labels. Many existing methods seek to generate pseudo labels for bridging the discrepancy between classification and localization, but usually only make use of limited contextual information for pseudo label generation. To alleviate this problem, we propose a representative snippet summarization and propagation framework. Our method seeks to mine the representative snippets in each video for propagating information between video snippets to generate better pseudo labels. For each video, its own representative snippets and the representative snippets from a memory bank are propagated to update the input features in an intraand inter-video manner. The pseudo labels are generated from the temporal class activation maps of the updated features to rectify the predictions of the main branch. Our method obtains superior performance in comparison to the existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in terms of average mAP on THUMOS14. Our code is available at&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2203.02925
AN  - PPRN:12111714
AD  - Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
M2  - Chinese Univ Hong Kong
M2  - Ctr Perceptual & Interact Intelligence
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Xiang, Tao
TI  - Few-Shot Temporal Action Localization with Query Adaptive Transformer
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing temporal action localization (TAL) works rely on a large number of training videos with exhaustive segment-level annotation, preventing them from scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this setting is not only unnatural actions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further, a novel FS-TAL model is proposed which maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both the new class and each video of that class simultaneously. This is achieved by introducing a query adaptive Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our method can outperform all the state of the art alternatives significantly in both single-domain and cross-domain scenarios.&nbsp;
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2110.10552
AN  - PPRN:11937186
AD  - Univers Surrey, Ctr Vis Speech & Signal, Proc CVSSP, Guildford, England
AD  - iFlyTek, Surrey Joint Res, Ctr Artificial Intelligence, Beijing, Peoples R China
M2  - Univers Surrey
M2  - iFlyTek
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Yang, Zichen
AU  - Qin, Jie
AU  - Huang, Di
A1  - Assoc Advancement Artificial Intelligence
TI  - ACGNet: Action Complement Graph Network for Weakly-Supervised Temporal Action Localization
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS' 14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-876-3
DA  - 2022 
PY  - 2022
SP  - 3090
EP  - 3098
AN  - WOS:000893636203020
AD  - Beihang Univ, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China
AD  - Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China
Y2  - 2023-02-17
ER  -

TY  - JOUR
AU  - Ju, Chen
AU  - Zhao, Peisen
AU  - Zhang, Ya
AU  - Wang, Yanfeng
AU  - Tian, Qi
TI  - Point-Level Temporal Action Localization: Bridging Fully-supervised Proposals to Weakly-supervised Losses
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Point-Level temporal action localization (PTAL) aims to localize actions in untrimmed videos with only one timestamp annotation for each action instance. Existing methods adopt the frame-level prediction paradigm to learn from the sparse single-frame labels. However, such a framework inevitably suffers from a large solution space. This paper attempts to explore the proposal-based prediction paradigm for point-level annotations, which has the advantage of more constrained solution space and consistent predictions among neighboring frames. The point-level annotations are first used as the keypoint supervision to train a keypoint detector. At the location prediction stage, a simple but effective mapper module, which enables back-propagation of training errors, is then introduced to bridge the fully-supervised framework with weak supervision. To our best of knowledge, this is the first work to leverage the fully-supervised paradigm for the point-level setting. Experiments on THUMOS14, BEOID, and GTEA verify the effectiveness of our proposed method both quantitatively and qualitatively, and demonstrate that our method outperforms state-of-the-art methods.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2012.08236
AN  - PPRN:22521437
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Lim, Geuntaek
AU  - Kim, Hyunwoo
AU  - Kim, Joonsoo
AU  - Choi, Yukyung
TI  - Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization (WTAL) aims to detect action instances in untrimmed videos using only video-level annotations. Since many existing works optimize WTAL models based on action classification labels, they encounter the task discrepancy problem (i.e., localization-by-classification). To tackle this issue, recent studies have attempted to utilize action category names as auxiliary semantic knowledge through vision-language pre-training (VLP). However, there are still areas where existing research falls short. Previous approaches primarily focused on leveraging textual information from language models but overlooked the alignment of dynamic human action and VLP knowledge in a joint space. Furthermore, the deterministic representation employed in previous studies struggles to capture fine-grained human motions. To address these problems, we propose a novel framework that aligns human action knowledge and VLP knowledge in a probabilistic embedding space. Moreover, we propose intra- and inter-distribution contrastive learning to enhance the probabilistic embedding space based on statistical similarities. Extensive experiments and ablation studies reveal that our method significantly outperforms all previous state-of-the-art methods. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.05955
AN  - PPRN:91350236
AD  - Sejong Univ, Seoul, South Korea
AD  - Elect & Telecommun Res Inst, Daejeon, South Korea
M2  - Elect & Telecommun Res Inst
Y2  - 2024-08-22
ER  -

TY  - JOUR
AU  - Yoon, Da-Hye
AU  - Cho, Nam-Gyu
AU  - Lee, Seong-Whan
TI  - A novel online action detection framework from untrimmed video streams
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision. It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOS'14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings. (C) 2020 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2020 OCT
PY  - 2020
VL  - 106
C7  - 107396
DO  - 10.1016/j.patcog.2020.107396
AN  - WOS:000541777200006
AD  - Korea Univ, Dept Comp & Radio Commun Engn, Seoul 02841, South Korea
AD  - Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea
AD  - Korea Univ, Dept Artificial Intelligence, Seoul 02841, South Korea
Y2  - 2020-10-01
ER  -

TY  - JOUR
AU  - Lee, Sangjin
AU  - Lim, Jaebin
AU  - Moon, Jinyoung
AU  - Jung, Chanho
TI  - An Improved Point-Level Supervision Method for Temporal Action Localization
T2  - IEEE ACCESS
M3  - Article
AB  - Recently, with the expansion of the video platform market, research has been actively conducted on temporal action localization (TAL) for detecting actions in atypical videos. Most learning methods for TAL include full and weak supervision (weak supervision with only action classes) approaches. Full supervision requires considerable time for labeling and weak supervision exhibits low localization performance owing to the lack of informative annotations. To solve this problem, point-level weak supervision using single-point timestamps within the temporal interval of action instances has been proposed, which demonstrates superior performance to weakly-supervised methods using only action classes of action instances. In this study, we proposed an improved point-level supervision mechanism that provides point-level annotations for each action and background instance. In addition, a widely used multiple instance learning (MIL)-based framework was used to verify the proposed method, and pseudo-labels were used for action instance boundary learning. Also, the background point loss was designed to leverage the added point-level annotations. The datasets used in the experiment were THUMOS14, GTEA, BEOID, and ActivityNet1.2, and improved results were obtained compared to existing point-level supervision. The code is available from https://github.com/sang9390/An-Improved-Point-Level-Supervision-Method-for-TAL.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2023 
PY  - 2023
VL  - 11
SP  - 71260
EP  - 71268
DO  - 10.1109/ACCESS.2023.3294572
AN  - WOS:001033502100001
AD  - Hanbat Natl Univ, Dept Elect Engn, Daejeon 34158, South Korea
AD  - Elect & Telecommun Res Inst ETRI, Daejeon 34129, South Korea
Y2  - 2023-08-03
ER  -

TY  - JOUR
AU  - Chen, Zhengyan
AU  - Liu, Hong
AU  - Zhang, Linlin
AU  - Liao, Xin
TI  - Multi-Dimensional Attention With Similarity Constraint for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised temporal action localization (WTAL) is a challenging task in understanding untrimmed videos, in which no frame-wise annotation is provided during training, only the video-level category label is available. Current methods mainly adopt temporal attention branches to conduct foreground-background separation with RGB and optical flow features simply concatenated, regardless of the discriminative spacial features and the complementarity between different modalities. In this work, we propose a Multi-Dimensional Attention (MDA) method to explore attention mechanism across three dimensions in weakly supervised action localization, i.e., 1) temporal attention that focuses on segments containing action instances, 2) channel attention that discovers the most relevant cues for action description, and 3) modal attention that fuses RGB and flow information adaptively based on feature magnitudes during background modeling. In addition, we introduce a similarity constraint loss to refine the action segment representation in feature space, which helps the network to detect less discriminative frames of an action to capture the full action boundaries. The proposed MDA with similarity constraints can be easily applied to existing action detection frameworks with few parameters. Extensive experiments on THUMOS'14 and ActivityNet v1.2 datasets show that the proposed method outperforms the current state-of-the-art WTAL approaches, and achieves comparable results with some advanced fully-supervised methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 4349
EP  - 4360
DO  - 10.1109/TMM.2022.3174344
AN  - WOS:001089390200020
AD  - Peking Univ, Shenzhen Grad Sch, Key Lab Machine Percept, Beijing 100871, Peoples R China
AD  - Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China
Y2  - 2023-11-15
ER  -

TY  - JOUR
AU  - Moniruzzaman, Md
AU  - Yin, Zhaozheng
TI  - Feature Weakening, Contextualization, and Discrimination for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Weakly-supervised Temporal Action Localization (W-TAL) aims to train a model to localize all action instances potentially from different classes in an untrimmed video, using a training dataset that has video-level action class labels but has no detailed annotations on the start and end timestamps of action instances. We propose to solve the W-TAL problem from the feature learning aspect, with a new architecture, termed F3-Net, which includes (1) a Feature Weakening (FW) module that can identify and randomly weaken either the most discriminative action or the most discriminative background features over the training iterations to force the network to precisely localize the action instances in both discriminative and ambiguous action-related frames, without spreading to the background intervals; (2) a Feature Contextualization (FC) module that can infer the global contexts among video segments and attentionally fuse them with the local contexts from individual video segments to generate more representative features; and (3) a Feature Discrimination (FD) module that can highlight the most discriminative video segments/classes corresponding to each class/segment, respectively, for localizing multiple action instances from different classes within a video. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our F3-Net, and the FW and FC are also effective plug-in modules to improve other methods. This project will be available at https://moniruzzamanmd.github.io/F3-Net/https://moniruzzamanmd.github.io/F3-Net/
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2024 
PY  - 2024
VL  - 26
SP  - 270
EP  - 283
DO  - 10.1109/TMM.2023.3263965
AN  - WOS:001140881500021
AD  - SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA
AD  - SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA
Y2  - 2024-02-08
ER  -

TY  - CPAPER
AU  - Su, Haisheng
AU  - Zhao, Xu
AU  - Lin, Tianwei
ED  - Jawahar, CV
ED  - Li, H
ED  - Mori, G
ED  - Schindler, K
TI  - Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action Localization
T2  - COMPUTER VISION - ACCV 2018, PT II
M3  - Proceedings Paper
CP  - 14th Asian Conference on Computer Vision (ACCV)
CL  - Perth, AUSTRALIA
AB  - Weakly supervised temporal action localization, which aims at temporally locating action instances in untrimmed videos using only video-level class labels during training, is an important yet challenging problem in video analysis. Many current methods adopt the "localization by classification" framework: first do video classification, then locate temporal area contributing to the results most. However, this framework fails to locate the entire action instances and gives little consideration to the local context. In this paper, we present a novel architecture called Cascaded Pyramid Mining Network (CPMN) to address these issues using two effective modules. First, to discover the entire temporal interval of specific action, we design a two-stage cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where new and complementary regions are mined through feeding the erased feature maps of discovered regions back to the system. Second, to exploit hierarchical contextual information in videos and reduce missing detections, we design a pyramid module which produces a scale-invariant attention map through combining the feature maps from different levels. Final, we aggregate the results of two modules to perform action localization via locating high score areas in temporal Class Activation Sequence (CAS). Extensive experiments conducted on THUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our method.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-20889-9
SN  - 978-3-030-20890-5
DA  - 2019 
PY  - 2019
VL  - 11362
SP  - 558
EP  - 574
DO  - 10.1007/978-3-030-20890-5_36
AN  - WOS:000492902300036
AD  - Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China
Y2  - 2019-11-06
ER  -

TY  - JOUR
AU  - Li, Ding
AU  - Yang, Xuebing
AU  - Tang, Yongqiang
AU  - Zhang, Chenyang
AU  - Zhang, Wensheng
AU  - Ma, Lizhuang
TI  - Active learning with effective scoring functions for semi-supervised temporal action localization
T2  - DISPLAYS
M3  - Article
AB  - Temporal Action Localization (TAL) aims to predict both action category and temporal boundary of action instances in untrimmed videos, i.e., start and end time. Existing works usually adopt fully-supervised solutions, however, one of the practical bottlenecks in these solutions is the large amount of labeled training data required. To reduce expensive human label cost, this paper focuses on a rarely investigated yet practical task named semi-supervised TAL and proposes an effective active learning method, named AL-STAL. We leverage four steps for actively selecting video samples with high informativeness and training the localization model, named Train, Query, Annotate, Append. Two scoring functions that consider the uncertainty of localization model are equipped in AL-STAL, thus facilitating the video sample ranking and selection. One takes entropy of predicted label distribution as measure of uncertainty, named Temporal Proposal Entropy (TPE). And the other introduces a new metric based on mutual information between adjacent action proposals, named Temporal Context Inconsistency (TCI). To validate the effectiveness of proposed method, we conduct extensive experiments on three benchmark datasets THUMOS'14, ActivityNet 1.3 and ActivityNet 1.2. Experiment results show that AL-STAL outperforms the existing competitors and achieves satisfying performance compared with fully-supervised learning.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0141-9382
SN  - 1872-7387
DA  - 2023 JUL
PY  - 2023
VL  - 78
C7  - 102434
DO  - 10.1016/j.displa.2023.102434
AN  - WOS:000981681400001
C6  - APR 2023
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China
AD  - Shanghai Jiao Tong Univ, Dept Comp & Engn, Shanghai 200240, Peoples R China
Y2  - 2023-05-25
ER  -

TY  - JOUR
AU  - Rathod, Vivek
AU  - Seybold, Bryan
AU  - Vijayanarasimhan, Sudheendra
AU  - Myers, Austin
AU  - Gu, Xiuye
AU  - Birodkar, Vighnesh
AU  - Ross, David A
TI  - Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Detecting actions in untrimmed videos should not be limited to a small, closed set of classes. We present a simple, yet effective strategy for open-vocabulary temporal action detection utilizing pretrained image-text co-embeddings. Despite being trained on static images rather than videos, we show that image-text co-embeddings enable openvocabulary performance competitive with fully-supervised models. We show that the performance can be further improved by ensembling the image-text features with features encoding local motion, like optical flow based features, or other modalities, like audio. In addition, we propose a more reasonable open-vocabulary evaluation setting for the ActivityNet data set, where the category splits are based on similarity rather than random assignment.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2212.10596
AN  - PPRN:35892516
AD  - Google Res, Mt View, CA 94043, USA
M2  - Google Res
Y2  - 2023-02-07
ER  -

TY  - JOUR
AU  - Xu, Mengmeng
AU  - Zhao, Chen
AU  - S. Rojas, David
AU  - Thabet, Ali
AU  - Ghanem, Bernard
TI  - G-TAD: Sub-Graph Localization for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches 51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is publicly available at https://github.com/frostinassiky/gtad.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1911.11462
AN  - PPRN:14362657
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Lin, Tianwei
AU  - Zhao, Xu
AU  - Shou, Zheng
TI  - Single Shot Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classification" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1710.06236
AN  - PPRN:12833707
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Chen, Guo
AU  - Zheng, Yin-Dong
AU  - Wang, Limin
AU  - Lu, Tong
A1  - Assoc Advancement Artificial Intelligence
TI  - DCAN: Improving Temporal Action Detection via Dual Context Aggregation
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-Fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.1% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-876-3
DA  - 2022 
PY  - 2022
SP  - 248
EP  - 257
AN  - WOS:000893636200028
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China
Y2  - 2023-02-17
ER  -

TY  - JOUR
AU  - Wang, Ning
AU  - Xiao, Yun
AU  - Peng, Xiaopeng
AU  - Chang, Xiaojun
AU  - Wang, Xuanhong
AU  - Fang, Dingyi
TI  - ContextDet: Temporal Action Detection with Adaptive Context Aggregation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD), which locates and recognizes action segments, remains a challenging task in video understanding due to variable segment lengths and ambiguous boundaries. Existing methods treat neighboring contexts of an action segment indiscriminately, leading to imprecise boundary predictions. We introduce a single-stage ContextDet framework, which makes use of large-kernel convolutions in TAD for the first time. Our model features a pyramid adaptive context aggragation (ACA) architecture, capturing long context and improving action discriminability. Each ACA level consists of two novel modules. The context attention module (CAM) identifies salient contextual information, encourages context diversity, and preserves context integrity through a context gating block (CGB). The long context module (LCM) makes use of a mixture of large- and small-kernel convolutions to adaptively gather long-range context and fine-grained local features. Additionally, by varying the length of these large kernels across the ACA pyramid, our model provides lightweight yet effective context aggregation and action discrimination. We conducted extensive experiments and compared our model with a number of advanced TAD methods on six challenging TAD benchmarks: MultiThumos, Charades, FineAction, EPIC-Kitchens 100, Thumos14, and HACS, demonstrating superior accuracy at reduced inference speed.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2410.15279
AN  - PPRN:118748409
AD  - Northwest Univ, Sch Informat Sci & Technol, Xian, Peoples R China
AD  - Rochester Inst Technol, Rochester, NY 14623, USA
AD  - Univ Technol Sydney, Australian Artificial Intelligence Inst, Ultimo, NSW 2007, Australia
AD  - Xian Univ Posts & Telecommun China, Sch Commun & Informat Engn, Xian, Peoples R China
AD  - Xian Univ Posts & Telecommun China, Sch Artificial Intelligence, Xian, Peoples R China
M2  - Rochester Inst Technol
M2  - Univ Technol Sydney
M2  - Xian Univ Posts & Telecommun China
M2  - Xian Univ Posts & Telecommun China
Y2  - 2024-11-20
ER  -

TY  - CPAPER
AU  - Xu, Mengmeng
AU  - Zhao, Chen
AU  - Rojas, David S.
AU  - Thabet, Ali
AU  - Ghanem, Bernard
A1  - IEEE
TI  - G-TAD: Sub-Graph Localization for Temporal Action Detection
T2  - 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2020)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches 51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is publicly available at https://github.com/frostinassiky/gtad.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-7281-7168-5
DA  - 2020 
PY  - 2020
SP  - 10153
EP  - 10162
DO  - 10.1109/CVPR42600.2020.01017
AN  - WOS:001309199903003
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
Y2  - 2024-11-01
ER  -

TY  - CPAPER
AU  - Guo, Dashan
AU  - Li, Wei
AU  - Xu, Ning
AU  - Sun, Jianhui
AU  - Fang, Xiangzhong
A1  - IEEE
TI  - REFINING PROPOSALS WITH NEIGHBORING CONTEXTS FOR TEMPORAL ACTION DETECTION
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Shanghai, PEOPLES R CHINA
AB  - While many methods have been proposed for generating temporal proposals, the performance of existing temporal detection pipelines is still limited by the quality of proposals. In this paper, we introduce a new refining model for temporal action detection, which incorporates the evaluation of Intersection-over-Union (IoU) value into the action classification, and then regresses a better segment using the neighboring contexts of one candidate proposal. To refine one candidate proposal, we augment it with two neighboring proposals of equal length, which capture the contextual information from the past and future segments. After extracting regional features for the augmented proposals, we utilize the dilated convolutions for contextual modeling to regress the offsets between the candidate proposal and the target segment within this augmented area. Extensive experiments on THUMOS14 demonstrate that our method successfully refines the generated proposals and achieves superior detection performance over other methods.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-5386-9552-4
DA  - 2019 
PY  - 2019
SP  - 1576
EP  - 1581
DO  - 10.1109/ICME.2019.00272
AN  - WOS:000501820600264
AD  - Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai, Peoples R China
Y2  - 2019-12-27
ER  -

TY  - JOUR
AU  - Li, Zhi
AU  - He, Lu
AU  - Xu, Huijuan
TI  - Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weakly-supervised models for general action detection cannot perform well in the fine-grained setting. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.11805
AN  - PPRN:11422541
Y2  - 2022-12-02
ER  -

TY  - JOUR
AU  - Raza, Muhammad Ahmed
AU  - Chen, Longfei
AU  - Nanbo, Li
AU  - Fisher, Robert B.
TI  - EatSense: Human centric, action recognition and localization dataset for understanding eating behaviors and quality of motion assessment
T2  - IMAGE AND VISION COMPUTING
M3  - Article
AB  - Current datasets for computer vision-based action recognition and localization cover a wide range of classes and challenging scenarios. However, these datasets don't cater to healthcare applications that involve long-term monitoring, tracking minor changes in movements over time for healthcare purposes, or completely modeling a specific human behavior that includes multiple sub-actions. Specifically, there are no existing datasets for research on either health monitoring on atomic-action-based eating behavior or for a full range of eating subactions that fully segment the main action. Addressing these gaps is valuable for extending research on the health monitoring of elderly people and is needed for creating richer and more complete descriptions of actions. This paper introduces a new benchmark dataset named EatSense that targets both the computer vision and healthcare communities and fills in the aforementioned gaps. EatSense is recorded while a person eats in an uncontrolled dining setting. The key features of EatSense are the introduction of challenging atomic actions for action recognition, the significantly diverse durations of actions that make it difficult for current temporal action localization frameworks to localize, the capability to model comprehensive eating behavior in terms of a sequence of action-based behaviors, and the simulation of minor variations in motion or performance. We conduct extensive experiments on EatSense with baseline deep learning-based approaches for benchmarking and hand-crafted feature-based approaches for explainable applications. We believe this dataset will benefit future researchers in building robust temporal action localization networks, behavior recognition, and performance assessment models for eating.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0262-8856
SN  - 1872-8138
DA  - 2023 SEP
PY  - 2023
VL  - 137
C7  - 104762
DO  - 10.1016/j.imavis.2023.104762
AN  - WOS:001049180800001
C6  - JUL 2023
AD  - Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Scotland
Y2  - 2023-08-27
ER  -

TY  - JOUR
AU  - Mokari, Mozhgan
AU  - Sadeghi, Khosrow Haj
TI  - Enhancing temporal action localization in an end-to-end network through estimation error incorporation
T2  - IMAGE AND VISION COMPUTING
M3  - Article
AB  - Temporal action localization presents a significant challenge in computer vision, as the development of an efficient method for this task remains elusive. The objective is to identify human activities within untrimmed videos, determining when and which actions occur in each video. While using trimmed videos could potentially resolve the localization problem and enhance classification accuracy, it is impractical for real-world applications as the trimming process itself requires human intervention. This highlights the importance of temporal localization. Due to the availability of several successful approaches for action recognition in trimmed video, conventional multi-stage methods for untrimmed video, commonly employ a network to generate activity proposals, followed by a separate network for classification. These disjoint networks are optimized individually and thus usually vary from the global optimum, leading to less precise candidate action proposals. To address this challenge, we propose a novel end-to-end neural network that utilizes error estimation for precise action localization and recognition in untrimmed videos. The proposed method performs the localization and classification of action instances simultaneously, thereby optimizing the corresponding networks concurrently. To increase the precision of the action proposal boundaries, the Regression module is innovatively utilized as part of the proposed end-toend network, along with the Evaluation and Classification modules. This module estimates the potential error in proposal time boundaries and enhances the result accuracy. We have conducted experiments on THUMOS 14 and ActivityNet-1.3, which are considered the most challenging datasets for temporal action localization. The novel, yet fairly simple, proposed network achieves remarkable performance improvement compared to the other stateof-the-art methods. This improvement, which is more pronounced in the cases of high temporal intersection with ground truth, is accomplished without requiring extra data or complicated architecture. By incorporating error estimation, we achieved improvement in mean Average Precision (mAP). The proposed approach particularly shines for the localization of challenging activities in the complex and diverse dataset ActivityNet-1.3. For instance, for the "drinking coffee" activity, the mean Average Precision (mAP) was enhanced fivefold compared to the best-reported results.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0262-8856
SN  - 1872-8138
DA  - 2024 MAY
PY  - 2024
VL  - 145
C7  - 104994
DO  - 10.1016/j.imavis.2024.104994
AN  - WOS:001223959900001
C6  - APR 2024
AD  - Sharif Univ Technol, Dept Elect Engn, Tehran, Iran
Y2  - 2024-05-23
ER  -

TY  - CPAPER
AU  - Ning, Ranyu
AU  - Zhang, Can
AU  - Zou, Yuexian
A1  - IEEE
TI  - SRF-NET: SELECTIVE RECEPTIVE FIELD NETWORK FOR ANCHOR-FREE TEMPORAL ACTION DETECTION
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021)
M3  - Proceedings Paper
CP  - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - ELECTR NETWORK
AB  - Temporal action detection (TAD) is a challenging task which aims to temporally localize and recognize the human action in untrimmed videos. Current mainstream one-stage TAD approaches localize and classify action proposals relying on pre-defined anchors, where the location and scale for action instances are set by designers. Obviously, such an anchor-based TAD method limits its generalization capability and will lead to performance degradation when videos contain rich action variation. In this study, we explore to remove the requirement of pre-defined anchors for TAD methods. A novel TAD model termed as Selective Receptive Field Network (SRF-Net) is developed, in which the location offsets and classification scores at each temporal location can be directly estimated in the feature map and SRF-Net is trained in an end-to-end manner. Innovatively, a building block called Selective Receptive Field Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust its receptive field size according to multiple scales of input information at each temporal location in the feature map. Extensive experiments are conducted on the THUMOS14 dataset, and superior results are reported comparing to state-of-the-art TAD approaches.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-7605-5
DA  - 2021 
PY  - 2021
SP  - 2460
EP  - 2464
DO  - 10.1109/ICASSP39728.2021.9414253
AN  - WOS:000704288402142
AD  - Peking Univ, Sch ECE, ADSPLAB, Shenzhen, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
Y2  - 2021-11-24
ER  -

TY  - CPAPER
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Li, Hongsheng
A1  - IEEE
TI  - Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - As a challenging task of high-level video understanding, weakly supervised temporal action localization has been attracting increasing attention. With only video annotations, most existing methods seek to handle this task with a localization-by-classification framework, which generally adopts a selector to select snippets of high probabilities of actions or namely the foreground. Nevertheless, the existing foreground selection strategies have a major limitation of only considering the unilateral relation from foreground to actions, which cannot guarantee the foreground-action consistency. In this paper, we present a framework named FAC-Net based on the I3D backbone, on which three branches are appended, named class-wise foreground classification branch, class-agnostic attention branch and multiple instance learning branch. First, our class-wise foreground classification branch regularizes the relation between actions and foreground to maximize the foreground-background separation. Besides, the class-agnostic attention branch and multiple instance learning branch are adopted to regularize the foreground-action consistency and help to learn a meaningful foreground classifier. Within each branch, we introduce a hybrid attention mechanism, which calculates multiple attention scores for each snippet, to focus on both discriminative and less-discriminative snippets to capture the full action boundaries. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our method.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 7982
EP  - 7991
DO  - 10.1109/ICCV48922.2021.00790
AN  - WOS:000797698908022
AD  - Chinese Univ Hong Kong, Multimedia Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
M2  - Ctr Perceptual & Interact Intelligence
Y2  - 2022-07-08
ER  -

TY  - JOUR
AU  - Song, Hao
AU  - Wu, Xinxiao
AU  - Zhu, Bing
AU  - Wu, Yuwei
AU  - Chen, Mei
AU  - Jia, Yunde
TI  - Temporal Action Localization in Untrimmed Videos Using Action Pattern Trees
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - In this paper, we present a novel framework of automatically localizing action instances based on action pattern trees (AP-Trees) in a long untrimmed video. For localizing action instances in videos with varied temporal lengths, we first split videos into sequential segments and then use the AP-Trees to produce precise temporal boundaries of action instances. The AP-Trees can exploit the temporal information between segments of videos based on the label vectors of segments, by learning the occurrence frequency and order of segments. In AP-Trees, nodes stand for action class labels of segments and edges represent the temporal relationships between two consecutive segments. Thus, we can discover the occurrence frequencies of segments by searching paths of AP-Trees. In order to obtain accurate labels of video segments, we introduce deep neural networks to annotate the segments by simultaneously leveraging the spatio-temporal information and the high-level semantic feature of segments. In the networks, informative action maps are generated by a global average pooling layer to retain the spatio-temporal information of segments. An overlap loss function is employed to further improve the precision of label vectors of segments by considering the temporal overlap between segments and the ground truth. The experiments on THUMOS2014, MSR ActionII, and MPII Cooking datasets demonstrate the effectiveness of the method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2019 MAR
PY  - 2019
VL  - 21
IS  - 3
SP  - 717
EP  - 730
DO  - 10.1109/TMM.2018.2866370
AN  - WOS:000460333800016
AD  - Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China
AD  - SUNY Albany, Dept Elect & Comp Engn, Albany, NY 12222 USA
Y2  - 2019-03-01
ER  -

TY  - JOUR
AU  - Gan, Ming-Gang
AU  - Zhang, Yan
AU  - Su, Shaowen
TI  - Temporal-visual proposal graph network for temporal action detection
T2  - APPLIED INTELLIGENCE
M3  - Article
AB  - Temporal action detection is usually divided into two stages: temporal action proposal generation and proposal classification. Most methods consider the proposal classification stage as an action recognition task. However, compared with trimmed videos, proposals generally contain part of the ground-truth action, lacking enough semantic information to predict their categories precisely. In this paper, we propose a novel temporal-visual proposal graph (TVPG) module to acquire sufficient semantic information for action proposal classification. The module first adopts a proposal graph construction strategy to select valuable neighbor proposals for each proposal and constructs them into an action proposal graph. Then, it applies a temporal graph convolution network and a visual graph convolution network in parallel on the graph to improve proposal feature quality by obtaining action information from neighbors. In the temporal graph convolution network, we design a novel temporal graph convolution operation that embeds temporal position relation information into proposal features and extracts the information from other proposals by temporal position relations. Based on the TVPG module, we construct an action proposal classification model named the temporal-visual proposal graph network (TVPGN) and perform extensive experiments on two benchmarks. The results show that TVPGN achieves competitive performance on both datasets.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-669X
SN  - 1573-7497
DA  - 2023 NOV
PY  - 2023
VL  - 53
IS  - 21
SP  - 26008
EP  - 26026
DO  - 10.1007/s10489-023-04947-0
AN  - WOS:001049168300001
C6  - AUG 2023
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2023-08-27
ER  -

TY  - JOUR
AU  - Yang, Wenfei
AU  - Zhang, Tianzhu
AU  - Mao, Zhendong
AU  - Zhang, Yongdong
AU  - Tian, Qi
AU  - Wu, Feng
TI  - Multi-Scale Structure-Aware Network for Weakly Supervised Temporal Action Detection
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly supervised temporal action detection has better scalability and practicability than fully supervised action detection in reality deployment. However, it is difficult to learn a robust model without temporal action boundary annotations. In this paper, we propose an en-to-end Multi-Scale Structure-Aware Network (MSA-Net) for weakly supervised temporal action detection by exploring both the global structure information of a video and the local structure information of actions. The proposed SA-Net enjoys several merits. First, to localize actions with different durations, each video is encoded into feature representations with different temporal scales. Second, based on the multi-scale feature representation, the proposed model has designed two effective structure modeling mechanisms including global structure modeling and local structure modeling, which can effectively learn discriminative structure aware representations for robust and complete action detection. To the best of our knowledge, this is the first work to fully explore the global and local structure information in a unified deep model for weakly supervised action detection. And extensive experimental results on two benchmark datasets demonstrate that the proposed MSA-Net performs favorably against state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 5848
EP  - 5861
DO  - 10.1109/TIP.2021.3089361
AN  - WOS:000668794500003
AD  - Univ Sci & Technol China, Sch Informat Sci, Hefei 230027, Peoples R China
AD  - Huawei, Cloud BU, Shenzhen 518129, Peoples R China
Y2  - 2021-07-09
ER  -

TY  - JOUR
AU  - Liu, Jichao
AU  - Wang, Chuanxu
AU  - Liu, Yun
TI  - A Novel Method for Temporal Action Localization and Recognition in Untrimmed Video Based on Time Series Segmentation
T2  - IEEE ACCESS
M3  - Article
AB  - Positioning of each action in a long complicated video is a challenging task in computer vision. To address this issue we propose a method with temporal boundary regression based on time series segmentation, which can generate proposals with flexible temporal duration. Firstly, we use a clustering algorithm to generate proposals, which is more efficient than sliding window method. It generates proposals by aggregating areas of high-probability behavior in time domain, and uses non-maximum suppression to remove redundancy. Then a multi-layer perceptron is used to refine boundary regression of behavior proposals, the process makes boundary coordinates closer to the real boundaries. Secondly, each behavioral proposal is represented by concatenating a three-subsegment feature description, which includes the proposal segment, its starting subsegment and its ending subsegment. Finally, the behavior proposal including a target action is identified by multi-layer perceptron. Our method is evaluated in two large data sets THUMOS14 and ActivityNet, which are commonly used in time series behavior detection task. The recognition rates can reach 30.1% and 33.19% respectively, which proves that the method can effectively improve the classification accuracy.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2019 
PY  - 2019
VL  - 7
SP  - 135204
EP  - 135209
DO  - 10.1109/ACCESS.2019.2940407
AN  - WOS:000560151400001
AD  - Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266100, Peoples R China
Y2  - 2020-12-10
ER  -

TY  - CPAPER
AU  - Narayan, Sanath
AU  - Cholakkal, Hisham
AU  - Hayat, Munawar
AU  - Khan, Fahad Shahbaz
AU  - Yang, Ming-Hsuan
AU  - Shao, Ling
A1  - IEEE
TI  - D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision. Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discriminative term incorporates a classification loss and utilizes a top-down attention mechanism to enhance the separability of latent foreground-background embeddings. The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive experiments are performed on multiple benchmarks, including THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on all datasets, achieving gains as high as 2.3% in terms of mAP at IoU=0.5 on THUMOS14. Source code is available at https://github.com/naraysa/D2-Net.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13588
EP  - 13597
DO  - 10.1109/ICCV48922.2021.01335
AN  - WOS:000798743203076
AD  - Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates
AD  - Mohamed Bin Zayed Univ AI, Abu Dhabi, U Arab Emirates
AD  - Monash Univ, Clayton, Vic, Australia
AD  - Linkoping Univ, Linkoping, Sweden
AD  - Univ Calif Merced, Merced, CA USA
AD  - Google Res, Mountain View, CA USA
AD  - Yonsei Univ, Seoul, South Korea
M2  - Incept Inst Artificial Intelligence
M2  - Mohamed Bin Zayed Univ AI
Y2  - 2022-06-24
ER  -

TY  - JOUR
AU  - Li, Tianyu
AU  - Bing, Bing
AU  - Wu, Xinxiao
TI  - Boundary discrimination and proposal evaluation for temporal action proposal generation
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Temporal action proposal generation for temporal action localization aims to capture temporal intervals that are likely to contain actions from untrimmed videos. Prevailing bottom-up proposal generation methods locate action boundaries (the start and the end) with high classifying probabilities. But for many actions, motions at boundaries are not discriminative, which makes action segments and background segments be classified into boundary classes, thereby generating low-overlap proposals. In this work, we propose a novel method that generates proposals by evaluating the continuity of video frames, and then locates the start and the end with low continuity. Our method consists of two modules: boundary discrimination and proposal evaluation. The boundary discrimination module trains a model to understand the relationship between two frames and uses the continuity of frames to generate proposals. The proposal evaluation module removes background proposals via a classification network, and evaluates the integrity of proposals with probability features by an integrity network. Extensive experiments are conducted on two challenging datasets: THUMOS14 and ActivityNet 1.3, and the results demonstrate that our method outperforms the state-of-the-art proposal generation methods.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2021 JAN
PY  - 2021
VL  - 80
IS  - 2
SP  - 2123
EP  - 2139
DO  - 10.1007/s11042-020-09703-x
AN  - WOS:000568478700008
C6  - SEP 2020
AD  - Beijing Inst Technol BIT, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China
M2  - Beijing Inst Technol BIT
Y2  - 2020-09-25
ER  -

TY  - JOUR
AU  - Li, Bairong
AU  - Guo, Biao
AU  - Zhu, Yuesheng
AU  - Yin, Jianfeng
AU  - Ji, Xiangli
TI  - Superframe-Based Temporal Proposals for Weakly Supervised Temporal Action Detection
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - The weakly supervised Temporal Action Detection (TAD) by using the video-level annotations can lighten the burden of labor consumption. However, the current methods for weakly supervised TAD do not take full advantage of the short-term consistency between consecutive frames and the long-term continuity inside an action, resulting in less accurate detecting boundaries of actions in untrimmed videos. In this paper, the SuperFrame-based Temporal Proposal (SFTP) is proposed, in which superframes are formed for representing a series of consecutive frames with high temporal consistency and their features are pooled from the features of frames through the integration function. Then, the temporal proposal is built based on the multiple consecutive superframes and the features of all proposals are generated from a pyramidal feature hierarchy. This hierarchy consists of the designed Structured Outer-Inner Context (SOIC) features formed from superframe features and is able to explicitly characterize the temporal continuity inside a proposal. Furthermore, a novel Scale-Wise Normalization Strategy (SWNS) is proposed to identify proposals, which can effectively detect multiple actions with different duration in one untrimmed video. Extensive experiments are conducted on two public datasets: THUMOS14 and ActivityNet1.2 for performance evaluation. Our experimental results have demonstrated that the proposed approach is able to detect the boundaries of actions more effectively and obtain competitive mAP (mean average precision) compared with other approaches.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 3628
EP  - 3641
DO  - 10.1109/TMM.2022.3163459
AN  - WOS:001069663600002
AD  - Peking Univ, Shenzhen Grad Sch, Shenzhen 518000, Peoples R China
Y2  - 2023-10-06
ER  -

TY  - JOUR
AU  - Yang, Jianhua
AU  - Wang, Ke
AU  - Li, Ruifeng
TI  - Actions as points: a simple and efficient detector for skeleton-based temporal action detection
T2  - MACHINE VISION AND APPLICATIONS
M3  - Article
AB  - Temporal action detection, aiming to determine the fragment and category of a human action simultaneously from continuous data stream, is still a challenge issue in the field of human-robot interaction, somatosensory game and security monitoring. In this paper, we present a novel one-stage skeleton-based TAD method, Action-CenterNet(ACNet) with a simple anchor-free and fully convolutional encoder-decoder pipeline. Our approach encodes skeleton position and motion data sequence from multiple persons into multi-channel skeleton images which are subsequently preprocessed by view invariant transform and translation-scale invariant. ACNet models each action fragment as a center point along the time dimension and generates a keypoint heatmap to locate and classify action fragments. To ensure the accurate temporal coordinates, the discretization error caused by the output stride of network is also learned. Compared with two-stage methods, ACNet is end-to-end differential and flexible. ACNet is also an anchor-free method avoiding the drawbacks of anchor boxes used in anchor-based TAD methods. Experimental results on PKU-MMD dataset, NTU RGB-D dataset and HITvs dataset reveal the excellent performance of our approach.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0932-8092
SN  - 1432-1769
DA  - 2023 MAR
PY  - 2023
VL  - 34
IS  - 2
C7  - 35
DO  - 10.1007/s00138-023-01377-3
AN  - WOS:000945336100001
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China
Y2  - 2023-03-27
ER  -

TY  - JOUR
AU  - Dang, Yuanjie
AU  - Zheng, Guozhu
AU  - Chen, Peng
AU  - Gao, Nan
AU  - Huan, Ruohong
AU  - Zhao, Dongdong
AU  - Liang, Ronghua
TI  - Learning Reliable Dense Pseudo-Labels for Point-Level Weakly-Supervised Action Localization
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Point-level weakly-supervised temporal action localization aims to accurately recognize and localize action segments in untrimmed videos, using only point-level annotations during training. Current methods primarily focus on mining sparse pseudo-labels and generating dense pseudo-labels. However, due to the sparsity of point-level labels and the impact of scene information on action representations, the reliability of dense pseudo-label methods still remains an issue. In this paper, we propose a point-level weakly-supervised temporal action localization method based on local representation enhancement and global temporal optimization. This method comprises two modules that enhance the representation capacity of action features and improve the reliability of class activation sequence classification, thereby enhancing the reliability of dense pseudo-labels and strengthening the model's capability for completeness learning. Specifically, we first generate representative features of actions using pseudo-label feature and calculate weights based on the feature similarity between representative features of actions and segments features to adjust class activation sequence. Additionally, we maintain the fixed-length queues for annotated segments and design a action contrastive learning framework between videos. The experimental results demonstrate that our modules indeed enhance the model's capability for comprehensive learning, particularly achieving state-of-the-art results at high IoU thresholds.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2024 APR 10
PY  - 2024
VL  - 56
IS  - 2
C7  - 145
DO  - 10.1007/s11063-024-11598-w
AN  - WOS:001199706800001
AD  - Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China
Y2  - 2024-04-14
ER  -

TY  - JOUR
AU  - Pramono, Rizard Renanda Adhi
AU  - Chen, Yie-Tarng
AU  - Fang, Wen-Hsien
TI  - Spatial-Temporal Action Localization With Hierarchical Self-Attention
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - This paper proposes a novel architecture for spatial-temporal action localization in videos. The new architecture first employs a two-stream 3D convolutional neural network (3D-CNN) to provide initial action detection. Next, a new Hierarchical Self-Attention Network (HiSAN), the core of this architecture, is developed to learn the spatial-temporal relationships of key actors. Spatial Gaussian priors (SGP) are also imbued to the bidirectional self-attention to enhance HiSAN in modelling the relationships of neighboring actors. Such a combination of 3D-CNN and SGP augmented HiSAN allows us to effectively extract both of the spatial context information and the long-term temporal dependency to improve action localization accuracy. Afterwards, a new fusion strategy is employed, which first re-scores the bounding boxes to settle the inconsistent detection scores caused by background clutter or occlusion, and then aggregates the motion and appearance information from the two-stream network with the motion saliency to alleviate the impact of camera movement. Finally, a tube association network based on the self-similarity of the actors' appearance and spatial information across frames is addressed to efficaciously construct the action tubes. Simulations on four widespread datasets reveal the efficacy of the new approach.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2022 
PY  - 2022
VL  - 24
SP  - 625
EP  - 639
DO  - 10.1109/TMM.2021.3056892
AN  - WOS:000753488100009
AD  - Natl Taiwan Univ Sci & Technol, Elect & Comp Engn, Taipei 106, Taiwan
Y2  - 2022-03-02
ER  -

TY  - JOUR
AU  - Wang, Tian
AU  - Hou, Boyao
AU  - Li, Zexian
AU  - Li, Zhe
AU  - Huang, Lei
AU  - Zhang, Baochang
AU  - Snoussi, Hichem
TI  - A Malleable Boundary Network for temporal action detection?
T2  - COMPUTERS & ELECTRICAL ENGINEERING
M3  - Article
AB  - Temporal action detection in untrimmed videos is a challenging task aiming to predict the boundary and category of action instances. It can be useful in transportation. In this study, we propose a two-stage framework Malleable Boundary Network (MB-Net) to adaptively regress proposals based on finer scores. In particular, MB-Net consists of a Potential Boundary Generator in the first stage and an Adaptive Proposal Detector in the second stage. First, the Potential Boundary Generator fuses multiple sets of flexible score sequences to obtain tentative proposals through a frame-level feature in an anchor-free way. Then, the Adaptive Proposal Detector employs parallel modules to filter, classify and regress proposals adaptively. Besides, we propose an easy-to-realize feature augmented method Structured Temporal Segment Pooling, which makes full use of the information throughout the whole proposal. Experiments show that MB -Net achieves state-of-the-art performance on popular benchmarks THUMOS-14 and Activity-1.3 with an improvement of 1.9% and 1.2%.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0045-7906
SN  - 1879-0755
DA  - 2022 OCT
PY  - 2022
VL  - 103
C7  - 108250
DO  - 10.1016/j.compeleceng.2022.108250
AN  - WOS:000865448600004
C6  - AUG 2022
AD  - Beihang Univ, Inst Artificial Intelligence, Beijing 100191, Peoples R China
AD  - Beihang Univ, Sch Automation Sci & Elect Engn, Beijing 100191, Peoples R China
AD  - ByteDance Co, Beijing 100191, Peoples R China
AD  - Univ Technol Troyes, LM2S, F-10004 Troyes, France
M2  - ByteDance Co
Y2  - 2022-11-04
ER  -

TY  - JOUR
AU  - Zhang, Shihui
AU  - Luo, Bingchun
AU  - Wang, Houlin
AU  - Gu, Yu
AU  - He, Jiacheng
TI  - Temporal action detection in videos with generative denoising diffusion
T2  - KNOWLEDGE-BASED SYSTEMS
M3  - Article
AB  - With generative denoising diffusion performing well in various fields, we propose a new generative model that redefines the temporal action detection task as a denoising diffusion process for noise proposals. Specifically, in the training stage, the proposed method adds random noise to the ground truth via multiple steps (e.g., 500 or 1000 steps) to completely confuse the ground truth. Then, utilizing the knowledge of the noise, a denoiser is trained to convert the random noise to the ground truth. In particular, the video features and time step are taken as conditions to obtain the improved denoiser. In the inference stage, the prediction results are obtained by inputting random noise proposals into the denoiser for multiple iteration steps. We further propose a novel joint denoising strategy (JDS) to improve the quality of denoising, which utilizes a prior prediction as a condition in inference. Extensive experimental results show that the proposed method achieves state -of -the -art performance on Thumos14, ActivityNet-1.3, and EPIC-Kitchens 100.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0950-7051
SN  - 1872-7409
DA  - 2024 JUL 19
PY  - 2024
VL  - 296
C7  - 111767
DO  - 10.1016/j.knosys.2024.111767
AN  - WOS:001238782500001
C6  - MAY 2024
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Harbin Inst Technol, Sch Comp Sci & Technol, Weihai, Shandong, Peoples R China
AD  - Key Lab Comp Virtual Technol & Syst Integrat Hebei, Qinhuangdao, Hebei, Peoples R China
M2  - Key Lab Comp Virtual Technol & Syst Integrat Hebei
Y2  - 2024-06-10
ER  -

TY  - JOUR
AU  - Wang, Jing
AU  - Wang, Chuanxu
TI  - Self-attention relational modeling and background suppression for weakly supervised temporal action localization
T2  - JOURNAL OF ELECTRONIC IMAGING
M3  - Article
AB  - Weakly supervised temporal action localization aims to locate the start and end boundaries of action instances and recognize the corresponding categories. Classical methods include random erasure, attention mechanism, and cross-temporal graph relationship modeling. Despite their great progress, there are still two challenges: localization integrity and background interference. Therefore, we propose a framework with self-attention relationship modeling and background suppression to address these issues. First, the input features of background frames are suppressed by the filtering module, which prevents interference from background noise. Second, a self-attention mechanism is designed to model the relationship between different segments in the video, which refines action features to encourage smoother temporal classification scores for completeness localization. Finally, under the guidance of classified loss L act, the refined segment features and foreground weights are further combined in an attention-weighted pool to achieve video-level prediction. The algorithm is experimentally verified on THUMOS14 and ActivityNet1.2 datasets and compared with other relevant literature, which proves its feasibility and effectiveness. (c) 2022 SPIE and IS&T
PU  - SPIE-SOC PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA
SN  - 1017-9909
SN  - 1560-229X
DA  - 2022 NOV 1
PY  - 2022
VL  - 31
IS  - 6
C7  - 063019
DO  - 10.1117/1.JEI.31.6.063019
AN  - WOS:000917034300050
AD  - Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao, Peoples R China
Y2  - 2023-02-18
ER  -

TY  - CPAPER
AU  - Xia, Kun
AU  - Wang, Le
AU  - Zhou, Sanping
AU  - Zheng, Nanning
AU  - Tang, Wei
A1  - IEEE COMP SOC
TI  - Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - The main challenge of Temporal Action Localization is to retrieve subtle human actions from various co-occurring ingredients, e.g., context and background, in an untrimmed video. While prior approaches have achieved substantial progress through devising advanced action detectors, they still suffer from these co-occurring ingredients which often dominate the actual action content in videos. In this paper, we explore two orthogonal but complementary aspects of a video snippet, i.e., the action features and the co-occurrence features. Especially, we develop a novel auxiliary task by decoupling these two types of features within a video snippet and recombining them to generate a new feature representation with more salient action information for accurate action localization. We term our method RefactorNet, which first explicitly factorizes the action content and regularizes its co-occurrence features, and then synthesizes a new actiondominated video representation. Extensive experimental results and ablation studies on THUMOS14 and ActivityNet v1.3 demonstrate that our new representation, combined with a simple action detector, can significantly improve the action localization performance.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 13874
EP  - 13883
DO  - 10.1109/CVPR52688.2022.01351
AN  - WOS:000870759106094
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Univ Illinois, Chicago, IL USA
Y2  - 2022-12-17
ER  -

TY  - CPAPER
AU  - Zhang, Chengwei
AU  - Xu, Yunlu
AU  - Cheng, Zhanzhan
AU  - Niu, Yi
AU  - Pu, Shiliang
AU  - Wu, Fei
AU  - Zou, Futai
A1  - ACM
TI  - Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal Action Localization
T2  - PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19)
M3  - Proceedings Paper
CP  - 27th ACM International Conference on Multimedia (MM)
CL  - Nice, FRANCE
AB  - Temporal action localization is an important yet challenging research topic due to its various applications. Since the frame-level or segment-level annotations of untrimmed videos require amounts of labor expenditure, studies on the weakly-supervised action detection have been springing up. However, most of existing frameworks rely on Class Activation Sequence (CAS) to localize actions by minimizing the video-level classification loss, which exploits the most discriminative parts of actions but ignores the minor regions. In this paper, we propose a novel weakly-supervised framework by adversarial learning of two modules for eliminating such demerits. Specifically, the first module is designed as a well-designed Seeded Sequence Growing (SSG) Network for progressively extending seed regions (namely the highly reliable regions initialized by a CAS-based framework) to their expected boundaries. The second module is a specific classifier for mining trivial or incomplete action regions, which is trained on the shared features after erasing the seeded regions activated by SSG. In this way, a whole network composed of these two modules can be trained in an adversarial manner. The goal of the adversary is to mine features that are difficult for the action classifier. That is, erasion from SSG will force the classifier to discover minor or even new action regions on the input feature sequence, and the classifier will drive the seeds to grow, alternately. At last, we could obtain the action locations and categories from the well-trained SSG and the classifier. Extensive experiments on two public benchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance of our proposed method compared with the state-of-the-arts.[GRAPHICS]
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-6889-6
DA  - 2019 
PY  - 2019
SP  - 738
EP  - 746
DO  - 10.1145/3343031.3351044
AN  - WOS:000509743400084
AD  - Shanghai Jiao Tong Univ, Shanghai, Peoples R China
AD  - Hikvis Res Inst, Hangzhou, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
M2  - Hikvis Res Inst
Y2  - 2020-02-12
ER  -

TY  - JOUR
AU  - Zheng, Jingye
AU  - Chen, Dihu
AU  - Hu, Haifeng
TI  - Multi-Scale Proposal Regression Network for Temporal Action Proposal Generation
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action detection, as a branch of video analysis, aims to locate the time points when the actions start and end, and classify the actions occurred in videos into correct categories. Generating high-quality proposals is a key step in temporal action detection task. In this paper, we introduce a novel network, named multi-scale proposal regression network (MPRN), for temporal action proposal generation. First, we take encoding visual features as input and predict action scores for time points, in order to group them to generate rough proposals. Then, we regress the proposal's boundaries to obtain more precise proposals via our multi-scale proposal regression network. Compared with SSN and TURN, our multi-scale regression segments are characterized by flexible boundaries. Experiments show that 1) Our method is better than other proposal generation methods on THUMOS-14 dataset and ActivityNet-v1.3 dataset. 2) The effectiveness of our method is due to its own architecture, not the selection of visual feature encoders. 3) Our proposal generation method can generate temporal proposals for unseen action classes, which shows the good generalization ability of our proposal generation method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2019 
PY  - 2019
VL  - 7
SP  - 183860
EP  - 183868
DO  - 10.1109/ACCESS.2019.2933360
AN  - WOS:000509587800003
AD  - Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China
Y2  - 2020-02-06
ER  -

TY  - JOUR
AU  - Huang, Linjiang
AU  - Huang, Yan
AU  - Ouyang, Wanli
AU  - Wang, Liang
TI  - Two-Branch Relational Prototypical Network for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - As a challenging task of high-level video understanding, weakly supervised temporal action localization has attracted more attention recently. With only video-level category labels, this task should indistinguishably identify the background and action categories frame by frame. However, it is non-trivial to achieve this in untrimmed videos, due to the unconstrained background, complex and multi-label actions. With the observation that these difficulties are mainly brought by the large variations within background and actions, we propose to address these challenges from the perspective of modeling variations. Moreover, it is desired to further reduce the variations, or learn compact features, so as to cast the problem of background identification as rejecting background and alleviate the contradiction between classification and detection. Accordingly, in this paper, we propose a two-branch relational prototypical network. The first branch, namely action-branch, adopts class-wise prototypes and mainly acts as an auxiliary to introduce priori knowledge about label dependencies and be a guide for the second branch. Meanwhile, the second branch, namely sub-branch, starts with multiple prototypes, namely sub-prototypes, to enable a powerful ability of modeling variations. As a further benefit, we elaborately design a multi-label clustering loss based on the sub-prototypes to learn compact features under the multi-label setting. The two branches are associated using the correspondences between two types of prototypes, leading to a special two-stage classifier in the s-branch, on the other hand, the two branches serve as regularization terms to each other, improving the final performance. Ablation studies find that the proposed model is capable of modeling classes with large variations and learning compact features. Extensive experimental evaluations on Thumos14, MultiThumos and ActivityNet datasets demonstrate the effectiveness of the proposed method and superior performance over state-of-the-art approaches.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2022 SEPT 1
PY  - 2022
VL  - 44
IS  - 9
SP  - 5729
EP  - 5746
DO  - 10.1109/TPAMI.2021.3076172
AN  - WOS:000836666600086
AD  - Chinese Acad Sci CASIA, Inst Automat, Beijing 100190, Peoples R China
AD  - Univ Sydney, SenseTime Comp Vis Res Grp, Camperdown, NSW 2006, Australia
Y2  - 2022-08-20
ER  -

TY  - CPAPER
AU  - Kim, Ho-Joong
AU  - Hong, Jung-Ho
AU  - Kong, Heejo
AU  - Lee, Seong-Whan
A1  - IEEE
TI  - TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD. To address this issue, we propose TE-TAD, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets. Code is available at: https://github.com/Dotori-HJ/TE-TAD
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18837
EP  - 18846
DO  - 10.1109/CVPR52733.2024.01782
AN  - WOS:001342515502017
AD  - Korea Univ, Dept Artificial Intelligence, Seoul, South Korea
AD  - Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea
Y2  - 2025-02-01
ER  -

TY  - CPAPER
AU  - Liu, Xiaolong
AU  - Bai, Song
AU  - Bai, Xiang
A1  - IEEE COMP SOC
TI  - An Empirical Study of End-to-End Temporal Action Detection
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Temporal action detection ('TAD) is an important yet challenging task in video understanding. It aims to simultaneously predict the semantic label and the temporal interval of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detection head upon the encoder is optimized for TAD. The effect of end-to-end learning is not systematically evaluated. Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learning over head-only learning and observe up to 11% performance improvement. Besides, we study the effects of multiple design choices that affect the TAD performance and speed, including detection head, video encoder, and resolution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while running more than 4x faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 19978
EP  - 19987
DO  - 10.1109/CVPR52688.2022.01938
AN  - WOS:000870783005079
AD  - Huazhong Univ Sci & Technol, Wuhan, Peoples R China
AD  - ByteDance Inc, Wuhan, Peoples R China
M2  - ByteDance Inc
Y2  - 2023-01-05
ER  -

TY  - JOUR
AU  - Zhao, Fan
AU  - Wang, Wen
AU  - Wu, Yu
AU  - Wang, Kaixuan
AU  - Kang, Xiaobing
TI  - A coarse-to-fine temporal action detection method combining light and heavy networks
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Temporal action detection aims to judge whether there existing a certain number of action instances in a long untrimmed videos and to locate the start and end time of each action. Even though the existing action detection methods have shown promising results in recent years with the widespread application of Convolutional Neural Network (CNN), it is still a challenging problem to accurately locate each action segment while ensuring real-time performance. In order to achieve a good tradeoff between detection efficiency and accuracy, we present a coarse-to-fine hierarchical temporal action detection method by using multi-scale sliding window mechanism. Since the complexity of the convolution operator is proportional to the number and the size of the input video clips, the idea of our proposed method is to first determine candidate action proposals and then perform the detection task on these candidate action proposals only with a view to reducing the overall complexity of the detection method. By making full use of the spatio-temporal information of video clips, a lightweight 3D-CNN classifier is first used to quickly determine whether the video clip is a candidate action proposal, avoiding the re-detection of a large number of non-action video clips by the heavyweight deep network. A heavyweight detector is designed to further improve the accuracy of action positioning by considering both boundary regression loss and category loss in the target loss function. In addition, the Non-Maximum Suppression (NMS) is performed to eliminate redundant detection results among the overlapping proposals. The mean Average Precision (mAP) is 40.6%, 51.7% and 20.4% on THUMOS14, ActivityNet and MPII Cooking dataset when the Intersection-over-Union (tIoU) threshold is set to 0.5, respectively. Experimental results show the superior performance of the proposed method on three challenging temporal activity detection datasets while achieving real-time speed. At the same time, our method can generate proposals for unseen action classes with high recalls.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2023 JAN
PY  - 2023
VL  - 82
IS  - 1
SP  - 879
EP  - 898
DO  - 10.1007/s11042-022-12720-7
AN  - WOS:000808527500001
C6  - JUN 2022
AD  - Xian Univ Technol, Dept Informat Sci, Xian 710054, Peoples R China
Y2  - 2022-06-20
ER  -

TY  - CPAPER
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning
T2  - COMPUTER VISION - ECCV 2022, PT III
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Existing temporal action detection (TAD) methods rely on generating an overwhelmingly large number of proposals per video. This leads to complex model designs due to proposal generation and/or per-proposal action instance evaluation and the resultant high computational cost. In this work, for the first time, we propose a proposal-free Temporal Action detection model via Global Segmentation mask (TAGS). Our core idea is to learn a global segmentation mask of each action instance jointly at the full video length. The TAGS model differs significantly from the conventional proposal-based methods by focusing on global temporal representation learning to directly detect local start and end points of action instances without proposals. Further, by modeling TAD holistically rather than locally at the individual proposal level, TAGS needs a much simpler model architecture with lower computational cost. Extensive experiments show that despite its simpler design, TAGS outperforms existing TAD methods, achieving new state-of-the-art performance on two benchmarks. Importantly, it is -20x faster to train and -1.6x more efficient for inference. Our PyTorch implementation of TAGS is available at https://github.com/sauradip/TAGS.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20061-8
SN  - 978-3-031-20062-5
DA  - 2022 
PY  - 2022
VL  - 13663
SP  - 645
EP  - 662
DO  - 10.1007/978-3-031-20062-5_37
AN  - WOS:000899240500037
AD  - Univ Surrey, CVSSP, Guildford, Surrey, England
AD  - IFlyTek Surrey Joint Res Ctr Artificial Intellige, London, England
AD  - Univ Surrey, Surrey Inst People Ctr Artificial Intelligence, Guildford, Surrey, England
M2  - IFlyTek Surrey Joint Res Ctr Artificial Intellige
Y2  - 2023-01-25
ER  -

TY  - CPAPER
AU  - Gao, Zhanning
AU  - Wang, Le
AU  - Zhang, Qilin
AU  - Niu, Zhenxing
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - AAAI
TI  - Video Imprint Segmentation for Temporal Action Detection in Untrimmed Videos
T2  - THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 33rd AAAI Conference on Artificial Intelligence / 31st Innovative Applications of Artificial Intelligence Conference / 9th AAAI Symposium on Educational Advances in Artificial Intelligence
CL  - Honolulu, HI
AB  - We propose a temporal action detection by spatial segmentation framework, which simultaneously categorize actions and temporally localize action instances in untrimmed videos. The core idea is the conversion of temporal detection task into a spatial semantic segmentation task. Firstly, the video imprint representation is employed to capture the spatial/temporal interdependences within/among frames and represent them as spatial proximity in a feature space. Subsequently, the obtained imprint representation is spatially segmented by a fully convolutional network. With such segmentation labels projected back to the video space, both temporal action boundary localization and per-frame spatial annotation can be obtained simultaneously. The proposed framework is robust to variable lengths of untrimmed videos, due to the underlying fixed-size imprint representations. The efficacy of the framework is validated in two public action detection datasets.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-809-1
DA  - 2019 
PY  - 2019
SP  - 8328
EP  - 8335
AN  - WOS:000486572502105
AD  - Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China
AD  - Alibaba Grp, Hangzhou, Zhejiang, Peoples R China
AD  - HERE Technol, Chicago, IL USA
AD  - Microsoft Cloud & AI, Redmond, WA USA
M2  - HERE Technol
M2  - Microsoft Cloud & AI
Y2  - 2019-01-01
ER  -

TY  - CPAPER
AU  - Ma, Xurui
AU  - Luo, Zhigang
AU  - Zhang, Xiang
AU  - Liao, Qing
AU  - Shen, Xingyu
AU  - Wang, Mengzhu
A1  - IEEE
TI  - Spatio-Temporal Action Detector with Self-Attention
T2  - 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - ELECTR NETWORK
AB  - In the field of spatio-temporal action detection, some current studies attempt to solve the problem of action detection by using the one-stage object detectors based on anchor-free. Albeit efficiency, more performance boosts are expected. Towards this goal, a Self-Attention MovingCenter Detector (SAMOC) is proposed, which is blessed with two attractive aspects: 1) to effectively capture motion cues, a spatio-temporal self-attention block is explored to reinforce feature representation by aggregating motion-dependent global contexts, and 2) a link branch serves to model the frame-level object dependency, which promotes the confidence scores of correct actions. Experiments on two benchmark datasets show that SAMOC with the proposed two aspects achieves the state-of-the-art and works in real-time as well.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-0-7381-3366-9
DA  - 2021 
PY  - 2021
DO  - 10.1109/IJCNN52387.2021.9533300
AN  - WOS:000722581700013
AD  - Natl Univ Def Technol, Coll Comp, Changsha 410073, Peoples R China
AD  - Natl Univ Def Technol, Sci & Technol Parallel & Distributed Lab, Changsha 410073, Hunan, Peoples R China
AD  - Natl Univ Def Technol, Inst Quantum, Changsha 410073, Hunan, Peoples R China
AD  - Natl Univ Def Technol, State Key Lab High Performance Comp, Changsha 410073, Hunan, Peoples R China
AD  - Harbin Inst Technol Shenzhen, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China
Y2  - 2022-01-07
ER  -

TY  - CPAPER
AU  - Pan, Binbin
AU  - Wang, Wenzhong
AU  - Luo, Bin
ED  - Pan, Z
ED  - Hei, X
TI  - RGB-Skeleton Fusion Network For Spatial-Temporal Action Detection
T2  - TWELFTH INTERNATIONAL CONFERENCE ON GRAPHICS AND IMAGE PROCESSING (ICGIP 2020)
M3  - Proceedings Paper
CP  - 12th International Conference on Graphics and Image Processing (ICGIP)
CL  - Xian, PEOPLES R CHINA
AB  - Due to most of the current algorithms use stacked RGB information for spatial-temporal action detection, the time sequence information is easily lost in the process of convolution and down-sampling, which makes it difficult to model spatial-temporal action and limits the development of action detection. Given the current advanced pose estimation algorithm that has achieved good detection accuracy, we propose an end-to-end network that fuses RGB with skeleton to solve the problem of spatial-temporal action detection. We use RGB to describe the appearance information of object and skeleton to describe the action information. Specifically, in the first part, we generate the initial classification and location proposals based on RGB information by the SSD network. Secondly, we generate frame-level skeleton information by the current advanced pose estimation algorithm, the skeleton helps the SSD network to filter negative samples during training, and then we stack the skeleton after completion and normalization, put it into LSTM network for classification. Finally, we fuse the outputs of the SSD network and LSTM network. We believe that the introduction of skeleton information can effectively address the problem of the insufficient capacity of RGB information for spatial-temporal action modeling. It is worth noting that our skeleton information is based on advanced attitude estimation algorithms rather than annotated. For the datasets, we select the single-person action videos in UCF101 and UCF50. The final experimental results show that our method can significantly improve the action modeling ability of the neural network, and show effective results in action detection.
PU  - SPIE-INT SOC OPTICAL ENGINEERING
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN  - 0277-786X
SN  - 1996-756X
SN  - 978-1-5106-4278-2
SN  - 978-1-5106-4277-5
DA  - 2021 
PY  - 2021
VL  - 11720
C7  - 1172006
DO  - 10.1117/12.2589340
AN  - WOS:000700371000005
AD  - Anhui Univ, Hefei, Peoples R China
Y2  - 2021-10-24
ER  -

TY  - CPAPER
AU  - Jiang, Zhiqiang
AU  - Yang, Jianhua
AU  - Jiang, Nan
AU  - Liu, Shuaiyan
AU  - Xie, Tao
AU  - Zhao, Lijun
AU  - Li, Ruifeng
ED  - Lan, X
ED  - Mei, X
ED  - Jiang, C
ED  - Zhao, F
ED  - Tian, Z
TI  - YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-Time Spatio-Temporal Action Detection
T2  - INTELLIGENT ROBOTICS AND APPLICATIONS, ICIRA 2024, PT II
M3  - Proceedings Paper
CP  - 17th International Conference on Intelligent Robotics and Applications
CL  - Xi'an Jiaotong University, Xi'an, PEOPLES R CHINA
AB  - Designing a real-time framework for the spatio-temporal action detection task is still a challenge. In this paper, we propose a novel real-time action detection framework, YOWOv2. In this new framework, YOWOv2 takes advantage of both the 3D backbone and 2D backbone for accurate action detection. A multi-level detection pipeline is designed to detect action instances of different scales. To achieve this goal, we carefully build a simple and efficient 2D backbone with a feature pyramid network to extract different levels of classification features and regression features. For the 3D backbone, we adopt the existing efficient 3D CNN to save development time. By combining 3D backbones and 2D backbones of different sizes, we design a YOWOv2 family including YOWOv2-Tiny, YOWOv2-Medium, and YOWOv2-Large. We also introduce the popular dynamic label assignment strategy and anchor-free mechanism to make the YOWOv2 consistent with the advanced model architecture design. With our improvement, YOWOv2 is significantly superior to YOWO, and can still keep real-time detection. Without any bells and whistles, YOWOv2 achieves 87.0% frame mAP and 52.8% video mAP with over 20 FPS on the UCF101-24. On the AVA, YOWOv2 achieves 21.7% frame mAP with over 20 FPS. Our code is available on https://github.com/ yjh0410/YOWOv2.
PU  - SPRINGER-VERLAG SINGAPORE PTE LTD
PI  - SINGAPORE
PA  - 152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE
SN  - 2945-9133
SN  - 1611-3349
SN  - 978-981-96-0773-0
SN  - 978-981-96-0774-7
DA  - 2025 
PY  - 2025
VL  - 15202
SP  - 33
EP  - 48
DO  - 10.1007/978-981-96-0774-7_3
AN  - WOS:001446523700003
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China
AD  - Yangtze River Delta HIT Robot Technol Res Inst, Wuhu 241007, Peoples R China
M2  - Yangtze River Delta HIT Robot Technol Res Inst
Y2  - 2025-04-11
ER  -

TY  - JOUR
AU  - Heyward, Joseph
AU  - Carreira, Joao
AU  - Damen, Dima
AU  - Zisserman, Andrew
AU  - Patraucean, Viorica
TI  - Perception Test 2023: A Summary of the First Challenge And Outcome
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The First Perception Test challenge was held as a half-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2023, with the goal of benchmarking state-of-the-art video models on the recently proposed Perception Test benchmark. The challenge had six tracks covering low-level and high-level tasks, with both a language and non-language interface, across video, audio, and text modalities, and covering: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, and grounded video question-answering. We summarise in this report the task descriptions, metrics, baselines, and results.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2312.13090
AN  - PPRN:86743295
AD  - Google DeepMind, London, England
AD  - Univ Bristol, Bristol, England
AD  - Univ Oxford, Oxford, England
M2  - Univ Bristol
M2  - Univ Oxford
Y2  - 2023-12-29
ER  -

TY  - JOUR
AU  - Xu, Yiming
AU  - Zhou, Fangjie
AU  - Wang, Li
AU  - Peng, Wei
AU  - Zhang, Kai
TI  - Optimization of Action Recognition Model Based on Multi-Task Learning and Boundary Gradient
T2  - ELECTRONICS
M3  - Article
AB  - Recently, people's demand for action recognition has extended from the initial high classification accuracy to the high accuracy of the temporal action detection. It is challenging to meet the two requirements simultaneously. The key to behavior recognition lies in the quantity and quality of the extracted features. In this paper, a two-stream convolutional network is used. A three-dimensional convolutional neural network (3D-CNN) is used to extract spatiotemporal features from the consecutive frames. A two-dimensional convolutional neural network (2D-CNN) is used to extract spatial features from the key-frames. The integration of the two networks is excellent for improving the model's accuracy and can complete the task of distinguishing the start-stop frame. In this paper, a multi-scale feature extraction method is presented to extract more abundant feature information. At the same time, a multi-task learning model is introduced. It can further improve the accuracy of classification via sharing the data between multiple tasks. The experimental result shows that the accuracy of the modified model is improved by 10%. Meanwhile, we propose the confidence gradient, which can optimize the distinguishing method of the start-stop frame to improve the temporal action detection accuracy. The experimental result shows that the accuracy has been enhanced by 11%.
PU  - MDPI
PI  - BASEL
PA  - MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2021 OCT
PY  - 2021
VL  - 10
IS  - 19
C7  - 2380
DO  - 10.3390/electronics10192380
AN  - WOS:000727241400001
AD  - Nantong Univ, Coll Elect Engn, Nantong 226019, Peoples R China
Y2  - 2021-12-13
ER  -

TY  - JOUR
AU  - Lu, Chongkai
AU  - Mak, Man-Wai
AU  - Li, Ruimin
AU  - Chi, Zheru
AU  - Fu, Hong
TI  - Progression-Guided Temporal Action Detection in Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We present a novel framework, Action Progression Network (APN), for temporal action detection (TAD) in videos. The framework locates actions in videos by detecting the action evolution process. To encode the action evolution, we quantify a complete action process into 101 ordered stages (0%, 1%, ..., 100%), referred to as action progressions. We then train a neural network to recognize the action progressions. The framework detects action boundaries by detecting complete action processes in the videos, e.g., a video segment with detected action progressions closely follow the sequence 0%, 1%, ..., 100%. The framework offers three major advantages: (1) Our neural networks are trained end-to-end, contrasting conventional methods that optimize modules separately; (2) The APN is trained using action frames exclusively, enabling models to be trained on action classification datasets and robust to videos with temporal background styles differing from those in training; (3) Our framework effectively avoids detecting incomplete actions and excels in detecting long-lasting actions due to the fine-grained and explicit encoding of the temporal structure of actions. Leveraging these advantages, the APN achieves competitive performance and significantly surpasses its counterparts in detecting long-lasting actions. With an IoU threshold of 0.5, the APN achieves a mean Average Precision (mAP) of 58.3% on the THUMOS14 dataset and 98.9% mAP on the DFMAD70 dataset.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2308.09268
AN  - PPRN:86000821
AD  - Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hong Kong, Peoples R China
AD  - Xidian Univ, Acad Adv Interdisciplinary Res, Xian, Shaanxi, Peoples R China
AD  - Educ Univ Hong Kong, Hong Kong, Peoples R China
M2  - Hong Kong Polytech Univ
M2  - Educ Univ Hong Kong
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Yuan, Yuan
AU  - Lyu, Yueming
AU  - Shen, Xi
AU  - W. Tsang, Ivor
AU  - Yeung, Dit-Yan
TI  - Marginalized Average Attentional Network for Weakly-Supervised Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features. Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others. Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from O($2^T$) to O($T^2$). Extensive experiments on two large-scale video datasets show that our MAAN achieves superior performance on weakly-supervised temporal action localization
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1905.08586
AN  - PPRN:22918725
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Shao, Jiayi
AU  - Wang, Xiaohan
AU  - Quan, Ruijie
AU  - Zheng, Junjun
AU  - Yang, Jiang
AU  - Yang, Yi
A1  - IEEE
TI  - Action Sensitivity Learning for Temporal Action Localization
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Temporal action localization (TAL), which involves recognizing and locating action instances, is a challenging task in video understanding. Most existing approaches directly predict action classes and regress offsets to boundaries, while overlooking the discrepant importance of each frame. In this paper, we propose an Action Sensitivity Learning framework (ASL) to tackle this task, which aims to assess the value of each frame and then leverage the generated action sensitivity to recalibrate the training procedure. We first introduce a lightweight Action Sensitivity Evaluator to learn the action sensitivity at the class level and instance level, respectively. The outputs of the two branches are combined to reweight the gradient of the two sub-tasks. Moreover, based on the action sensitivity of each frame, we design an Action Sensitive Contrastive Loss to enhance features, where the action-aware frames are sampled as positive pairs to push away the action-irrelevant frames. The extensive studies on various action localization benchmarks (i.e., MultiThumos, Charades, Ego4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) show that ASL surpasses the state-of-the-art in terms of average-mAP under multiple types of scenarios, e.g., single-labeled, densely-labeled and egocentric.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 13411
EP  - 13423
DO  - 10.1109/ICCV51070.2023.01238
AN  - WOS:001169499005081
AD  - Zhejiang Univ, ReLER Lab, CCAI, Hangzhou, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
Y2  - 2024-04-06
ER  -

TY  - JOUR
AU  - Pan, Junting
AU  - Chen, Siyu
AU  - Zheng Shou, Mike
AU  - Liu, Yu
AU  - Shao, Jing
AU  - Li, Hongsheng
TI  - Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71 mAP). The code is available online.1
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2006.07976
AN  - PPRN:11628588
AD  - Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Hong Kong
AD  - Columbia Univ, New York, NY 10027, USA
AD  - Xidian Univ, Sch CST, Xidian, Peoples R China
AD  - SenseTime Res, Beijing, Peoples R China
M2  - Columbia Univ
M2  - Xidian Univ
M2  - SenseTime Res
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Zhang, Can
AU  - Cao, Meng
AU  - Yang, Dongming
AU  - Chen, Jie
AU  - Zou, Yuexian
TI  - CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Most existing models follow the &ldquo;localization by classification&rdquo; procedure: locate temporal regions contributing most to the video-level classification. Generally, they process each snippet (or frame) individually and thus overlook the fruitful temporal context relation. Here arises the single snippet cheating issue: &ldquo;hard&rdquo; snippets are too vague to be classified. In this paper, we argue that learning by comparing helps identify these hard snippets and we propose to utilize snippet Contrastive learning to Localize Actions, CoLA for short. Specifically, we propose a Snippet Contrast (SniCo) Loss to refine the hard snippet representation in feature space, which guides the network to perceive precise temporal boundaries and avoid the temporal interval interruption. Besides, since it is infeasible to access frame-level annotations, we introduce a Hard Snippet Mining algorithm to locate the potential hard snippets. Substantial analyses verify that this mining strategy efficaciously captures the hard snippets and SniCo Loss leads to more informative feature representation. Extensive experiments show that CoLA achieves state-of-the-art results on THUMOS&rsquo;14 and ActivityNet v1.2 datasets.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.16392
AN  - PPRN:12820329
AD  - Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
M2  - Peng Cheng Lab
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Bock, Marius
AU  - Kuehne, Hilde
AU  - Van Laerhoven, Kristof
AU  - Moeller, Michael
TI  - WEAR: An Outdoor Sports Dataset for Wearable and Egocentric Activity Recognition
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Research has shown the complementarity of camera- and inertial-based data for modeling human activities, yet datasets with both egocentric video and inertial-based sensor data remain scarce. In this paper, we introduce WEAR, an outdoor sports dataset for both vision- and inertial-based human activity recognition (HAR). Data from 22 participants performing a total of 18 different workout activities was collected with synchronized inertial (acceleration) and camera (egocentric video) data recorded at 11 different outside locations. WEAR provides a challenging prediction scenario in changing outdoor environments using a sensor placement, in line with recent trends in real-world applications. Benchmark results show that through our sensor placement, each modality interestingly offers complementary strengths and weaknesses in their prediction performance. Further, in light of the recent success of single-stage Temporal Action Localization (TAL) models, we demonstrate their versatility of not only being trained using visual data, but also using raw inertial data and being capable to fuse both modalities by means of simple concatenation. The dataset and code to reproduce experiments is publicly available via: mariusbock.github.io/wear/.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2304.05088
AN  - PPRN:58044388
AD  - Univ Siegen, Ubiquitous Comp, Siegen, Germany
AD  - Univ Tuebingen, Multimodal Learning, Tuebingen, Germany
AD  - Univ Siegen, Comp Vis, Siegen, Germany
M2  - Univ Tuebingen
Y2  - 2024-11-06
ER  -

TY  - CPAPER
AU  - Lin, Chuming
AU  - Xu, Chengming
AU  - Luo, Donghao
AU  - Wang, Yabiao
AU  - Tai, Ying
AU  - Wang, Chengjie
AU  - Li, Jilin
AU  - Huang, Feiyue
AU  - Fu, Yanwei
A1  - IEEE COMP SOC
TI  - Learning Salient Boundary Feature for Anchor-free Temporal Action Localization
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video. While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/TencentYoutuResearch/ActionDetection - AFSD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 3319
EP  - 3328
DO  - 10.1109/CVPR46437.2021.00333
AN  - WOS:000739917303051
AD  - Tencent, Youtu Lab, Shenzhen, Peoples R China
AD  - Fudan Univ, Shanghai, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Gupta, Akshita
AU  - Mittal, Gaurav
AU  - Magooda, Ahmed
AU  - Yu, Ye
AU  - Taylor, Graham W.
AU  - Chen, Mei
TI  - LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Gated Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2 (ViT-g) and leveraging them beyond head-only transfer learning.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.01282
AN  - PPRN:88364961
AD  - Microsoft, Hyderabad, Pakistan
AD  - Univ Guelph, Guelph, ON, Canada
AD  - Vector Inst AI, Toronto, ON, Canada
M2  - Microsoft
M2  - Vector Inst AI
Y2  - 2024-08-11
ER  -

TY  - JOUR
AU  - Qing, Zhiwu
AU  - Wang, Xiang
AU  - Sang, Yongpeng
AU  - Gao, Changxin
AU  - Zhang, Shiwei
AU  - Sang, Nong
TI  - Temporal Fusion Network for Temporal Action Localization:Submission to ActivityNet Challenge 2020 (Task E)
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report analyzes a temporal action localization method we used in the HACS competition which is hosted in Activitynet Challenge 2020.The goal of our task is to locate the start time and end time of the action in the untrimmed video, and predict action category.Firstly, we utilize the video-level feature information to train multiple video-level action classification models. In this way, we can get the category of action in the video.Secondly, we focus on generating high quality temporal proposals.For this purpose, we apply BMN to generate a large number of proposals to obtain high recall rates. We then refine these proposals by employing a cascade structure network called Refine Network, which can predict position offset and new IOU under the supervision of ground truth.To make the proposals more accurate, we use bidirectional LSTM, Nonlocal and Transformer to capture temporal relationships between local features of each proposal and global features of the video data.Finally, by fusing the results of multiple models, our method obtains 40.55% on the validation set and 40.53% on the test set in terms of mAP, and achieves Rank 1 in this challenge.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2006.07520
AN  - PPRN:22868767
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Ren, Huan
AU  - Yang, Wenfei
AU  - Zhang, Tianzhu
AU  - Zhang, Yongdong
TI  - Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to localize and recognize actions in untrimmed videos with only video-level category labels during training. Without instance-level annotations, most existing methods follow the Segment-based Multiple Instance Learning (S-MIL) framework, where the predictions of segments are supervised by the labels of videos. However, the objective for acquiring segment-level scores during training is not consistent with the target for acquiring proposal-level scores during testing, leading to suboptimal results. To deal with this problem, we propose a novel Proposal-based Multiple Instance Learning (P-MIL) framework that directly classifies the candidate proposals in both the training and testing stages, which includes three key designs: 1) a surrounding contrastive feature extraction module to suppress the discriminative short proposals by considering the surrounding contrastive information, 2) a proposal completeness evaluation module to inhibit the low-quality proposals with the guidance of the completeness pseudo labels, and 3) an instance-level rank consistency loss to achieve robust detection by leveraging the complementarity of RGB and FLOW modalities. Extensive experimental results on two challenging benchmarks including THUMOS14 and ActivityNet demonstrate the superior performance of our method.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2305.17861
AN  - PPRN:72753394
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Deep Space Explorat Lab, Nanjing, Peoples R China
M2  - Univ Sci & Technol China
M2  - Deep Space Explorat Lab
Y2  - 2023-06-23
ER  -

TY  - CPAPER
AU  - Lee, Pilhyeon
AU  - Wang, Jinglu
AU  - Lu, Yan
AU  - Byun, Hyeran
A1  - Assoc Advancement Artificial Intelligence
TI  - Weakly-supervised Temporal Action Localization by Uncertainty Modeling
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THU-MOS' 14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL- Uncertainty- Modeling.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
DA  - 2021 
PY  - 2021
VL  - 35
SP  - 1854
EP  - 1862
AN  - WOS:000680423501105
AD  - Yonsei Univ, Dept Comp Sci, Seoul, South Korea
AD  - Microsoft Res Asia, Beijing, Peoples R China
AD  - Yonsei Univ, Grad Sch AI, Seoul, South Korea
Y2  - 2021-09-02
ER  -

TY  - JOUR
AU  - Yoon, Da-Hye
AU  - Cho, Nam-Gyu
AU  - Lee, Seong-Whan
TI  - A Novel Online Action Detection Framework from Untrimmed Video Streams
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision. It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOS'14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2003.07734
AN  - PPRN:22639689
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Wang, Yu
AU  - Li, Yadong
AU  - Wang, Hongbin
A1  - IEEE
TI  - Two-Stream Networks for Weakly-Supervised Temporal Action Localization with Semantic-Aware Mechanisms
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Weakly-supervised temporal action localization aims to detect action boundaries in untrimmed videos with only video-level annotations. Most existing schemes detect temporal regions that are most responsive to video-level classification, but they overlook the semantic consistency between frames. In this paper, we hypothesize that snippets with similar representations should be considered as the same action class despite the absence of supervision signals on each snippet. To this end, we devise a learnable dictionary where entries are the class centroids of the corresponding action categories. The representations of snippets identified as the same action category are induced to be close to the same class centroid, which guides the network to perceive the semantics of frames and avoid unreasonable localization. Besides, we propose a two-stream framework that integrates the attention mechanism and the multiple-instance learning strategy to extract fine-grained clues and salient features respectively. Their complementarity enables the model to refine temporal boundaries. Finally, the developed model is validated on the publicly available THUMOS-14 and ActivityNet-1.3 datasets, where substantial experiments and analyses demonstrate that our model achieves remarkable advances over existing methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 18878
EP  - 18887
DO  - 10.1109/CVPR52729.2023.01810
AN  - WOS:001062531303019
AD  - Ant Grp, Hangzhou, Peoples R China
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Lee, Pilhyeon
AU  - Wang, Jinglu
AU  - Lu, Yan
AU  - Byun, Hyeran
TI  - Weakly-supervised Temporal Action Localization by Uncertainty Modeling
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2006.07006
AN  - PPRN:15044839
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Ren, Huan
AU  - Yang, Wenfei
AU  - Zhang, Tianzhu
AU  - Zhang, Yongdong
A1  - IEEE
TI  - Proposal-based Multiple Instance Learning for Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Weakly-supervised temporal action localization aims to localize and recognize actions in untrimmed videos with only video-level category labels during training. Without instance-level annotations, most existing methods follow the Segment-based Multiple Instance Learning (S-MIL) framework, where the predictions of segments are supervised by the labels of videos. However, the objective for acquiring segment-level scores during training is not consistent with the target for acquiring proposal-level scores during testing, leading to suboptimal results. To deal with this problem, we propose a novel Proposal-based Multiple Instance Learning (P-MIL) framework that directly classifies the candidate proposals in both the training and testing stages, which includes three key designs: 1) a surrounding contrastive feature extraction module to suppress the discriminative short proposals by considering the surrounding contrastive information, 2) a proposal completeness evaluation module to inhibit the low-quality proposals with the guidance of the completeness pseudo labels, and 3) an instance-level rank consistency loss to achieve robust detection by leveraging the complementarity of RGB and FLOW modalities. Extensive experimental results on two challenging benchmarks including THUMOS14 and ActivityNet demonstrate the superior performance of our method. Our code is available at github.com/RenHuan1999/CVPR2023_P-MIL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 2394
EP  - 2404
DO  - 10.1109/CVPR52729.2023.00237
AN  - WOS:001058542602070
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Deep Space Explorat Lab, Hefei, Peoples R China
M2  - Deep Space Explorat Lab
Y2  - 2023-11-08
ER  -

TY  - JOUR
AU  - Narayan, Sanath
AU  - Cholakkal, Hisham
AU  - Hayat, Munawar
AU  - Khan, Fahad Shahbaz
AU  - Yang, Ming-Hsuan
AU  - Shao, Ling
TI  - D2-Net: Weakly-Supervised Action Localization via Discriminative Embeddings and Denoised Activations
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision. Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formulation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discriminative term incorporates a classification loss and utilizes a top-down attention mechanism to enhance the separability of latent foreground-background embeddings. The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, activations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive experiments are performed on multiple benchmarks, including THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on all datasets, achieving gains as high as 2.3% in terms of mAP at IoU=0.5 on THUMOS14.&nbsp;
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2012.06440
AN  - PPRN:11916290
AD  - Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates
AD  - Mohamed Bin Zayed Univ AI, Abu Dhabi, U Arab Emirates
AD  - Monash Univ, Clayton, Australia
AD  - Linkoping Univ, Linkoping, Sweden
AD  - Univ Calif Merced, Merced, CA, USA
AD  - Google Res, Mt View, CA, USA
AD  - Yonsei Univ, Seoul, South Korea
M2  - Incept Inst Artificial Intelligence
M2  - Mohamed Bin Zayed Univ AI
M2  - Monash Univ
M2  - Linkoping Univ
M2  - Univ Calif Merced
M2  - Google Res
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Yin, Yuan
AU  - Huang, Yifei
AU  - Furuta, Ryosuke
AU  - Sato, Yoichi
TI  - Proposal-based Temporal Action Localization with Point-level Supervision
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Point-level supervised temporal action localization (PTAL) aims at recognizing and localizing actions in untrimmed videos where only a single point (frame) within every action instance is annotated in training data. Without temporal annotations, most previous works adopt the multiple instance learning (MIL) framework, where the input video is segmented into non-overlapped short snippets, and action classification is performed independently on every short snippet. We argue that the MIL framework is suboptimal for PTAL because it operates on separated short snippets that contain limited temporal information. Therefore, the classifier only focuses on several easy-to-distinguish snippets instead of discovering the whole action instance without missing any relevant snippets. To alleviate this problem, we propose a novel method that localizes actions by generating and evaluating action proposals of flexible duration that involve more comprehensive temporal information. Moreover, we introduce an efficient clustering algorithm to efficiently generate dense pseudo labels that provide stronger supervision, and a fine-grained contrastive loss to further refine the quality of pseudo labels. Experiments show that our proposed method achieves competitive or superior performance to the state-of-the-art methods and some fully-supervised methods on four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2310.05511
AN  - PPRN:85582746
AD  - Univ Tokyo, Inst Ind Sci, Tokyo, Japan
Y2  - 2023-10-17
ER  -

TY  - JOUR
AU  - Ju, Chen
AU  - Li, Zeqian
AU  - Zhao, Peisen
AU  - Zhang, Ya
AU  - Zhang, Xiaopeng
AU  - Tian, Qi
AU  - Wang, Yanfeng
AU  - Xie, Weidi
TI  - Multi-modal Prompting for Low-Shot Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we consider the problem of temporal action localization under low-shot (zero-shot & few-shot) scenario, with the goal of detecting and classifying the action instances from arbitrary categories within some untrimmed videos, even not seen at training time. We adopt a Transformer-based two-stage action localization architecture with class-agnostic action proposal, followed by open-vocabulary classification. We make the following contributions. First, to compensate image-text foundation models with temporal motions, we improve category-agnostic action proposal by explicitly aligning embeddings of optical flows, RGB and texts, which has largely been ignored in existing low-shot methods. Second, to improve open-vocabulary action classification, we construct classifiers with strong discriminative power, i.e., avoid lexical ambiguities. To be specific, we propose to prompt the pre-trained CLIP text encoder either with detailed action descriptions (acquired from large-scale language models), or visually-conditioned instance-specific prompt vectors. Third, we conduct thorough experiments and ablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior performance of our proposed model, outperforming existing state-of-the-art approaches by one significant margin.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.11732
AN  - PPRN:46956653
AD  - Shanghai Jiao Tong Univ, CMIC, Shanghai, Peoples R China
AD  - Huawei Cloud & AI, Shenzhen, Guangdong, Peoples R China
M2  - Huawei Cloud & AI
Y2  - 2023-04-06
ER  -

TY  - CPAPER
AU  - Liu, Ziyi
AU  - Wang, Le
AU  - Tang, Wei
AU  - Yuan, Junsong
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - Assoc Advancement Artificial Intelligence
TI  - Weakly Supervised Temporal Action Localization Through Learning Explicit Subspaces for Action and Context
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to localize temporal starts and ends of action instances in a video under only video-level supervision. Existing WS-TAL methods rely on deep features learned for action recognition. However, due to the mismatch between classification and localization, these features cannot distinguish the frequently co-occurring contextual background, i.e., the context, and the actual action instances. We term this challenge action-context confusion, and it will adversely affect the action localization accuracy. To address this challenge, we introduce a framework that learns two feature subspaces respectively for actions and their context. By explicitly accounting for action visual elements, the action instances can be localized more precisely without the distraction from the context. To facilitate the learning of these two feature subspaces with only video-level categorical labels, we leverage the predictions from both spatial and temporal streams for snippets grouping. In addition, an unsupervised learning task is introduced to make the proposed module focus on mining temporal information. The proposed approach outperforms state-of-the-art WS-TAL methods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3 datasets.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
DA  - 2021 
PY  - 2021
VL  - 35
SP  - 2242
EP  - 2250
AN  - WOS:000680423502038
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Univ Illinois, Chicago, IL USA
AD  - SUNY Buffalo, Buffalo, NY USA
AD  - Wormpex AI Res, Bellevue, WA USA
M2  - Wormpex AI Res
Y2  - 2021-09-02
ER  -

TY  - JOUR
AU  - Lin, Chuming
AU  - Xu, Chengming
AU  - Luo, Donghao
AU  - Wang, Yabiao
AU  - Tai, Ying
AU  - Wang, Chengjie
AU  - Li, Jilin
AU  - Huang, Feiyue
AU  - Fu, Yanwei
TI  - Learning Salient Boundary Feature for Anchor-free Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video.While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/TencentYoutuResearch/ActionDetection-AFSD.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.13137
AN  - PPRN:11474314
AD  - Tencent, Youtu Lab, Shenzhen, Peoples R China
AD  - Fudan Univ, Shanghai, Peoples R China
M2  - Tencent
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Shao, Jiayi
AU  - Wang, Xiaohan
AU  - Quan, Ruijie
AU  - Zheng, Junjun
AU  - Yang, Jiang
AU  - Yang, Yi
TI  - Action Sensitivity Learning for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action localization (TAL), which involves recognizing and locating action instances, is a challenging task in video understanding. Most existing approaches directly predict action classes and regress offsets to boundaries, while overlooking the discrepant importance of each frame. In this paper, we propose an Action Sensitivity Learning framework (ASL) to tackle this task, which aims to assess the value of each frame and then leverage the generated action sensitivity to recalibrate the training procedure. We first introduce a lightweight Action Sensitivity Evaluator to learn the action sensitivity at the class level and instance level, respectively. The outputs of the two branches are combined to reweight the gradient of the two sub-tasks. Moreover, based on the action sensitivity of each frame, we design an Action Sensitive Contrastive Loss to enhance features, where the action-aware frames are sampled as positive pairs to push away the action-irrelevant frames. The extensive studies on various action localization benchmarks (i.e., MultiThumos, Charades, Ego4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) show that ASL surpasses the state-of-the-art in terms of average-mAP under multiple types of scenarios, e.g., single-labeled, densely-labeled and egocentric.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2305.15701
AN  - PPRN:72716045
AD  - Zhejiang Univ, ReLER Lab, CCAI, Hangzhou, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
M2  - Zhejiang Univ
Y2  - 2023-09-27
ER  -

TY  - CPAPER
AU  - Ju, Chen
AU  - Zheng, Kunhao
AU  - Liu, Jinxiang
AU  - Zhao, Peisen
AU  - Zhang, Ya
AU  - Chang, Jianlong
AU  - Tian, Qi
AU  - Wang, Yanfeng
A1  - IEEE
TI  - Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Weakly-supervised temporal action localization (WTAL) learns to detect and classify action instances with only category labels. Most methods widely adopt the off-the-shelf Classification-Based Pre-training (CBP) to generate video features for action localization. However, the different optimization objectives between classification and localization, make temporally localized results suffer from the serious incomplete issue. To tackle this issue without additional annotations, this paper considers to distill free action knowledge from Vision-Language Pre-training (VLP), as we surprisingly observe that the localization results of vanilla VLP have an over-complete issue, which is just complementary to the CBP results. To fuse such complementarity, we propose a novel distillation-collaboration framework with two branches acting as CBP and VLP respectively. The framework is optimized through a dual-branch alternate training strategy. Specifically, during the B step, we distill the confident background pseudo-labels from the CBP branch; while during the F step, the confident foreground pseudo-labels are distilled from the VLP branch. As a result, the dual-branch complementarity is effectively fused to promote one strong alliance. Extensive experiments and ablation studies on THUMOS14 and ActivityNet1.2 reveal that our method significantly outperforms state-of-the-art methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 14751
EP  - 14762
DO  - 10.1109/CVPR52729.2023.01417
AN  - WOS:001062522107008
AD  - Shanghai Jiao Tong Univ, CMIC, Shanghai, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
AD  - Huawei Cloud, Shenzhen, Peoples R China
Y2  - 2023-11-15
ER  -

TY  - CPAPER
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Deng, Jiankang
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
A1  - IEEE
TI  - DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code is available at https://github.com/sauradip/DiffusionTAD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 10328
EP  - 10340
DO  - 10.1109/ICCV51070.2023.00951
AN  - WOS:001169499002072
AD  - Univ Surrey, CVSSP, Surrey, England
AD  - IFlyTek Surrey Joint Res Ctr Artificial Intellige, Surrey, England
AD  - Surrey Inst People Centred Artificial Intelligenc, Surrey, England
AD  - Imperial Coll London, London, England
M2  - IFlyTek Surrey Joint Res Ctr Artificial Intellige
M2  - Surrey Inst People Centred Artificial Intelligenc
Y2  - 2024-04-06
ER  -

TY  - JOUR
AU  - Wu, Lifang
AU  - Xin, Chang
AU  - Li, Zun
AU  - Cui, Di
TI  - Dual-branch Cross-scale Feature Interaction for Temporal Action Detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal Action Detection (TAD) is aim to predict action boundary and category simultaneously. Most existing RGB-based methods model temporal dependency using pyramid -style features without interaction among different scales, which usually result in inaccurate prediction for long-term actions. The reason is that features at different scales involve information with different granularity, which is suitable for either prediction of action boundary or category. In this paper, we present a novel Dual -branch Cross -scale Feature Interaction (DCFI) method that directly exchanges different scale information from both temporal and spatial perspective for TAD. To be specific, in one branch, a cross -scale temporal transformer module is devised to enable both semantic and temporal communications among different scale features with a merge -to -split mechanism. While the other branch designs a cross -scale spatial mixer module to mine the most salient spatial difference between consecutive and long-term frames via a scale -mixer. Benefiting from these two modules, DCFI achieves comprehensive temporal as well as spatial interaction across all feature scales, and thus accurately predicts the boundaries of different time -span action instances. Extensive experiments on two challenging benchmarks, i.e. ., THUMOS-14 and ActivityNet-1.3, demonstrate that our DCFI achieves new state-of-the-art performance with only RGB.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2024 SEP 7
PY  - 2024
VL  - 597
C7  - 128087
DO  - 10.1016/j.neucom.2024.128087
AN  - WOS:001264418700001
C6  - JUL 2024
AD  - Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China
Y2  - 2024-07-16
ER  -

TY  - JOUR
AU  - Chen, Xu
AU  - Gao, Chenqiang
AU  - Li, Chaoyu
AU  - Yang, Yi
AU  - Meng, Deyu
TI  - Infrared Action Detection in the Dark via Cross-Stream Attention Mechanism
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Action detection plays an important role in video understanding and attracts considerable attention in the last decade. However, current action detection methods are mainly based on visible videos, and few of them consider scenes with low-light, where actions are difficult to be detected by existing methods, or even by human eyes. Compared with visible videos, infrared videos are more suitable for the dark environment and resistant to background clutter. In this paper, we investigate the temporal action detection problem in the dark by using infrared videos, which is, to the best of our knowledge, the first attempt in the action detection community. Our model takes the whole video as input, a Flow Estimation Network (FEN) is employed to generate the optical flow for infrared data, and it is optimized with the whole network to obtain action-related motion representations. After feature extraction, the infrared stream and flow stream are fed into a Selective Cross-stream Attention (SCA) module to narrow the performance gap between infrared and visible videos. The SCA emphasizes informative snippets and focuses on the more discriminative stream automatically. Then we adopt a snippet-level classifier to obtain action scores for all snippets and link continuous snippets into final detections. All these modules are trained in an end-to-end manner. We collect an Infrared action Detection (InfDet) dataset obtained in the dark and conduct extensive experiments to verify the effectiveness of the proposed method. Experimental results show that our proposed method surpasses state-of-the-art temporal action detection methods designed for visible videos, and it also achieves the best performance compared with other infrared action recognition methods on both InfAR and Infrared-Visible datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2022 
PY  - 2022
VL  - 24
SP  - 288
EP  - 300
DO  - 10.1109/TMM.2021.3050069
AN  - WOS:000745524300022
AD  - Chongqing Univ Posts & Telecommunt, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China
AD  - Chongqing Key Lab Signal & Informat Proc, Chongqing 400065, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Sch Automat, Chongqing 400065, Peoples R China
AD  - Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo, NSW 2007, Australia
AD  - Macau Univ Sci & Technol, Macau Inst Syst Engn, Taipa 999078, Macao, Peoples R China
AD  - Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Shanxi, Peoples R China
M2  - Chongqing Univ Posts & Telecommunt
M2  - Chongqing Key Lab Signal & Informat Proc
Y2  - 2022-02-01
ER  -

TY  - JOUR
AU  - Hu, Xuejiao
AU  - Dai, Jingzhao
AU  - Li, Ming
AU  - Li, Yang
AU  - Du, Sidan
TI  - An efficient action proposal processing approach for temporal action detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal action detection is a fundamental yet challenging task in video understanding. It is important to process the action proposals for action classification and temporal boundary localization. Some methods process action proposals by exploiting the relations between them. However, learning the relations between numerous action proposals is time-consuming and requires huge computation and memory storage. Each proposal contains contextual information extracted from video segments, and redundant information aggregation has a negative impact on the final detection performance. In this paper, we exploit an efficient model which processes each proposal individually and learn intra-proposal features adequately, avoiding the interference of redundant information to achieve more effective detection. We also design relational learning models based on mean pooling, self-attention, and temporal convolution to compare with the intra-proposal learning model. Extensive experiments show that our method outperforms the relation learning models and achieves competitive performance on the two standard benchmarks. Moreover, efficiency experiments also verify that our model is more efficient than the relation learning methods.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2025 MAR 28
PY  - 2025
VL  - 623
C7  - 129294
DO  - 10.1016/j.neucom.2024.129294
AN  - WOS:001402283500001
C6  - JAN 2025
AD  - Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China
Y2  - 2025-01-26
ER  -

TY  - CPAPER
AU  - Liu, Zhikang
AU  - Wang, Zilei
AU  - Zhao, Yan
AU  - Tian, Ye
ED  - Jawahar, CV
ED  - Li, H
ED  - Mori, G
ED  - Schindler, K
TI  - SMC: Single-Stage Multi-location Convolutional Network for Temporal Action Detection
T2  - COMPUTER VISION - ACCV 2018, PT II
M3  - Proceedings Paper
CP  - 14th Asian Conference on Computer Vision (ACCV)
CL  - Perth, AUSTRALIA
AB  - Temporal action detection in untrimmed videos is an important and challenging visual task. State-of-the-art works always adopt a multi-stage pipeline, i.e., a class-agnostic segment proposal followed by a multi-label action classification. This pipeline is computationally slow and hard to optimize as each stage need be trained separately. Moreover, a desirable method should go beyond segment-level localization and make dense predictions with precise boundaries. We introduce a novel detection model in this paper, Single-stage Multi-location Convolutional Network (SMC), which completely eliminates the proposal generation and spatio-temporal feature resampling, and predicts frame-level action locations with class probabilities in a unified end-to-end network. Specifically, we associate a set of multi-scale default locations with each feature map cell in multiple layers, then predict the location offsets to the default locations, as well as action categories. SMC in practice is faster than the existing methods (753 FPS on a Titan X Maxwell GPU) and achieves state-of-the-art performance on THUMOS'14 and MEXaction2.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-20889-9
SN  - 978-3-030-20890-5
DA  - 2019 
PY  - 2019
VL  - 11362
SP  - 179
EP  - 195
DO  - 10.1007/978-3-030-20890-5_12
AN  - WOS:000492902300012
AD  - Univ Sci & Technol China, Dept Automat, Hefei, Anhui, Peoples R China
AD  - Megvii Inc Face, Beijing, Peoples R China
M2  - Megvii Inc Face
Y2  - 2019-11-06
ER  -

TY  - JOUR
AU  - Zhou, Xuyang
AU  - Wang, Ye
AU  - Tao, Fei
AU  - Yu, Hong
AU  - Liu, Qun
TI  - Hierarchical chat-based strategies with MLLMs for Spatio-temporal action detection
T2  - INFORMATION PROCESSING & MANAGEMENT
M3  - Article
AB  - Spatio-temporal action detection (STAD) in football matches is challenging due to the subtle, fast-paced actions involving multiple participants. Multimodal large language models (MLLMs) often fail to capture these nuances with standard prompts, producing results lacking the detailed descriptions needed to improve visual features. To address this issue, we propose a prompt strategy called Hierarchical Chat-Based Strategies (HCBS). Specifically, this strategy enables MLLMs to form a chain of thought (CoT), gradually generating content with increasingly detailed information. We conduct extensive experiments on three datasets: 126 videos from Multisports, 43 videos from J-HMDB, and 147 videos from UCF101-24, all focus on the football sections. Compared to baseline tasks, our method improves performance by 30.3%, 26.1%, and 25.5% on these three datasets, respectively. Through the experiment of Hierarchy Verification, we demonstrate that HCBS effectively guides MLLMs in generating hierarchical descriptions. Additionally, using HCBS to guide MLLMs in content generation, we create a frame-level description dataset with 120,511 frame descriptions across the three datasets. Our code and dataset are available at the following link: https://github.com/TristanAlkaid/HCBS/.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0306-4573
SN  - 1873-5371
DA  - 2025 JUL
PY  - 2025
VL  - 62
IS  - 4
C7  - 104094
DO  - 10.1016/j.ipm.2025.104094
AN  - WOS:001428646700001
C6  - FEB 2025
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Computat Intelligence, Chongqing, Peoples R China
AD  - Shanghai Feathervibe Tech, Shanghai, Peoples R China
M2  - Shanghai Feathervibe Tech
Y2  - 2025-03-01
ER  -

TY  - JOUR
AU  - Lu, Chong-Kai
AU  - Mak, Man-Wai
AU  - Li, Ruimin
AU  - Chi, Zheru
AU  - Fu, Hong
TI  - Action Progression Networks for Temporal Action Detection in Videos
T2  - IEEE ACCESS
M3  - Article
AB  - This study introduces an innovative Temporal Action Detection (TAD) model that is distinguished by its lightweight structure and capability for end-to-end training, delivering competitive performance. Traditional TAD approaches often rely on pre-trained models for feature extraction, compromising on end-to-end training for efficiency, yet encounter challenges due to misalignment with tasks and data shifts. Our method addresses these challenges by processing untrimmed videos on a snippet basis, facilitating a snippet-level TAD model that is trained end-to-end. Central to our approach is a novel frame-level label, termed "action progressions," designed to encode temporal localization information. The prediction of action progressions not only enables our snippet-level model to incorporate temporal information effectively but also introduces a granular temporal encoding for the evolution of actions, enhancing the precision of detection. Beyond a streamlined pipeline, our model introduces several novel capabilities: 1) It directly learns from raw videos, unlike prevalent TAD methods that depend on frozen, pre-trained feature extraction models; 2) It is flexible for training with trimmed and untrimmed videos; 3) It is the first TAD model to avoid the detection of incomplete actions; and 4) It can accurately detect long-lasting actions or those with clear evolutionary patterns. Utilizing these advantages, our model achieves commendable performance on benchmark datasets, securing averaged mean Average Precision (mAP) scores of 54.8%, 30.5%, and 78.7% on THUMOS14, ActivityNet-1.3, and DFMAD, respectively.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2024 
PY  - 2024
VL  - 12
SP  - 126829
EP  - 126844
DO  - 10.1109/ACCESS.2024.3451503
AN  - WOS:001316135100001
AD  - Hong Kong Polytech Univ, Dept Elect & Elect Engn, Hong Kong, Peoples R China
AD  - Xidian Univ, Acad Adv Interdisciplinary Res, Xian 710126, Shaanxi, Peoples R China
AD  - Educ Univ Hong Kong, Dept Math & Informat Technol, Hong Kong, Peoples R China
Y2  - 2024-09-25
ER  -

TY  - CPAPER
AU  - Dang, Yuanjie
AU  - Shou, Haoyu
AU  - Chen, Peng
AU  - Gao, Nan
AU  - Huan, Ruohong
AU  - Zhang, Yilong
ED  - Lin, Z
ED  - Cheng, MM
ED  - He, R
ED  - Ubul, K
ED  - Silamu, W
ED  - Zha, H
ED  - Zhou, J
ED  - Liu, CL
TI  - Focus on Subtle Actions: Semantic and Saliency Knowledge Co-Propagation Method for Weakly-Supervised Temporal Action Localization
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2024, PT X
M3  - Proceedings Paper
CP  - 7th Chinese Conference on Pattern Recognition and Computer Vision
CL  - Urumqi, PEOPLES R CHINA
AB  - Weakly-supervised temporal action localization (WTAL) aims to classify and localize actions with only video-level labels. Existing methods use more complex networks and pre-trained backbone to mine visual information, but fail to effectively focus on the subtle-motion subjects in the video, thus generating large errors in the presence of large-scale background disturbance and weak temporal contextual correlation. To enhance the model's ability to focus on subtle-motion subjects, we propose a Semantic and Saliency Knowledge Co-Propagation (S(2)KPro) method that consists of a Class-Aware Branch, a Saliency-Aware Branch, a Knowledge Interaction Module and a Knowledge Internalization Module collaborating with each other to achieve knowledge propagation. Specifically, two branches are introduced to acquire semantic and saliency knowledge respectively and establish an initial connection between two types of knowledge. The Knowledge Interaction Module propagates knowledge corresponding to branches from one branch to the other through co-distillation and achieve the integration of complementary information in both knowledge. The Knowledge Internalization Module mines out key snippets with both critical semantic and saliency information and ambiguous snippets containing unbalanced semantic and saliency content, for which we generates more accurate feature representations through contrastive learning to condense the shared information of the key snippets and alleviate the information imbalance of the ambiguous snippets. Extensive experiments demonstrate that S2KPro achieves state-of-the-art results on the THUMOS14 and ActivityNet1.2 datasets. The code is available at https://github.com/shouhyshy/S2KPro.
PU  - SPRINGER-VERLAG SINGAPORE PTE LTD
PI  - SINGAPORE
PA  - 152 BEACH ROAD, #21-01/04 GATEWAY EAST, SINGAPORE, 189721, SINGAPORE
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-981-97-8791-3
SN  - 978-981-97-8792-0
DA  - 2025 
PY  - 2025
VL  - 15040
SP  - 342
EP  - 356
DO  - 10.1007/978-981-97-8792-0_24
AN  - WOS:001416898800024
AD  - Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310014, Peoples R China
Y2  - 2025-03-07
ER  -

TY  - CPAPER
AU  - Fang, Zhenying
AU  - Zhu, Suguo
AU  - Yu, Jun
AU  - Tian, Qi
A1  - IEEE
TI  - PCPCAD: PROPOSAL COMPLEMENTARY ACTION DETECTOR
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Shanghai, PEOPLES R CHINA
AB  - Temporal action detection is still a challenging task. This task not only requires correct classification, but also needs to accurately detect the start and end times of each action. In this paper, we present a novel proposal complementary action detector (PCAD) to deal with video streams under continuous, untrimmed conditions. Our approach first uses a simple fully 3D convolutional (Conv3D) network to encode the video streams and then generates candidate temporal proposals for activities by using anchor segments. To generate more precise proposals, we also designed a boundary proposal network (BPN) to offer some complementary information for the candidate proposals. Finally, we learn an efficient classifier to classify the generated proposals into different activities and refine their temporal boundaries at the same time. Our model can achieve end-to-end training by jointly optimizing classification loss and regression loss. When evaluating on THUMOS'14 detection benchmark, PCAD achieves the state-of-the-art performance in high-speed models.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-5386-9552-4
DA  - 2019 
PY  - 2019
SP  - 424
EP  - 429
DO  - 10.1109/ICME.2019.00080
AN  - WOS:000501820600072
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou, Zhejiang, Peoples R China
AD  - Huawei Noahs Ark Lab, Hangzhou, Zhejiang, Peoples R China
AD  - Univ Texas San Antonio, San Antonio, TX USA
Y2  - 2019-12-27
ER  -

TY  - JOUR
AU  - Zhang, Shihui
AU  - Wang, Houlin
AU  - Wang, Lei
AU  - Han, Xueqiang
AU  - Tian, Qing
TI  - CGCN: Context graph convolutional network for few-shot temporal action localization
T2  - INFORMATION PROCESSING & MANAGEMENT
M3  - Article
AB  - Localizing human actions in videos has attracted extensive attention from industry and academia. Few-Shot Temporal Action Localization (FS-TAL) aims to detect human actions in untrimmed videos using a limited number of training samples. Existing FS-TAL methods usually ignore the semantic context between video snippets, making it difficult to detect actions during the query process. In this paper, we propose a novel FS-TAL method named Context Graph Convolutional Network (CGCN) which employs multi-scale graph convolution to aggregate semantic context between video snippets in addition to exploiting their temporal context. Specifically, CGCN constructs a graph for each scale of a video, where each video snippet is a node, and the relationships between the snippets are edges. There are three types of edges, namely sequence edges, intra-action edges, and inter-action edges. CGCN establishes sequence edges to enhance temporal expression. Intra-action edges utilize hyperbolic space to encapsulate context among video snippets within each action, while inter-action edges leverage Euclidean space to capture similar semantics between different actions. Through graph convolution on each scale, CGCN enables the acquisition of richer and context-aware video representations. Experiments demonstrate CGCN outperforms the second-best method by 4.5%/0.9% and 4.3%/0.9% mAP on the ActivityNet and THUMOS14 datasets in oneshot/five-shot scenarios, respectively, at tIoU@0.5. The source code can be found in https: //github.com/mugenggeng/CGCN.git.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0306-4573
SN  - 1873-5371
DA  - 2025 JAN
PY  - 2025
VL  - 62
IS  - 1
C7  - 103926
DO  - 10.1016/j.ipm.2024.103926
AN  - WOS:001336605000001
C6  - OCT 2024
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Univ Alabama Birmingham, Dept Comp Sci, Birmingham, AL USA
AD  - Hebei Univ Econ & Business, Sch Management Sci & Engn, Shijiazhuang, Hebei, Peoples R China
AD  - Key Lab Comp Virtual Technol & Syst Integrat Hebei, Qinhuangdao, Hebei, Peoples R China
M2  - Key Lab Comp Virtual Technol & Syst Integrat Hebei
Y2  - 2024-10-27
ER  -

TY  - JOUR
AU  - Cheng, Yi
AU  - Sun, Ying
AU  - Fan, Hehe
AU  - Zhuo, Tao
AU  - Lim, Joo-Hwee
AU  - Kankanhalli, Mohan
TI  - Entropy guided attention network for weakly-supervised action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net. (C) 2022 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2022 SEP
PY  - 2022
VL  - 129
C7  - 108718
DO  - 10.1016/j.patcog.2022.108718
AN  - WOS:000832702600005
C6  - APR 2022
AD  - Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore
AD  - Agcy Sci Technol & Res, Ctr Frontier AI Res, Singapore 138632, Singapore
AD  - Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore
AD  - Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore
AD  - Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China
Y2  - 2022-08-10
ER  -

TY  - CPAPER
AU  - Chen, Keke
AU  - Shu, Xiangbo
AU  - Xie, Guo-Sen
AU  - Yan, Rui
AU  - Tang, Jinhui
A1  - ACM
TI  - Foreground/Background-Masked Interaction Learning for Spatio-temporal Action Detection
T2  - PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023
M3  - Proceedings Paper
CP  - 31st ACM International Conference on Multimedia (MM)
CL  - Ottawa, CANADA
AB  - Spatio-temporal Action Detection (SAD) aims to recognize the multi-class actions, and meanwhile locate their spatio-temporal occurrence in untrimmed videos. Besides relying on the inherent inter-actor interactions, most previous SAD approaches model actor interactions between multi-actors and the whole frames or special parts (e.g., objects/hands). However, such approaches are relatively graceless by 1) roughly treating all various actors to equivalently interact with frames/parts or by 2) sumptuously borrowing multiple costly detectors to acquire the special parts. To solve the above dilemma, we propose a novel Foreground/Background-masked Interaction Learning (dubbed as FBI Learning) framework to learn the multi-actor features by attentively interacting with the hands-down foreground and background frames. Specifically, we first design a new Mask-guided Cross Attention (MCA) mechanism that calculates the masked cross-attentions to capture the compact relations between the actors and foreground/background regions. Next, we present a new Actor-guided Feature Aggregation (AFA) scheme that integrates foreground- and background-interacted actor features with the learnable actor-based weights. Finally, we construct a long-term feature bank that associates temporal context information to facilitate action classification. Extensive experiments are conducted on commonly available UCF101-24, MultiSports, and AVA v2.1/v2.2 datasets, which illustrate the competitive performance of FBI Learning against the state-of-the-art methods.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0108-5
DA  - 2023 
PY  - 2023
SP  - 2381
EP  - 2390
DO  - 10.1145/3581783.3611945
AN  - WOS:001199449102050
AD  - Nanjing Univ Sci & Technol, Nanjing, Peoples R China
AD  - Nanjing Univ, Nanjing, Peoples R China
Y2  - 2024-07-31
ER  -

TY  - JOUR
AU  - Hu, Yufan
AU  - Fu, Jie
AU  - Chen, Mengyuan
AU  - Gao, Junyu
AU  - Dong, Jianfeng
AU  - Fan, Bin
AU  - Liu, Hongmin
TI  - Learning Proposal-Aware Re-Ranking for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Weakly-supervised temporal action localization (WTAL) aims to localize and classify action instances in untrimmed videos with only video-level labels available. Despite the remarkable success of existing methods, whose generated proposals are commonly far more than the ground-truth action instances, it still makes sense to improve the ranking accuracy of the generated proposals since users in real-world scenarios usually prioritize the action proposals with the highest confidence scores. The inaccuracy of the proposal ranking mainly comes from two aspects: For one thing, the traditional proposal generation manner entirely relies on snippet-level perception, resulting in a significant yet unnoticed gap with the target of proposal-level localization. For another, existing methods commonly employ a hand-crafted proposal generation manner, a post-process that does not participate in model optimization. To address the above issues, we propose an end-to-end trained two-stage method, termed as Learning Proposal-aware Re-ranking (LPR) for WTAL. In the first stage, we design a proposal-aware feature learning module to inject the proposal-aware contextual information into each snippet, and then the enhanced features are utilized for predicting initial proposals. Furthermore, to perform effective and efficient proposal re-ranking, in the second stage, we contrast the proposals attached with high confidence scores with our constructed multi-scale foreground/background prototypes for further optimization. Evaluated by both the vanilla and Top- $k$ mAP metrics, results of extensive experiments on two popular benchmarks demonstrate the effectiveness of our proposed method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 JAN
PY  - 2024
VL  - 34
IS  - 1
SP  - 207
EP  - 220
DO  - 10.1109/TCSVT.2023.3283430
AN  - WOS:001138814400041
AD  - Univ Sci & Technol Beijing, Key Lab Intelligent Bion Unmanned Syst, Minist Educ, Sch Intelligence Sci & Technol, Beijing 100083, Peoples R China
AD  - Univ Sci & Technol Beijing, Inst Artificial Intelligence, Beijing 100083, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China
AD  - Zhejiang Gongshang Univ, Coll Comp & Informat Engn, Hangzhou 310018, Peoples R China
Y2  - 2024-02-08
ER  -

TY  - JOUR
AU  - Tan, Jing
AU  - Zhao, Xiaotong
AU  - Shi, Xintian
AU  - Kang, Bing
AU  - Wang, Limin
TI  - PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric.&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2210.11035
AN  - PPRN:22129175
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Tencent, Platform & Content Grp PCG, Shenzhen, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Nanjing Univ
M2  - Tencent
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Hu, Xin
AU  - Li, Kai
AU  - Patel, Deep
AU  - Kruus, Erik
AU  - Min, Martin Renqiang
AU  - Ding, Zhengming
A1  - IEEE
TI  - Weakly-Supervised Temporal Action Localization with Multi-Modal Plateau Transformers
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to jointly localize and classify action segments in untrimmed videos with only video-level annotations. To leverage video-level annotations, most existing methods adopt the multiple instance learning paradigm where frame-/snippet-level action predictions are first produced and then aggregated to form a video-level prediction. Although there are trials to improve snippet-level predictions by modeling temporal relationships, we argue that those implementations have not sufficiently exploited such information. In this paper, we propose Multi-Modal Plateau Transformers (M2PT) for WS-TAL by simultaneously exploiting temporal relationships among snippets, complementary information across data modalities, and temporal coherence among consecutive snippets. Specifically, M2PT explores a dual-Transformer architecture for RGB and optical flow modalities, which models intra-modality temporal relationship with a self-attention mechanism and inter-modality temporal relationship with a cross-attention mechanism. To capture the temporal coherence that consecutive snippets are supposed to be assigned with the same action, M2PT deploys a Plateau model to refine the temporal localization of action segments. Experimental results on popular benchmarks demonstrate that our proposed M2PT achieves state-of-the-art performance.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 979-8-3503-6547-4
DA  - 2024 
PY  - 2024
SP  - 2704
EP  - 2713
DO  - 10.1109/CVPRW63382.2024.00276
AN  - WOS:001327781702087
AD  - Tulane Univ, New Orleans, LA 70118 USA
AD  - NEC Labs Amer, Princeton, NJ USA
Y2  - 2025-03-05
ER  -

TY  - JOUR
AU  - Li, Bairong
AU  - Zhu, Yuesheng
AU  - Liu, Ruixin
AU  - Weng, Zhenyu
TI  - Learning frame-level affinity with video-level labels for weakly supervised temporal action detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Weakly supervised temporal action detection aims at localizing actions with only video-level labels rather than lots of frame-level labels. To this end, previous methods train a classification network for mining discernible action frames as detection results. However, the classification network is known to only concentrate on local discernible frames rather than the entire action instance. Therefore, substantial numbers of indiscernible action frames are not detected and the detection results are incomplete. To alle-viate this issue, we propose a novel method to facilitate the detection of indiscernible frames based on learning frame-level affinities. In the proposed method, we design a network (named Affinity Network) for predicting affinities between pairs of adjacent frames. Then, the affinities are used as tran-sition probabilities to propagate local responses to indiscernible frames. As a result, the responses of indiscernible frames can be enhanced and the detection of them can be facilitated. For learning the net-work, we propose strategies to synthesize frame-pair and video-pair training samples, which are con-ducive to learn frame-level affinities with only video-level labels. The experimental results on THUMOS14 dataset and ActivityNet1.2 dataset show that the detection performance of our framework outperforms most previous weakly supervised action detection methods, and is even as competitive as some fully supervised action detection methods. (c) 2021 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2021 NOV 6
PY  - 2021
VL  - 463
SP  - 109
EP  - 121
DO  - 10.1016/j.neucom.2021.07.059
AN  - WOS:000698041600009
C6  - AUG 2021
AD  - Peking Univ, Shenzhen Grad Sch, Shenzhen, Peoples R China
Y2  - 2021-10-02
ER  -

TY  - JOUR
AU  - Chen, Yunze
AU  - Chen, Mengjuan
AU  - Gu, Qingyi
TI  - Class-wise boundary regression by uncertainty in temporal action detection
T2  - IET IMAGE PROCESSING
M3  - Article
AB  - Temporal action detection is a crucial aspect of video understanding. It aims to classify the action as well as locate the start and end boundaries of the action in the untrimmed videos. As deep learning is frequently utilized, the accuracy of annotation is crucial to boundary localization. However, it is observed that some annotation instances are ambiguous and the ambiguity varies between categories. To solve the problem above, a Gaussian model is built to estimate the boundary uncertainty for each instance. Based on instance uncertainty, category uncertainty is applied to describe the uncertainty of each category. By combining instance and category uncertainty, the boundaries of the selected proposals are refined and the ranking of candidate proposals is adjusted. Furthermore, overcorrection is avoided for categories with a high level of uncertainty. With the uncertainty approach, state-of-the-art performance is achieved: 57.5% on THUMOS14 (mAP@0.5) and 35.4% on ActivityNet (mAP@Avg).
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 1751-9659
SN  - 1751-9667
DA  - 2022 DEC
PY  - 2022
VL  - 16
IS  - 14
SP  - 3854
EP  - 3862
DO  - 10.1049/ipr2.12599
AN  - WOS:000836034700001
C6  - AUG 2022
AD  - Chinese Acad Sci, Ctr Precis Sensing & Control, Inst Automat, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China
Y2  - 2022-08-13
ER  -

TY  - CPAPER
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Li, Hongsheng
A1  - IEEE COMP SOC
TI  - Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Weakly supervised temporal action localization aims to localize temporal boundaries of actions and simultaneously identify their categories with only video-level category labels. Many existing methods seek to generate pseudo labels for bridging the discrepancy between classification and localization, but usually only make use of limited contextual information for pseudo label generation. To alleviate this problem, we propose a representative snippet summarization and propagation framework. Our method seeks to mine the representative snippets in each video for propagating information between video snippets to generate better pseudo labels. For each video, its own representative snippets and the representative snippets from a memory bank are propagated to update the input features in an introand inter-video manner. The pseudo labels are generated from the temporal class activation maps of the updated features to rectify the predictions of the main branch. Our method obtains superior performance in comparison to the existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in terms of average mAP on THUMOS14. Our code is available at https://github.com/LeonHLJ/RSKP.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 3262
EP  - 3271
DO  - 10.1109/CVPR52688.2022.00327
AN  - WOS:000867754203050
AD  - Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
M2  - Ctr Perceptual & Interact Intelligence
Y2  - 2022-12-17
ER  -

TY  - CPAPER
AU  - Zhang, Chen-Lin
AU  - Wu, Jianxin
AU  - Li, Yin
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - ActionFormer: Localizing Moments of Actions with Transformers
T2  - COMPUTER VISION - ECCV 2022, PT IV
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer-a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU = 0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at https://github.com/happyharrycn/actionformer.release.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-19771-0
SN  - 978-3-031-19772-7
DA  - 2022 
PY  - 2022
VL  - 13664
SP  - 492
EP  - 510
DO  - 10.1007/978-3-031-19772-7_29
AN  - WOS:000898297000029
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - 4Paradigm Inc, Beijing, Peoples R China
AD  - Univ Wisconsin Madison, Madison, WI USA
M2  - 4Paradigm Inc
Y2  - 2023-01-25
ER  -

TY  - CPAPER
AU  - Pan, Junting
AU  - Chen, Siyu
AU  - Shou, Mike Zheng
AU  - Liu, Yu
AU  - Shao, Jing
AU  - Li, Hongsheng
A1  - IEEE COMP SOC
TI  - Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor-Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71 mAP). The code is available online.(1)
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 464
EP  - 474
DO  - 10.1109/CVPR46437.2021.00053
AN  - WOS:000739917300044
AD  - Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China
AD  - Columbia Univ, New York, NY 10027 USA
AD  - Xidian Univ, Sch CST, Xian, Peoples R China
AD  - SenseTime Res, Hong Kong, Peoples R China
M2  - SenseTime Res
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Hu, Junshan
AU  - Guo, Chaoxu
AU  - Zhang, Liansheng
AU  - Wang, Biao
AU  - Ge, Tiezheng
AU  - Jiang, Yuning
AU  - Li, Houqiang
ED  - ACM
TI  - Estimation of Reliable Proposal Quality for Temporal Action Detection
T2  - PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022
M3  - Proceedings Paper
CP  - 30th ACM International Conference on Multimedia (MM)
CL  - Lisboa, PORTUGAL
AB  - Temporal action detection (TAD) aims to locate and recognize the actions in an untrimmed video. Anchor-free methods have made remarkable progress which mainly formulate TAD into two tasks: classification and localization using two separate branches. This paper reveals the temporal misalignment between the two tasks hindering further progress. To address this, we propose a new method that gives insights into moment and region perspectives simultaneously to a he two tasks by acquiring reliable proposal quality. For the moat perspective. Boundary Evaluate Module (BEM) is designed which focuses on local appearance and motion evolvement to estimate boundary quality and adopts a multi-scale manner to deal with varied action durations. For the region perspective, we introduce Region Evaluate Module (REM) which uses a new and efficient sampling method for proposal feature representation containing more contextual information compared with point feature to refine category score and proposal boundary. The proposed Boundary Evaluate Module and Region Evaluate Module (BREM) are generic, and they can be easily integrated with other anchor-free TAD methods to achieve superior performance. In our experiments, BREM is combined with two different frameworks and improves the performance on THUMOS14 by 3.6% and 1.0% respectively, reaching a new state-of-the-art (63.6% average mAP). Meanwhile, a competitive result of 36.2% average mAP is achieved on ActivityNet-1.3 with the consistent improvement of BREM. The codes are released at https://github.com/Junshan233/BREM.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9203-7
DA  - 2022 
PY  - 2022
SP  - 6685
EP  - 6695
DO  - 10.1145/3503161.3548029
AN  - WOS:001150372706072
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - BOOK
AU  - Vo, Khoa
Z2  -  
TI  - Towards Comprehensive and Interpretable Video Understanding
M3  - Dissertation/Thesis
SN  - 9798302874283
DA  - 2024 
PY  - 2024
AN  - PQDT:121296856
AD  - University of Arkansas, Computer Science, Arkansas, United States
M2  - University of Arkansas
ER  -

TY  - CPAPER
AU  - Tang, Xiaojun
AU  - Fan, Junsong
AU  - Luo, Chuanchen
AU  - Zhang, Zhaoxiang
AU  - Zhang, Man
AU  - Yang, Zongyuan
A1  - IEEE
TI  - DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION, ICCV
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them omit that ambiguous snippets deliver contradictory information, which would reduce the discriminability of linked snippets. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at https://github. com/XiaojunTang22/ICCV2023-DDGNet.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 6599
EP  - 6609
AN  - WOS:001159644306082
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
AD  - HKISI CAS, Ctr Artificial Intelligence & Robot, Hongkong, Peoples R China
AD  - UCAS, Beijing, Peoples R China
M2  - HKISI CAS
Y2  - 2024-04-14
ER  -

TY  - JOUR
AU  - Xia, Hui-fen
AU  - Zhan, Yong-zhao
TI  - Deep cascaded action attention network for weakly-supervised temporal action localization
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Weakly-supervised temporal action localization (W-TAL) is to locate the boundaries of action instances and classify them in an untrimmed video, which is a challenging task due to only video-level labels during training. Existing methods mainly focus on the most discriminative action snippets of a video by using top-k multiple instance learning (MIL), and ignore the usage of less discriminative action snippets and non-action snippets. This makes the localization performance improve limitedly. In order to mine the less discriminative action snippets and distinguish the non-action snippets better in a video, a novel method based on deep cascaded action attention network is proposed. In this method, the deep cascaded action attention mechanism is presented to model not only the most discriminative action snippets, but also different levels of less discriminative action snippets by introducing threshold erasing, which ensures the completeness of action instances. Besides, the entropy loss for non-action is introduced to restrict the activations of non-action snippets for all action categories, which are generated by aggregating the bottom-k activation scores along the temporal dimension. Thereby, the action snippets can be distinguished from non-action snippets better, which is beneficial to the separation of action and non-action snippets and enables the action instances more accurate. Ultimately, our method can facilitate more precise action localization. Extensive experiments conducted on THUMOS14 and ActivityNet1.3 datasets show that our method outperforms state-of-the-art methods at several t-IoU thresholds.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2023 AUG
PY  - 2023
VL  - 82
IS  - 19
SP  - 29769
EP  - 29787
DO  - 10.1007/s11042-023-14670-0
AN  - WOS:000950119600004
C6  - MAR 2023
AD  - Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China
AD  - Changzhou Vocat Inst Mechatron Technol, Changzhou 213164, Jiangsu, Peoples R China
AD  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept &, Zhenjiang 212013, Jiangsu, Peoples R China
M2  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept &
Y2  - 2023-04-03
ER  -

TY  - JOUR
AU  - Bi, Mingwen
AU  - Li, Jiaqi
AU  - Liu, Xinliang
AU  - Zhang, Qingchuan
AU  - Yang, Zhenghong
TI  - Action-Aware Network with Upper and Lower Limit Loss for Weakly-Supervised Temporal Action Localization
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Weakly-supervised temporal action localization aims to detect the temporal boundaries of action instances in untrimmed videos only by relying on video-level action labels. The main challenge of the research is to accurately segment the action from the background in the absence of frame-level labels. Previous methods consider the action-related context in the background as the main factor restricting the segmentation performance. Most of them take action labels as pseudo-labels for context and suppress context frames in class activation sequences using the attention mechanism. However, this only applies to fixed shots or videos with a single theme. For videos with frequent scene switching and complicated themes, such as casual shots of unexpected events and secret shots, the strong randomness and weak continuity of the action cause the assumption not to be valid. In addition, the wrong pseudo-labels will enhance the weight of context frames, which will affect the segmentation performance. To address above issues, in this paper, we define a new video frame division standard (action instance, action-related context, no-action background), propose an Action-aware Network with Upper and Lower loss AUL-Net, which limits the activation of context to a reasonable range through a two-branch weight-sharing framework with a three-branch attention mechanism, so that the model has wider applicability while accurately suppressing context and background. We conducted extensive experiments on the self-built food safety video dataset FS-VA, and the results show that our method outperforms the state-of-the-art model.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2023 AUG
PY  - 2023
VL  - 55
IS  - 4
SP  - 4307
EP  - 4324
DO  - 10.1007/s11063-022-11042-x
AN  - WOS:000860229400002
C6  - SEP 2022
AD  - China Agr Univ, Coll Informat & Elect Engn, Beijing, Peoples R China
AD  - Minist Agr & Rural Affairs, Key Lab Agr Informatizat Standardizat, Beijing, Peoples R China
AD  - Beijing Technol & Business Univ, Natl Engn Res Ctr Agriprod Qual Traceabil, Beijing, Peoples R China
Y2  - 2022-10-08
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Peng, Houwen
AU  - Zhang, Dingwen
AU  - Fu, Jianlong
AU  - Han, Junwei
TI  - Revisiting Anchor Mechanisms for Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2020 
PY  - 2020
VL  - 29
SP  - 8535
EP  - 8548
DO  - 10.1109/TIP.2020.3016486
AN  - WOS:000564245800001
AD  - Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China
AD  - Microsoft Res Asia, Beijing 100080, Peoples R China
AD  - Xidian Univ, Sch Mechanoelect Engn, Xian 710071, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Ye, Yuancheng
AU  - Yang, Xiaodong
AU  - Tian, YingLi
TI  - Discovering spatio-temporal action tubes
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
M3  - Article
AB  - In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101. (C) 2018 Elsevier Inc. All rights reserved.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1047-3203
SN  - 1095-9076
DA  - 2019 JAN
PY  - 2019
VL  - 58
SP  - 515
EP  - 524
DO  - 10.1016/j.jvcir.2018.12.019
AN  - WOS:000457668100050
AD  - CUNY, City Coll, New York, NY 10021 USA
AD  - CUNY, Grad Ctr, New York, NY 10016 USA
AD  - NVIDIA Res, Santa Clara, CA USA
M2  - NVIDIA Res
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Shi, Dingfeng
AU  - Zhong, Yujie
AU  - Cao, Qiong
AU  - Ma, Lin
AU  - Li, Jia
AU  - Tao, Dacheng
TI  - TriDet: Temporal Action Detection with Relative Boundary Modeling
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer to mitigate the rank loss problem of self-attention that takes place in the video features and aggregate information across different temporal granularities. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency. 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.07347
AN  - PPRN:46483684
AD  - Beihang Univ, VRLab, Beijing, Peoples R China
AD  - Meituan Inc, Beijing, Peoples R China
AD  - JD Explore Acad, Beijing, Peoples R China
M2  - Meituan Inc
M2  - JD Explore Acad
Y2  - 2023-11-09
ER  -

TY  - JOUR
AU  - Liu, Shuming
AU  - Sui, Lin
AU  - Zhang, Chen-Lin
AU  - Mu, Fangzhou
AU  - Zhao, Chen
AU  - Ghanem, Bernard
TI  - Harnessing Temporal Causality for Advanced Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - As a fundamental task in long-form video understanding, temporal action detection (TAD) aims to capture inherent temporal relations in untrimmed videos and identify candidate actions with precise boundaries. Over the years, various networks, including convolutions, graphs, and transformers, have been explored for effective temporal modeling for TAD. However, these modules typically treat past and future information equally, overlooking the crucial fact that changes in action boundaries are essentially causal events. Inspired by this insight, we propose leveraging the temporal causality of actions to enhance TAD representation by restricting the model's access to only past or future context. We introduce CausalTAD, which combines causal attention and causal Mamba to achieve state-of-the-art performance on multiple benchmarks. Notably, with CausalTAD, we ranked 1st in the Action Recognition, Action Detection, and Audio-Based Interaction Detection tracks at the EPIC-Kitchens Challenge 2024, as well as 1st in the Moment Queries track at the Ego4D Challenge 2024. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.17792
AN  - PPRN:91101782
AD  - KAUST, Thuwal, Saudi Arabia
AD  - 4Paradigm Inc, Beijing, Peoples R China
AD  - Moonshot AI, New York City, NY, USA
AD  - NVIDIA, Santa Clara, CA, USA
M2  - 4Paradigm Inc
M2  - Moonshot AI
M2  - NVIDIA
Y2  - 2024-08-02
ER  -

TY  - CPAPER
AU  - Koh, Thean Chun
AU  - Yeo, Chai Kiat
AU  - Jing, Xuan
A1  - IEEE
TI  - Online Spatio-temporal Action Detection for Eldercare
T2  - 2023 IEEE CONFERENCE ON ARTIFICIAL INTELLIGENCE, CAI
M3  - Proceedings Paper
CP  - IEEE Conference on Artificial Intelligence (IEEE CAI)
CL  - Santa Clara, CA
AB  - Using AI technologies for assisted living of the elderly greatly facilitates care provision by caregivers and healthcare professionals. This paper proposes a lightweight model for real-time detection of human actions focusing on the elderly using a conventional RGB camera and an AI edge device. Our model analyzes and predicts the actions as the video frames arrive live from the camera and utilize spatio-temporal action detection to detect and locate the human actions with minimum latency. It can also apply to scenarios involving multiple people. We evaluate the proposed method using the popular public action detection dataset, AVA as well as an in-house self-collected dataset. The results show that our model can accurately detect the various actions in real-time at 15.2 fps using a resource-constrained edge device, offering significant potential for applications in various smart monitoring systems.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-3984-0
DA  - 2023 
PY  - 2023
SP  - 126
EP  - 127
DO  - 10.1109/CAI54212.2023.00061
AN  - WOS:001046447800051
AD  - Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore
AD  - NCS Pte Ltd, NEXT Prod & Platform, Singapore, Singapore
M2  - NCS Pte Ltd
Y2  - 2023-09-06
ER  -

TY  - JOUR
AU  - Zhang, Pengfei
AU  - Cao, Yu
AU  - Liu, Benyuan
TI  - Multi-Stream Single Shot Spatial-Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (1) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (2) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1908.08178
AN  - PPRN:21456588
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Wang, Xiang
AU  - Qing, Zhiwu
AU  - Huang, Ziyuan
AU  - Feng, Yutong
AU  - Zhang, Shiwei
AU  - Jiang, Jianwen
AU  - Tang, Mingqian
AU  - Gao, Changxin
AU  - Sang, Nong
TI  - Proposal Relation Network for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report presents our solution for temporal action detection task in AcitivityNet Challenge 2021. The purpose of this task is to locate and identify actions of interest in long untrimmed videos. The crucial challenge of the task comes from that the temporal duration of action varies dramatically, and the target actions are typically embedded in a background of irrelevant activities. Our solution builds on BMN [10], and mainly contains three steps: 1) action classification and feature encoding by Slowfast [6], CSN [13] and ViViT [1]; 2) proposal generation. We improve BMN by embedding the proposed Proposal Relation Network (PRN), by which we can generate proposals of high quality; 3) action detection. We calculate the detection results by assigning the proposals with corresponding classification results. Finally, we ensemble the results under different settings and achieve 44.7% on the test set, which improves the champion result in ActivityNet 2020 [17] by 1.9% in terms of average mAP.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2106.11812
AN  - PPRN:11863494
AD  - Huazhong Univ Sci & Technol, Intelligent Control Sch Artificial Intelligence & Automation, Key Lab Image Proc, Wuhan, People R China
AD  - Alibaba Grp, Hangzhou, People R China
M2  - Huazhong Univ Sci & Technol
M2  - Alibaba Grp
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Zhao, Chen
AU  - Thabet, Ali
AU  - Ghanem, Bernard
TI  - Video Self-Stitching Graph Network for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy a major proportion in the datasets, but tend to have the lowest performance. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2011.14598
AN  - PPRN:11950273
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
M2  - King Abdullah Univ Sci & Technol KAUST
Y2  - 2024-04-17
ER  -

TY  - JOUR
AU  - Alwassel, Humam
AU  - Caba Heilbron, Fabian
AU  - Ghanem, Bernard
TI  - Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - State-of-the-art temporal action detectors inefficiently search the entire video for specific actions. Despite the encouraging progress these methods achieve, it is crucial to design automated approaches that only explore parts of the video which are the most relevant to the actions being searched for. To address this need, we propose the new problem of action spotting in video, which we define as finding a specific action in a video while observing a small portion of that video. Inspired by the observation that humans are extremely efficient and accurate in spotting and finding action instances in video, we propose Action Search, a novel Recurrent Neural Network approach that mimics the way humans spot actions. Moreover, to address the absence of data recording the behavior of human annotators, we put forward the Human Searches dataset, which compiles the search sequences employed by human annotators spotting actions in the AVA and THUMOS14 datasets. We consider temporal action localization as an application of the action spotting problem. Experiments on the THUMOS14 dataset reveal that our model is not only able to explore the video efficiently (observing on average 17.3% of the video) but it also accurately finds human activities with 30.8% mAP.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1706.04269
AN  - PPRN:22708801
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Su, Haisheng
AU  - Zhao, Xu
AU  - Lin, Tianwei
TI  - Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization, which aims at temporally locating action instances in untrimmed videos using only video-level class labels during training, is an important yet challenging problem in video analysis. Many current methods adopt the "localization by classification" framework: first do video classification, then locate temporal area contributing to the results most. However, this framework fails to locate the entire action instances and gives little consideration to the local context. In this paper, we present a novel architecture called Cascaded Pyramid Mining Network (CPMN) to address these issues using two effective modules. First, to discover the entire temporal interval of specific action, we design a two-stage cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where new and complementary regions are mined through feeding the erased feature maps of discovered regions back to the system. Second, to exploit hierarchical contextual information in videos and reduce missing detections, we design a pyramid module which produces a scale-invariant attention map through combining the feature maps from different levels. Final, we aggregate the results of two modules to perform action localization via locating high score areas in temporal Class Activation Sequence (CAS). Extensive experiments conducted on THUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our method.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1810.11794
AN  - PPRN:22618460
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Zhang, Quan
AU  - Qi, Yuxin
AU  - Tang, Xi
AU  - Yuan, Rui
AU  - Lin, Xi
AU  - Zhang, Ke
AU  - Yuan, Chun
TI  - Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2501.11124
AN  - PPRN:120863587
AD  - Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China
AD  - Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China
AD  - Shanghai Key Lab Integrated Adm Technol Informat Secur, Shanghai, Peoples R China
AD  - Soochow Univ, Sch Comp Sci & Technol, Suzhou, Peoples R China
M2  - Shanghai Jiao Tong Univ
M2  - Shanghai Key Lab Integrated Adm Technol Informat Secur
M2  - Soochow Univ
Y2  - 2025-03-15
ER  -

TY  - JOUR
AU  - Tirupattur, Praveen
AU  - Duarte, Kevin
AU  - Rawat, Yogesh
AU  - Shah, Mubarak
TI  - Modeling Multi-Label Action Dependencies for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Real-world videos contain many complex actions with inherent relationships between action classes. In this work, we propose an attention-based architecture that models these action relationships for the task of temporal action localization in untrimmed videos. As opposed to previous works that leverage video-level co-occurrence of actions, we distinguish the relationships between actions that occur at the same time-step and actions that occur at different time-steps (i.e. those which precede or follow each other). We define these distinct relationships as action dependencies. We propose to improve action localization performance by modeling these action dependencies in a novel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer consists of two branches: a Co-occurrence Dependency Branch and a Temporal Dependency Branch to model co-occurrence action dependencies and temporal action dependencies, respectively. We observe that existing metrics used for multi-label classification do not explicitly measure how well action dependencies are modeled, therefore, we propose novel metrics that consider both co-occurrence and temporal dependencies between action classes. Through empirical evaluation and extensive analysis, we show improved performance over state-of-the-art methods on multi-label action localization benchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.03027
AN  - PPRN:11804539
AD  - Univ Cent Florida, Orlando, FL 32816, USA
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Li, Zhilin
AU  - Wang, Zilei
AU  - Liu, Qinying
ED  - Williams, B
ED  - Chen, Y
ED  - Neville, J
TI  - Actionness Inconsistency-Guided Contrastive Learning for Weakly-Supervised Temporal Action Localization
T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 2
M3  - Proceedings Paper
CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence
CL  - Washington, DC
AB  - Weakly-supervised temporal action localization (WTAL) aims to detect action instances given only video-level labels. To address the challenge, recent methods commonly employ a two-branch framework, consisting of a class-aware branch and a class-agnostic branch. In principle, the two branches are supposed to produce the same actionness activation. However, we observe that there are actually many inconsistent activation regions. These inconsistent regions usually contain some challenging segments whose semantic information (action or background) is ambiguous. In this work, we propose a novel Actionness Inconsistency-guided Contrastive Learning (AICL) method which utilizes the consistent segments to boost the representation learning of the inconsistent segments. Specifically, we first define the consistent and inconsistent segments by comparing the predictions of two branches and then construct positive and negative pairs between consistent segments and inconsistent segments for contrastive learning. In addition, to avoid the trivial case where there is no consistent sample, we introduce an action consistency constraint to control the difference between the two branches. We conduct extensive experiments on THUMOS14, ActivityNet v1.2, and ActivityNet v1.3 datasets, and the results show the effectiveness of AICL with state-of-the-art performance. Our code is available at https://github.com/lizhilin-ustc/AAAI2023-AICL.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - *****************
DA  - 2023 
PY  - 2023
SP  - 1513
EP  - 1521
AN  - WOS:001243761100012
AD  - Univ Sci & Technol China, Hefei, Peoples R China
Y2  - 2024-08-01
ER  -

TY  - JOUR
AU  - Su, Rui
AU  - Ouyang, Wanli
AU  - Zhou, Luping
AU  - Xu, Dong
TI  - Improving Action Localization by Progressive Cross-stream Cooperation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatio-temporal action localization consists of three levels of tasks: spatial localization, action classification, and temporal segmentation. In this work, we propose a new Progressive Cross-stream Cooperation (PCSC) framework to use both region proposals and features from one stream (i.e. Flow/RGB) to help another stream (i.e. RGB/Flow) to iteratively improve action localization results and generate better bounding boxes in an iterative fashion. Specifically, we first generate a larger set of region proposals by combining the latest region proposals from both streams, from which we can readily obtain a larger set of labelled training samples to help learn better action detection models. Second, we also propose a new message passing approach to pass information from one stream to another stream in order to learn better representations, which also leads to better action detection models. As a result, our iterative framework progressively improves action localization results at the frame level. To improve action localization results at the video level, we additionally propose a new strategy to train class-specific actionness detectors for better temporal segmentation, which can be readily learnt by focusing on "confusing" samples from the same action class. Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB demonstrate the effectiveness of our newly proposed approaches for spatio-temporal action localization in realistic scenarios.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1905.11575
AN  - PPRN:22918017
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Meng, Lingwen
AU  - Ban, Guobang
AU  - Xi, Guanghui
AU  - Guo, Siqi
TI  - Pseudo label refining for semi-supervised temporal action localization
T2  - PLOS ONE
M3  - Article
AB  - The training of temporal action localization models relies heavily on a large amount of manually annotated data. Video annotation is more tedious and time-consuming compared with image annotation. Therefore, the semi-supervised method that combines labeled and unlabeled data for joint training has attracted increasing attention from academics and industry. This study proposes a method called pseudo-label refining (PLR) based on the teacher-student framework, which consists of three key components. First, we propose pseudo-label self-refinement which features in a temporal region interesting pooling to improve the boundary accuracy of TAL pseudo label. Second, we design a module named boundary synthesis to further refined temporal interval in pseudo label with multiple inference. Finally, an adaptive weight learning strategy is tailored for progressively learning pseudo labels with different qualities. The method proposed in this study uses ActionFormer and BMN as the detector and achieves significant improvement on the THUMOS14 and ActivityNet v1.3 datasets. The experimental results show that the proposed method significantly improve the localization accuracy compared to other advanced SSTAL methods at a label rate of 10% to 60%. Further ablation experiments show the effectiveness of each module, proving that the PLR method can improve the accuracy of pseudo-labels obtained by teacher model reasoning.
PU  - PUBLIC LIBRARY SCIENCE
PI  - SAN FRANCISCO
PA  - 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN  - 1932-6203
DA  - 2025 FEB 5
PY  - 2025
VL  - 20
IS  - 2
C7  - e0318418
DO  - 10.1371/journal.pone.0318418
AN  - WOS:001416747900036
AD  - Guizhou Power Grid Co Ltd, Elect Power Res Inst, Guiyang, Peoples R China
Y2  - 2025-03-06
ER  -

TY  - JOUR
AU  - Du, Jia-Run
AU  - Feng, Jia-Chang
AU  - Lin, Kun-Yu
AU  - Hong, Fa-Ting
AU  - Wu, Xiao-Ming
AU  - Qi, Zhongang
AU  - Shan, Ying
AU  - Zheng, Wei-Shi
TI  - Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Weakly Supervised Temporal Action Localization (WSTAL) aims to localize and classify action instances in long untrimmed videos with only video-level category labels. Due to the lack of snippet-level supervision for indicating action boundaries, previous methods typically assign pseudo labels for unlabeled snippets. However, since some action instances of different categories are visually similar, it is non-trivial to exactly label the (usually) one action category for a snippet, and incorrect pseudo labels would impair the localization performance. To address this problem, we propose a novel method from a category exclusion perspective, named Progressive Complementary Learning (ProCL), which gradually enhances the snippet-level supervision. Our method is inspired by the fact that video-level labels precisely indicate the categories that all snippets surely do not belong to, which is ignored by previous works. Accordingly, we first exclude these surely non-existent categories by a complementary learning loss. And then, we introduce the background-aware pseudo complementary labeling in order to exclude more categories for snippets of less ambiguity. Furthermore, for the remaining ambiguous snippets, we attempt to reduce the ambiguity by distinguishing foreground actions from the background. Extensive experimental results show that our method achieves new state-of-the-art performance on two popular benchmarks, namely THUMOS14 and ActivityNet1.3.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2206.11011
AN  - PPRN:12186916
AD  - Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China
AD  - ARC Lab, Shenzhen, Peoples R China
AD  - Tencent PCG, Shenzhen, Peoples R China
M2  - ARC Lab
M2  - Tencent PCG
Y2  - 2024-09-23
ER  -

TY  - CPAPER
AU  - Tirupattur, Praveen
AU  - Duarte, Kevin
AU  - Rawat, Yogesh S.
AU  - Shah, Mubarak
A1  - IEEE COMP SOC
TI  - Modeling Multi-Label Action Dependencies for Temporal Action Localization
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Real-world videos contain many complex actions with inherent relationships between action classes. In this work, we propose an attention-based architecture that models these action relationships for the task of temporal action localization in untrimmed videos. As opposed to previous works that leverage video-level co-occurrence of actions, we distinguish the relationships between actions that occur at the same time-step and actions that occur at different time-steps (i.e. those which precede or follow each other). We define these distinct relationships as action dependencies. We propose to improve action localization performance by modeling these action dependencies in a novel attention-based Multi-Label Action Dependency (MLAD) layer. The MLAD layer consists of two branches: a Co-occurrence Dependency Branch and a Temporal Dependency Branch to model co-occurrence action dependencies and temporal action dependencies, respectively. We observe that existing metrics used for multi-label classification do not explicitly measure how well action dependencies are modeled, therefore, we propose novel metrics that consider both co-occurrence and temporal dependencies between action classes. Through empirical evaluation and extensive analysis, we show improved performance over state-of-the-art methods on multi-label action localization benchmarks (MultiTHUMOS and Charades) in terms of fmAP and our proposed metric.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 1460
EP  - 1470
DO  - 10.1109/CVPR46437.2021.00151
AN  - WOS:000739917301065
AD  - Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Zhu, Zixin
AU  - Tang, Wei
AU  - Wang, Le
AU  - Zheng, Nanning
AU  - Hua, Gang
TI  - Enriching Local and Global Contexts for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action classification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three sub-networks: L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two existing models to be the P-Net in our experiments. The efficacy of our proposed method is validated by experimental results on the THUMOS14 (54.3\% at tIoU@0.5) and ActivityNet v1.3 (56.01\% at tIoU@0.5) datasets, which outperforms recent states of the art. Code is available at https://github.com/buxiangzhiren/ContextLoc.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2107.12960
AN  - PPRN:11880022
AD  - Xian Jiaotong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Univ Illinois Chicago, Chicago, IL, USA
AD  - Wormpex AI Res, Bellevue, WA, USA
M2  - Univ Illinois Chicago
M2  - Wormpex AI Res
Y2  - 2023-03-17
ER  -

TY  - CPAPER
AU  - Yang, Le
AU  - Zheng, Ziwei
AU  - Han, Yizeng
AU  - Cheng, Hao
AU  - Song, Shiji
AU  - Huang, Gao
AU  - Li, Fan
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - DyFADet: Dynamic Feature Aggregation for Temporal Action Detection
T2  - COMPUTER VISION-ECCV 2024, PT XLVI
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Recent proposed neural network-based Temporal Action Detection (TAD) models are inherently limited to extracting the discriminative representations and modeling action instances with various lengths from complex scenes by shared-weights detection heads. Inspired by the successes in dynamic neural networks, in this paper, we build a novel dynamic feature aggregation (DFA) module that can simultaneously adapt kernel weights and receptive fields at different timestamps. Based on DFA, the proposed dynamic encoder layer aggregates the temporal features within the action time ranges and guarantees the discriminability of the extracted representations. Moreover, using DFA helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the multi-scale features with adjusted parameters and learned receptive fields better to detect the action instances with diverse ranges from videos. With the proposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves promising performance on a series of challenging TAD benchmarks, including HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment QueriesV1.0, and FineAction. Code is released to https://github.com/yangle15/DyFADet-pytorch.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-72951-5
SN  - 978-3-031-72952-2
DA  - 2025 
PY  - 2025
VL  - 15104
SP  - 305
EP  - 322
DO  - 10.1007/978-3-031-72952-2_18
AN  - WOS:001346379200018
AD  - Xi An Jiao Tong Univ, Xian, Peoples R China
AD  - Alibaba Grp, DAMO Acad, Beijing, Peoples R China
AD  - HKUST GZ, Guangzhou, Peoples R China
AD  - Tsinghua Univ, Beijing, Peoples R China
M2  - HKUST GZ
Y2  - 2024-12-03
ER  -

TY  - JOUR
AU  - Yang, Min
AU  - Chen, Guo
AU  - Zheng, Yin-Dong
AU  - Lu, Tong
AU  - Wang, Limin
TI  - BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD) is extensively studied in the video understanding community by generally following the object detection pipeline in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling, and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low detection efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We extensively investigate the existing techniques in each component for this baseline and, more importantly, perform end-to-end training over the entire pipeline thanks to the simplicity of design. As a result, this simple BasicTAD yields an astounding and real-time RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as PlusTAD). Empirical results demonstrate that our PlusTAD is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Meanwhile, we also perform in-depth visualization and error analysis on our proposed method and try to provide more insights into the TAD problem. Our approach can serve as a strong baseline for future TAD research. The code and model are released at 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2205.02717
AN  - PPRN:58069724
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
Y2  - 2023-04-26
ER  -

TY  - JOUR
AU  - Liu, Shuming
AU  - Xu, Mengmeng
AU  - Zhao, Chen
AU  - Zhao, Xu
AU  - Ghanem, Bernard
TI  - ETAD: A Unified Framework for Efficient Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Untrimmed video understanding such as temporal action detection (TAD) often suffers from the pain of huge demand for computing resources. Because of long video durations and limited GPU memory, most action detectors can only operate on pre-extracted features rather than the original videos, and they still require a lot of computation to achieve high detection performance. To alleviate the heavy computation problem in TAD, in this work, we first propose an efficient action detector with detector proposal sampling, based on the observation that performance saturates at a small number of proposals. This detector is designed with several important techniques, such as LSTM-boosted temporal aggregation and cascaded proposal refinement to achieve high detection quality as well as low computational cost. To enable joint optimization of this action detector and the feature encoder, we also propose encoder gradient sampling, which selectively back-propagates through video snippets and tremendously reduces GPU memory consumption. With the two sampling strategies and the effective detector, we build a unified framework for efficient end-to-end temporal action detection (ETAD), making real-world untrimmed video understanding tractable. ETAD achieves state-of-the-art performance on both THUMOS-14 and ActivityNet-1.3. Interestingly, on ActivityNet-1.3, it reaches 37.78% average mAP, while only requiring 6 mins of training time and 1.23 GB memory based on pre-extracted features. With end-to-end training, it reduces the GPU memory footprint by more than 70% with even higher performance (38.21% average mAP), as compared with traditional end-to-end methods. Code is available at&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2205.07134
AN  - PPRN:12356331
AD  - King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia
AD  - Shanghai Jiao Tong Univ, Shanghai, Peoples R China
M2  - King Abdullah Univ Sci & Technol
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Zeng, Yingsen
AU  - Zhong, Yujie
AU  - Feng, Chengjian
AU  - Ma, Lin
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection
T2  - COMPUTER VISION-ECCV 2024, PT XLVI
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Temporal Action Detection (TAD) focuses on detecting predefined actions, while Moment Retrieval (MR) aims to identify the events described by open-ended natural language within untrimmed videos. Despite that they focus on different events, we observe they have a significant connection. For instance, most descriptions in MR involve multiple actions from TAD. In this paper, we aim to investigate the potential synergy between TAD and MR. Firstly, we propose a unified architecture, termed Unified Moment Detection (UniMD), for both TAD and MR. It transforms the inputs of the two tasks, namely actions for TAD or events for MR, into a common embedding space, and utilizes two novel query-dependent decoders to generate a uniform output of classification score and temporal segments. Secondly, we explore the efficacy of two task fusion learning approaches, pre-training and co-training, in order to enhance the mutual benefits between TAD and MR. Extensive experiments demonstrate that the proposed task fusion learning scheme enables the two tasks to help each other and outperform the separately trained counterparts. Impressively, UniMD achieves state-of-the-art results on three paired datasets Ego4D, Charades-STA, and ActivityNet. Our code is available at https://github.com/yingsen1/UniMD.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-72951-5
SN  - 978-3-031-72952-2
DA  - 2025 
PY  - 2025
VL  - 15104
SP  - 286
EP  - 304
DO  - 10.1007/978-3-031-72952-2_17
AN  - WOS:001346379200017
AD  - Meituan Inc, Beijing, Peoples R China
M2  - Meituan Inc
Y2  - 2024-12-03
ER  -

TY  - CPAPER
AU  - Liu, Shuming
AU  - Zhang, Chen-Lin
AU  - Zhao, Chen
AU  - Ghanem, Bernard
A1  - IEEE
TI  - End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Recently, temporal action detection (TAD) has seen significant performance improvement with end-to-end training. However, due to the memory bottleneck, only models with limited scales and limited data volumes can afford end-to-end training, which inevitably restricts TAD performance. In this paper, we reduce the memory consumption for end-to-end training, and manage to scale up the TAD backbone to 1 billion parameters and the input video to 1,536 frames, leading to significant detection performance. The key to our approach lies in our proposed temporal-informative adapter (TIA), which is a novel lightweight module that reduces training memory. Using TIA, we free the humongous backbone from learning to adapt to the TAD task by only updating the parameters in TIA. TIA also leads to better TAD representation by temporally aggregating context from adjacent frames throughout the backbone. We evaluate our model across four representative datasets. Owing to our efficient design, we are able to train end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the first end-to-end model to outper-form the best feature-based methods. Code is available at https://github.com/sming256/AdaTAD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18591
EP  - 18601
DO  - 10.1109/CVPR52733.2024.01759
AN  - WOS:001342515501088
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
AD  - 4Paradigm Inc, Beijing, Peoples R China
M2  - 4Paradigm Inc
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Nguyen, Trung Thanh
AU  - Kawanishi, Yasutomo
AU  - Komamizu, Takahiro
AU  - Ide, Ichiro
TI  - One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced video analysis approach that expands Closed-vocabulary Temporal Action Detection (Closedvocab TAD) capabilities. Closed-vocab TAD is typically confined to localizing and classifying actions based on a predefined set of categories. In contrast, Open-vocab TAD goes further and is not limited to these predefined categories. This is particularly useful in real-world scenarios where the variety of actions in videos can be vast and not always predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage approach, which involves generating action proposals and then identifying those actions. However, errors made during the first stage can adversely affect the subsequent action identification accuracy. Additionally, existing studies face challenges in handling actions of different durations owing to the use of fixed temporal processing methods. Therefore, we propose a 1-stage approach consisting of two primary modules: Multi-scale Video Analysis (MVA) and Video-Text Alignment (VTA). The MVA module captures actions at varying temporal resolutions, overcoming the challenge of detecting actions with diverse durations. The VTA module leverages the synergy between visual and textual modalities to precisely align video segments with corresponding action labels, a critical step for accurate action identification in Open-vocab scenarios. Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed that the proposed method achieved superior results compared to the other methods in both Open-vocab and Closed-vocab settings. This serves as a strong demonstration of the effectiveness of the proposed method in the TAD task.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.19542
AN  - PPRN:88700358
AD  - Nagoya Univ, Grad Sch Informat, Nagoya, Aichi 4648601, Japan
AD  - RIKEN, Guardian Robot Project, Informat R&D & Strategy Headquarters, Seika, Kyoto 6190288, Japan
AD  - Nagoya Univ, Math & Data Sci Ctr, Nagoya, Aichi 4648601, Japan
M2  - Nagoya Univ
M2  - Nagoya Univ
Y2  - 2024-05-17
ER  -

TY  - CPAPER
AU  - Nguyen, Trung Thanh
AU  - Kawanishi, Yasutomo
AU  - Komamizu, Takahiro
AU  - Ide, Ichiro
A1  - IEEE
TI  - One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features
T2  - 2024 IEEE 18TH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2024
M3  - Proceedings Paper
CP  - 18th International Conference on Automatic Face and Gesture Recognition (FG)
CL  - Istanbul, TURKEY
AB  - Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced video analysis approach that expands Closed-vocabulary Temporal Action Detection (Closedvocab TAD) capabilities. Closed-vocab TAD is typically confined to localizing and classifying actions based on a predefined set of categories. In contrast, Open-vocab TAD goes further and is not limited to these predefined categories. This is particularly useful in real-world scenarios where the variety of actions in videos can be vast and not always predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage approach, which involves generating action proposals and then identifying those actions. However, errors made during the first stage can adversely affect the subsequent action identification accuracy. Additionally, existing studies face challenges in handling actions of different durations owing to the use of fixed temporal processing methods. Therefore, we propose a 1-stage approach consisting of two primary modules: Multi-scale Video Analysis (MVA) and Video-Text Alignment (VTA). The MVA module captures actions at varying temporal resolutions, overcoming the challenge of detecting actions with diverse durations. The VTA module leverages the synergy between visual and textual modalities to precisely align video segments with corresponding action labels, a critical step for accurate action identification in Open-vocab scenarios. Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed that the proposed method achieved superior results compared to the other methods in both Open-vocab and Closed-vocab settings. This serves as a strong demonstration of the effectiveness of the proposed method in the TAD task.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2326-5396
SN  - 979-8-3503-9494-8
SN  - 979-8-3503-9495-5
DA  - 2024 
PY  - 2024
DO  - 10.1109/FG59268.2024.10581896
AN  - WOS:001270976600015
AD  - Nagoya Univ, Grad Sch Informat, Nagoya, Aichi 4648601, Japan
AD  - RIKEN, Guardian Robot Project, Informat R&D & Strategy Headquarters, Seika, Kyoto 6190288, Japan
AD  - Nagoya Univ, Math & Data Sci Ctr, Nagoya, Aichi 4648601, Japan
Y2  - 2024-10-16
ER  -

TY  - CPAPER
AU  - Tan, Jing
AU  - Zhao, Xiaotong
AU  - Shi, Xintian
AU  - Kang, Bin
AU  - Wang, Limin
ED  - Koyejo, S
ED  - Mohamed, S
ED  - Agarwal, A
ED  - Belgrave, D
ED  - Cho, K
ED  - Oh, A
TI  - PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35 (NEURIPS 2022)
M3  - Proceedings Paper
CP  - 36th Conference on Neural Information Processing Systems (NeurIPS)
CL  - ELECTR NETWORK
AB  - Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for finegrained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detectionmAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD.
PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI  - LA JOLLA
PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN  - 1049-5258
SN  - 978-1-7138-7108-8
DA  - 2022 
PY  - 2022
AN  - WOS:001213811606056
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Tencent, PCG, Shenzhen, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Wu, Tao
AU  - Ge, Shuqiu
AU  - Qin, Jie
AU  - Wu, Gangshan
AU  - Wang, Limin
TI  - Open-Vocabulary Spatio-Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatio-temporal action detection (STAD) is an important fine-grained video understanding task. Current methods require box and label supervision for all action classes in advance. However, in real-world applications, it is very likely to come across new action classes not seen in training because the action category space is large and hard to enumerate. Also, the cost of data annotation and model training for new classes is extremely high for traditional methods, as we need to perform detailed box annotations and re-train the whole network from scratch. In this paper, we propose a new challenging setting by performing open-vocabulary STAD to better mimic the situation of action detection in an open world. Open-vocabulary spatio-temporal action detection (OV-STAD) requires training a model on a limited set of base classes with box and label supervision, which is expected to yield good generalization performance on novel action classes. For OV-STAD, we build two benchmarks based on the existing STAD datasets and propose a simple but effective method based on pretrained video-language models (VLM). To better adapt the holistic VLM for the fine-grained action detection task, we carefully fine-tune it on the localized video region-text pairs. This customized fine-tuning endows the VLM with better motion understanding, thus contributing to a more accurate alignment between video regions and texts. Local region feature and global video feature fusion before alignment is adopted to further improve the action detection performance by providing global context. Our method achieves a promising performance on novel classes.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2405.10832
AN  - PPRN:89093456
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Nanjing Univ Aeronaut & Astronaut
M2  - Shanghai AI Lab
Y2  - 2024-06-15
ER  -

TY  - JOUR
AU  - Chen, Siyu
AU  - Pan, Junting
AU  - Song, Guanglu
AU  - Zhang, Manyuan
AU  - Shao, Hao
AU  - Lin, Ziyi
AU  - Shao, Jing
AU  - Li, Hongsheng
AU  - Liu, Yu
TI  - 1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report introduces our winning solution to the spatio-temporal action localization track, AVA-Kinetics Crossover, in ActivityNet Challenge 2020. Our entry is mainly based on Actor-Context-Actor Relation Network. We describe technical details for the new AVA-Kinetics dataset, together with some experimental results. Without any bells and whistles, we achieved 39.62 mAP on the test set of AVA-Kinetics, which outperforms other entries by a large margin. Code will be available at: https://github.com/Siyu-C/ACAR-Net.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2006.09116
AN  - PPRN:22867940
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Su, Rui
AU  - Ouyang, Wanli
AU  - Zhou, Luping
AU  - Xu, Dong
A1  - IEEE Comp Soc
TI  - Improving Action Localization by Progressive Cross-stream Cooperation
T2  - 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)
M3  - Proceedings Paper
CP  - 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Long Beach, CA
AB  - Spatio-temporal action localization consists of three levels of tasks: spatial localization, action classification, and temporal segmentation. In this work, we propose a new Progressive Cross-stream Cooperation (PCSC) framework to use both region proposals and features from one stream (i.e. Flow/RGB) to help another stream (i.e. RGB/Flow) to iteratively improve action localization results and generate better bounding boxes in an iterative fashion. Specifically, we first generate a larger set of region proposals by combining the latest region proposals from both streams, from which we can readily obtain a larger set of labelled training samples to help learn better action detection models. Second, we also propose a new message passing approach to pass information from one stream to another stream in order to learn better representations, which also leads to better action detection models. As a result, our iterative framework progressively improves action localization results at the frame level. To improve action localization results at the video level, we additionally propose a new strategy to train class-specific actionness detectors for better temporal segmentation, which can be readily learnt by focusing on "confusing" samples from the same action class. Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB demonstrate the effectiveness of our newly proposed approaches for spatio-temporal action localization in realistic scenarios.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1063-6919
SN  - 978-1-7281-3293-8
DA  - 2019 
PY  - 2019
SP  - 12008
EP  - 12017
DO  - 10.1109/CVPR.2019.01229
AN  - WOS:000542649305064
AD  - Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW, Australia
AD  - SenseTime Comp Vis Res Grp, Sydney, NSW, Australia
M2  - SenseTime Comp Vis Res Grp
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Luo, Jing
AU  - Yang, Yulin
AU  - Liu, Rongkai
AU  - Chen, Li
AU  - Fei, Hongxiao
AU  - Hu, Chao
AU  - Shi, Ronghua
AU  - Zou, You
TI  - A Tracking-Based Two-Stage Framework for Spatio-Temporal Action Detection
T2  - ELECTRONICS
M3  - Article
AB  - Spatio-temporal action detection (STAD) is a task receiving widespread attention and has numerous application scenarios, such as video surveillance and smart education. Current studies follow a localization-based two-stage detection paradigm, which exploits a person detector for action localization and a feature processing model with a classifier for action classification. However, many issues occur due to the imbalance between task settings and model complexity in STAD. Firstly, the model complexity of heavy offline person detectors adds to the inference overhead. Secondly, the frame-level actor proposals are incompatible with the video-level feature aggregation and Region-of-Interest feature pooling in action classification, which limits the detection performance under diverse action motions and results in low detection accuracy. In this paper, we propose a tracking-based two-stage spatio-temporal action detection framework called TrAD. The key idea of TrAD is to build video-level consistency and reduce model complexity in our STAD framework by generating action track proposals among multiple video frames instead of actor proposals in a single frame. In particular, we utilize tailored tracking to simulate the behavior of human cognitive actions and used the captured motion trajectories as video-level proposals. We then integrate a proposal scaling method and a feature aggregation module into action classification to enhance feature pooling for detected tracks. Evaluations in the AVA dataset demonstrate that TrAD achieves SOTA performance with 29.7 mAP, while also facilitating a 58% reduction in overall computation compared to SlowFast.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2024 FEB
PY  - 2024
VL  - 13
IS  - 3
C7  - 479
DO  - 10.3390/electronics13030479
AN  - WOS:001159995800001
AD  - Cent South Univ, Sch Comp, Changsha 410000, Peoples R China
AD  - Hunan Hanma Technol Co Ltd, Changsha 410083, Peoples R China
AD  - Cent South Univ, Sch Elect Informat, Changsha 410000, Peoples R China
AD  - Cent South Univ, Hunan Res Base Educ Sci Res Educ Informatizat 14 5, Changsha 410083, Peoples R China
AD  - Cent South Univ, Informat & Networking Ctr, Changsha 410083, Peoples R China
M2  - Hunan Hanma Technol Co Ltd
Y2  - 2024-02-19
ER  -

TY  - CPAPER
AU  - Zhang, Can
AU  - Yang, Tianyu
AU  - Weng, Junwu
AU  - Cao, Meng
AU  - Wang, Jue
AU  - Zou, Yuexian
A1  - IEEE COMP SOC
TI  - Unsupervised Pre-training for Temporal Action Localization Tasks
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video classification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrepancy between video-level classification and clip-level localization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as Pseudo Action Localization (PAL) to Unsupervisedly Pre-train feature encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal regions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different temporal positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video representation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivariant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show that PAL can utilize large-scale unlabeled video data to significantly boost the performance of existing TAL methods. Our codes and models will be made publicly available at https://github.com/zhang-can/UP-TAL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 14011
EP  - 14021
DO  - 10.1109/CVPR52688.2022.01364
AN  - WOS:000870759107011
AD  - Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China
AD  - Tencent AI Lab, Bellevue, WA USA
M2  - Tencent AI Lab
Y2  - 2022-12-17
ER  -

TY  - CPAPER
AU  - Xia, Ziying
AU  - Cheng, Jian
AU  - Liu, Siyu
AU  - Hu, Yongxiang
AU  - Wang, Shiguang
AU  - Zhang, Yijie
AU  - Dang, Liwan
A1  - IEEE
TI  - Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Point-level weakly-supervised temporal action localization (P-TAL) aims to localize action instances in untrimmed videos through the use of single-point annotations in each instance. Existing methods predict the class activation sequences without any boundary information, and the unreliable sequences result in a significant misalignment between the quality of proposals and their corresponding confidence. In this paper, we surprisingly observe the most salient frame tend to appear in the central region of the each instance and is easily annotated by humans. Guided by the temporal saliency information, we present a novel proposal-level plug-in framework to relearn the aligned confidence of proposals generated by the base locators. The proposed approach consists of Center Score Learning (CSL) and Alignment-based Boundary Adaptation (ABA). In CSL, we design a novel center label generated by the point annotations for predicting aligned center scores. During inference, we first fuse the center scores with the predicted action probabilities to obtain the aligned confidence. ABA utilizes the both aligned confidence and IoU information to enhance localization completeness. Extensive experiments demonstrate the generalization and effectiveness of the proposed framework, showcasing state-of-the-art or competitive performances across three benchmarks. Our code is available at https://github.com/zyxia1009/CVPR2024-TSPNet.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18440
EP  - 18450
DO  - 10.1109/CVPR52733.2024.01745
AN  - WOS:001342515501074
AD  - Univ Elect Sci & Technol China, Chengdu, Peoples R China
AD  - Second Res Inst Civil Aviat Adm China, Beijing, Peoples R China
M2  - Second Res Inst Civil Aviat Adm China
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Li, Zhilin
AU  - Wang, Zilei
AU  - Liu, Qinying
TI  - Weakly supervised temporal action localization with actionness-guided false positive suppression
T2  - NEURAL NETWORKS
M3  - Article
AB  - Weakly supervised temporal action localization aims to locate the temporal boundaries of action instances in untrimmed videos using video-level labels and assign them the corresponding action category. Generally, it is solved by a pipeline called "localization-by-classification", which finds the action instances by classifying video snippets. However, since this approach optimizes the video-level classification objective, the generated activation sequences often suffer interference from class-related scenes, resulting in a large number of false positives in the prediction results. Many existing works treat background as an independent category, forcing models to learn to distinguish background snippets. However, under weakly supervised conditions, the background information is fuzzy and uncertain, making this method extremely difficult. To alleviate the impact of false positives, we propose a new actionness-guided false positive suppression framework. Our method seeks to suppress false positive backgrounds without introducing the background category. Firstly, we propose a self-training actionness branch to learn class-agnostic actionness, which can minimize the interference of class-related scene information by ignoring the video labels. Secondly, we propose a false positive suppression module to mine false positive snippets and suppress them. Finally, we introduce the foreground enhancement module, which guides the model to learn the foreground with the help of the attention mechanism as well as class-agnostic actionness. We conduct extensive experiments on three benchmarks (THUMOS14, ActivityNet1.2, and ActivityNet1.3). The results demonstrate the effectiveness of our method in suppressing false positives and it achieves the state -of -the -art performance. Code: https://github.com/lizhilin-ustc/AFPS.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0893-6080
SN  - 1879-2782
DA  - 2024 JUL
PY  - 2024
VL  - 175
C7  - 106307
DO  - 10.1016/j.neunet.2024.106307
AN  - WOS:001230269200001
C6  - APR 2024
AD  - Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China
Y2  - 2024-05-31
ER  -

TY  - CPAPER
AU  - Zhang, Can
AU  - Cao, Meng
AU  - Yang, Dongming
AU  - Chen, Jie
AU  - Zou, Yuexian
A1  - IEEE COMP SOC
TI  - CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Weakly-supervised temporal action localization (WS-TAL) aims to localize actions in untrimmed videos with only video-level labels. Most existing models follow the "localization by classification" procedure: locate temporal regions contributing most to the video-level classification. Generally, they process each snippet (or frame) individually and thus overlook the fruitful temporal context relation. Here arises the single snippet cheating issue: "hard" snippets are too vague to be classified. In this paper, we argue that learning by comparing helps identify these hard snippets and we propose to utilize snippet Contrastive learning to Localize Actions, CoLA for short. Specifically, we propose a Snippet Contrast (SniCo) Loss to refine the hard snippet representation in feature space, which guides the network to perceive precise temporal boundaries and avoid the temporal interval interruption. Besides, since it is infeasible to access frame-level annotations, we introduce a Hard Snippet Mining algorithm to locate the potential hard snippets. Substantial analyses verify that this mining strategy efficaciously captures the hard snippets and SniCo Loss leads to more informative feature representation. Extensive experiments show that CoLA achieves state-of-the-art results on THUMOS'14 and ActivityNet v1.2 datasets.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 16005
EP  - 16014
DO  - 10.1109/CVPR46437.2021.01575
AN  - WOS:000742075006023
AD  - Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Li, Guozhang
AU  - Cheng, De
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Wang, Xiaoyu
AU  - Gao, Xinbo
A1  - IEEE
TI  - Boosting Weakly-Supervised Temporal Action Localization with Text Information
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage the text information to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose a Text-Segment Mining (TSM) mechanism, which constructs a text description based on the action class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Completion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 10648
EP  - 10657
DO  - 10.1109/CVPR52729.2023.01026
AN  - WOS:001062522102090
AD  - Xidian Univ, Xian, Peoples R China
AD  - Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China
AD  - Chinese Univ Hong Kong Shenzhen, Shenzhen, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing, Peoples R China
Y2  - 2023-11-15
ER  -

TY  - CPAPER
AU  - Richard, Alexander
AU  - Iqbal, Ahsan
AU  - Gall, Juergen
A1  - IEEE
TI  - Enhancing Temporal Action Localization with Transfer Learning from Action Recognition
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Temporal localization of actions in videos has been of increasing interest in recent years. However, most existing approaches rely on complex architectures that are either expensive to train, inefficient at inference time, or require thorough and careful architecture engineering. Classical action recognition on pre-segmented clips, on the other hand, benefits from sophisticated deep architectures that paved the way for highly reliable video clip classifiers. In this paper, we propose to use transfer learning to leverage the good results from action recognition for temporal localization. We apply a network that is inspired by the classical bag-of-words model for transfer learning and show that the resulting framewise class posteriors already provide good results without explicit temporal modeling. Further, we show that combining these features with a deep but simple convolutional network achieves state of the art results on two challenging action localization datasets.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2473-9936
SN  - 978-1-7281-5023-9
DA  - 2019 
PY  - 2019
SP  - 1533
EP  - 1540
DO  - 10.1109/ICCVW.2019.00191
AN  - WOS:000554591601073
AD  - Univ Bonn, Bonn, Germany
Y2  - 2020-09-02
ER  -

TY  - CPAPER
AU  - Islam, Ashraful
AU  - Long, Chengjiang
AU  - Radke, Richard
A1  - Assoc Advancement Artificial Intelligence
TI  - A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an "action-ness" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THU-MOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
DA  - 2021 
PY  - 2021
VL  - 35
SP  - 1637
EP  - 1645
AN  - WOS:000680423501082
AD  - Rensselaer Polytech Inst, Troy, NY 12181 USA
AD  - JD Digits AI Lab, Pullman, WA USA
M2  - JD Digits AI Lab
Y2  - 2021-09-02
ER  -

TY  - CPAPER
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - Semi-supervised Temporal Action Detection with Proposal-Free Masking
T2  - COMPUTER VISION - ECCV 2022, PT III
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - Existing temporal action detection (TAD) methods rely on a large number of training data with segment-level annotations. Collecting and annotating such a training set is thus highly expensive and unscalable. Semi-supervised TAD (SS-TAD) alleviates this problem by leveraging unlabeled videos freely available at scale. However, SS-TAD is also a much more challenging problem than supervised TAD, and consequently much under-studied. Prior SS-TAD methods directly combine an existing proposal-based TAD method and a SSL method. Due to their sequential localization (e.g., proposal generation) and classification design, they are prone to proposal error propagation. To overcome this limitation, in this work we propose a novel Semi - supervised Temporal action detection model based on PropOsal - free Temporal mask (SPOT) with a parallel localization (mask generation) and classification architecture. Such a novel design effectively eliminates the dependence between localization and classification by cutting off the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for prediction refinement, and a new pretext task for self-supervised model pre-training. Extensive experiments on two standard benchmarks show that our SPOT outperforms state-of-the-art alternatives, often by a large margin. The PyTorch implementation of SPOT is available at https://github.com/sauradip/SPOT
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20061-8
SN  - 978-3-031-20062-5
DA  - 2022 
PY  - 2022
VL  - 13663
SP  - 663
EP  - 680
DO  - 10.1007/978-3-031-20062-5_38
AN  - WOS:000899240500038
AD  - Univ Surrey, CVSSP, Guildford, Surrey, England
AD  - IFlyTek Surrey Joint Res Ctr Artificial Intellige, London, England
AD  - Univ Surrey, Surrey Inst People Ctr Artificial Intelligence, Guildford, Surrey, England
M2  - IFlyTek Surrey Joint Res Ctr Artificial Intellige
Y2  - 2023-01-25
ER  -

TY  - JOUR
AU  - Li, Guozhang
AU  - Cheng, De
AU  - Ding, Xinpeng
AU  - Wang, Nannan
AU  - Wang, Xiaoyu
AU  - Gao, Xinbo
TI  - Boosting Weakly-Supervised Temporal Action Localization with Text Information
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage the text information to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose a Text-Segment Mining (TSM) mechanism, which constructs a text description based on the action class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Completion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2305.00607
AN  - PPRN:66515883
AD  - Xidian Univ, Xian, Peoples R China
AD  - Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China
AD  - Chinese Univ Hong Kong, Shenzhen, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing, Peoples R China
M2  - Xidian Univ
M2  - Hong Kong Univ Sci & Technol
M2  - Chongqing Univ Posts & Telecommun
Y2  - 2023-08-25
ER  -

TY  - JOUR
AU  - Zhang, Huaxin
AU  - Wang, Xiang
AU  - Xu, Xiaohao
AU  - Qing, Zhiwu
AU  - Gao, Changxin
AU  - Sang, Nong
TI  - HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully supervised methods. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2308.12608
AN  - PPRN:83903499
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Wuhan, Peoples R China
AD  - Univ Michigan, Ann Arbor, MI, USA
M2  - Huazhong Univ Sci & Technol
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Liu, Ziyi
AU  - Wang, Le
AU  - Zhang, Qilin
AU  - Tang, Wei
AU  - Yuan, Junsong
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - Assoc Advancement Artificial Intelligence
TI  - ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - The object of Weakly-supervised Temporal Action Localization (WS -TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS -TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS -TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground-Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
DA  - 2021 
PY  - 2021
VL  - 35
SP  - 2233
EP  - 2241
AN  - WOS:000680423502037
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - HERE Technol, Sunnyvale, CA USA
AD  - Univ Illinois, Chicago, IL USA
AD  - SUNY Buffalo, Buffalo, NY USA
AD  - Wormpex AI Res, Bellevue, WA USA
M2  - HERE Technol
M2  - Wormpex AI Res
Y2  - 2021-09-02
ER  -

TY  - JOUR
AU  - Liu, Qinying
AU  - Wang, Zilei
AU  - Rong, Shenghai
AU  - Li, Junjie
AU  - Zhang, Yixin
TI  - Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2312.14138
AN  - PPRN:86787128
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China
M2  - Univ Sci & Technol China
M2  - Hefei Comprehens Natl Sci Ctr
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Liu, Qinying
AU  - Wang, Zilei
AU  - Rong, Shenghai
AU  - Li, Junjie
AU  - Zhang, Yixin
A1  - IEEE
TI  - Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 10399
EP  - 10409
DO  - 10.1109/ICCV51070.2023.00957
AN  - WOS:001169499002078
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China
M2  - Hefei Comprehens Natl Sci Ctr
Y2  - 2024-04-06
ER  -

TY  - JOUR
AU  - Li, Fuwei
TI  - Automated Assessment of Student Mental Health Through Image Processing Technology
T2  - TRAITEMENT DU SIGNAL
M3  - Article
AB  - Students inAvocational education face high levels of academic and employment pressure, significantly impacting their mental health, academic performance, and career development. Traditional mental health assessment methods, relying on questionnaires or interviews, often lag in timeliness and are limited in their ability to reflect real-time changes in students' mental states. Recently, the application of image processing technology in mental health monitoring has gained attention, as it allows for faster, more accurate detection of emotional changes by capturing features like facial expressions and postural behaviors. However, existing approaches often focus on singular emotional features or are limited to static images, failing to leverage the combined potential of dynamic information and subtle facial expressions. This paper proposes a dual-analysis method based on temporal action detection and micro-expression recognition to comprehensively assess students' body language and emotional changes. This approach enables accurate monitoring of mental health status, providing technical support forAa psychological support system tailored to vocational education students.
PU  - INT INFORMATION & ENGINEERING TECHNOLOGY ASSOC
PI  - EDMONTON
PA  - #2020, SCOTIA PLACE TOWER ONE, 10060 JASPER AVE, EDMONTON, AB T5J 3R8, CANADA
SN  - 0765-0019
SN  - 1958-5608
DA  - 2024 OCT
PY  - 2024
VL  - 41
IS  - 5
SP  - 2549
EP  - 2557
DO  - 10.18280/ts.410528
AN  - WOS:001359895400028
AD  - Zibo Vocat Inst, Intelligent Mfg Coll, Zibo 255300, Peoples R China
Y2  - 2024-11-28
ER  -

TY  - JOUR
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Li, Hongsheng
TI  - Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - As a challenging task of high-level video understanding, weakly supervised temporal action localization has been attracting increasing attention. With only video annotations, most existing methods seek to handle this task with a localization-by-classification framework, which generally adopts a selector to select snippets of high probabilities of actions or namely the foreground. Nevertheless, the existing foreground selection strategies have a major limitation of only considering the unilateral relation from foreground to actions, which cannot guarantee the foreground-action consistency. In this paper, we present a framework named FAC-Net based on the I3D backbone, on which three branches are appended, named class-wise foreground classification branch, class-agnostic attention branch and multiple instance learning branch. First, our class-wise foreground classification branch regularizes the relation between actions and foreground to maximize the foreground-background separation. Besides, the class-agnostic attention branch and multiple instance learning branch are adopted to regularize the foreground-action consistency and help to learn a meaningful foreground classifier. Within each branch, we introduce a hybrid attention mechanism, which calculates multiple attention scores for each snippet, to focus on both discriminative and less-discriminative snippets to capture the full action boundaries. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our method. Our code is available at https://github.com/LeonHLJ/FAC-Net.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2108.06524
AN  - PPRN:11898089
AD  - Chinese Univ Hong Kong, Multimedia Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Beijing, Peoples R China
M2  - Chinese Univ Hong Kong
M2  - Ctr Perceptual & Interact Intelligence
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Shi, Dingfeng
AU  - Zhong, Yujie
AU  - Cao, Qiong
AU  - Zhang, Jing
AU  - Ma, Lin
AU  - Li, Jia
AU  - Tao, Dacheng
TI  - ReAct: Temporal Action Detection with Relational Queries
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component.&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.07097
AN  - PPRN:10627343
AD  - Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China
AD  - Meituan Inc, Beijing, Peoples R China
AD  - JD Explore Acad, Beijing, Peoples R China
AD  - Univ Sydney, Sydney, Australia
M2  - Meituan Inc
M2  - JD Explore Acad
M2  - Univ Sydney
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Zhang, Huaxin
AU  - Wang, Xiang
AU  - Xu, Xiaohao
AU  - Qing, Zhiwu
AU  - Gao, Changxin
AU  - Sang, Nong
ED  - Wooldridge, M
ED  - Dy, J
ED  - Natarajan, S
TI  - HR-Pro: Point-Supervised Temporal Action Localization via Hierarchical Reliability Propagation
T2  - THIRTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 7
M3  - Proceedings Paper
CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
CL  - Vancouver, CANADA
AB  - Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HRPro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - *****************
DA  - 2024 
PY  - 2024
SP  - 7115
EP  - 7123
AN  - WOS:001239937300069
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Wuhan, Peoples R China
AD  - Univ Michigan, Ann Arbor, MI 48109 USA
Y2  - 2024-08-15
ER  -

TY  - JOUR
AU  - Lee, Pilhyeon
AU  - Kim, Taeoh
AU  - Shim, Minho
AU  - Wee, Dongyoon
AU  - Byun, Hyeran
TI  - Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection aims to predict the time intervals and the classes of action instances in the video. Despite the promising performance, existing two-stream models exhibit slow inference speed due to their reliance on computationally expensive optical flow. In this paper, we introduce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowledge of the motion modality. Specifically, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform action localization. The dual-branch design and the asymmetric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addition, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the benchmarks verify the effectiveness of the proposed method in enhancing RGB-based action detectors. Notably, our framework is agnostic to backbones and detection heads, bringing consistent gains across different model combinations.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.17285
AN  - PPRN:52057573
AD  - Yonsei Univ, Seoul, South Korea
AD  - AI Tech, Naver Cloud, Seoul, South Korea
M2  - AI Tech
Y2  - 2023-04-08
ER  -

TY  - CPAPER
AU  - Lee, Pilhyeon
AU  - Kim, Taeoh
AU  - Shim, Minho
AU  - Wee, Dongyoon
AU  - Byun, Hyeran
A1  - IEEE
TI  - Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Temporal action detection aims to predict the time intervals and the classes of action instances in the video. Despite the promising performance, existing two-stream models exhibit slow inference speed due to their reliance on computationally expensive optical flow. In this paper, we introduce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowledge of the motion modality. Specifically, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform action localization. The dual-branch design and the asymmetric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addition, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the benchmarks verify the effectiveness of the proposed method in enhancing RGB-based action detectors. Notably, our framework is agnostic to backbones and detection heads, bringing consistent gains across different model combinations.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 2373
EP  - 2383
DO  - 10.1109/CVPR52729.2023.00235
AN  - WOS:001058542602068
AD  - Yonsei Univ, Seoul, South Korea
AD  - Naver Cloud, AI Tech, Seongnam, South Korea
M2  - Naver Cloud
Y2  - 2023-11-08
ER  -

TY  - CPAPER
AU  - Zhu, Zixin
AU  - Wang, Le
AU  - Tang, Wei
AU  - Liu, Ziyi
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - Assoc Advancement Artificial Intelligence
TI  - Learning Disentangled Classification and Localization Representations for Temporal Action Localization
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - A common approach to Temporal Action Localization (TAL) is to generate action proposals and then perform action classification and localization on them. For each proposal, existing methods universally use a shared proposal-level representation for both tasks. However, our analysis indicates that this shared representation focuses on the most discriminative frames for classification, e.g., "take-offs" rather than "runups" in distinguishing "high jump" and "long jump", while frames most relevant to localization, such as the start and end frames of an action, are largely ignored. In other words, such a shared representation can not simultaneously handle both classification and localization tasks well, and it makes precise TAL difficult. To address this challenge, this paper disentangles the shared representation into classification and localization representations. The disentangled classification representation focuses on the most discriminative frames, and the disentangled localization representation focuses on the action phase as well as the action start and end. Our model can be divided into two sub-networks, i.e., the disentanglement network and the context-based aggregation network. The disentanglement network is an autoencoder to learn orthogonal hidden variables of classification and localization. The context-based aggregation network aggregates the classification and localization representations by modeling local and global contexts. We evaluate our proposed method on two popular benchmarks for TAL, which outperforms all state-of-the-art methods.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-876-3
DA  - 2022 
PY  - 2022
SP  - 3644
EP  - 3652
AN  - WOS:000893636203081
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Univ Illinois, Chicago, IL USA
AD  - Wormpex AI Res, Bellevue, WA USA
M2  - Wormpex AI Res
Y2  - 2023-02-17
ER  -

TY  - JOUR
AU  - Liu, Qinying
AU  - Wang, Zilei
AU  - Chen, Ruoxi
AU  - Li, Zhilin
TI  - Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Weakly-supervised temporal action localization (WTAL) intends to detect action instances with only weak supervision, e . g ., video-level labels. The current de facto pipeline locates action instances by thresholding and grouping continuous high-score regions on temporal class activation sequences. In this route, the capacity of the model to recognize the relationships between adjacent snippets is of vital importance which determines the quality of the action boundaries. However, it is error-prone since the variations between adjacent snippets are typically subtle, and unfortunately this is overlooked in the literature. To tackle the issue, we propose a novel WTAL approach named Convex Combination Consistency between Neighbors (C3BN). C3BN consists of two key ingredients: a micro data augmentation strategy that increases the diversity in-between adjacent snippets by convex combination of adjacent snippets, and a macro-micro consistency regularization that enforces the model to be invariant to the transformations w.r.t. video semantics, snippet predictions, and snippet representations. Consequently, fine-grained patterns in-between adjacent snippets are enforced to be explored, thereby resulting in a more robust action boundary localization. Experimental results demonstrate the effectiveness of C3BN3 BN on top of various baselines for WTAL with video-level and point-level supervisions. Code is at C3BN.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2205.00400
AN  - PPRN:86786188
Y2  - 2024-05-21
ER  -

TY  - JOUR
AU  - Cao, Meng
AU  - Zhang, Can
AU  - Chen, Long
AU  - Zheng Shou, Mike
AU  - Zou, Yuexian
TI  - Deep Motion Prior for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-theart performance on three challenging benchmarks, including THUMOS&rsquo;14, ActivityNet v1.2 and v1.3.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2108.05607
AN  - PPRN:12053433
AD  - Peking Univ, Sch Elect andComputer Engn, Shenzhen, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Columbia Univ, Dept Elect Engn, New York City, NY, USA
AD  - Natl Univ Singapore, SG, Queenstown, Singapore
M2  - Peking Univ
M2  - Peng Cheng Lab
M2  - Columbia Univ
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Reka, Aglind
AU  - Borza, Diana Laura
AU  - Reilly, Dominick
AU  - Balazia, Michal
AU  - Bremond, Francois
TI  - Introducing Gating and Context into Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Detection (TAD), the task of localizing and classifying actions in untrimmed video, remains challenging due to action overlaps and variable action durations. Recent findings suggest that TAD performance is dependent on the structural design of transformers rather than on the self-attention mechanism. Building on this insight, we propose a refined feature extraction process through lightweight, yet effective operations. First, we employ a local branch that employs parallel convolutions with varying window sizes to capture both fine-grained and coarse-grained temporal features. This branch incorporates a gating mechanism to select the most relevant features. Second, we introduce a context branch that uses boundary frames as key-value pairs to analyze their relationship with the central frame through cross-attention. The proposed method captures temporal dependencies and improves contextual understanding. Evaluations of the gating mechanism and context branch on challenging datasets (THUMOS14 and EPIC-KITCHEN 100) show a consistent improvement over the baseline and existing methods.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2409.04205
AN  - PPRN:91790495
AD  - Inria Sophia Antipolis, Valbonne, France
AD  - Univ North Carolina Charlotte, Charlotte, NC, USA
AD  - Babes Bolyai Univ, Cluj Napoca, Romania
M2  - Univ North Carolina Charlotte
M2  - Babes Bolyai Univ
Y2  - 2024-09-23
ER  -

TY  - JOUR
AU  - Yang, Zhenheng
AU  - Gao, Jiyang
AU  - Nevatia, Ram
TI  - Spatio-Temporal Action Detection with Cascade Proposal and Location Anticipation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this work, we address the problem of spatio-temporal action detection in temporally untrimmed videos. It is an important and challenging task as finding accurate human actions in both temporal and spatial space is important for analyzing large-scale video data. To tackle this problem, we propose a cascade proposal and location anticipation (CPLA) model for frame-level action detection. There are several salient points of our model: (1) a cascade region proposal network (casRPN) is adopted for action proposal generation and shows better localization accuracy compared with single region proposal network (RPN); (2) action spatio-temporal consistencies are exploited via a location anticipation network (LAN) and thus frame-level action detection is not conducted independently. Frame-level detections are then linked by solving an linking score maximization problem, and temporally trimmed into spatio-temporal action tubes. We demonstrate the effectiveness of our model on the challenging UCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1708.00042
AN  - PPRN:12702753
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Kim, Jihwan
AU  - Lee, Miso
AU  - Cho, Cheol-Ho
AU  - Lee, Jihyun
AU  - Heo, Jae-Pil
TI  - Prediction-Feedback DETR for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal Action Detection (TAD) is fundamental yet challenging for real-world video applications. Leveraging the unique benefits of transformers, various DETR-based approaches have been adopted in TAD. However, it has recently been identified that the attention collapse in self-attention causes the performance degradation of DETR for TAD. Building upon previous research, this paper newly addresses the attention collapse problem in cross-attention within DETR-based TAD methods. Moreover, our findings reveal that cross-attention exhibits patterns distinct from predictions, indicating a short-cut phenomenon. To resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR), which utilizes predictions to restore the collapse and align the cross- and self-attention with predictions. Specifically, we devise novel prediction-feedback objectives using guidance from the relations of the predictions. As a result, Pred-DETR significantly alleviates the collapse and achieves state-of-the-art performance among DETR-based methods on various challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and FineAction.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.16729
AN  - PPRN:91803627
AD  - Sungkyunkwan Univ, Seoul, South Korea
Y2  - 2025-01-27
ER  -

TY  - JOUR
AU  - Dang, Duc Manh Nguyen
AU  - Duong, Viet Hang
AU  - Wang, Jia Ching
AU  - Duc, Nhan Bui
TI  - YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Our new framework, YOWOv3, is an enhanced version of YOWOv2 that we provide in this study with a focus on Spatio Temporal Action Detection task. This framework is made by offering a more accessible approach to experiment deeply with different configurations and to easily customize different model components, which minimizes the amount of labor needed to comprehend and alter the source code. YOWOv3 outperforms YOWOv2 on two popular datasets (UCF101-24 and AVAv2.2) for Human Action Detection and Recognition. In particular, the prior model, YOWOv2, with 109.7M parameters and 53.6 GFLOPs, obtains a mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively. On the other hand, our model, YOWOv3, obtains a mAP of 20.31% on AVAv2.2 and 88.33% on UCF101-24, by utilizing just 39.8 GFLOPs and 59.8M parameters. The outcomes show that YOWOv3 achieves equivalent performance with a significant reduction in the number of parameters and GFLOPs.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.02623
AN  - PPRN:91248435
AD  - Univ Informat Technol, Ho Chi Minh City, Vietnam
AD  - Natl Cent Univ, Taoyuan City, Taiwan
M2  - Natl Cent Univ
Y2  - 2024-08-21
ER  -

TY  - CPAPER
AU  - Shi, Dingfeng
AU  - Zhong, Yujie
AU  - Cao, Qiong
AU  - Ma, Lin
AU  - Li, Jia
AU  - Tao, Dacheng
A1  - IEEE
TI  - TriDet: Temporal Action Detection with Relative Boundary Modeling
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer to mitigate the rank loss problem of self-attention that takes place in the video features and aggregate information across different temporal granularities. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency. The code is released to https://github.com/dingfengshi/TriDet.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 18857
EP  - 18866
DO  - 10.1109/CVPR52729.2023.01808
AN  - WOS:001062531303017
AD  - Beihang Univ, VRLab, Beijing, Peoples R China
AD  - Meituan Inc, Beijing, Peoples R China
AD  - JD Explore Acad, Beijing, Peoples R China
M2  - Meituan Inc
M2  - JD Explore Acad
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Zhang, Haiping
AU  - Ma, Conghao
AU  - Yu, Dongjin
AU  - Guan, Liming
AU  - Wang, Dongjing
AU  - Hu, Zepeng
AU  - Liu, Xu
TI  - MTSCANet: Multi temporal resolution temporal semantic context aggregation network
T2  - IET COMPUTER VISION
M3  - Article
AB  - Temporal action localisation is a challenging task, and video context is crucial to localisation actions. Most existing cases that incorporate temporal and semantic contexts into video features suffer from single contextual representation and blurred temporal boundaries. In this study, a multi-temporal resolution pyramid structure model is proposed. Firstly, a temporal-semantic context aggregation module (TSCF) is designed to assign different attention weights to temporal contexts and combine them with multi-level semantics into video features. Secondly, for the problem of large differences in the time span between different actions in the video, a local-global attention module is designed to combine local and global temporal dependencies for each temporal point to obtain a more flexible and robust representation of contextual relations. The redundant representation of the convolution kernel is reduced by modifying the convolution and the arithmetic power is redeployed at a microscopic granularity. To verify the effectiveness of the model, extensive experiments on three challenging datasets are performed. On THUMOS14, the best performance is obtained in IoU@0.3-0.6 with an average mAP of 47.02%. On ActivityNet-1.3, an average mAP of 34.94% was obtained. On HACS, an average mAP of 28.46% was achieved.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 1751-9632
SN  - 1751-9640
DA  - 2023 APR
PY  - 2023
VL  - 17
IS  - 3
SP  - 366
EP  - 378
DO  - 10.1049/cvi2.12163
AN  - WOS:000895160700001
C6  - DEC 2022
AD  - Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Informat Engn, Hangzhou, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou 310000, Peoples R China
Y2  - 2022-12-23
ER  -

TY  - JOUR
AU  - Zheng, Jingye
AU  - Chen, Dihu
AU  - Hu, Haifeng
TI  - Boundary Adjusted Network Based on Cosine Similarity for Temporal Action Proposal Generation
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Detecting temporal actions in long and untrimmed videos is a challenging and important field in computer vision. Generating high-quality proposals is a key step in temporal action detection. A high-quality proposal usually contains two main characteristics. One is the temporal overlaps between proposals and action instances should be as large as possible. The another one is the number of generated proposals should be as few as possible. Inspired by the similarity comparison in face recognition and the similarity of action in same action segment, we design a module to compare the similarity for visual features extracted from visual feature encoder. We find out time points where the similarity of features changes shapely to generate candidate proposals. Then, we train a classifier to evaluate the candidate proposals whether contains or not contains action instances. The experiments suggest that our method outperforms other temporal action proposal generation methods in THUMOS-14 dataset and ActivityNet-v1.3 dataset. In addition, our method still outperforms other methods when using different visual features extracted from different networks.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2021 AUG
PY  - 2021
VL  - 53
IS  - 4
SP  - 2813
EP  - 2828
DO  - 10.1007/s11063-021-10500-2
AN  - WOS:000650807900001
C6  - MAY 2021
AD  - Sun Yat Sen Univ, Guangzhou, Peoples R China
Y2  - 2021-06-04
ER  -

TY  - JOUR
AU  - Zhang, Haiping
AU  - Zhou, Fuxing
AU  - Ma, Conghao
AU  - Wang, Dongjing
AU  - Zhang, Wanjun
TI  - MCMNET: Multi-Scale Context Modeling Network for Temporal Action Detection
T2  - SENSORS
M3  - Article
AB  - Temporal action detection is a very important and challenging task in the field of video understanding, especially for datasets with significant differences in action duration. The temporal relationships between the action instances contained in these datasets are very complex. For such videos, it is necessary to capture information with a richer temporal distribution as much as possible. In this paper, we propose a dual-stream model that can model contextual information at multiple temporal scales. First, the input video is divided into two resolution streams, followed by a Multi-Resolution Context Aggregation module to capture multi-scale temporal information. Additionally, an Information Enhancement module is added after the high-resolution input stream to model both long-range and short-range contexts. Finally, the outputs of the two modules are merged to obtain features with rich temporal information for action localization and classification. We conducted experiments on three datasets to evaluate the proposed approach. On ActivityNet-v1.3, an average mAP (mean Average Precision) of 32.83% was obtained. On Charades, the best performance was obtained, with an average mAP of 27.3%. On TSU (Toyota Smarthome Untrimmed), an average mAP of 33.1% was achieved.
PU  - MDPI
PI  - BASEL
PA  - Gross-peteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
DA  - 2023 SEP
PY  - 2023
VL  - 23
IS  - 17
C7  - 7563
DO  - 10.3390/s23177563
AN  - WOS:001061244800001
AD  - Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Informat Engn, Hangzhou 310005, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou 310018, Peoples R China
Y2  - 2023-09-17
ER  -

TY  - JOUR
AU  - Jiao, Yanyan
AU  - Yang, Wenzhu
AU  - Xing, Wenjie
AU  - Zeng, Shuang
AU  - Geng, Lei
TI  - TAN: a temporal-aware attention network with context-rich representation for boosting proposal generation
T2  - COMPLEX & INTELLIGENT SYSTEMS
M3  - Article
AB  - Temporal action proposal generation in an untrimmed video is very challenging, and comprehensive context exploration is critically important to generate accurate candidates of action instances. This paper proposes a Temporal-aware Attention Network (TAN) that localizes context-rich proposals by enhancing the temporal representations of boundaries and proposals. Firstly, we pinpoint that obtaining precise location information of action instances needs to consider long-distance temporal contexts. To this end, we propose a Global-Aware Attention (GAA) module for boundary-level interaction. Specifically, we introduce two novel gating mechanisms into the top-down interaction structure to incorporate multi-level semantics into video features effectively. Secondly, we design an efficient task-specific Adaptive Temporal Interaction (ATI) module to learn proposal associations. TAN enhances proposal-level contextual representations in a wide range by utilizing multi-scale interaction modules. Extensive experiments on the ActivityNet-1.3 and THUMOS-14 demonstrate the effectiveness of our proposed method, e.g., TAN achieves 73.43% in AR@1000 on THUMOS-14 and 69.01% in AUC on ActivityNet-1.3. Moreover, TAN significantly improves temporal action detection performance when equipped with existing action classification frameworks.
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN  - 2199-4536
SN  - 2198-6053
DA  - 2024 JUN
PY  - 2024
VL  - 10
IS  - 3
SP  - 3691
EP  - 3708
DO  - 10.1007/s40747-024-01343-0
AN  - WOS:001167206000001
C6  - FEB 2024
AD  - Hebei Univ, Sch Cyber Secur & Comp, Baoding 071002, Peoples R China
AD  - Hebei Univ, Machine Vis Engn Res Ctr, Baoding 071002, Peoples R China
Y2  - 2024-03-06
ER  -

TY  - JOUR
AU  - Yang, Rui
AU  - Zhang, Hui
AU  - Qiu, Mulan
AU  - Wang, Min
TI  - MLSTIF: multi-level spatio-temporal and human-object interaction feature fusion network for spatio-temporal action detection
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - Human-object interaction (HOI) information provides cues or constraints for spatio-temporal action detection (STAD). Existing STAD models that incorporate HOI relationships utilize heavyweight 3D backbones to extract temporal information and require matching many region proposals during the HOI feature generation stage, which results in significant computational overhead. To address these limitations, we propose a multi-level spatio-temporal and interaction feature fusion (MLSTIF) network, whose 2D and 3D backbones are implemented via You Only Look Once (YOLO)v7 and 3D-EfficientNetv2, respectively, to extract multi-level spatial and spatio-temporal features. To exploit the spatio-temporal information within videos, a spatial and spatio-temporal feature fusion (SSTFF) module is designed to fuse motion information acquired from multi-level spatio-temporal features into spatial features at corresponding levels, and to more effectively extract moving target-related interaction content from videos, a decoupled deformable HOI transformer (Decoupled Deformable HOITR) module is designed to produce higher-order classification and regression interaction features, which improves the accuracy of the spatio-temporal locations and action categories of moving targets. Compared with that of all similar methods used in the experiments, the MLSTIF improves the accuracy by 1.4-11.3% on the UCF101-24 dataset and by 0.9-11.4% on the J-HMDB dataset. Among similar methods, some models have at least three times the computational cost of MLSTIF. Compared with that of the models with comparable computational costs, such as the YOWO series and YWOM, the accuracy of MLSTIF is improved by 1.4-7.2% on the AVA v2.2 dataset.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2025 JUN
PY  - 2025
VL  - 31
IS  - 3
C7  - 201
DO  - 10.1007/s00530-025-01796-4
AN  - WOS:001471839200001
AD  - Nanjing Normal Univ, Key Lab Virtual Geog Environm, Minist Educ, Nanjing 210023, Peoples R China
AD  - Nanjing Normal Univ, Sch Geog, Nanjing 210023, Peoples R China
AD  - Jiangsu Ctr Collaborat Innovat Geog Informat Resou, Nanjing 210023, Peoples R China
AD  - State Key Lab Cultivat Base Geog Environm Evolut J, Nanjing 210023, Peoples R China
AD  - Nanjing Normal Univ, Sch Comp & Elect Informat, Nanjing 210023, Peoples R China
M2  - State Key Lab Cultivat Base Geog Environm Evolut J
Y2  - 2025-04-26
ER  -

TY  - CPAPER
AU  - Ramaswamy, Akshaya
AU  - Seemakurthy, Karthik
AU  - Gubbi, Jayavardhana
AU  - Purushothaman, Balamuralidhar
A1  - IEEE COMP SOC
TI  - Spatio-temporal action detection and localization using a hierarchical LSTM
T2  - 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW 2020)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Video analysis is gaining importance in the recent past due to its usefulness in a wide variety of applications. The efficiency of a video analytics engine primarily depends on its ability to extract the spatio-temporal features, which has enough discriminative. Inspired by the way the human visual system operates, we propose a hierarchical architecture to capture the spatio-temporal information from a given input video at different time scales. The proposed architecture has a 3D Inception module followed by two layers of modified Convolutional Long Short Term Memory (ConvLSTM) as the fundamental unit. At each level, we consolidate the LSTM cell and hidden states to the next level by using an visual attention-based pooling approach. The proposed network is used for video action detection and localization application that is the foundational element for video analysis. UCF101 and AVA datasets are used to show that the recognition accuracy achieved by the proposed algorithm advances the state-of-the-art in spatio-temporal action detection and localization application.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 978-1-7281-9360-1
DA  - 2020 
PY  - 2020
SP  - 3303
EP  - 3312
DO  - 10.1109/CVPRW50498.2020.00390
AN  - WOS:000788279003043
AD  - TCS Res & Innovat, Embedded Syst & Robot, Bangalore, Karnataka, India
M2  - TCS Res & Innovat
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Li, Guozhang
AU  - Cheng, De
AU  - Wang, Nannan
AU  - Li, Jie
AU  - Gao, Xinbo
TI  - Neighbor-Guided Pseudo-Label Generation and Refinement for Single-Frame Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Due to the sparse single-frame annotations, current Single-Frame Temporal Action Localization (SF-TAL) methods generally employ threshold-based pseudo-label generation strategies. However, these approaches suffer from inefficient data utilization, as only parts of unlabeled frames with confidence scores surpassing a predefined threshold are selected for training. Moreover, the variability of single-frame annotations and unreliable model predictions introduce pseudo-label noise. To address these challenges, we propose two strategies by using the relationship of the video segments with their neighbors': 1) temporal neighbor-guided soft pseudo-label generation (TNPG); and 2) semantic neighbor-guided pseudo-label refinement (SNPR). TNPG utilizes a local-global self-attention mechanism in a transformer encoder to capture temporal neighbor information while focusing on the whole video. Then the generated self-attention map is multiplied by the network predictions to propagate information between labeled and unlabeled frames, and produce soft pseudo-label for all segments. Despite this, label noise persists due to unreliable model predictions. To mitigate this, SNPR refines pseudo-labels based on the assumption that predictions should resemble their semantic nearest neighbor'. Specifically, we search for semantic nearest neighbors of each video segment by cosine similarity in the feature space. Then the refined soft pseudo-labels can be obtained by a weight combination of the original pseudo-label and the semantic nearest neighbors;. Finally, the model can be trained with the refined pseudo-labels, and the performance has been greatly improved. Comprehensive experimental results on different benchmarks show that we achieve state-of-the-art performances on THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2024 
PY  - 2024
VL  - 33
SP  - 2419
EP  - 2430
DO  - 10.1109/TIP.2024.3378477
AN  - WOS:001193988100003
AD  - Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Telecommun Engn, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China
Y2  - 2024-05-09
ER  -

TY  - JOUR
AU  - Zhao, Tao
AU  - Han, Junwei
AU  - Yang, Le
AU  - Zhang, Dingwen
TI  - Equivalent Classification Mapping for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly supervised temporal action localization is a newly emerging yet widely studied topic in recent years. The existing methods can be categorized into two localization-by-classification pipelines, i.e., the pre-classification pipeline and the post-classification pipeline. The pre-classification pipeline first performs classification on each video snippet, and then, aggregates the snippet-level classification scores to obtain the video-level classification score. In contrast, the post-classification pipeline aggregates the snippet-level features first and then predicts the video-level classification score based on the aggregated feature. Although the classifiers in these two pipelines are used in different ways, the role they play is exactly the same-to classify the given features to identify the corresponding action categories. To this end, an ideal classifier can make both pipelines work. This inspires us to simultaneously learn these two pipelines in a unified framework to obtain an effective classifier. Specifically, in the proposed learning framework, we implement two parallel network streams to model the two localization-by-classification pipelines simultaneously and make the two network streams share the same classifier. This achieves the novel Equivalent Classification Mapping (ECM) mechanism. Moreover, we discover that an ideal classifier may possess two characteristics: 1) the frame-level classification scores obtained from the pre-classification stream and the feature aggregation weights in the post-classification stream should be consistent; and 2) the classification results of these two streams should be identical. Based on these two characteristics, we further introduce a weight-transition module and an equivalent training strategy into the proposed learning framework, which assists to thoroughly mine the equivalence mechanism. Comprehensive experiments are conducted on three benchmarks and ECM achieves accurate action localization results.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 MAR 1
PY  - 2023
VL  - 45
IS  - 3
SP  - 3019
EP  - 3031
DO  - 10.1109/TPAMI.2022.3178957
AN  - WOS:000965379500001
AD  - Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China
Y2  - 2023-05-14
ER  -

TY  - JOUR
AU  - Xia, Huifen
AU  - Zhan, Yongzhao
AU  - Liu, Honglin
AU  - Ren, Xiaopeng
TI  - Enhancing action discrimination via category-specific frame clustering for weakly-supervised temporal action localization
T2  - FRONTIERS OF INFORMATION TECHNOLOGY & ELECTRONIC ENGINEERING
M3  - Article
AB  - Temporal action localization (TAL) is a task of detecting the start and end timestamps of action instances and classifying them in an untrimmed video. As the number of action categories per video increases, existing weakly-supervised TAL (W-TAL) methods with only video-level labels cannot provide sufficient supervision. Single-frame supervision has attracted the interest of researchers. Existing paradigms model single-frame annotations from the perspective of video snippet sequences, neglect action discrimination of annotated frames, and do not pay sufficient attention to their correlations in the same category. Considering a category, the annotated frames exhibit distinctive appearance characteristics or clear action patterns. Thus, a novel method to enhance action discrimination via category-specific frame clustering for W-TAL is proposed. Specifically, the K-means clustering algorithm is employed to aggregate the annotated discriminative frames of the same category, which are regarded as exemplars to exhibit the characteristics of the action category. Then, the class activation scores are obtained by calculating the similarities between a frame and exemplars of various categories. Category-specific representation modeling can provide complimentary guidance to snippet sequence modeling in the mainline. As a result, a convex combination fusion mechanism is presented for annotated frames and snippet sequences to enhance the consistency properties of action discrimination, which can generate a robust class activation sequence for precise action classification and localization. Due to the supplementary guidance of action discriminative enhancement for video snippet sequences, our method outperforms existing single-frame annotation based methods. Experiments conducted on three datasets (THUMOS14, GTEA, and BEOID) show that our method achieves high localization performance compared with state-of-the-art methods.
PU  - ZHEJIANG UNIV PRESS
PI  - Hangzhou
PA  - Xixi Campus, Zhejiang University, No. 148 Tianmushan Road, Hangzhou, Zhejiang, PEOPLES R CHINA
SN  - 2095-9184
SN  - 2095-9230
DA  - 2024 JUN
PY  - 2024
VL  - 25
IS  - 6
SP  - 809
EP  - 823
DO  - 10.1631/FITEE.2300024
AN  - WOS:001263329500002
AD  - Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China
AD  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept &, Zhenjiang 212013, Peoples R China
AD  - Changzhou Vocat Inst Mechatron Technol, Changzhou 213164, Peoples R China
M2  - Jiangsu Engn Res Ctr Big Data Ubiquitous Percept &
Y2  - 2024-07-20
ER  -

TY  - JOUR
AU  - Huang, Jing
AU  - Zhao, Peng
AU  - Wang, Guiqin
AU  - Yang, Shusen
AU  - Lin, Jie
TI  - Self-attention-based long temporal sequence modeling method for temporal action detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal Action Detection (TAD) is a basic and complex task in video understanding. It aims at detecting both the localization and category of actions in a video. The anchor-free TAD methods directly predict the action classes at each location and regress the distances to the boundaries. However, current anchor-free models based on convolutional neural network encode spatiotemporal sequences by 3D convolution networks. Due to the limited receptive field and the basic prior of the translation invariance, effective long temporal sequence modeling cannot be achieved. As a result, these methods cannot effectively detect temporal boundaries. To solve this problem, we design a novel end-to-end self-attention temporal enhancement TAD model, which introduces the Temporal Enhancement module to enhance the temporal feature encoding of the videos and expand the receptive field. Extensive experiments demonstrate that the self-attention Temporal Enhancement model yields an effective improvement on previous work, which improves the performance on THUMOS14 by 1.2%, reaching 53.2% on average mAP. Meanwhile, a competitive result of 34.7% average mAP is achieved on ActivityNet-1.3.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2023 OCT 14
PY  - 2023
VL  - 554
C7  - 126617
DO  - 10.1016/j.neucom.2023.126617
AN  - WOS:001054044200001
C6  - AUG 2023
AD  - Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China
AD  - Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Peoples R China
AD  - Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China
AD  - Minist Educ Key Lab Intelligent Networks & Network, Xian 710049, Peoples R China
M2  - Natl Engn Lab Big Data Analyt
M2  - Minist Educ Key Lab Intelligent Networks & Network
Y2  - 2023-09-03
ER  -

TY  - JOUR
AU  - Li, Yong
AU  - Liang, Qiming
AU  - Gan, Bo
AU  - Cui, Xiaolong
TI  - Action Recognition and Detection Based on Deep Learning: A Comprehensive Summary
T2  - CMC-COMPUTERS MATERIALS & CONTINUA
M3  - Review
AB  - Action recognition and detection is an important research topic in computer vision, which can be divided into action recognition and action detection. At present, the distinction between action recognition and action detection is not clear, and the relevant reviews are not comprehensive. Thus, this paper summarized the action recognition and detection methods and datasets based on deep learning to accurately present the research status in this field. Firstly, according to the way that temporal and spatial features are extracted from the model, the commonly used models of action recognition are divided into the two stream models, the temporal models, the spatiotemporal models and the transformer models according to the architecture. And this paper briefly analyzes the characteristics of the four models and introduces the accuracy of various algorithms in common data sets. Then, from the perspective of tasks to be completed, action detection is further divided into temporal action detection and spatiotemporal action detection, and commonly used datasets are introduced. From the perspectives of the two stage method and one-stage method, various algorithms of temporal action detection are reviewed, and the various algorithms of spatiotemporal action detection are summarized in detail. Finally, the relationship between different parts of action recognition and detection is discussed, the difficulties faced by the current research are summarized in detail, and future development was prospected.
PU  - TECH SCIENCE PRESS
PI  - HENDERSON
PA  - 871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA
SN  - 1546-2218
SN  - 1546-2226
DA  - 2023 
PY  - 2023
VL  - 77
IS  - 1
SP  - 1
EP  - 23
DO  - 10.32604/cmc.2023.042494
AN  - WOS:001108210300013
AD  - Engn Univ PAP, Coll Informat Engn, Xian 710086, Peoples R China
AD  - PAP Heilongjiang Prov, Heihe Detachment, Heihe 164300, Peoples R China
AD  - Naval Univ Engn, Natl Key Lab Sci & Technol Electromagnet Energy, Wuhan 430033, Peoples R China
AD  - Engn Univ PAP, Joint Lab Counter Terrorism Command & Informat En, Xian 710086, Peoples R China
M2  - PAP Heilongjiang Prov
Y2  - 2024-01-11
ER  -

TY  - JOUR
AU  - Song, Yeongtaek
AU  - Kim, Incheol
TI  - Spatio-Temporal Action Detection in Untrimmed Videos by Using Multimodal Features and Region Proposals
T2  - SENSORS
M3  - Article
AB  - This paper proposes a novel deep neural network model for solving the spatio-temporal-action-detection problem, by localizing all multiple-action regions and classifying the corresponding actions in an untrimmed video. The proposed model uses a spatio-temporal region proposal method to effectively detect multiple-action regions. First, in the temporal region proposal, anchor boxes were generated by targeting regions expected to potentially contain actions. Unlike the conventional temporal region proposal methods, the proposed method uses a complementary two-stage method to effectively detect the temporal regions of the respective actions occurring asynchronously. In addition, to detect a principal agent performing an action among the people appearing in a video, the spatial region proposal process was used. Further, coarse-level features contain comprehensive information of the whole video and have been frequently used in conventional action-detection studies. However, they cannot provide detailed information of each person performing an action in a video. In order to overcome the limitation of coarse-level features, the proposed model additionally learns fine-level features from the proposed action tubes in the video. Various experiments conducted using the LIRIS-HARL and UCF-10 datasets confirm the high performance and effectiveness of the proposed deep neural network model.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
DA  - 2019 MAR 1
PY  - 2019
VL  - 19
IS  - 5
C7  - 1085
DO  - 10.3390/s19051085
AN  - WOS:000462540400104
AD  - Kyonggi Univ, Dept Comp Sci, 154-42 Gwanggyosan Ro, Suwon 16227, South Korea
Y2  - 2019-04-10
ER  -

TY  - JOUR
AU  - Cui, Di
AU  - Xin, Chang
AU  - Wu, Lifang
AU  - Wang, Xiangdong
TI  - ConvTransformer Attention Network for temporal action detection
T2  - KNOWLEDGE-BASED SYSTEMS
M3  - Article
AB  - Boundary detection is a challenging problem in Temporal Action Detection (TAD). While transformer-based methods achieve satisfactory results by incorporating self-attention mechanisms to model global dependencies for boundary detection, they face two key issues. Firstly, they lack explicit learning of local relationships; this limitation results in imprecise boundary detection when subtle appearance changes occur between adjacent clips. Secondly, transformer-based methods lead to feature convergence across multiple actions due to the self-attention mechanism's tendency to distribute focus across the entire input video, resulting in the prediction of imprecisely overlapping actions. To address these challenges, we introduce the ConvTransformer Attention Network (CTAN), a novel framework comprised of two primary components: (1) The Temporal Attention Block (TAB), a temporal attention mechanism designed to emphasize critical temporal positions enriched with essential action-related features. (2) The ConvTransformer Block (CTB), which employs a hybrid structure for capturing nuanced appearance changes locally and action transitions globally. Facilitated with these components, CTAN is adept at focusing on motion features between overlapping actions, and precisely capturing both local differences between adjacent clips and global action transitions. The extensive experiments on multiple datasets, including THUMOS14, MultiTHUMOS, and ActivityNet, confirm the effectiveness of CTAN.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0950-7051
SN  - 1872-7409
DA  - 2024 SEP 27
PY  - 2024
VL  - 300
C7  - 112264
DO  - 10.1016/j.knosys.2024.112264
AN  - WOS:001280628700001
C6  - JUL 2024
AD  - Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China
AD  - Jimei Univ, Sch Phys Educ, Xiamen 361021, Fujian, Peoples R China
Y2  - 2024-08-05
ER  -

TY  - JOUR
AU  - An, Xin
AU  - Zhao, Peng
AU  - Wang, Guiqin
AU  - Zhao, Cong
AU  - Yang, Shusen
TI  - Transformer feature collapse of Temporal Action Detection via Multi-granularity Semantic Enhancement
T2  - NEUROCOMPUTING
M3  - Article
AB  - Transformer-based models for Temporal Action Detection have achieved significant performance improvements, where the Multi-Head Self-Attention (MHSA) mechanism has played a pivotal role. However, owing to MHSA's tendency to map different patches into similar latent representations, existing methodologies are afflicted with the issue of temporal feature collapse, resulting in high similarity among temporal points and consequently increasing the difficulty in distinguishing action from background. To address the issue, we propose a Multi-granularity Semantic Enhancement (MSE) Block to learn multi-granularity semantic information from different feature spaces. The MSE Block comprises three core components: Local Discriminative Information Modeling (LDM), Global Temporal Information Modeling (GTM), and Adaptive Fusion Module (AFM). LDM facilitates the capture of discriminative information via a multi-scale convolutional group for local detail enhancement, subsequently, GTM employs the MHSA for global temporal context interaction, and AFM adaptively fuses all enhanced features to achieve multi-granularity semantic representation enhancement. Extensive experiments validate the superiority of our method, yielding state-of-the-art performance of 70.5% on THUMOS14, 39.3% on HACS, and 36.9% on ActivityNet-1.3. The code is released at https://github.com/ XinAn9508/MSE_for_TAD.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2025 APR 14
PY  - 2025
VL  - 626
C7  - 129543
DO  - 10.1016/j.neucom.2025.129543
AN  - WOS:001426324500001
C6  - FEB 2025
AD  - Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China
AD  - Xi An Jiao Tong Univ, Sch Math & Stat, Xian 710049, Peoples R China
AD  - Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China
AD  - Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China
M2  - Natl Engn Lab Big Data Analyt
Y2  - 2025-02-26
ER  -

TY  - JOUR
AU  - Sui, Lin
AU  - Mu, Fangzhou
AU  - Li, Yin
TI  - NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to the Ego4D Moment Queries Challenge 2023
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This report describes our submission to the Ego4D Mo-ment Queries Challenge 2023. Our submission extends ActionFormer [12], a latest method for temporal action localization. Our extension combines an improved ground-truth assignment strategy during training and a refined version of SoftNMS at inference time. Our solution is ranked 2nd on the public leaderboard with 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge.  
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2307.02025
AN  - PPRN:73790261
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Univ Wisconsin Madison, Madison, WI, USA
M2  - Univ Wisconsin Madison
Y2  - 2023-07-16
ER  -

TY  - JOUR
AU  - Yang, Wenfei
AU  - Zhang, Tianzhu
AU  - Zhang, Yongdong
AU  - Wu, Feng
TI  - Uncertainty Guided Collaborative Training for Weakly Supervised and Unsupervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - In weakly supervised (WSAL) and unsupervised temporal action localization (UAL), the target is to simultaneously localize temporal boundaries and identify category labels of actions with only video-level category labels (WSAL) or category numbers in a dataset (UAL) during training. Among existing methods, attention based methods have achieved superior performance in both tasks by highlighting action segments with foreground attention weights. However, without the segment-level supervision on the attention weight learning, the quality of the attention weight hinders the performance of these methods. In this paper, we propose a novel Uncertainty Guided Collaborative Training (UGCT) strategy to alleviate this problem, which mainly includes two key designs: (1) The first design is an online pseudo label generation module, in which the RGB and FLOW streams work collaboratively to learn from each other. (2) The second design is an uncertainty aware learning module, which can mitigate the noise in the generated pseudo labels. These two designs work together to promote the model performance effectively and efficiently by exchanging information between RGB and FLOW streams. Extensive experimental results on two benchmark datasets with three attention based methods demonstrate the effectiveness of the proposed method, e.g, more than 7.0% performance gain for mAP@IoU=0.5 on THUMOS14 dataset.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 APR 1
PY  - 2023
VL  - 45
IS  - 4
SP  - 5252
EP  - 5267
DO  - 10.1109/TPAMI.2022.3200399
AN  - WOS:000947840300081
AD  - Univ Sci & Technol China, Sch Informat Sci, Hefei 230027, Peoples R China
Y2  - 2023-03-31
ER  -

TY  - CPAPER
AU  - He, Bo
AU  - Yang, Xitong
AU  - Kang, Le
AU  - Cheng, Zhiyu
AU  - Zhou, Xin
AU  - Shrivastava, Abhinav
A1  - IEEE COMP SOC
TI  - ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Weakly-supervised temporal action localization aims to recognize and localize action segments in untrimmed videos given only video-level action labels for training. Without the boundary information of action segments, existing methods mostly rely on multiple instance learning (MIL), where the predictions of unlabeled instances (i.e., video snippets) are supervised by classifying labeled bags (i.e., untrimmed videos). However, this formulation typically treats snippets in a video as independent instances, ignoring the underlying temporal structures within and across action segments. To address this problem, we propose ASM-Loc, a novel WTAL framework that enables explicit, action-aware segment modeling beyond standard MIL-based methods. Our framework entails three segment-centric components: (i) dynamic segment sampling for compensating the contribution of short actions; (ii) infra- and inter-segment attention for modeling action dynamics and capturing temporal dependencies; (iii) pseudo instance-level supervision for improving action boundary prediction. Furthermore, a multistep refinement strategy is proposed to progressively improve action proposals along the model training process. Extensive experiments on THUMOS-14 and ActivityNet-v1.3 demonstrate the effectiveness of our approach, establishing new state of the art on both datasets.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 13915
EP  - 13925
DO  - 10.1109/CVPR52688.2022.01355
AN  - WOS:000870759107002
AD  - Univ Maryland, College Pk, MD 20742 USA
AD  - Baidu Res, Sunnyvale, CA USA
Y2  - 2022-12-17
ER  -

TY  - JOUR
AU  - Tang, Yue
AU  - Wu, Yawen
AU  - Zhou, Peipei
AU  - Hu, Jingtong
TI  - Enabling Weakly Supervised Temporal Action Localization From On-Device Learning of the Video Stream
T2  - IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
M3  - Article
M3  - Proceedings Paper
CP  - ACM/IEEE Int Conf on Hardware/Software Codesign and Syst Synthesis / Int Conf on Compilers, Architectures, and Synthesis for Embedded Syst / Int Conf on Embedded Software part of the Embedded Syst Week
CL  - ELECTR NETWORK
AB  - Detecting actions in videos have been widely applied in on-device applications, such as cars, robots, etc. Practical on-device videos are always untrimmed with both action and background. It is desirable for a model to both recognize the class of action and localize the temporal position where the action happens. Such a task is called temporal action location (TAL), which is always trained on the cloud where multiple untrimmed videos are collected and labeled. It is desirable for a TAL model to continuously and locally learn from new data, which can directly improve the action detection precision while protecting customers' privacy. However, directly training a TAL model on the device is nontrivial. To train a TAL model which can precisely recognize and localize each action, tremendous video samples with temporal annotations are required. However, annotating videos frame by frame is exorbitantly time consuming and expensive. Although weakly supervised temporal action localization (W-TAL) has been proposed to learn from untrimmed videos with only video-level labels, such an approach is also not suitable for on-device learning scenarios. In practical on-device learning applications, data are collected in streaming. For example, the camera on the device keeps collecting video frames for hours or days, and the actions of nearly all classes are included in a single long video stream. Dividing such a long video stream into multiple video segments requires lots of human effort, which hinders the exploration of applying the TAL tasks to realistic on-device learning applications. To enable W-TAL models to learn from a long, untrimmed streaming video, we propose an efficient video learning approach that can directly adapt to new environments. We first propose a self-adaptive video dividing approach with a contrast score-based segment merging approach to convert the video stream into multiple segments. Then, we explore different sampling strategies on the TAL tasks to request as few labels as possible. To the best of our knowledge, we are the first attempt to directly learn from the on-device, long video stream. Experimental results on the THUMOS'14 dataset show that the performance of our approach is comparable to the current W-TAL state-of-the-art (SOTA) work without any laborious manual video splitting.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0278-0070
SN  - 1937-4151
DA  - 2022 NOV
PY  - 2022
VL  - 41
IS  - 11
SP  - 3910
EP  - 3921
DO  - 10.1109/TCAD.2022.3197536
AN  - WOS:000877295000033
AD  - Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15261 USA
Y2  - 2022-11-24
ER  -

TY  - CPAPER
AU  - Zhu, Zixin
AU  - Tang, Wei
AU  - Wang, Le
AU  - Zheng, Nanning
AU  - Hua, Gang
A1  - IEEE
TI  - Enriching Local and Global Contexts for Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action classification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three subnetworks: L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two existing models to be the P-Net in our experiments. The efficacy of our proposed method is validated by experimental results on the THUMOS14 (54.3% at tIoU@0.5) and ActivityNet v1.3 (56.01% at tIoU@0.5) datasets, which outperforms recent states of the art. Code is available at https://github.com/buxiangzhiren/ContextLoc.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13496
EP  - 13505
DO  - 10.1109/ICCV48922.2021.01326
AN  - WOS:000798743203067
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China
AD  - Univ Illinois, Chicago, IL USA
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
M2  - Wormpex AI Res
Y2  - 2022-06-24
ER  -

TY  - JOUR
AU  - Prakash, Varun Ganjigunte
AU  - Kohli, Manu
AU  - Prathosh, Aragulla Prasad
AU  - Juneja, Monica
AU  - Gupta, Manushree
AU  - Sairam, Smitha
AU  - Sitaraman, Sadasivan
AU  - Bangalore, Anjali Sanjeev
AU  - Kommu, John Vijay Sagar
AU  - Saini, Lokesh
AU  - Utage, Prashant Ramesh
AU  - Goyal, Nishant
TI  - Video-based real-time assessment and diagnosis of autism spectrum disorder using deep neural networks
T2  - EXPERT SYSTEMS
M3  - Article
AB  - Human action recognition (HAR) in untrimmed videos can make insightful predictions of human behaviour. Previous work on HAR-included models trained on spatial and temporal annotations and could classify limited actions from trimmed videos. These methods reported limitations such as (1) performance degradation due to the lack of precision temporal regions proposal and (2) poor adaptability of the models in the clinical domain because of unrelated actions of interest. We propose an innovative method that could analyse untrimmed behavioural videos to recommend actions of interest leading to diagnostic and functional assessments for children with Autism Spectrum Disorder (ASD). Our method entails end-to-end behaviour action recognition (BAR) pipeline, including child detection, temporal action localization, and actions of interest identification and classification. The model trained on the data of 400 ASD children and 125 with other developmental delays (ODD) accurately identified ASD, ODD, and Neurotypical children with 79.7%, 77.2%, and 80.8% accuracy, respectively. The model's performance on an independent benchmark Self-Stimulatory Behaviour Dataset (SSBD) reported top-1 accuracy of 78.57% for combined localization with action recognition, significantly higher than the earlier reported outcomes.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 0266-4720
SN  - 1468-0394
DA  - 2025 JAN
PY  - 2025
VL  - 42
IS  - 1
DO  - 10.1111/exsy.13253
AN  - WOS:000945029100001
C6  - MAR 2023
AD  - CogniAble, Gurgaon, India
AD  - Indian Inst Sci, Dept Elect Commun Engn, Signal Proc Bldg West, Bengaluru, India
AD  - Lok Nayak Hosp, Maulana Azad Med Coll & Associated, Dept Pediat, New Delhi, India
AD  - VMMC & Safdarjung Hosp, Dept Psychiat, OPD Bldg, New Delhi, India
AD  - Lok Nayak Hosp, Ctr Excellence, Early Intervent Ctr, Dept Pediat, New Delhi, India
AD  - SMS Med Coll & Hosp, Sir Padampat Mother & Child Hlth Inst, Neurodev Div, Jaipur, India
AD  - ICON Ctr Child Dev & Assisted Learning, Aurangabad, India
AD  - NIMHANS, Adolescent Psychiat Ctr, Dept Child & Adolescent Psychiat, 2nd Floor, Bengaluru, India
AD  - All India Inst Med Sci, Dept Pediat, Jodhpur, India
AD  - Utage Child Dev Ctr, Hyderabad, India
AD  - Cent Inst Psychiat, Ranchi, India
M2  - CogniAble
M2  - Lok Nayak Hosp
M2  - ICON Ctr Child Dev & Assisted Learning
M2  - Utage Child Dev Ctr
Y2  - 2023-03-07
ER  -

TY  - CPAPER
AU  - Zhao, Chen
AU  - Liu, Shuming
AU  - Mangalam, Karttikeya
AU  - Ghanem, Bernard
A1  - IEEE
TI  - Re<SUP>2</SUP>TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and complex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re(2)TAL, which rewires pretrained video backbones for reversible TAL. Re(2)TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a large variety of reversible networks are easily obtained from existing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible versions. Re(2)TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9% at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods. Code is available at https://github.com/coolbay/Re2TAL.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 10637
EP  - 10647
DO  - 10.1109/CVPR52729.2023.01025
AN  - WOS:001062522102089
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
AD  - Univ Calif Berkeley, Berkeley, CA USA
Y2  - 2023-11-15
ER  -

TY  - JOUR
AU  - Fu, Jie
AU  - Gao, Junyu
AU  - Xu, Changsheng
TI  - Semantic and Temporal Contextual Correlation Learning for Weakly-Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Weakly-supervised temporal action localization (WSTAL) aims to automatically identify and localize action instances in untrimmed videos with only video-level labels as supervision. In this task, there exist two challenges: (1) how to accurately discover the action categories in an untrimmed video (what to discover); (2) how to elaborately focus on the integral temporal interval of each action instance (where to focus). Empirically, to discover the action categories, discriminative semantic information should be extracted, while robust temporal contextual information is beneficial for complete action localization. However, most existing WSTAL methods ignore to explicitly and jointly model the semantic and temporal contextual correlation information for the above two challenges. In this article, a Semantic and Temporal Contextual Correlation Learning Network (STCL-Net) with the semantic (SCL) and temporal contextual correlation learning (TCL) modules is proposed, which achieves both accurate action discovery and complete action localization by modeling the semantic and temporal contextual correlation information for each snippet in the inter- and intra-video manners respectively. It is noteworthy that the two proposed modules are both designed in a unified dynamic correlation-embedding paradigm. Extensive experiments are performed on different benchmarks. On all the benchmarks, our proposed method exhibits superior or comparable performance in comparison to the existing state-of-the-art models, especially achieving gains as high as 7.2% in terms of the average mAP on THUMOS-14. In addition, comprehensive ablation studies also verify the effectiveness and robustness of each component in our model.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2023 OCT
PY  - 2023
VL  - 45
IS  - 10
SP  - 12427
EP  - 12443
DO  - 10.1109/TPAMI.2023.3287208
AN  - WOS:001068816800057
AD  - Zhengzhou Univ, Zhengzhou 450001, Henan, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China
Y2  - 2023-10-18
ER  -

TY  - JOUR
AU  - Wu, Lanxi
AU  - Xu, Luhui
TI  - Local and global context cooperation for temporal action detection
T2  - MULTIMEDIA SYSTEMS
M3  - Article
AB  - Temporal action detection (TAD) is a fundamental task for video understanding. The task aims to locate the start and end boundaries of action instances and identify their corresponding categories within untrimmed videos. Distinguishing between similar actions in a video is still a difficult task. To enable fine-grained differentiation of temporal localization and action classification, it is necessary to introduce more temporal cues when building visual representations of similar actions. To address this issue, We propose a new method called the local and global context cooperation (LGCC), which aims to construct discriminative visual representations by combining short-term, medium-term, and long-term dependencies. The LGCC method comprises two main components: a local relation module and a global relation module. Specifically, we design a novel short-term and medium-term temporal context aggregation module (SMTCA). The aim of this module is to capture local context cues within an action instance to construct short-term context dependencies, and uses different dilation rates to expand the scope of information collection to establish medium-term context dependencies. The local relation module consists of multiple SMTCAs, which are used to obtain more temporal cues for fine-grained modeling. The global relation module employs multi-head self-attention to capture complex long-term context dependencies. We also design the LRGR module, combining local and global relation modules to create more expressive temporal features, improving action classification and boundary detection. Extensive experiments are conducted on the THUMOS14, ActivityNet1.3, and EPIC-Kitchens 100 datasets. LGCC achieves an average mAP of 68.9% on THUMOS14 and 36.6% on ActivityNet1.3. For the EPIC-Kitchens 100, the average mAP performance on the verb and noun tasks is 25.2% and 23.3%, respectively. The results show that LGCC achieves state-of-the-art performance.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0942-4962
SN  - 1432-1882
DA  - 2024 DEC
PY  - 2024
VL  - 30
IS  - 6
C7  - 334
DO  - 10.1007/s00530-024-01511-9
AN  - WOS:001348852600002
AD  - Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin, Peoples R China
AD  - Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin, Peoples R China
Y2  - 2024-11-12
ER  -

TY  - CPAPER
AU  - Shi, Dingfeng
AU  - Zhong, Yujie
AU  - Cao, Qiong
AU  - Zhang, Jing
AU  - Ma, Lin
AU  - Li, Jia
AU  - Tao, Dacheng
ED  - Avidan, S
ED  - Brostow, G
ED  - Cisse, M
ED  - Farinella, GM
ED  - Hassner, T
TI  - ReAct: Temporal Action Detection with Relational Queries
T2  - COMPUTER VISION, ECCV 2022, PT X
M3  - Proceedings Paper
CP  - 17th European Conference on Computer Vision (ECCV)
CL  - Tel Aviv, ISRAEL
AB  - This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-20079-3
SN  - 978-3-031-20080-9
DA  - 2022 
PY  - 2022
VL  - 13670
SP  - 105
EP  - 121
DO  - 10.1007/978-3-031-20080-9_7
AN  - WOS:000897089200007
AD  - Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing, Peoples R China
AD  - Meituan Inc, Beijing, Peoples R China
AD  - JD Explore Acad, Beijing, Peoples R China
AD  - Univ Sydney, Camperdown, Australia
M2  - Meituan Inc
M2  - JD Explore Acad
Y2  - 2023-01-21
ER  -

TY  - JOUR
AU  - Luo, Wang
AU  - Ren, Huan
AU  - Zhangd, Tianzhu
AU  - Yang, Wenfei
AU  - Zhang, Yongdong
TI  - Adaptive Prototype Learning for Weakly-supervised Temporal Action Localization.
T2  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
M3  - Journal Article
AB  - Weakly-supervised Temporal Action Localization (WTAL) aims to localize action instances with only video-level labels during training, where two primary issues are localization incompleteness and background interference. To relieve these two issues, recent methods adopt an attention mechanism to activate action instances and simultaneously suppress background ones, which have achieved remarkable progress. Nevertheless, we argue that these two issues have not been well resolved yet. On the one hand, the attention mechanism adopts fixed weights for different videos, which are incapable of handling the diversity of different videos, thus deficient in addressing the problem of localization incompleteness. On the other hand, previous methods only focus on learning the foreground attention and the attention weights usually suffer from ambiguity, resulting in difficulty of suppressing background interference. To deal with the above issues, in this paper we propose an Adaptive Prototype Learning (APL) method for WTAL, which includes two key designs: (1) an Adaptive Transformer Network (ATN) to explicitly model background and learn video-adaptive prototypes for each specific video, (2) an OT-based Collaborative (OTC) training strategy to guide the learning of prototypes and remove the ambiguity of the foreground-background separation by introducing an Optimal Transport (OT) algorithm into the collaborative training scheme between RGB and FLOW streams. These two key designs can work together to learn video-adaptive prototypes and solve the above two issues, achieving robust localization. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our proposed APL performs favorably against state-of-the-art methods.
SN  - 1941-0042
DA  - 2024 Aug 20 (Epub 2024 Aug 20)
PY  - 2024
VL  - PP
DO  - 10.1109/TIP.2024.3431915
AN  - MEDLINE:39163178
Y2  - 2024-08-22
ER  -

TY  - JOUR
AU  - Han, Jing
AU  - Zhu, Junwei
AU  - Cui, Yiyin
AU  - Bai, Lianfa
AU  - Yue, Jiang
TI  - Action detection by double hierarchical multi-structure space-time statistical matching model
T2  - OPTICAL REVIEW
M3  - Article
AB  - Aimed at the complex information in videos and low detection efficiency, an actions detection model based on neighboring Gaussian structure and 3D LARK features is put forward. We exploit a double hierarchical multi-structure space-time statistical matching model (DMSM) in temporal action localization. First, a neighboring Gaussian structure is presented to describe the multi-scale structural relationship. Then, a space-time statistical matching method is proposed to achieve two similarity matrices on both large and small scales, which combines double hierarchical structural constraints in model by both the neighboring Gaussian structure and the 3D LARK local structure. Finally, the double hierarchical similarity is fused and analyzed to detect actions. Besides, the multi-scale composite template extends the model application into multi-view. Experimental results of DMSM on the complex visual tracker benchmark data sets and THUMOS 2014 data sets show the promising performance. Compared with other state-of-the-art algorithm, DMSM achieves superior performances.
PU  - OPTICAL SOC JAPAN
PI  - TOKYO
PA  - KUDAN-KITA BLDG 5F, 1-12-3, KUDAN-KITA CHIYODA-KU, TOKYO, 102, JAPAN
SN  - 1340-6000
SN  - 1349-9432
DA  - 2018 JUN
PY  - 2018
VL  - 25
IS  - 3
SP  - 301
EP  - 315
DO  - 10.1007/s10043-018-0420-9
AN  - WOS:000433083600002
AD  - Nanjing Univ Sci & Technol, Jiangsu Key Lab Spectral Imaging & Intelligent Se, Nanjing 210094, Jiangsu, Peoples R China
Y2  - 2018-06-11
ER  -

TY  - JOUR
AU  - Lin, Xudong
AU  - Shou, Zheng
AU  - Chang, Shih-Fu
TI  - Towards Train-Test Consistency for Semi-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recently, Weakly-supervised Temporal Action Localization (WTAL) has been densely studied but there is still a large gap between weakly-supervised models and fully-supervised models. It is practical and intuitive to annotate temporal boundaries of a few examples and utilize them to help WTAL models better detect actions. However, the train-test discrepancy of action localization strategy prevents WTAL models from leveraging semi-supervision for further improvement. At training time, attention or multiple instance learning is used to aggregate predictions of each snippet for video-level classification; at test time, they first obtain action score sequences over time, then truncate segments of scores higher than a fixed threshold, and post-process action segments. The inconsistent strategy makes it hard to explicitly supervise the action localization model with temporal boundary annotations at training time. In this paper, we propose a Train-Test Consistent framework, TTC-Loc. In both training and testing time, our TTC-Loc localizes actions by comparing scores of action classes and predicted threshold, which enables it to be trained with semi-supervision. By fixing the train-test discrepancy, our TTC-Loc significantly outperforms the state-of-the-art performance on THUMOS'14, ActivityNet 1.2 and 1.3 when only video-level labels are provided for training. With full annotations of only one video per class and video-level labels for the other videos, our TTC-Loc further boosts the performance and achieves 33.4\% mAP (IoU threshold 0.5) on THUMOS's 14.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1910.11285
AN  - PPRN:13099185
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Islam, Ashraful
AU  - Long, Chengjiang
AU  - Radke, Richard
TI  - A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an "action-ness" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2101.00545
AN  - PPRN:11491305
AD  - Rensselaer Polytech Inst, Troy, NY 12180, USA
AD  - JD Digits AI Lab, Beijing, Peoples R China
M2  - JD Digits AI Lab
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Shou, Zheng
AU  - Chan, Jonathan
AU  - Zareian, Alireza
AU  - Miyazawa, Kazuyuki
AU  - Chang, Shih-Fu
TI  - CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. We will update the camera-ready version and publish the source codes online soon.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1703.01515
AN  - PPRN:12537322
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Zhang, Bowen
AU  - Chen, Hao
AU  - Wang, Meng
AU  - Xiong, Yuanjun
TI  - Online Action Detection in Streaming Videos with Time Buffers
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We formulate the problem of online temporal action detection in live streaming videos, acknowledging one important property of live streaming videos that there is normally a broadcast delay between the latest captured frame and the actual frame viewed by the audience. The standard setting of the online action detection task requires immediate prediction after a new frame is captured. We illustrate that its lack of consideration of the delay is imposing unnecessary constraints on the models and thus not suitable for this problem. We propose to adopt the problem setting that allows models to make use of the small `buffer time' incurred by the delay in live streaming videos. We design an action start and end detection framework for this online with buffer setting with two major components: flattened I3D and window-based suppression. Experiments on three standard temporal action detection benchmarks under the proposed setting demonstrate the effectiveness of the proposed framework. We show that by having a suitable problem setting for this problem with wide-applications, we can achieve much better detection accuracy than off-the-shelf online action detection models.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2010.03016
AN  - PPRN:22846552
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Yu, Jiahao
AU  - Hong, Jiang
A1  - IEEE
TI  - SARNet: Self-attention Assisted Ranking Network for Temporal Action Proposal Generation
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC)
M3  - Proceedings Paper
CP  - IEEE International Conference on Systems, Man, and Cybernetics (SMC)
CL  - ELECTR NETWORK
AB  - Temporal action detection is a fundamental yet challenging video understanding task. The calculation of confidence score for each generated action proposals remains the bottleneck of this task. Given that the continuity of videos is beneficial for self-supervised learning, in this paper we propose Self-attention Assisted Ranking Network (SARNet), which uses a self-attention mechanism to assist the ranking and retrieval of generated proposals. Our method incorporates a discriminative and a generative constraint to train the self-attention weight. Extensive experiments on THUMOS14 demonstrate that our method achieves a considerable improvement of average recall with a small number of proposals, and brings the mAP up to 30% at tIoU threshold 0.7 for the first time.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1062-922X
SN  - 978-1-6654-4207-7
DA  - 2021 
PY  - 2021
SP  - 1062
EP  - 1067
DO  - 10.1109/SMC52423.2021.9659016
AN  - WOS:000800532001007
AD  - East China Normal Univ, Sch Data Sci & Engn, Shanghai, Peoples R China
Y2  - 2022-07-01
ER  -

TY  - JOUR
AU  - Sui, Lin
AU  - Chen-Lin, Zhang
AU  - Gu, Lixin
AU  - Han, Feng
TI  - A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction.&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2206.03064
AN  - PPRN:22217637
AD  - 4Paradigm Inc, Beijing, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - DataElem Inc, Beijing, Peoples R China
M2  - 4Paradigm Inc
M2  - DataElem Inc
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Liu, Qinying
AU  - Wang, Zilei
A1  - Assoc Advancement Artificial Intelligence
TI  - Progressive Boundary Refinement Network for Temporal Action Detection
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
CL  - New York, NY
AB  - Temporal action detection is a challenging task due to vagueness of action boundaries. To tackle this issue, we propose an end-to-end progressive boundary refinement network (PBRNet) in this paper. PBRNet belongs to the family of one-stage detectors and is equipped with three cascaded detection modules for localizing action boundary more and more precisely. Specifically, PBRNet mainly consists of coarse pyramidal detection, refined pyramidal detection, and fine-grained detection. The first two modules build two feature pyramids to perform the anchor-based detection, and the third one explores the frame-level features to refine the boundaries of each action instance. In the fined-grained detection module, three frame-level classification branches are proposed to augment the frame-level features and update the confidence scores of action instances. Evidently, PBRNet integrates the anchor-based and frame-level methods. We experimentally evaluate the proposed PBRNet and comprehensively investigate the effect of the main components. The results show PBRNet achieves the state-of-the-art detection performances on two popular benchmarks: THUMOS'14 and ActivityNet, and meanwhile possesses a high inference speed.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
DA  - 2020 
PY  - 2020
VL  - 34
SP  - 11612
EP  - 11619
AN  - WOS:000668126804008
AD  - Univ Sci & Technol China, Dept Automat, Hefei, Anhui, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Phan, Thinh
AU  - Vo, Khoa
AU  - Le, Duy
AU  - Doretto, Gianfranco
AU  - Adjeroh, Donald
AU  - Le, Ngan
TI  - ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationship between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2311.00729
AN  - PPRN:85983615
AD  - Univ Arkansas, AICV Lab, Fayetteville, AR 72701, USA
AD  - FPT Software AI Ctr, Hanoi City, Vietnam
AD  - West Virginia Univ, Morgantown, WV 26506, USA
M2  - FPT Software AI Ctr
M2  - West Virginia Univ
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Wang, Wenju
AU  - Gu, Zehua
AU  - Tang, Bang
AU  - Wang, Sen
AU  - Hao, Jianfei
TI  - ACSF-ED: Adaptive Cross-Scale Fusion Encoder-Decoder for Spatio-Temporal Action Detection
T2  - CMC-COMPUTERS MATERIALS & CONTINUA
M3  - Article
AB  - Current spatio-temporal action detection methods lack sufficient capabilities in extracting and comprehending spatio-temporal information. This paper introduces an end-to-end Adaptive Cross-Scale Fusion Encoder-Decoder (ACSF-ED) network to predict the action and locate the object efficiently. In the Adaptive Cross-Scale Fusion Spatio-Temporal Encoder (ACSF ST-Encoder), the Asymptotic Cross-scale Feature-fusion Module (ACCFM) is designed to address the issue of information degradation caused by the propagation of high-level semantic information, thereby extracting high-quality multi-scale features to provide superior features for subsequent spatio-temporal information modeling. Within the Shared-Head Decoder structure, a shared classification and regression detection head is constructed. A multi-constraint loss function composed of one-to-one, one-to-many, and contrastive denoising losses is designed to address the problem of insufficient constraint force in predicting results with traditional methods. This loss function enhances the accuracy of model classification predictions and improves the proximity of regression position predictions to ground truth objects. The proposed method model is evaluated on the popular dataset UCF101-24 and JHMDB-21. Experimental results demonstrate that the proposed method achieves an accuracy of 81.52% on the Frame-mAP metric, surpassing current existing methods.
PU  - TECH SCIENCE PRESS
PI  - HENDERSON
PA  - 871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA
SN  - 1546-2218
SN  - 1546-2226
DA  - 2025 
PY  - 2025
VL  - 82
IS  - 2
SP  - 2389
EP  - 2414
DO  - 10.32604/cmc.2024.057392
AN  - WOS:001435460100001
AD  - Univ Shanghai Sci & Technol, Coll Publishing, Shanghai 200093, Peoples R China
AD  - Shanghai Baosight Software Co Ltd, Inst Informat Technol, Shanghai 200940, Peoples R China
M2  - Shanghai Baosight Software Co Ltd
Y2  - 2025-03-08
ER  -

TY  - CPAPER
AU  - Ma, Xurui
AU  - Zhang, Xiang
AU  - Wu, Chengkun
AU  - Xu, Chuanfu
AU  - Liu, Jie
AU  - Luo, Zhigang
ED  - Osten, W
ED  - Nikolaev, D
ED  - Zhou, J
TI  - Joint Motion Context and Clip Augmentation for Spatio-temporal Action Detection
T2  - FOURTEENTH INTERNATIONAL CONFERENCE ON MACHINE VISION (ICMV 2021)
M3  - Proceedings Paper
CP  - 14th International Conference on Machine Vision (ICMV)
CL  - Rome, ITALY
AB  - This paper endeavors to leverage spatio-temporal visual cues to improve video-based action detection. As a result, a NOn-Local Action detector based on anchor-free called NOLA is proposed, which is built off a recent moving center detector (MOC) and further extends it by efficiently aggregating long-range spatio-temporal information. In detail, a significantly efficient spatio-temporal motion-aware non-local block is explored to provide global motion contexts for the entire predictive branches of MOC. This byproduct can make the large batch samples run on a resource limited device. Besides, a light-weighted data augmentation method termed clip augmentation designed for video-based tasks is proposed, which serves to improve the generalization ability of the detector with economical scale-and-addition operation. NOLA works with two above schemes in real-time as well. Experiments on two benchmark datasets show that NOLA significantly exceeds MOC. Compared to other existing methods,, NOLA reaches the state-of-the-art, in terms of video-level mean of average precision (video mAP).
PU  - SPIE-INT SOC OPTICAL ENGINEERING
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN  - 0277-786X
SN  - 1996-756X
SN  - 978-1-5106-5045-9
SN  - 978-1-5106-5044-2
DA  - 2022 
PY  - 2022
VL  - 12084
C7  - 120840T
DO  - 10.1117/12.2623446
AN  - WOS:000799214600028
AD  - Natl Univ Def Technol, Changsha, Peoples R China
Y2  - 2022-06-22
ER  -

TY  - CPAPER
AU  - Kang, Hyolim
AU  - Hyun, Jeongseok
AU  - An, Joungbin
AU  - Yu, Youngjae
AU  - Kim, Seon Joo
ED  - Leonardis, A
ED  - Ricci, E
ED  - Roth, S
ED  - Russakovsky, O
ED  - Sattler, T
ED  - Varol, G
TI  - ActionSwitch: Class-Agnostic Detection of Simultaneous Actions in Streaming Videos
T2  - COMPUTER VISION - ECCV 2024, PT X
M3  - Proceedings Paper
CP  - 18th European Conference on Computer Vision (ECCV)
CL  - Milan, ITALY
AB  - Online Temporal Action Localization (On-TAL) is a critical task that aims to instantaneously identify action instances in untrimmed streaming videos as soon as an action concludes-a major leap from frame-based Online Action Detection (OAD). Yet, the challenge of detecting overlapping actions is often overlooked even though it is a common scenario in streaming videos. Current methods that can address concurrent actions depend heavily on class information, limiting their flexibility. This paper introduces ActionSwitch, the first class-agnostic On-TAL framework capable of detecting overlapping actions. By obviating the reliance on class information, ActionSwitch provides wider applicability to various situations, including overlapping actions of the same class or scenarios where class information is unavailable. This approach is complemented by the proposed "conservativeness loss", which directly embeds a conservative decision-making principle into the loss function for On-TAL. Our ActionSwitch achieves state-of-the-art performance in complex datasets, including Epic-Kitchens 100 targeting the challenging egocentric view and FineAction consisting of fine-grained actions.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-72683-5
SN  - 978-3-031-72684-2
DA  - 2025 
PY  - 2025
VL  - 15068
SP  - 383
EP  - 400
DO  - 10.1007/978-3-031-72684-2_22
AN  - WOS:001353686900022
AD  - Yonsei Univ, Seoul, South Korea
Y2  - 2024-12-03
ER  -

TY  - JOUR
AU  - Liu, Ziyi
AU  - Wang, Le
AU  - Zhang, Qilin
AU  - Tang, Wei
AU  - Zheng, Nanning
AU  - Hua, Gang
TI  - Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Given only video-level action categorical labels during training, weakly-supervised temporal action localization (WS-TAL) learns to detect action instances and locates their temporal boundaries in untrimmed videos. Compared to its fully supervised counterpart, WS-TAL is more cost-effective in data labeling and thus favorable in practical applications. However, the coarse video-level supervision inevitably incurs ambiguities in action localization, especially in untrimmed videos containing multiple action instances. To overcome this challenge, we observe that significant temporal contrasts among video snippets, e.g., caused by temporal discontinuities and sudden changes, often occur around true action boundaries. This motivates us to introduce a Contrast-based Localization EvaluAtioN Network (CleanNet), whose core is a new temporal action proposal evaluator, which provides fine-grained pseudo supervision by leveraging the temporal contrasts among snippet-level classification predictions. As a result, the uncertainty in locating action instances can be resolved via evaluating their temporal contrast scores. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Besides, we also explore the usage of temporal contrast on temporal action proposal (TAP) generation task, which we believe is the first attempt with the weak supervision setting. Experiments on the THUMOS14, ActivityNet v1.2 and v1.3 datasets validate the efficacy of our method against existing state-of-the-art WS-TAL algorithms.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2022 SEPT 1
PY  - 2022
VL  - 44
IS  - 9
SP  - 5886
EP  - 5902
DO  - 10.1109/TPAMI.2021.3078798
AN  - WOS:000836666600095
AD  - Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China
AD  - ABB Corp Res Ctr, Raleigh, NC 27606 USA
AD  - Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA
AD  - Wormpex AI Res, Bellevue, WA 98004 USA
M2  - Wormpex AI Res
Y2  - 2022-08-20
ER  -

TY  - CPAPER
AU  - Alyahya, Munirah
AU  - Alghannam, Shahad
AU  - Alhussan, Taghreed
A1  - IEEE
TI  - Temporal Driver Action Localization using Action Classification Methods
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Driver distraction recognition is an essential computer vision task that can play a key role in increasing traffic safety and reducing traffic accidents. In this paper, we propose a temporal driver action localization (TDAL) framework for classifying driver distraction actions, as well as identifying the start and end time of a given driver action. The TDAL framework consists of three stages: preprocessing, 'which takes untrimmed video as input and generates multiple clips; action classification, which classifies the clips; and finally, the classifier output is sent to the temporal action localization to generate the start and end times of the distracted actions. The proposed framework achieves an F1 score of 27.06% on Track 3A2 dataset of NVIDIA AI City 2022 Challenge. The findings show that the TDAL framework contributes to fine-grained driver distraction recognition and paves the way for the development of smart and safe transportation. Code will be available soon.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3318
EP  - 3325
DO  - 10.1109/CVPRW56347.2022.00375
AN  - WOS:000861612703048
AD  - Saudi Technol & Secur Comprehens Control Co, Riyadh, Saudi Arabia
M2  - Saudi Technol & Secur Comprehens Control Co
Y2  - 2022-12-07
ER  -

TY  - CPAPER
AU  - Pardo, Alejandro
AU  - Alwassel, Humam
AU  - Heilbron, Fabian Caba
AU  - Thabet, Ali
AU  - Ghanem, Bernard
A1  - IEEE
TI  - RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization
T2  - 2021 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WACV 2021
M3  - Proceedings Paper
CP  - IEEE Winter Conference on Applications of Computer Vision (WACV)
CL  - ELECTR NETWORK
AB  - Video action detectors are usually trained using datasets with fully-supervised temporal annotations. Building such datasets is an expensive task. To alleviate this problem, recent methods have tried to leverage weak labeling, where videos are untrimmed and only a video-level label is available. In this paper, we propose RefineLoc, a novel weakly-supervised temporal action localization method. RefineLoc uses an iterative refinement approach by estimating and training on snippet-level pseudo ground truth at every iteration. We show the benefit of this iterative approach and present an extensive analysis of five different pseudo ground truth generators. We show the effectiveness of our model on two standard action datasets, ActivityNet v1.2 and THUMOS14. RefineLoc shows competitive results with the state-of-the-art in weakly-supervised temporal localization. Additionally, our iterative refinement process is able to significantly improve the performance of two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2472-6737
SN  - 978-0-7381-4266-1
DA  - 2021 
PY  - 2021
SP  - 3318
EP  - 3327
DO  - 10.1109/WACV48630.2021.00336
AN  - WOS:000693397600132
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
AD  - Adobe Res, San Francisco, CA USA
Y2  - 2021-09-25
ER  -

TY  - JOUR
AU  - Tang, Yue
AU  - Wu, Yawen
AU  - Zhou, Peipei
AU  - Hu, Jingtong
AU  - Hu, Jingtong
TI  - Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Detecting actions in videos have been widely applied in on-device applications such as cars, robots, etc. Practical on-device videos are always untrimmed with both action and background. It is desirable for a model to both recognize the class of action and localize the temporal position where the action happens. Such a task is called temporal action location (TAL), which is always trained on the cloud where multiple untrimmed videos are collected and labeled. It is desirable for a TAL model to continuously and locally learn from new data, which can directly improve the action detection precision while protecting customers&rsquo; privacy. However, directly training a TAL model on the device is non-trivial. To train a TAL model which can precisely recognize and localize each action, tremendous video samples with temporal annotations are required. However, annotating videos frame by frame is exorbitantly time-consuming and expensive. Although weakly-supervised temporal action localization (W-TAL) has been proposed to learn from untrimmed videos with only video-level labels, such an approach is also not suitable for on-device learning scenarios. In practical on-device learning applications, data are collected in streaming. For example, the camera on the device keeps collecting video frames for hours or days, and the actions of nearly all classes are included in a single long video stream. Dividing such a long video stream into multiple video segments requires lots of human effort, which hinders the exploration of applying the TAL tasks to realistic on-device learning applications. To enable W-TAL models to learn from a long, untrimmed streaming video, we propose an efficient video learning approach that can directly adapt to new environments. We first propose a self-adaptive video dividing approach with a contrast score-based segment merging approach to convert the video stream into multiple segments. Then, we explore different sampling strategies on the TAL tasks to request as few labels as possible. To the best of our knowledge, we are the first attempt to directly learn from the on-device, long video stream. Experimental results on the THUMOS&rsquo;14 dataset show that the performance of our approach is comparable to the current W-TAL state-of-the-art (SOTA) work without any laborious manual video splitting.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.12673
AN  - PPRN:12662751
AD  - Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15261, USA
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Yang, Jianhua
AU  - Wang, Ke
AU  - Li, Ruifeng
AU  - Perner, Petra
TI  - Cascading spatio-temporal attention network for real-time action detection
T2  - MACHINE VISION AND APPLICATIONS
M3  - Article
AB  - Accurately detecting human actions in video has many applications, such as video surveillance and somatosensory games. In this paper, we propose a spatial-aware attention module (SAM) and a temporal-aware attention module (TAM) for spatio-temporal action detection in videos. SAM first concatenates the feature maps of consecutive frames on the channel and then uses dilated convolutional layer followed by a sigmoid function to generate a spatial attention map. The resulting attention map contains spatial information from consecutive frames, so it helps the detector focus on salient spatial features to achieve more accurate localization of action instances in consecutive frames. TAM deploys several fully connected layers to generate a temporal attention map. The temporal attention map focuses on the temporal association of each spatial feature; it can capture the temporal association of action instances, thereby improving the detector to track actions. To evaluate the effectiveness of SAM and TAM, we build an efficient and strong anchor-free action detector, cascading spatio-temporal attention network, equipped with a 2D backbone and SAM and TAM modules. Extensive experiments on two benchmarks, JHMDB and UCF101-24, demonstrate the preferable performance of SAM and TAM.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0932-8092
SN  - 1432-1769
DA  - 2023 NOV
PY  - 2023
VL  - 34
IS  - 6
C7  - 110
DO  - 10.1007/s00138-023-01457-4
AN  - WOS:001073437400003
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China
AD  - Harbin Inst Technol, Zhengzhou Res Inst, Zhengzhou 450000, Peoples R China
AD  - FutureLab Artificial Intelligence IBaI 2, Dresden, Germany
M2  - FutureLab Artificial Intelligence IBaI 2
Y2  - 2023-10-12
ER  -

TY  - JOUR
AU  - Zhao, Yisheng
AU  - Zhu, Huaiyu
AU  - Huan, Ruohong
AU  - Bao, Yaoqi
AU  - Pan, Yun
TI  - Heterogeneous Graph Network for Action Detection
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Spatio-temporal action detection is a fundamental task that detects persons and recognizes their actions from videos. It requires reasoning about the spatial-temporal interactions between persons and their surroundings. Recently, more modalities have been found by researchers, which puts higher demands on the reasoning capability of the method, yet a method capable of holistic reasoning is still lacking. To this end, we propose a heterogeneous graph network, which aims to reason the spatial-temporal interactions among different types of nodes (video entities) and edges (inter-entity relations). Concretely, it includes spatial and temporal graphs, which are alternately updated. The spatial graph contains nodes of person appearance, person pose, object appearance, and hand interaction, and the temporal graph has person nodes at different moments. For information aggregation, we propose a person-centric heterogeneous graph reasoning algorithm, which introduces heterogeneity into the graphs through node-type-specific projections and modulated edge-type-specific representations. We find that the introduction of heterogeneity enriches the model's ability to understand multi-modality, which facilitates better parsing of complex semantic relations in videos and potentially leads to further mining of spatial-temporal interactions between entities in the future. Experimental results on four public datasets demonstrate the superiority of our method. Code is available at https://github.com/actiondetection.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 SEP
PY  - 2024
VL  - 34
IS  - 9
SP  - 7962
EP  - 7974
DO  - 10.1109/TCSVT.2024.3383477
AN  - WOS:001409508700013
AD  - Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China
AD  - Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China
AD  - Xidian Univ, Hangzhou Inst Technol, Hangzhou 311200, Peoples R China
Y2  - 2025-02-07
ER  -

TY  - JOUR
AU  - Yao, Leiyue
AU  - Yang, Wei
AU  - Huang, Wei
AU  - Jiang, Nan
AU  - Zhou, Bingbing
TI  - Multi-scale feature learning and temporal probing strategy for one-stage temporal action localization
T2  - INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS
M3  - Article
AB  - The aim of temporal action localization (TAL) is to determine the start and end frames of an action in a video. In recent years, TAL has attracted considerable attention because of its increasing applications in video understanding and retrieval. However, precisely estimating the duration of an action in the temporal dimension is still a challenging problem. In this paper, we propose an effective one-stage TAL method based on a self-defined motion data structure, called a dense joint motion matrix (DJMM), and a novel temporal detection strategy. Our method provides three main contributions. First, compared with mainstream motion images, DJMMs can preserve more pre-processed motion features and provides more precise detail representations. Furthermore, DJMMs perfectly solve the temporal information loss problem caused by motion trajectory overlaps within a certain time period. Second, a spatial pyramid pooling (SPP) layer, which is widely used in the object detection and tracking fields, is innovatively incorporated into the proposed method for multi-scale feature learning. Moreover, the SPP layer enables the backbone convolutional neural network (CNN) to receive DJMMs of any size in the temporal dimension. Third, a large-scale-first temporal detection strategy inspired by a well-developed Chinese text segmentation algorithm is proposed to address long-duration videos. Our method is evaluated on two benchmark data sets and one self-collected data set: Florence-3D, UTKinect-Action3D and HanYue-3D. The experimental results show that our method achieves competitive action recognition accuracy and high TAL precision, and its time efficiency and few-shot learning capabilities enable it to be utilized for real-time surveillance.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 0884-8173
SN  - 1098-111X
DA  - 2022 JUL
PY  - 2022
VL  - 37
IS  - 7
SP  - 4092
EP  - 4112
DO  - 10.1002/int.22713
AN  - WOS:000708079100001
C6  - OCT 2021
AD  - Jiangxi Univ Technol, Sch Informat Engn, 99 ZiYang Rd, Nanchang 330098, Jiangxi, Peoples R China
AD  - Nanchang Univ, Sch Informat Engn, Nanchang, Jiangxi, Peoples R China
AD  - East China Jiaotong Univ, Sch Informat Engn, Nanchang, Jiangxi, Peoples R China
M2  - Jiangxi Univ Technol
Y2  - 2021-10-27
ER  -

TY  - JOUR
AU  - Su, Haisheng
AU  - Zhao, Xu
AU  - Lin, Tianwei
AU  - Liu, Shuming
AU  - Hu, Zhilan
TI  - Transferable Knowledge-Based Multi-Granularity Fusion Network for Weakly Supervised Temporal Action Detection
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Despite remarkable progress, temporal action detection is still limited for real application due to the great amount of manual annotations. This issue motivates interest in addressing this task under weak supervision, namely, locating the action instances using only video-level class labels. Many current works on this task are mainly based on the Class Activation Sequence (CAS), which is generated by the video classification network to describe the probability of each snippet being in a specific action class of the video. However, the CAS generated by a simple classification network can only focus on local discriminative parts instead of locating the entire interval of target actions. In this paper, we present a novel framework to handle this issue. Specifically, we propose to utilize convolutional kernels with varied dilation rates to enlarge the receptive fields, which can transfer the discriminative information to the surrounding non-discriminative regions. Then, we design a cascaded module with the proposed Online Adversarial Erasing (OAE) mechanism to further mine more relevant regions of target actions by feeding the erased-feature maps of discovered regions back into the system. In addition, inspired by the transfer learning method, we adopt an additional module to transfer the knowledge from trimmed videos to untrimmed videos to promote the classification performance on untrimmed videos. Finally, we employ a boundary regression module embedded with Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries based on the enhanced CAS. Extensive experiments are conducted on two challenging datasets, THUMOS14 and ActivityNet-1.3, and the experimental results clearly demonstrate the superiority of our unified framework.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2021 
PY  - 2021
VL  - 23
SP  - 1503
EP  - 1515
DO  - 10.1109/TMM.2020.2999184
AN  - WOS:000655830300004
AD  - Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China
AD  - Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China
AD  - Huawei Co Ltd, Cent Media Technol Inst, Shenzhen 518129, Peoples R China
Y2  - 2021-06-17
ER  -

TY  - JOUR
AU  - Yao, Guangle
AU  - Lei, Tao
AU  - Liu, Xianyuan
AU  - Jiang, Ping
TI  - Temporal Action Detection in Untrimmed Videos from Fine to Coarse Granularity
T2  - APPLIED SCIENCES-BASEL
M3  - Article
AB  - Temporal action detection in long, untrimmed videos is an important yet challenging task that requires not only recognizing the categories of actions in videos, but also localizing the start and end times of each action. Recent years, artificial neural networks, such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) improve the performance significantly in various computer vision tasks, including action detection. In this paper, we make the most of different granular classifiers and propose to detect action from fine to coarse granularity, which is also in line with the people's detection habits. Our action detection method is built in the proposal then classification' framework. We employ several neural network architectures as deep information extractor and segment-level (fine granular) and window-level (coarse granular) classifiers. Each of the proposal and classification steps is executed from the segment to window level. The experimental results show that our method not only achieves detection performance that is comparable to that of state-of-the-art methods, but also has a relatively balanced performance for different action categories.
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN  - 2076-3417
DA  - 2018 OCT
PY  - 2018
VL  - 8
IS  - 10
C7  - 1924
DO  - 10.3390/app8101924
AN  - WOS:000448653700219
AD  - Chinese Acad Sci, Inst Opt & Elect, Chengdu 610209, Sichuan, Peoples R China
AD  - Univ Elect Sci & Technol China, Sch Optoelect Informat, 4,Sect 2,North Jianshe Rd, Chengdu 610054, Sichuan, Peoples R China
AD  - Univ Chinese Acad Sci, 19 A Yuquan Rd, Beijing 100039, Peoples R China
Y2  - 2018-11-13
ER  -

TY  - CPAPER
AU  - Stippel, Christian
AU  - Heitzinger, Thomas
AU  - Kampel, Martin
ED  - Kothe, U
ED  - Rother, C
TI  - A Trimodal Dataset: RGB, Thermal, and Depth for Human Segmentation and Temporal Action Detection
T2  - PATTERN RECOGNITION, DAGM GCPR 2023
M3  - Proceedings Paper
CP  - 45th Annual Conference of the German-Association-for-Pattern-Recognition (DAGM GCPR)
CL  - Heidelberg, GERMANY
AB  - Computer vision research and popular datasets are predominantly based on the RGB modality. However, traditional RGB datasets have limitations in lighting conditions and raise privacy concerns. Integrating or substituting with thermal and depth data offers a more robust and privacy-preserving alternative. We present TRISTAR (https://zenodo.org/record/7996570, https://github.com/Stippler/tristar), a public TRImodal Segmentation and acTion ARchive comprising registered sequences of RGB, depth, and thermal data. The dataset encompasses 10 unique environments, 18 camera angles, 101 shots, and 15,618 frames which include human masks for semantic segmentation and dense labels for temporal action detection and scene understanding. We discuss the system setup, including sensor configuration and calibration, as well as the process of generating ground truth annotations. On top, we conduct a quality analysis of our proposed dataset and provide benchmark models as reference points for human segmentation and action detection. By employing only modalities of thermal and depth, these models yield improvements in both human segmentation and action detection.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-54604-4
SN  - 978-3-031-54605-1
DA  - 2024 
PY  - 2024
VL  - 14264
SP  - 18
EP  - 33
DO  - 10.1007/978-3-031-54605-1_2
AN  - WOS:001212397400002
AD  - TU Wien, Comp Vis Lab, Vienna, Austria
Y2  - 2024-05-31
ER  -

TY  - JOUR
AU  - Raza, Asif
AU  - Yang, Bang
AU  - Zou, Yuexian
TI  - Zero-Shot Temporal Action Detection by Learning Multimodal Prompts and Text-Enhanced Actionness
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Zero-shot temporal action detection (ZS-TAD), aiming to recognize and detect new and unseen video actions, is an emerging and challenging task with limited solutions. Recent studies have adapted the vision-language pre-trained model CLIP for this task in a parameter-efficient fine-tuning fashion to achieve open-vocabulary detection. However, they suffer from insufficient vision-text alignment because of the dual-stream structure of CLIP and yield inferior TAD results due to the lack of accurate action prior. In this paper, we target the above limitations and propose to learn multimodal Prompts and Text-Enhanced Actionness (mProTEA) for ZS-TAD. Specifically, we insert learnable layer-wise prompts into the vision and text branches of the frozen CLIP and establish a strong coupling between them, resulting in multimodal prompts that can boost cross-modal alignment. To ease computation costs, we propose to conduct multimodal prompt learning on an image recognition dataset with rich concepts (e.g., ImageNet) first and then keep them frozen during TAD fine-tuning. For improving TAD, we introduce text-enhanced actionness modeling, where we leverage the concise semantics of text to assist the calculation of class-agnostic actionness scores, to offer accurate prior information for both action classification and localization. With the above designs, our mProTEA excels in extensive TAD experiments, surpassing the strong competitor STALE by 5.1% on ActivityNet under the zero-shot setting and achieving state-of-the-art performance in conventional supervised scenarios. Ablation studies confirm the effectiveness of our proposals and show superior domain generalization of multimodal prompts learned on ImageNet against the other 10 image recognition datasets.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2024 NOV
PY  - 2024
VL  - 34
IS  - 11
SP  - 11000
EP  - 11012
DO  - 10.1109/TCSVT.2024.3414275
AN  - WOS:001398274800036
AD  - Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518052, Peoples R China
Y2  - 2025-02-13
ER  -

TY  - JOUR
AU  - Eun, Hyunjun
AU  - Lee, Sumin
AU  - Moon, Jinyoung
AU  - Park, Jongyoul
AU  - Jung, Chanho
AU  - Kim, Changick
TI  - SRG: Snippet Relatedness-Based Temporal Action Proposal Generator
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Recent temporal action proposal generation approaches have suggested integrating segment- and snippet score-based methodologies to produce proposals with high recall and accurate boundaries. In this paper, different from such a hybrid strategy, we focus on the potential of the snippet score-based approach. Specifically, we propose a new snippet score-based method, named Snippet Relatedness-based Generator (SRG), with a novel concept of "snippet relatedness". Snippet relatedness represents which snippets are related to a specific action instance. To effectively learn this snippet relatedness, we present "pyramid non-local operations" for locally and globally capturing long-range dependencies among snippets. By employing these components, SRG first produces a 2D relatedness score map that enables the generation of various temporal intervals reliably covering most action instances with high overlap. Then, SRG evaluates the action confidence scores of these temporal intervals and refines their boundaries to obtain temporal action proposals. On THUMOS-14 and ActivityNet-1.3 datasets, SRG outperforms state-of-the-art methods for temporal action proposal generation. Furthermore, compared to competing proposal generators, SRG leads to significant improvements in temporal action detection.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2020 NOV
PY  - 2020
VL  - 30
IS  - 11
SP  - 4232
EP  - 4244
DO  - 10.1109/TCSVT.2019.2953187
AN  - WOS:000583761300030
AD  - Korea Adv Inst Sci & Technol KAIST, Sch Elect Engn, Daejeon 34141, South Korea
AD  - Elect & Telecommun Res Inst ETRI, Daejeon 34129, South Korea
AD  - Hanbat Natl Univ, Dept Elect Engn, Daejeon 34158, South Korea
Y2  - 2020-12-14
ER  -

TY  - JOUR
AU  - Xie, Ting-Ting
AU  - Tzelepis, Christos
AU  - Patras, Ioannis
TI  - Temporal Action Localization with Variance-Aware Networks
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This work addresses the problem of temporal action localization with Variance-Aware Networks (VAN), i.e., DNNs that use second-order statistics in the input and/or the output of regression tasks. We first propose a network (VANp) that when presented with the second-order statistics of the input, i.e., each sample has a mean and a variance, it propagates the mean and the variance throughout the network to deliver outputs with second order statistics. In this framework, both the input and the output could be interpreted as Gaussians. To do so, we derive differentiable analytic solutions, or reasonable approximations, to propagate across commonly used NN layers. To train the network, we define a differentiable loss based on the KL-divergence between the predicted Gaussian and a Gaussian around the ground truth action borders, and use standard back-propagation. Importantly, the variances propagation in VANp does not require any additional parameters, and during testing, does not require any additional computations either. In action localization, the means and the variances of the input are computed at pooling operations, that are typically used to bring arbitrarily long videos to a vector with fixed dimensions. Second, we propose two alternative formulations that augment the first (respectively, the last) layer of a regression network with additional parameters so as to take in the input (respectively, predict in the output) both means and variances. Results in the action localization problem show that the incorporation of second order statistics improves over the baseline network, and that VANp surpasses the accuracy of virtually all other two-stage networks without involving any additional parameters.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.11254
AN  - PPRN:22635089
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Shi, Haichao
AU  - Zhang, Xiao-Yu
AU  - Li, Changsheng
AU  - Gong, Lixing
AU  - Li, Yong
AU  - Bao, Yongjun
ED  - ACM
TI  - Dynamic Graph Modeling for Weakly-Supervised Temporal Action Localization
T2  - PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022
M3  - Proceedings Paper
CP  - 30th ACM International Conference on Multimedia (MM)
CL  - Lisboa, PORTUGAL
AB  - Weakly supervised action localization is a challenging task that aims to localize action instances in untrimmed videos given only video-level supervision. Existing methods mostly distinguish action from background via attentive feature fusion with RGB and optical flow modalities. Unfortunately, this strategy fails to retain the distinct characteristics of each modality, leading to inaccurate localization under hard-to-discriminate cases such as action-context interference and in-action stationary period. As an action is typically comprised of multiple stages, an intuitive solution is to model the relation between the finer-grained action segments to obtain a more detailed analysis. In this paper, we propose a dynamic graph-based method, namely DGCNN, to explore the two-stream relation between action segments. To be specific, segments within a video which are likely to be actions are dynamically selected to construct an action graph. For each graph, a triplet adjacency matrix is devised to explore the temporal and contextual correlations between the pseudo action segments, which consists of three components, i.e., mutual importance, feature similarity, and high-level contextual similarity. The two-stream dynamic pseudo graphs, along with the pseudo background segments, are used to derive more detailed video representation. For action localization, a non-local based temporal refinement module is proposed to fully leverage the temporal consistency between consecutive segments. Experimental results on three datasets, i.e., THUMOS14, ActivityNet v1.2 and v1.3, demonstrate that our method is superior to the state-of-the-arts.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9203-7
DA  - 2022 
PY  - 2022
SP  - 3820
EP  - 3828
DO  - 10.1145/3503161.3548077
AN  - WOS:001150372703093
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China
AD  - Beijing Inst Technol, Beijing, Peoples R China
AD  - JD com, Beijing, Peoples R China
M2  - JD com
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Zhao, Tao
AU  - Han, Junwei
AU  - Yang, Le
AU  - Zhang, Dingwen
TI  - Equivalent Classification Mapping for Weakly Supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action localization is a newly emerging yet widely studied topic in recent years. The existing methods can be categorized into two localization-by-classification pipelines, i.e., the pre-classification pipeline and the post-classification pipeline. The pre-classification pipeline first performs classification on each video snippet and then aggregate the snippet-level classification scores to obtain the video-level classification score. In contrast, the post-classification pipeline aggregates the snippet-level features first and then predicts the video-level classification score based on the aggregated feature. Although the classifiers in these two pipelines are used in different ways, the role they play is exactly the same---to classify the given features to identify the corresponding action categories. To this end, an ideal classifier can make both pipelines work. This inspires us to simultaneously learn these two pipelines in a unified framework to obtain an effective classifier. Specifically, in the proposed learning framework, we implement two parallel network streams to model the two localization-by-classification pipelines simultaneously and make the two network streams share the same classifier. This achieves the novel Equivalent Classification Mapping (ECM) mechanism. Moreover, we discover that an ideal classifier may possess two characteristics: 1) The frame-level classification scores obtained from the pre-classification stream and the feature aggregation weights in the post-classification stream should be consistent; 2) The classification results of these two streams should be identical. Based on these two characteristics, we further introduce a weight-transition module and an equivalent training strategy into the proposed learning framework, which assists to thoroughly mine the equivalence mechanism.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.07728
AN  - PPRN:15960590
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Xu, Mengmeng
AU  - Perez-Rua, Juan-Manuel
AU  - Zhu, Xiatian
AU  - Ghanem, Bernard
AU  - Martinez, Brais
ED  - Ranzato, M
ED  - Beygelzimer, A
ED  - Dauphin, Y
ED  - Liang, PS
ED  - Vaughan, JW
TI  - Low-Fidelity Video Encoder Optimization for Temporal Action Localization
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)
M3  - Proceedings Paper
CP  - 35th Annual Conference on Neural Information Processing Systems (NeurIPS)
CL  - ELECTR NETWORK
AB  - Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline, first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder - trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical flow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-fi/lofi_action_localization.
PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI  - LA JOLLA
PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN  - 1049-5258
SN  - *****************
DA  - 2021 
PY  - 2021
VL  - 34
AN  - WOS:000922928207014
AD  - Samsung AI Ctr Cambridge, Cambridge, England
AD  - King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia
M2  - Samsung AI Ctr Cambridge
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Ma, Fan
AU  - Zhu, Linchao
AU  - Yang, Yi
AU  - Zha, Shengxin
AU  - Kundu, Gourab
AU  - Feiszli, Matt
AU  - Shou, Zheng
TI  - SF-Net: Single-Frame Supervision for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we study an intermediate form of supervision, i.e., single-frame supervision, for temporal action localization (TAL). To obtain the single-frame supervision, the annotators are asked to identify only a single frame within the temporal window of an action. This can significantly reduce the labor cost of obtaining full supervision which requires annotating the action boundary. Compared to the weak supervision that only annotates the video-level label, the single-frame supervision introduces extra temporal action signals while maintaining low annotation overhead. To make full use of such single-frame supervision, we propose a unified system called SF-Net. First, we propose to predict an actionness score for each video frame. Along with a typical category score, the actionness score can provide comprehensive information about the occurrence of a potential action and aid the temporal boundary refinement during inference. Second, we mine pseudo action and background frames based on the single-frame annotations. We identify pseudo action frames by adaptively expanding each annotated single frame to its nearby, contextual frames and we mine pseudo background frames from all the unannotated frames across multiple videos. Together with the ground-truth labeled frames, these pseudo-labeled frames are further used for training the classifier. In extensive experiments on THUMOS14, GTEA, and BEOID, SF-Net significantly improves upon state-of-the-art weakly-supervised methods in terms of both segment localization and single-frame localization. Notably, SF-Net achieves comparable results to its fully-supervised counterpart which requires much more resource intensive annotations. The code is available at https://github.com/Flowerfan/SF-Net.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2003.06845
AN  - PPRN:13134625
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Yao, Leiyue
AU  - Hu, Weilong
AU  - Xiong, Keyun
A1  - IEEE
TI  - Recognizing A Complex Human Behaviour with Zero Video Training Sample Base on Human Montion Data Encoding
T2  - 2023 INTERNATIONAL CONFERENCE ON DATA SECURITY AND PRIVACY PROTECTION, DSPP
M3  - Proceedings Paper
CP  - International Conference on Data Security and Privacy Protection (DSPP)
CL  - Xian, PEOPLES R CHINA
AB  - Complex human behaviour (CHB) recognition is more meaningful, but also more challenging than human action recognition (HAR). Understanding the semantic information included in CHB video is helpful to develop advanced human-machine interaction applications. However, utilizing the existing action recognition models can hardly gain satisfactory accuracy because of the CHBs' long duration and unenumerable characteristics. In this paper, we break through this bottleneck from the semantic perspective via motion data encoding and semantic similarity judgment. The contributions of our method are mainly reflected in three aspects. First, a CHB is innovatively defined as the combination of the human basic actions which are composed in time order. Thus, the unenumerable CHBs can be represented by limited human actions. Second, to automatically construct the action combination of a CHB, a skeleton-based multi-scale CNN model and an efficient one-stage temporal action localization model are put forward to exactly extract action and its duration from a CHB video. Third, the word-vector in the nature language processing field is creatively imported into our method. Therefore, the action words can be encoded by a set of float-point numbers. Moreover, the word-vector is helpful to judge the similarities of different action combinations. The proposed method is evaluated on a self-collected dataset. Through extensive experiments, the results prove the method's feasibility and universality. Moreover, its use of only human skeleton motion data provides privacy assurances to users.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-0315-5
DA  - 2023 
PY  - 2023
SP  - 174
EP  - 181
DO  - 10.1109/DSPP58763.2023.10404592
AN  - WOS:001168583200021
AD  - Jiangxi Univ Chinese Med, Key Lab Artificial Intelligence Chinese Med, Nanchang, Peoples R China
AD  - Hanlin Hangyu Tianjin Ind Co Ltd, Tianjin, Peoples R China
AD  - Jiangxi Univ Chinese Med, Coll Comp Sci, Nanchang, Peoples R China
M2  - Hanlin Hangyu Tianjin Ind Co Ltd
Y2  - 2024-03-19
ER  -

TY  - CPAPER
AU  - Zhao, Zixuan
AU  - Wang, Dongqi
AU  - Zhao, Xu
A1  - IEEE
TI  - Movement Enhancement toward Multi-Scale Video Feature Representation for Temporal Action Detection
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Boundary localization is a challenging problem in Temporal Action Detection (TAD), in which there are two main issues. First, the submergence of movement feature, i.e. the movement information in a snippet is covered by the scene information. Second, the scale of action, that is, the proportion of action segments in the entire video, is considerably variable. In this work, we first design a Movement Enhance Module (MEM) to highlight movement feature for better action location, and then, we propose a Scale Feature Pyramid Network (SFPN) to detect multi-scale actions in videos. For Movement Enhance Module, firstly, Movement Feature Extractor (MFE) is designed to get the movement feature. Secondly, we propose a Multi-Relation Enhance Module (MREM) to grasp valuable information correlation both locally and temporally. For Scale Feature Pyramid Network, we design a U-Shape Module to model different scale actions, moreover, we design the training and inference strategy of different scales, ensuring that each pyramid layer is only responsible for actions at a specific scale. These two innovations are integrated as the Movement Enhance Network (MENet), and extensive experiments conducted on two challenging benchmarks demonstrate its effectiveness. MENet outperforms other representative TAD methods on ActivityNet-1.3 and THUMOS-14.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 13509
EP  - 13518
DO  - 10.1109/ICCV51070.2023.01247
AN  - WOS:001169499005090
AD  - Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China
Y2  - 2024-04-06
ER  -

TY  - JOUR
AU  - Kim, Jihwan
AU  - Lee, Miso
AU  - Heo, Jae-Pil
TI  - Self-Feedback DETR for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Detection (TAD) is challenging but fundamental for real-world video applications. Recently, DETR-based models have been devised for TAD but have not performed well yet. In this paper, we point out the problem in the self-attention of DETR for TAD; the attention modules focus on a few key elements, called temporal collapse problem. It degrades the capability of the encoder and decoder since their self-attention modules play no role. To solve the problem, we propose a novel framework, Self-DETR, which utilizes cross-attention maps of the decoder to reactivate self-attention modules. We recover the relationship between encoder features by simple matrix multiplication of the cross-attention map and its transpose. Likewise, we also get the information within decoder queries. By guiding collapsed self-attention maps with the guidance map calculated, we settle down the temporal collapse of self-attention modules in the encoder and decoder. Our extensive experiments demonstrate that Self-DETR resolves the temporal collapse problem by keeping high diversity of attention over all layers. Moreover, it is validated that our simple framework achieves a new state-of-the-art performance on THUMOS14 and outperforms all the DETR-based approaches on ActivityNet-v1.3.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2308.10570
AN  - PPRN:81821444
AD  - Sungkyunkwan Univ, Seoul, South Korea
Y2  - 2023-09-08
ER  -

TY  - JOUR
AU  - Kim, Jihwan
AU  - Lee, Miso
AU  - Heo, Jae-Pil
TI  - Long-term Pre-training for Temporal Action Detection with Transformers
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.13152
AN  - PPRN:91525400
AD  - Sungkyunkwan Univ, Seoul, South Korea
Y2  - 2024-09-26
ER  -

TY  - CPAPER
AU  - Kim, Jihwan
AU  - Lee, Miso
AU  - Heo, Jae-Pil
A1  - IEEE
TI  - Self-Feedback DETR for Temporal Action Detection
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - Temporal Action Detection (TAD) is challenging but fundamental for real-world video applications. Recently, DETR-based models have been devised for TAD but have not performed well yet. In this paper, we point out the problem in the self-attention of DETR for TAD; the attention modules focus on a few key elements, called temporal collapse problem. It degrades the capability of the encoder and decoder since their self-attention modules play no role. To solve the problem, we propose a novel framework, Self-DETR, which utilizes cross-attention maps of the decoder to reactivate self-attention modules. We recover the relationship between encoder features by simple matrix multiplication of the cross-attention map and its transpose. Likewise, we also get the information within decoder queries. By guiding collapsed self-attention maps with the guidance map calculated, we settle down the temporal collapse of self-attention modules in the encoder and decoder. Our extensive experiments demonstrate that Self-DETR resolves the temporal collapse problem by keeping high diversity of attention over all layers. Moreover, it is validated that our simple framework achieves a new state-of-the-art performance on THUMOS14 and outperforms all the DETR-based approaches on ActivityNet-v1.3.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 10252
EP  - 10262
DO  - 10.1109/ICCV51070.2023.00944
AN  - WOS:001169499002065
AD  - Sungkyunkwan Univ, Seoul, South Korea
Y2  - 2024-04-06
ER  -

TY  - CPAPER
AU  - Zhang, Hongcheng
AU  - Zhao, Xu
A1  - IEEE
TI  - SPATIO-TEMPORAL MOTION AGGREGATION NETWORK FOR VIDEO ACTION DETECTION
T2  - 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP)
M3  - Proceedings Paper
CP  - 47th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - Singapore, SINGAPORE
AB  - Recognizing action patterns and detecting action instances are vital for spatial temporal action detection task, which aims to recognize the actions of interest in untrimmed videos and localize them in both space and time. The mainstream action tubelet detectors, however, ignore the conflicts in features between localization and classification, and use localization features for temporal modeling, which leads to ineffective action classification. In this paper, we propose the Spatio-Temporal Motion Aggregation mechanism for integrating the local motion feature from a short term snippet and the longer spatio-temporal information to predict the action category. We design the Class-Agnostic Center Localization module to perform action instance center localization in the Class-Agnostic manner. Besides, Movement and Size Regression is proposed for movement estimation and spatial extent detection by using Gaussian kernels to encode training samples. These three modules work together to generate the tubelet detection results, which could be further linked to yield video-level tubes with a matching strategy. Our detector achieves the state-of-the-art performance in both frame-mAP and video-mAP metrics, on the UCF-24 and JHMDB datasets.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-6654-0540-9
DA  - 2022 
PY  - 2022
SP  - 2180
EP  - 2184
DO  - 10.1109/ICASSP43922.2022.9746817
AN  - WOS:000864187902091
AD  - Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China
Y2  - 2023-01-06
ER  -

TY  - JOUR
AU  - Deng, Bowen
AU  - Zhao, Shuangliang
AU  - Liu, Dongchang
TI  - TadML: A fast temporal action detection with Mechanics-MLP
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Detection(TAD) is a crucial but challenging task in video understanding. It is aimed at detecting both the type and start-end frame for each action instance in a long, untrimmed video. Most current models adopt both RGB and Optical-Flow streams for the TAD task. Thus, original RGB frames must be converted manually into Optical-Flow frames with additional computation and time cost, which is an obstacle to achieve real-time processing. At present, many models adopt two-stage strategies, which would slow the inference speed down and complicatedly tuning on proposals generating. By comparison, we propose a one-stage anchor-free temporal localization method with RGB stream only, in which a novel Newtonian Mechanics-MLP architecture is established. It has comparable accuracy with all existing state-of-the-art models, while surpasses the inference speed of these methods by a large margin. The typical inference speed in this paper is astounding 4.44 video per second on THUMOS14. In applications, because there is no need to convert optical flow, the inference speed will be faster. It also proves that MLP has great potential in downstream tasks such as TAD. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2206.02997
AN  - PPRN:87504021
AD  - Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China
AD  - Guangxi Univ, Nanning 69121, Peoples R China
M2  - Guangxi Univ
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Yang, Wenfei
AU  - Zhang, Tianzhu
AU  - Yu, Xiaoyuan
AU  - Qi, Tian
AU  - Zhang, Yongdong
AU  - Wu, Feng
A1  - IEEE COMP SOC
TI  - Uncertainty Guided Collaborative Training for Weakly Supervised Temporal Action Detection
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Weakly supervised temporal action detection aims to localize temporal boundaries of actions and identify their categories simultaneously with only video-level category labels during training. Among existing methods, attention based methods have achieved superior performance by separating action and non-action segments. However, without the segment-level ground-truth supervision, the quality of the attention weight hinders the performance of these methods. To alleviate this problem, we propose a novel Uncertainty Guided Collaborative Training (UGCT) strategy, which mainly includes two key designs: (1) The first design is an online pseudo label generation module, in which the RGB and FLOW streams work collaboratively to learn from each other. (2) The second design is an uncertainty aware learning module, which can mitigate the noise in the generated pseudo labels. These two designs work together to promote the model performance effectively and efficiently by imposing pseudo label supervision on attention weight learning. Experimental results on three state-of-the-art attention based methods demonstrate that the proposed training strategy can significantly improve the performance of these methods, e.g., more than 4% for all three methods in terms of mAP@IoU=0.5 on the THUMOS14 dataset.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 53
EP  - 63
DO  - 10.1109/CVPR46437.2021.00012
AN  - WOS:000739917300006
AD  - Univ Sci & Technol China, Hefei, Anhui, Peoples R China
AD  - Huawei Cloud, Shenzhen, Peoples R China
Y2  - 2022-02-03
ER  -

TY  - JOUR
AU  - Wang, Xiang
AU  - Zhang, Huaxin
AU  - Zhang, Shiwei
AU  - Gao, Changxin
AU  - Shao, Yuanjie
AU  - Sang, Nong
TI  - Context-aware Proposal Network for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This technical report presents our first place winning solution for temporal action detection task in CVPR-2022 AcitivityNet Challenge. The task aims to localize tem-poral boundaries of action instances with specific classes in long untrimmed videos. Recent mainstream attempts are based on dense boundary matchings and enumerate all possible combinations to produce proposals. We ar-gue that the generated proposals contain rich contextual information, which may benefits detection confidence pre-diction. To this end, our method mainly consists of the following three steps: 1) action classification and feature extraction by Slowfast [10], CSN [20], TimeSformer [4], TSP [1], I3D-flow [7], VGGish-audio [11], TPN [33] and ViViT [3]; 2) proposal generation. Our proposed Context-aware Proposal Network (CPN) builds on top of BMN [16], GTAD [32] and PRN [26] to aggregate contextual informa-tion by randomly masking some proposal features. 3) action detection. The final detection prediction is calculated by as-signing the proposals with corresponding video-level clas-sification results. Finally, we ensemble the results under different feature combination settings and achieve 45.8% performance on the test set, which improves the champion result in CVPR-2021 ActivityNet Challenge [26] by 1.1% in terms of average mAP.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2206.09082
AN  - PPRN:12383750
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automation, Lab Image Proc & Intelligent Control, Hubei, Peoples R China
AD  - Alibaba Grp, Hubei, Peoples R China
AD  - Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Hubei, Peoples R China
M2  - Huazhong Univ Sci & Technol
M2  - Alibaba Grp
M2  - Huazhong Univ Sci & Technol
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Zeng, Yingsen
AU  - Zhong, Yujie
AU  - Feng, Chengjian
AU  - Ma, Lin
TI  - UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal Action Detection (TAD) focuses on detecting predefined actions, while Moment Retrieval (MR) aims to identify the events described by open-ended natural language within untrimmed videos. Despite that they focus on different events, we observe they have a significant connection. For instance, most descriptions in MR involve multiple actions from TAD. In this paper, we aim to investigate the potential synergy between TAD and MR. Firstly, we propose a unified architecture, termed Uni fied M oment D etection ( UniMD ), for both TAD and MR. It transforms the inputs of the two tasks, namely actions for TAD or events for MR, into a common embedding space, and utilizes two novel query-dependent decoders to generate a uniform output of classification score and temporal segments. Secondly, we explore the efficacy of two task fusion learning approaches, pre-training and co-training, in order to enhance the mutual benefits between TAD and MR. Extensive experiments demonstrate that the proposed task fusion learning scheme enables the two tasks to help each other and outperform the separately trained counterparts. Impressively, UniMD achieves state-of-the-art results on three paired datasets Ego4D, Charades-STA, and ActivityNet. Our code is available at https://github.com/yingsen1/UniMD.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.04933
AN  - PPRN:88446600
AD  - Meituan Inc, Hong Kong, Peoples R China
M2  - Meituan Inc
Y2  - 2024-07-23
ER  -

TY  - JOUR
AU  - Liu, Shuming
AU  - Zhang, Chen-Lin
AU  - Zhao, Chen
AU  - Ghanem, Bernard
TI  - End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recently, temporal action detection (TAD) has seen significant performance improvement with end-to-end training. However, due to the memory bottleneck, only models with limited scales and limited data volumes can afford end-to-end training, which inevitably restricts TAD performance. In this paper, we reduce the memory consumption for end-to-end training, and manage to scale up the TAD backbone to 1 billion parameters and the input video to 1,536 frames, leading to significant detection performance. The key to our approach lies in our proposed temporal-informative adapter (TIA), which is a novel lightweight module that reduces training memory. Using TIA, we free the humongous backbone from learning to adapt to the TAD task by only updating the parameters in TIA. TIA also leads to better TAD representation by temporally aggregating context from adjacent frames throughout the backbone. We evaluate our model across four representative datasets. Owing to our efficient design, we are able to train end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the first end-to-end model to outperform the best feature-based methods. Code is available at https://github.com/sming256/AdaTAD.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2311.17241
AN  - PPRN:88600435
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
AD  - 4Paradigm Inc, Singapore, Singapore
M2  - King Abdullah Univ Sci & Technol KAUST
M2  - 4Paradigm Inc
Y2  - 2024-04-30
ER  -

TY  - CPAPER
AU  - Sui, Lin
AU  - Zhang, Chen-Lin
AU  - Gu, Lixin
AU  - Han, Feng
A1  - IEEE
TI  - A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector
T2  - 2023 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - 23rd IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction. Our code will be released at https://github.com/4paradigm-CV/SE-STAD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-6654-9346-8
DA  - 2023 
PY  - 2023
SP  - 5988
EP  - 5997
DO  - 10.1109/WACV56688.2023.00594
AN  - WOS:000971500206012
AD  - 4Paradigm Inc, Beijing, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - DataElem Inc, Beijing, Peoples R China
M2  - 4Paradigm Inc
M2  - DataElem Inc
Y2  - 2023-07-22
ER  -

TY  - CPAPER
AU  - Phan, Thinh
AU  - Vo, Khoa
AU  - Let, Duy
AU  - Doretto, Gianfranco
AU  - Adjeroht, Donald
AU  - Le, Ngan
A1  - IEEE Comp Soc
TI  - ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection
T2  - 2024 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION, WACV 2024
M3  - Proceedings Paper
CP  - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationship between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEE-TAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories. Code is available at https://github.com/UARK-AICV/ZEETAD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 979-8-3503-1892-0
SN  - 979-8-3503-1893-7
DA  - 2024 
PY  - 2024
SP  - 7031
EP  - 7040
DO  - 10.1109/WACV57701.2024.00689
AN  - WOS:001222964607018
AD  - Univ Arkansas, AICV Lab, Fayetteville, AR 72701 USA
AD  - FPT Software AI Ctr, Hanoi, Vietnam
AD  - West Virginia Univ, Morgantown, WV 26506 USA
M2  - FPT Software AI Ctr
Y2  - 2025-03-05
ER  -

TY  - JOUR
AU  - Zhong, Jia-Xing
AU  - Li, Nannan
AU  - Kong, Weijie
AU  - Zhang, Tao
AU  - H. Li, Thomas
AU  - Li, Ge
TI  - Step-by-step Erasion, One-by-one Collection: A Weakly Supervised Temporal Action Detector
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised temporal action detection is a Herculean task in understanding untrimmed videos, since no supervisory signal except the video-level category label is available on training data. Under the supervision of category labels, weakly supervised detectors are usually built upon classifiers. However, there is an inherent contradiction between classifier and detector; i.e., a classifier in pursuit of high classification performance prefers top-level discriminative video clips that are extremely fragmentary, whereas a detector is obliged to discover the whole action instance without missing any relevant snippet. To reconcile this contradiction, we train a detector by driving a series of classifiers to find new actionness clips progressively, via step-by-step erasion from a complete video. During the test phase, all we need to do is to collect detection results from the one-by-one trained classifiers at various erasing steps. To assist in the collection process, a fully connected conditional random field is established to refine the temporal localization outputs. We evaluate our approach on two prevailing datasets, THUMOS'14 and ActivityNet. The experiments show that our detector advances state-of-the-art weakly supervised temporal action detection results, and even compares with quite a few strongly supervised methods.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.02929
AN  - PPRN:22700296
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Girmaji, Rohit
AU  - Jain, Siddharth
AU  - Beri, Bhav
AU  - Bansal, Sarthak
AU  - Gandhi, Vineet
TI  - Minimalistic Video Saliency Prediction via Efficient Decoder & Spatio Temporal Action Cues
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper introduces ViNet-S, a 36MB model based on the ViNet architecture with a U-Net design, featuring a lightweight decoder that significantly reduces model size and parameters without compromising performance. Additionally, ViNet-A (148MB) incorporates spatio-temporal action localization (STAL) features, differing from traditional video saliency models that use action classification backbones. Our studies show that an ensemble of ViNet-S and ViNet-A, by averaging predicted saliency maps, achieves state-of-the-art performance on three visual-only and six audio-visual saliency datasets, outperforming transformer-based models in both parameter efficiency and real-time performance, with ViNet-S reaching over 1000fps.
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2502.00397
AN  - PPRN:121095560
AD  - IIIT Hyderabad, CVIT, Hyderabad, India
Y2  - 2025-03-22
ER  -

TY  - JOUR
AU  - Fang, Qihang
AU  - Tang, Chengcheng
AU  - Ma, Shugao
AU  - Yang, Yanchao
TI  - BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Trainin
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Skeleton-based motion representations are robust for action localization and understanding for their invariance to perspective, lighting, and occlusion, compared with images. Yet, they are often ambiguous and incomplete when taken out of context, even for human annotators. As infants discern gestures before associating them with words, actions can be conceptualized before being grounded with labels. Therefore, we propose the first unsupervised pre-training framework, Boundary-Interior Decoding (BID) , that partitions a skeleton-based motion sequence into discovered semantically meaningful pre -action segments. By fine -tuning our pre-training network with a small number of annotated data, we show results out-performing SOTA methods by a large margin.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2403.07354
AN  - PPRN:88709809
AD  - Univ Hong Kong, Hong Kong, Peoples R China
AD  - Meta Real Labs, Beijing, Peoples R China
M2  - Meta Real Labs
Y2  - 2024-05-18
ER  -

TY  - JOUR
AU  - Mo, Sicheng
AU  - Mu, Fangzhou
AU  - Li, Yin
TI  - A Simple Transformer-Based Model for Ego4D Natural Language Queries Challenge
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This report describes Badgers@UW-Madison, our submission to the Ego4D Natural Language Queries (NLQ) Challenge. Our solution inherits the point-based event representation from our prior work on temporal action localization, and develops a Transformer-based model for video grounding. Further, our solution integrates several strong video features including SlowFast, Omnivore and EgoVLP. Without bells and whistles, our submission based on a single model achieves 12.64% Mean R@1 and is ranked 2nd on the public leaderboard. Meanwhile, our method garners 28.45% (18.03%) R@5 at tIoU=0.3 (0.5), surpassing the top-ranked solution by up to 5.5 absolute percentage points.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2211.08704
AN  - PPRN:22951958
AD  - Univ Wisconsin Madison, Madison, WI 53706, USA
M2  - Univ Wisconsin Madison
Y2  - 2022-12-08
ER  -

TY  - BOOK
AU  - Moniruzzaman, Md.
Z2  -  
TI  - Human Action Analysis From Cameras and Wearable Sensors: Recognition, Localization, Anticipation, Online Detection, and Pose Estimation
M3  - Dissertation/Thesis
SN  - 9798302178398
DA  - 2024 
PY  - 2024
AN  - PQDT:120967512
AD  - State University of New York at Stony Brook, Computer Science, New York, United States
M2  - State University of New York at Stony Brook
ER  -

TY  - JOUR
AU  - Ruan, Xianghui
AU  - Dai, Meng
AU  - Chen, Zhuokun
AU  - You, Zeng
AU  - Zhang, Yaowen
AU  - Li, Yuanqing
AU  - Dou, Zulin
AU  - Tan, Mingkui
TI  - Temporal Micro-Action Localization for Videofluoroscopic Swallowing Study
T2  - IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS
M3  - Article
AB  - Videofluoroscopic swallowing study (VFSS) visualizes the swallowing movement by using X-ray fluoroscopy, which is the most widely used method for dysphagia examination. To better facilitate swallowing assessment, the temporal parameter is one of the most important indicators. However, most information of that acquire is hand-crafted and elaborated, which is time-consuming and difficult to ensure objectivity and accuracy. In this article, we propose to formulate this task as a temporal action localization task and solve it using deep neural networks. However, the action of VFSS has the following characteristics such as small motion targets, small action amplitudes, large sample variances, short duration, and variations in duration. Furthermore, all existing methods often rely on daily behaviors, which makes locating and recognizing micro-actions more challenging. To address the above issues, we first collect and annotate the VFSS micro-action dataset, which includes 847 VFSS data from 71 subjects, due to the lack of benchmarks. We then introduce a coarse-to-fine mechanism to handle the short and repeated nature of micro-actions, which can significantly enhancing micro-action localization accuracy. Moreover, we propose a Variable-Size Window Generator method, which improves the model's characterization performance and addresses the issue of different action timings, leading to further improvements in localization accuracy. The results of our experiments demonstrate the superiority of our method, with significantly improved performance (46.10% vs. 37.70%).
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2168-2194
SN  - 2168-2208
DA  - 2023 DEC
PY  - 2023
VL  - 27
IS  - 12
SP  - 5904
EP  - 5913
DO  - 10.1109/JBHI.2023.3313255
AN  - WOS:001147165700020
AD  - South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China
AD  - Guizhou Minzu Univ, Guiyang 550025, Peoples R China
AD  - Pazhou Lab, Guangzhou 510320, Peoples R China
AD  - Sun Yat Sen Univ, Affiliated Hosp 3, Dept Rehabil Med, Guangzhou 510630, Peoples R China
AD  - South China Univ Technol, Sch Future Technol, Guangzhou 510006, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 510320, Peoples R China
AD  - South China Univ Technol, Coll Automat Sci & Engn, Guangzhou 510640, Peoples R China
Y2  - 2024-02-03
ER  -

TY  - JOUR
AU  - Shin, Yoonho
AU  - Park, Sanghoon
AU  - Han, Youngsub
AU  - Jeon, Byoung-Ki
AU  - Lee, Soonyoung
AU  - Kang, Byung Jun
TI  - Soccer-CLIP: Vision Language Model for Soccer Action Spotting
T2  - IEEE ACCESS
M3  - Article
AB  - In the rapidly advancing field of computer vision, the application of multimodal models-specifically, vision-language frameworks-has shown substantial promise for complex tasks such as video-based action spotting. This paper introduces Soccer-CLIP, a vision-language model specially designed for soccer action spotting. Soccer-CLIP incorporates an innovative domain-specific prompt engineering strategy, leveraging large language models (LLMs) to refine textual representations for precise alignment with soccer-specific actions. Our model integrates both visual and textual features to enhance recognition accuracy of critical soccer events. With the temporal augmentation techniques devised for input videos, Soccer-CLIP builds upon existing methodologies to address the inherent challenges of temporally sparse event annotations within video sequences. Evaluations on the SoccerNet Action Spotting benchmark demonstrate that Soccer-CLIP outperforms previous state-of-the-art models, exploring the effectiveness of our model's capacity to capture domain-specific contextual nuances. This work represents a significant advancement in automated sports analysis, providing a robust and adaptable framework for broader applications in video recognition and temporal action localization tasks.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2025 
PY  - 2025
VL  - 13
SP  - 44354
EP  - 44365
DO  - 10.1109/ACCESS.2025.3549293
AN  - WOS:001445065100010
AD  - LG UPlus, Seoul 07795, South Korea
AD  - LG AI Res, Seoul 07336, South Korea
M2  - LG UPlus
M2  - LG AI Res
Y2  - 2025-03-21
ER  -

TY  - CPAPER
AU  - Xu, Lei
AU  - Liu, Jiexi
AU  - Chen, Bo
ED  - Peng, C
ED  - Sun, J
TI  - Large Receptive Field Boundary Matching Networks for Generating Better Proposals
T2  - 2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)
M3  - Proceedings Paper
CP  - 40th Chinese Control Conference (CCC)
CL  - Shanghai, PEOPLES R CHINA
AB  - Temporal Action Detection is a research hotspot in Video Understanding. It is necessary not only to give the specific moments of the beginning and end of each action instance in the video, but also to give the category of the action instance. At present, most of the methods in temporal action detection are divided into two steps: The video is divided into a series of video clips to determine whether each clip is an action instance, and the fragments that are not an action instance are deleted; Segments that may be action instances are then classified to get the fmal result. At present, the difficulties in action detection research mainly include the following two points: 1) The boundary is not clear. Different from action recognition, action detection requires precise positioning, but an action in life is often not very defmite. It is also the reason why the mean average precision(mAP) of action detection is low at present. 2) The time span is large. In life, an action often spans a very long scale. Short movements such as waving can last for a few seconds, while long movements such as rock climbing or cycling can last for tens of minutes, which makes it extremely difficult for us to extract proposals. In view of the above difficulties, this paper proposes a large receptive field boundary matching network(LRFBMN) model which takes advantage of the relationship between proposals to improve the accuracy of proposal generation. The model is mainly divided into two parts: 1) Clipping feature map is processed by large kernel convolution, and then proposal feature is generated by ROI-pooling; 2) The proposals are arranged in a certain order to form a fixed graph, and the information exchange between graph nodes is realized by using convolution with dilation. Through experiments, this model is 2.35% higher than baseline and 1.06% higher than state-of-the-art in THUMOS14 data set.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-2927
SN  - 978-988-15638-0-4
DA  - 2021 
PY  - 2021
SP  - 7196
EP  - 7200
AN  - WOS:000931046707054
AD  - Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China
AD  - Key Lab Complex Syst Intelligent Control & Decis, Beijing 100081, Peoples R China
M2  - Key Lab Complex Syst Intelligent Control & Decis
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Li, Ping
AU  - Cao, Jiachen
AU  - Ye, Xingchao
TI  - Prototype contrastive learning for point-supervised temporal action detection
T2  - EXPERT SYSTEMS WITH APPLICATIONS
M3  - Article
AB  - Detecting temporal actions in a video with only single-frame annotation in each action instance or segment, a.k.a., point-level supervision, has emerged as a more challenging task, compared to fully-supervised setting where per-frame annotations are available. Generally, it faces the label sparsity problem and the serious class -imbalance problem which are not fully explored in previous works. To address them, this paper develops an efficient pseudo-label generation approach to yield more positive samples and negative samples for providing supervision, i.e., the Prototype Contrastive Learning (PCL) based point-supervised temporal action detection framework. PCL aims at explicitly discovering the class relations between labeled and unlabeled frames by adopting prototype learning, and generates pseudo labels by estimating the semantic similarity of pair-wise frames in the embedding space. Meanwhile, it imposes the class relation constraint onto the action and background prototypes by introducing contrastive representation learning, i.e., the prototypes in distinct classes are pushed away and those within the same class are pulled closer. This allows learning the discriminative representations of prototypes that well comply with the data distribution of video frames. These prototype representations are treated as the hidden pattern proxies of different classes, and their semantic relations help to generate pseudo labels for unlabeled frames. Empirical studies on three benchmarks including GTEA, BEOID, and THUMOS14, have demonstrated the favorable performance of the proposed method.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0957-4174
SN  - 1873-6793
DA  - 2023 MAR 1
PY  - 2023
VL  - 213
C7  - 118965
DO  - 10.1016/j.eswa.2022.118965
AN  - WOS:000877842700009
C6  - OCT 2022
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2022-11-13
ER  -

TY  - CPAPER
AU  - Yu, Jun
AU  - Wang, Leilei
AU  - Lu, Renjie
AU  - Yang, Shuoping
AU  - Li, Renda
AU  - Wang, Lei
AU  - Chen, Minchuan
AU  - Zhu, Qingying
AU  - Wang, Shaojun
AU  - Xiao, Jing
A1  - ACM
TI  - Relative Boundary Modeling: A High-Resolution Cricket Bowl Release Detection Framework with I3D Features
T2  - PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS, MMSPORTS 2023
M3  - Proceedings Paper
CP  - 6th ACM International Workshop on Multimedia Content Analysis in Sports (MMSports)
CL  - Ottawa, CANADA
AB  - Cricket Bowl Release Detection aims to segment specific portions of bowl release actions occurring in multiple videos, with a focus on detecting the entire time window of this action. Unlike traditional detection tasks that identify action categories at a specific moment, this task involves identifying events that typically span around 100 frames and require recognizing all instances of the bowl release action in the video. Strictly speaking, this task falls under a branch of temporal action detection. With the advancement of deep neural networks, recent works have proposed deep learning-based approaches to address this task. However, due to the challenge of unclear action boundaries in videos, many existing methods perform poorly on the DeepSportradar Cricket Bowl Release Dataset. To more accurately identify specific portions of the bowl release action in videos, we adopt a one-stage architecture based on Relative Boundary Modeling. Specifically, our method consists of three stages. In the first stage, we use the Inflated 3D ConvNet (I3D) model to extract spatio-temporal features from the input videos. In the second stage, we utilize Temporal Action Detection with Relative Boundary Modeling (TriDet) to model the boundaries of the bowl release action's specific portions based on the relative relationships between different time moments, thereby predicting the action's time window. Lastly, as the target events typically span around 100 frames and the predicted time windows may exhibit overlapping regions based on confidence scores, we implement a post-processing step to merge and filter these outputs, resulting in the final submission results. We conducted extensive experiments to demonstrate that our proposed method achieves superior performance. Additionally, we evaluated the training techniques of existing approaches. Our proposed method achieves a PQ score of 0.519, an SQ score of 0.822, and an RQ score of 0.632 on the challenge set of the DeepSportradar Cricket Bowl Release Dataset. Through this approach, our team, USTC_IAT_United, won the third place in the first phase of the DeepSportradar Cricket Bowl Release Challenge.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0269-3
DA  - 2023 
PY  - 2023
SP  - 151
EP  - 159
DO  - 10.1145/3606038.3616167
AN  - WOS:001148097700020
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Ping An Technol, Shenzhen, Peoples R China
M2  - Ping An Technol
Y2  - 2024-02-10
ER  -

TY  - CPAPER
AU  - Manh Tung Tran
AU  - Minh Quan Vu
AU  - Ngoc Duong Hoang
AU  - Khac-Hoai Nam Bui
A1  - IEEE
TI  - An Effective Temporal Localization Method with Multi-View 3D Action Recognition for Untrimmed Naturalistic Driving Videos
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Naturalistic driving studies with computer vision techniques have become an emergent research issue. The objective is to classify the distracted behavior actions by drivers. Specifically, this issue is regarded as temporal action localization (TAL) of untrimmed videos, which is a challenging task in the research field of video analysis. Particularly, TAL remains as one of the most challenging unsolved problems in computer vision that requires not only the recognition of action but the localization of the start and end times of each action. Most state-of-the-art approaches adopt complex architectures, which are expensive training and inefficient inference time. In this study, we propose a new framework for untrimmed naturalistic driving videos by utilizing the results from 3D action recognition with video clip classification for short temporal and spatial correlation. Then, simple post-processing based on data-driven is presented for long temporal correlation in untrimmed videos. The proposed method is evaluated on the AI City Challenge 2022 dataset for Naturalistic Driving Action Recognition. Accordingly, our method achieves the top 1 on the public leaderboard of the challenge.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3167
EP  - 3172
DO  - 10.1109/CVPRW56347.2022.00357
AN  - WOS:000861612703030
AD  - Viettel Grp, Viettel Cyperspace Ctr, Hanoi, Vietnam
M2  - Viettel Grp
Y2  - 2022-12-07
ER  -

TY  - JOUR
AU  - Gaidon, Adrien
AU  - Harchaoui, Zaid
AU  - Schmid, Cordelia
TI  - Temporal Localization of Actions with Actoms
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - We address the problem of localizing actions, such as opening a door, in hours of challenging video data. We propose a model based on a sequence of atomic action units, termed "actoms," that are semantically meaningful and characteristic for the action. Our actom sequence model (ASM) represents an action as a sequence of histograms of actom-anchored visual features, which can be seen as a temporally structured extension of the bag-of-features. Training requires the annotation of actoms for action examples. At test time, actoms are localized automatically based on a nonparametric model of the distribution of actoms, which also acts as a prior on an action's temporal structure. We present experimental results on two recent benchmarks for action localization "Coffee and Cigarettes" and the "DLSBP" dataset. We also adapt our approach to a classification-by-localization set-up and demonstrate its applicability on the challenging "Hollywood 2" dataset. We show that our ASM method outperforms the current state of the art in temporal action localization, as well as baselines that localize actions with a sliding window method.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2013 NOV
PY  - 2013
VL  - 35
IS  - 11
SP  - 2782
EP  - 2795
DO  - 10.1109/TPAMI.2013.65
AN  - WOS:000324830900016
AD  - Xerox Res Ctr Europe, F-38240 Meylan, France
AD  - INRIA Grenoble Rhone Alpes, F-38330 Montbonnot St Martin, France
M2  - INRIA Grenoble Rhone Alpes
Y2  - 2013-11-05
ER  -

TY  - CPAPER
AU  - Zhou, Jingqiu
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Liu, Si
AU  - Li, Hongsheng
A1  - IEEE
TI  - Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - The task of weakly supervised temporal action localization targets at generating temporal boundaries for actions of interest, meanwhile the action category should also be classified. Pseudo-label-based methods, which serve as an effective solution, have been widely studied recently. However, existing methods generate pseudo labels during training and make predictions during testing under different pipelines or settings, resulting in a gap between training and testing. In this paper, we propose to generate high-quality pseudo labels from the predicted action boundaries. Nevertheless, we note that existing post-processing, like NMS, would lead to information loss, which is insufficient to generate high-quality action boundaries. More importantly, transforming action boundaries into pseudo labels is quite challenging, since the predicted action instances are generally overlapped and have different confidence scores. Besides, the generated pseudo-labels can be fluctuating and inaccurate at the early stage of training. It might repeatedly strengthen the false predictions if there is no mechanism to conduct self-correction. To tackle these issues, we come up with an effective pipeline for learning better pseudo labels. Firstly, we propose a Gaussian weighted fusion module to preserve information of action instances and obtain high-quality action boundaries. Second, we formulate the pseudo-label generation as an optimization problem under the constraints in terms of the confidence scores of action instances. Finally, we introduce the idea of. pseudo labels, which enables the model with the ability of self-correction. Our method achieves superior performance to existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains of 1.9% on THUMOS14 and 3.7% on ActivityNet1.3 in terms of average mAP. Our code is available at https://github.com/zhou745/GauFuse_WSTAL.git.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 23003
EP  - 23012
DO  - 10.1109/CVPR52729.2023.02203
AN  - WOS:001062531307032
AD  - Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Xidian Univ, Xian, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
AD  - Beihang Univ, Beijing, Peoples R China
M2  - Ctr Perceptual & Interact Intelligence
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Zhang, Hao
AU  - Li, Zheng
AU  - Yang, Jiahui
AU  - Wang, Xin
AU  - Guo, Caili
AU  - Feng, Chunyan
TI  - Revisiting Hard Negative Mining in Contrastive Learning for Visual Understanding
T2  - ELECTRONICS
M3  - Article
AB  - Efficiently mining and distinguishing hard negatives is the key to Contrastive Learning (CL) in various visual understanding tasks. By properly emphasizing the penalty of hard negatives, Hard Negative Mining (HNM) can improve the CL performance. However, there is no method to quantitatively analyze the penalty strength of hard negatives, which makes training difficult to converge. In this paper, we propose a method for measuring and controlling the penalty strength. We first define a penalty strength metric to provides a quantitative analysis tool for HNM. Then, we propose a Triplet loss with Penalty Strength Control (T-PSC), which can balance the penalty strength of hard negatives and the difficulty of model optimization. In order to verify the effectiveness of the proposed T-PSC method in different modalities, we applied it to two visual understanding tasks: Image-Text Retrieval (ITR) for multi-model processing, and Temporal Action Localization (TAL) for video processing. T-PSC can be applied to existing ITR and TAL models in a plug-and-play manner without any changes. Experiments combined with existing models show that a reasonable control of the penalty strength can speed up training and improve the performance on higher-level tasks.
PU  - MDPI
PI  - BASEL
PA  - MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2023 DEC
PY  - 2023
VL  - 12
IS  - 23
C7  - 4884
DO  - 10.3390/electronics12234884
AN  - WOS:001116177600001
AD  - Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Key Lab Network Syst Architecture & Conver, Beijing 100876, Peoples R China
Y2  - 2023-12-19
ER  -

TY  - JOUR
AU  - Zhou, Jingqiu
AU  - Huang, Linjiang
AU  - Wang, Liang
AU  - Liu, Si
AU  - Li, Hongsheng
TI  - Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The task of weakly supervised temporal action localization targets at generating temporal boundaries for actions of interest, meanwhile the action category should also be classified. Pseudo-label-based methods, which serve as an effective solution, have been widely studied recently. However, existing methods generate pseudo labels during training and make predictions during testing under different pipelines or settings, resulting in a gap between training and testing. In this paper, we propose to generate high-quality pseudo labels from the predicted action boundaries. Nevertheless, we note that existing post-processing, like NMS, would lead to information loss, which is insufficient to generate high-quality action boundaries. More importantly, transforming action boundaries into pseudo labels is quite challenging, since the predicted action instances are generally overlapped and have different confidence scores. Besides, the generated pseudo-labels can be fluctuating and inaccurate at the early stage of training. It might repeatedly strengthen the false predictions if there is no mechanism to conduct self-correction. To tackle these issues, we come up with an effective pipeline for learning better pseudo labels. Firstly, we propose a Gaussian weighted fusion module to preserve information of action instances and obtain high-quality action boundaries. Second, we formulate the pseudo-label generation as an optimization problem under the constraints in terms of the confidence scores of action instances. Finally, we introduce the idea of $Delta$ pseudo labels, which enables the model with the ability of self-correction. Our method achieves superior performance to existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains of 1.9% on THUMOS14 and 3.7% on ActivityNet1.3 in terms of average mAP.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.07978
AN  - PPRN:63587208
AD  - Chinese Univ Hong Kong, CUHK, Sense Time Joint Lab, Hong Kong, Peoples R China
AD  - Ctr Perceptual & Interact Intelligence, Hong Kong, Peoples R China
AD  - Xidian Univ, Xian, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Beijing, Peoples R China
AD  - Beihang Univ, Beijing, Peoples R China
M2  - Chinese Univ Hong Kong
M2  - Ctr Perceptual & Interact Intelligence
M2  - Xidian Univ
M2  - Beihang Univ
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Gan, Ming-Gang
AU  - Zhang, Yan
TI  - Content Temporal Relation Network for temporal action proposal generation
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Temporal action proposal generation is an essential step for untrimmed video analysis and gains much attention from academia. However, most of the prior works predict the confidence score of each proposal separately and neglect the relations between proposals, limiting their performance. In this work, we design a novel Content Temporal Relation Network (CTRNet) to generate temporal action proposals by exploring the content and temporal semantic relations between proposals simultaneously. Specifically, we design a proposal feature map generation layer to convert the temporal semantic relations of proposals into spatial relations. Based on the proposal feature map, we propose a content-temporal relation module, which applies a novel adaptive -dilated convolution to model the temporal semantic relations between proposals and designs a content-adaptive convolution operation to explore the content semantic relation between proposals. Considering the temporal and content semantic relations between proposals, CTRNet has learned discriminative proposal features to improve performance. Extensive experiments are performed on two mainstream temporal action detection datasets, and CTRNet significantly outperforms the previous state-of-the-art methods. The codes are available at https://github.com/YanZhang-bit/CTRNet.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2024 MAY
PY  - 2024
VL  - 149
C7  - 110245
DO  - 10.1016/j.patcog.2023.110245
AN  - WOS:001155008000001
C6  - JAN 2024
AD  - Beijing Inst Technol, Sch Automat, State Key Lab Intelligent Control & Decis Complex, Beijing 100081, Peoples R China
Y2  - 2024-02-09
ER  -

TY  - JOUR
AU  - Wu, Tao
AU  - Cao, Mengqi
AU  - Gao, Ziteng
AU  - Wu, Gangshan
AU  - Wang, Limin
TI  - STMixer: A One-Stage Sparse Action Detector
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - Traditional video action detectors typically adopt the two-stage pipeline, where a person detector is first employed to generate actor boxes and then 3D RoIAlign is used to extract actor-specific features for action recognition. This detection paradigm requires multi-stage training and inference, and the feature sampling is only constrained inside the box, failing to effectively leverage richer context information outside. Recently, several query-based action detectors have been proposed to predict action instances in an end-to-end manner. However, they still lack adaptability in feature sampling and decoding, thus suffering from the issues of inferior performance or slower convergence. In this paper, we propose two core designs for a more flexible one-stage sparse action detector. First, we present a query-based adaptive feature sampling module, which endows the detector with the flexibility of mining a group of discriminative features from the entire spatio-temporal domain. Second, we devise a decoupled feature mixing module, which dynamically attends to and mixes video features along the spatial and temporal dimensions respectively for better feature decoding. Based on these designs, we instantiate two detection pipelines, that is, STMixer-K for keyframe action detection and STMixer-T for action tubelet detection. Without bells and whistles, our STMixer detectors obtain the state-of-the-art results on five challenging spatio-temporal action detection benchmarks for keyframe action detection or action tube detection.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2024 OCT
PY  - 2024
VL  - 46
IS  - 10
SP  - 6842
EP  - 6857
DO  - 10.1109/TPAMI.2024.3387127
AN  - WOS:001308236900014
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
AD  - Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China
M2  - Shanghai Artificial Intelligence Lab
Y2  - 2024-09-19
ER  -

TY  - JOUR
AU  - Yuan, Yunfeng
AU  - Yang, Wenzhu
AU  - Luo, Zifei
AU  - Gou, Ruru
TI  - Temporal Context Modeling Network with Local-Global Complementary Architecture for Temporal Proposal Generation
T2  - ELECTRONICS
M3  - Article
AB  - Temporal Action Proposal Generation (TAPG) is a promising but challenging task with a wide range of practical applications. Although state-of-the-art methods have made significant progress in TAPG, most ignore the impact of the temporal scales of action and lack the exploitation of effective boundary contexts. In this paper, we propose a simple but effective unified framework named Temporal Context Modeling Network (TCMNet) that generates temporal action proposals. TCMNet innovatively uses convolutional filters with different dilation rates to address the temporal scale issue. Specifically, TCMNet contains a BaseNet with dilated convolutions (DBNet), an Action Completeness Module (ACM), and a Temporal Boundary Generator (TBG). The DBNet aims to model temporal information. It handles input video features through different dilated convolutional layers and outputs a feature sequence as the input of ACM and TBG. The ACM aims to evaluate the confidence scores of densely distributed proposals. The TBG is designed to enrich the boundary context of an action instance. The TBG can generate action boundaries with high precision and high recall through a local-global complementary structure. We conduct comprehensive evaluations on two challenging video benchmarks: ActivityNet-1.3 and THUMOS14. Extensive experiments demonstrate the effectiveness of the proposed TCMNet on tasks of temporal action proposal generation and temporal action detection.
PU  - MDPI
PI  - BASEL
PA  - MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 2079-9292
DA  - 2022 SEP
PY  - 2022
VL  - 11
IS  - 17
C7  - 2674
DO  - 10.3390/electronics11172674
AN  - WOS:000851199800001
AD  - Hebei Univ, Sch Cyber Secur & Comp, Baoding 071002, Peoples R China
AD  - Hebei Univ, Hebei Machine Vis Engn Res Ctr, Baoding 071002, Peoples R China
Y2  - 2022-09-14
ER  -

TY  - JOUR
AU  - Shi, Baifeng
AU  - Dai, Qi
AU  - Hoffman, Judy
AU  - Saenko, Kate
AU  - Darrell, Trevor
AU  - Xu, Huijuan
TI  - Temporal Action Detection with Multi-level Supervision
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - &nbsp; &nbsp; Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification task. To alleviate the main error of action incompleteness (i.e., missing parts of actions) in SSAD baselines, we further design an unsupervised foreground attention (UFA) module utilizing the "independence" between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. An information bottleneck (IB) suppressing the scene information in non-action frames while preserving the action information is designed to help overcome the accompanying action-context confusion problem in OSAD baselines. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2011.11893
AN  - PPRN:10695182
AD  - Peking Univ, Beijing, Peoples R China
AD  - Microsoft Res Asia, Beijing, Peoples R China
AD  - Georgia Tech, Atlanta, GA 30332, USA
AD  - Boston Univ, MIT, IBM Watson AI Lab, Boston, MA 02215, USA
AD  - UC Berkeley, Berkeley, CA 94720, USA
M2  - Peking Univ
M2  - Microsoft Res Asia
M2  - Georgia Tech
M2  - UC Berkeley
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Yang, Jianhua
AU  - Dai, Kun
TI  - YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Designing a real-time framework for the spatiotemporal action detection task is still a challenge. In this paper, we propose a novel real-time action detection framework, YOWOv2. In this new framework, YOWOv2 takes advantage of both the 3D backbone and 2D backbone for accurate action detection. A multi-level detection pipeline is designed to detect action instances of different scales. To achieve this goal, we carefully build a simple and efficient 2D backbone with a feature pyramid network to extract different levels of classification features and regression features. For the 3D backbone, we adopt the existing efficient 3D CNN to save development time. By combining 3D backbones and 2D backbones of different sizes, we design a YOWOv2 family including YOWOv2-Tiny, YOWOv2-Medium, and YOWOv2-Large. We also introduce the popular dynamic label assignment strategy and anchor-free mechanism to make the YOWOv2 consistent with the advanced model architecture design. With our improvement, YOWOv2 is significantly superior to YOWO, and can still keep real-time detection. Without any bells and whistles, YOWOv2 achieves 87.0% frame mAP and 52.8% video mAP with over 20 FPS on the UCF101-24. On the AVA, YOWOv2 achieves 21.7% frame mAP with over 20 FPS. 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2302.06848
AN  - PPRN:73232921
AD  - Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China
AD  - Wuhu Robot Ind Technol Res Inst, Harbin Inst Technol, Wuhu 241000, Peoples R China
M2  - Wuhu Robot Ind Technol Res Inst
Y2  - 2023-08-24
ER  -

TY  - JOUR
AU  - Liu, Meng
AU  - Nie, Liqiang
AU  - Wang, Yunxiao
AU  - Wang, Meng
AU  - Rui, Yong
TI  - A Survey on Video Moment Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video moment localization, also known as video moment retrieval, aiming to search a target segment within a video described by a given natural language query. Beyond the task of temporal action localization whereby the target actions are pre-defined, video moment retrieval can query arbitrary complex activities. In this survey paper, we aim to present a comprehensive review of existing video moment localization techniques, including supervised, weakly supervised, and unsupervised ones. We also review the datasets available for video moment localization and group results of related work. In addition, we discuss promising future directions for this field, in particular large-scale datasets and interpretable video moment localization models.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2306.07515
AN  - PPRN:73315782
AD  - Shandong Jianzhu Univ, Jinan, Peoples R China
AD  - Harbin Inst Technol Shenzhen, Shenzhen, Peoples R China
AD  - Shandong Univ, Jinan, Peoples R China
AD  - Hefei Univ Technol, Hefei, Peoples R China
AD  - Lenovo Co Ltd, Beijing, Peoples R China
M2  - Shandong Jianzhu Univ
M2  - Shandong Univ
M2  - Hefei Univ Technol
M2  - Lenovo Co Ltd
Y2  - 2023-06-24
ER  -

TY  - JOUR
AU  - Heyward, Joseph
AU  - Carreira, Joao
AU  - Damen, Dima
AU  - Zisserman, Andrew
AU  - Patraucean, Viorica
TI  - Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Following the successful 2023 edition, we organised the Second Perception Test challenge as a half-day workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking state-of-the-art video models and measuring the progress since last year using the Perception Test benchmark. This year, the challenge had seven tracks (up from six last year) and covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities; the additional track covered hour-long video understanding and introduced a novel video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks were: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, grounded video question-answering, and hour-long video question-answering. We summarise in this report the challenge tasks and results, and introduce in detail the novel hour-long video QA benchmark 1h-walk VQA.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2411.19941
AN  - PPRN:119587885
AD  - Google DeepMind, London, England
AD  - Univ Bristol, Bristol, England
AD  - Univ Oxford, Oxford, England
M2  - Google DeepMind
M2  - Univ Bristol
Y2  - 2025-01-11
ER  -

TY  - JOUR
AU  - Dou, Peng
AU  - Zeng, Ying
AU  - Wang, Zhuoqun
AU  - Hu, Haifeng
TI  - Multiple Temporal Pooling Mechanisms for Weakly Supervised Temporal Action Localization
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Recent action localization works learn in a weakly supervised manner to avoid the expensive cost of human labeling. Those works are mostly based on the Multiple Instance Learning framework, where temporal pooling is an indispensable part that usually relies on the guidance of snippet-level Class Activation Sequences (CAS). However, we observe that previous works only leverage a simple convolutional neural network for the generation of CAS, which ignores the weak discriminative foreground action segments and the background ones, and meanwhile, the relationship between different actions has not been considered. To solve this problem, we propose multiple temporal pooling mechanisms (MTP) for a more sufficient information utilization. Specifically, with the design of the Foreground Variance Branch, Dual Foreground Attention Branch and Hybrid Attention Fine-tuning Branch, MTP can leverage more effective information from different aspects and generate different CASs to guide the learning of temporal pooling. Moreover, different loss functions are designed for a better optimization of individual branches, aiming to effectively distinguish the action from the background. Our method shows excellent results on the THUMOS14 and ActivityNet1.2 datasets.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2023 MAY
PY  - 2023
VL  - 19
IS  - 3
C7  - 108
DO  - 10.1145/3567828
AN  - WOS:001011930300008
AD  - Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Guangdong, Peoples R China
Y2  - 2023-07-27
ER  -

TY  - JOUR
AU  - Yang, Jian
AU  - Liu, Kang
AU  - Zhao, Manqi
AU  - Li, Shengyang
TI  - Video process detection for space electrostatic suspension material experiment in China's Space Station
T2  - ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
M3  - Article
AB  - The purpose of video process detection is to identify key processes of interest and associated semantic categories, which is an important computer vision task for localizing key processes in space science experiments. To detect experimental processes in space electrostatic suspension material experiment on China's Space Station, we propose a Slow -and -Fast Dual -Stream Network. It extracts multi -scale temporal semantic information by designing a Slow -and -Fast Temporal Stream structure and effectively detects the experimental processes with various temporal scales. In addition, the Two Path Temporal Pyramid Network (TP2N) enhances the bi-direction flow among multi -scale features on the temporal dimension to alleviate boundary sensitivity. The validation experiments on our established dataset demonstrate the effectiveness of our proposed method with 62.43% mAP, outperforming the related state-of-the-art temporal action localization methods with nearly 7% mAP gain. This paper is the first application of deep learning in space electrostatic suspension material experiments on China's Space Station and further provides important technical support for the identification of key processes in space science experiments.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0952-1976
SN  - 1873-6769
DA  - 2024 MAY
PY  - 2024
VL  - 131
C7  - 107804
DO  - 10.1016/j.engappai.2023.107804
AN  - WOS:001154615900001
C6  - JAN 2024
AD  - Chinese Acad Sci, Technol & Engn Ctr Space Utilizat, Beijing 100094, Peoples R China
AD  - Chinese Acad Sci, Key Lab Space Utilizat, Beijing 100094, Peoples R China
AD  - Univ Chinese Acad Sci, Beijing 100049, Peoples R China
Y2  - 2024-02-09
ER  -

TY  - CPAPER
AU  - Shi, Baifeng
AU  - Dai, Qi
AU  - Mu, Yadong
AU  - Wang, Jingdong
A1  - IEEE
TI  - Weakly-Supervised Action Localization by Generative Attention Modeling
T2  - 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Weakly-supervised temporal action localization is a problem of learning an action localization model with only video-level action labeling available. The general framework largely relies on the classification activation, which employs an attention model to identify the action-related frames and then categorizes them into different classes. Such method results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves, since they are closely related to the specific classes. To solve the problem, in this paper we propose to model the class-agnostic frame-wise probability conditioned on the frame attention using conditional Variational Auto-Encoder (VAE). With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model, i.e., conditional VAE, is learned to model the likelihood of each frame given the attention. By maximizing the conditional probability with respect to the attention, the action and non-action frames are well separated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of our method and effectiveness in handling action-context confusion problem. Code is now available on GitHub(1).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-7281-7168-5
DA  - 2020 
PY  - 2020
SP  - 1006
EP  - 1016
DO  - 10.1109/CVPR42600.2020.00109
AN  - WOS:000620679501024
AD  - Peking Univ, Beijing, Peoples R China
AD  - Microsoft Res Asia, Beijing, Peoples R China
Y2  - 2021-03-14
ER  -

TY  - JOUR
AU  - Zhu, Suguo
AU  - Yang, Xiaoxian
AU  - Yu, Jun
AU  - Fang, Zhenying
AU  - Wang, Meng
AU  - Huang, Qingming
TI  - Proposal Complementary Action Detection
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Temporal action detection not only requires correct classification but also needs to detect the start and end times of each action accurately. However, traditional approaches always employ sliding windows or actionness to predict the actions, and it is different to train to model with sliding windows or actionness by end-toend means. In this article, we attempt a different idea to detect the actions end-to-end, which can calculate the probabilities of actions directly through one network as one part of the results. We present PCAD, a novel proposal complementary action detector to deal with video streams under continuous, untrimmed conditions. Our approach first uses a simple fully 3D convolutional network to encode the video streams and then generates candidate temporal proposals for activities by using anchor segments. To generate more precise proposals, we also design a boundary proposal network to offer some complementary information for the candidate proposals. Finally, we learn an efficient classifier to classify the generated proposals into different activities and refine their temporal boundaries at the same time. Our model can achieve end-to-end training by jointly optimizing classification loss and regression loss. When evaluating on the THUMOS'14 detection benchmark, PCAD achieves state-of-the-art performance in high-speed models.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2020 JUL
PY  - 2020
VL  - 16
IS  - 2
C7  - 64
DO  - 10.1145/3361845
AN  - WOS:000583712600007
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China
AD  - Shanghai Polytech Univ, Sch Comp & Informat Engn, Shanghai, Peoples R China
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing, Peoples R China
Y2  - 2020-12-08
ER  -

TY  - JOUR
AU  - Murtaza, Fiza
AU  - Yousaf, Muhammad Haroon
AU  - Velastin, Sergio A.
TI  - TAB: Temporally aggregated bag-of-discriminant-words for temporal action proposals
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - In this work, we propose a new method to generate temporal action proposals from long untrimmed videos named Temporally Aggregated Bag-of-Discriminant-Words (TAB). TAB is based on the observation that there are many overlapping frames in action and background temporal regions of untrimmed videos, which cause difficulties in segmenting actions from non-action regions. TAB solves this issue by extracting class-specific codewords from the action and background videos and extracting the discriminative weights of these codewords based on their ability to discriminate between these two classes. We integrate these discriminative weights with Bag of Word encoding, which we then call Bag-of-Discriminant-Words (BoDW). We sample the untrimmed videos into non-overlapping snippets and temporally aggregate the BoDW representation of multiple snippets into action proposals using a binary classifier trained on trimmed videos in a single pass. TAB can be used with different types of features, including those computed by deep networks. We present the effectiveness of the TAB proposal extraction method on two challenging temporal action detection datasets: MSR-II and Thumos14, where it improves upon state-of-the-art with recall rates of 87.0% and 82.0% respectively at a temporal intersection over union ratio of 0.8.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2019 JUN
PY  - 2019
VL  - 183
SP  - 42
EP  - 52
DO  - 10.1016/j.cviu.2019.04.008
AN  - WOS:000469164200005
AD  - Univ Engn & Technol Taxila, Dept Comp Engn, Taxila 47050, Pakistan
AD  - Univ Engn & Technol Taxila, Swarm Robot Lab NCRA, Taxila 47050, Pakistan
AD  - Cortex Vis Syst Ltd, London SE1 9LQ, England
AD  - Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England
AD  - Univ Carlos III Madrid, Dept Comp Sci, Appl Artificial Intelligence Res Grp, Madrid 28270, Spain
M2  - Cortex Vis Syst Ltd
Y2  - 2019-06-10
ER  -

TY  - JOUR
AU  - Zeng, Runhao
AU  - Chen, Xiaoyong
AU  - Liang, Jiaming
AU  - Wu, Huisi
AU  - Cao, Guangzhong
AU  - Guo, Yong
TI  - Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2403.20254
AN  - PPRN:88342708
AD  - Shenzhen MSU BIT Univ, Artificial Intelligence Res Inst, Shenzhen, Peoples R China
AD  - Guangdong Hong Kong Macao Joint Lab Emot Intelligence & Pervas Comp, Hong Kong, Peoples R China
AD  - Shenzhen Univ, Shenzhen, Peoples R China
AD  - South China Univ Technol, Guangzhou, Peoples R China
M2  - Shenzhen MSU BIT Univ
M2  - Guangdong Hong Kong Macao Joint Lab Emot Intelligence & Pervas Comp
M2  - South China Univ Technol
Y2  - 2024-04-17
ER  -

TY  - JOUR
AU  - Hu, Junshan
AU  - Guo, Chaoxu
AU  - Zhuang, Liansheng
AU  - Wang, Biao
AU  - Ge, Tiezheng
AU  - Jiang, Yuning
AU  - Li, Houqiang
TI  - Estimation of Reliable Proposal Quality for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action detection (TAD) aims to locate and recognize the actions in an untrimmed video. Anchor-free methods have made remarkable progress which mainly formulate TAD into two tasks: classification and localization using two separate branches. This paper reveals the temporal misalignment between the two tasks hindering further progress. To address this, we propose a new method that gives insights into moment and region perspectives simultaneously to align the two tasks by acquiring reliable proposal quality. For the moment perspective, Boundary Evaluate Module (BEM) is designed which focuses on local appearance and motion evolvement to estimate boundary quality and adopts a multi-scale manner to deal with varied action durations. For the region perspective, we introduce Region Evaluate Module (REM) which uses a new and efficient sampling method for proposal feature representation containing more contextual information compared with point feature to refine category score and proposal boundary. The proposed Boundary Evaluate Module and Region Evaluate Module (BREM) are generic, and they can be easily integrated with other anchor-free TAD methods to achieve superior performance. In our experiments, BREM is combined with two different frameworks and improves the performance on THUMOS14 by 3.6% and 1.0% respectively, reaching a new state-of-the-art (63.6% average mAP). Meanwhile, a competitive result of 36.2% average mAP is achieved on ActivityNet-1.3 with the consistent improvement of BREM.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2204.11695
AN  - PPRN:13482625
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
M2  - Univ Sci & Technol China
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Zhu, Yuhan
AU  - Zhang, Guozhen
AU  - Tan, Jing
AU  - Wu, Gangshan
AU  - Wang, Limin
TI  - Dual DETRs for Multi-Label Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the querybased framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub -optimal boundary localization. To address this issue, we propose a new Dual -level query -based TAD framework, namely DualDETR, to detect actions from both instance -level and boundary -level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two -branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two -branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual -level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi -label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-ofthe-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.00653
AN  - PPRN:88367331
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Chinese Univ Hong Kong, Hong Kong, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Nanjing Univ
M2  - Chinese Univ Hong Kong
Y2  - 2024-04-18
ER  -

TY  - JOUR
AU  - Nag, Sauradip
AU  - Xu, Mengmeng
AU  - Zhu, Xiatian
AU  - Perez-Rua, Juan-Manuel
AU  - Ghanem, Bernard
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
TI  - Multi-Modal Few-Shot Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Few-shot (FS) and zero-shot (ZS) learning are two different approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, which can be considered as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new class names jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multi-modal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-class variation, we further design a query feature regulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14 demonstrate that our MUPPET outperforms state-of-the-art alternative methods, often by a large margin. We also show that our MUPPET can be easily extended to tackle the few-shot object detection problem and again achieves the state-of-the-art performance on MS-COCO dataset.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2211.14905
AN  - PPRN:49747446
AD  - Univ Surrey, CVSSP, Surrey, England
AD  - Meta, London, England
AD  - Surrey Inst People Centred Artificial Intelligence, Surrey, England
AD  - KAUST, Thuwal, Saudi Arabia
AD  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence, London, England
M2  - Univ Surrey
M2  - Meta
M2  - Surrey Inst People Centred Artificial Intelligence
M2  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence
Y2  - 2023-04-07
ER  -

TY  - CPAPER
AU  - He, Jiayu
AU  - Li, Guohui
AU  - Lei, Jun
A1  - IEEE COMP SOC
TI  - Feature Pyramid Hierarchies for Multi-scale Temporal Action Detection
T2  - 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
M3  - Proceedings Paper
CP  - 25th International Conference on Pattern Recognition (ICPR)
CL  - ELECTR NETWORK
AB  - Temporal action detection is a challenging but promising task in video content analysis. It is in great demand in the field of public safety. The main difficulty of the task is precisely localizing activities in the video especially those short duration activities. And most of the existing methods can not achieve a satisfactory detection result. Our method addresses a key point to improve detection accuracy, which is to use multi-scale feature maps for regression and classification. In this paper, we introduce a novel network based on classification following proposal framework. In our network, a 3D feature pyramid hierarchies is built to enhance the ability of detecting short duration activities. The input RGB/Flow frames are first encoded by a 3D feature pyramid hierarchies, and this subnet produces multi-level feature maps. Then temporal proposal subnet uses these features to pick out proposals which might contain activity segments. Finally a pyramid region of interest (Rid) pooling pipeline and two fully connected layers reuse muti-level feature maps to refine the temporal boundaries of proposals and classify them. We use late feature fusion scheme to combine RGB and Flow information. The network is trained end-to-end and we evaluate it in THUMOS'14 dataset. Our network achieves a good result among typical methods. A further ablation test demonstrate that pyramid hierarchies is effective to improve detecting short duration activity segments.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1051-4651
SN  - 978-1-7281-8808-9
DA  - 2021 
PY  - 2021
SP  - 2158
EP  - 2165
DO  - 10.1109/ICPR48806.2021.9411986
AN  - WOS:000678409202035
AD  - Natl Univ Def Technol, Sci & Technol Informat Syst Engn Lab, Changsha, Hunan, Peoples R China
Y2  - 2021-08-25
ER  -

TY  - JOUR
AU  - Wang, Houlin
AU  - Zhang, Shihui
AU  - Tian, Qing
AU  - Wang, Lei
AU  - Luo, Bingchun
AU  - Han, Xueqiang
TI  - Multi-scale Graph Convolutional Network for understanding human action in videos
T2  - ADVANCED ENGINEERING INFORMATICS
M3  - Article
AB  - Temporal action detection aims to classify and locate human action in videos, which has been a difficult challenge in the field of smart transportation and intelligent manufacturing. In general, an action consists of multiple action units, and understanding the relationships between different action units is beneficial for detection. Most existing methods simply use the temporal context and ignore the relationships between the action units. In this paper, we propose a Multi-scale Graph Convolutional Network (MGCN), which can capture the different relationships between action units. Specifically, MGCN includes a Graph Pyramid Module (GPM), which divides a video into different scales and builds a graph for each scale. In each graph, we model action units as nodes and their relationships as edges. In addition, we propose GPM-T, a generalized version of GPM that can be plugged into state-of-the-art methods (e.g., G-TAD, TemporalMaxer) to enhance their performance. Experimental results show that at tIoU 0.5, MGCN reaches 72.8%, 33.8%, 20.2% and 17.9% mAP on THUMOS14, MultiTHUMOS, EPIC-Kitchens 100 (verb and noun), surpassing existing state-of-the-art methods. In addition, GPM-T improves the mAP of G-TAD and TemporalMaxer from 42.2% and 71.8% to 46.2% and 72.1%, respectively. The source code can be found at https://github.com/mugenggeng/GPM-T.git.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 1474-0346
SN  - 1873-5320
DA  - 2025 MAY
PY  - 2025
VL  - 65
C7  - 103166
DO  - 10.1016/j.aei.2025.103166
AN  - WOS:001426534900001
C6  - FEB 2025
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Univ Alabama Birmingham, Dept Comp Sci, Birmingham, AL USA
AD  - Hebei Univ Econ & Business, Sch Management Sci & Informat Engn, Shijiazhuang, Hebei, Peoples R China
AD  - Key Lab Comp Virtual Technol & Syst Integrat Hebei, Qinhuangdao, Hebei, Peoples R China
M2  - Key Lab Comp Virtual Technol & Syst Integrat Hebei
Y2  - 2025-02-26
ER  -

TY  - CPAPER
AU  - Zhu, Yuhan
AU  - Zhang, Guozhen
AU  - Tan, Jing
AU  - Wu, Gangshan
AU  - Wang, Limin
A1  - IEEE
TI  - Dual DETRs for Multi-Label Temporal Action Detection
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level ( i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18559
EP  - 18569
DO  - 10.1109/CVPR52733.2024.01756
AN  - WOS:001342515501085
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Chinese Univ Hong Kong, Hong Kong, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Ignat, Oana
AU  - Castro, Santiago
AU  - Zhou, Yuhang
AU  - Bao, Jiajun
AU  - Shan, Dandan
AU  - Mihalcea, Rada
TI  - When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2202.08138
AN  - PPRN:12069113
AD  - Univ Michigan, Ann Arbor, MI 48109, USA
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Wang, Tian
AU  - Lei, Shiye
AU  - Jiang, Youyou
AU  - Chang, Choi
AU  - Snoussi, Hichem
AU  - Shan, Guangcun
TI  - Accelerating temporal action proposal generation via high performance computing
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Temporal action recognition always depends on temporal action proposal generation to hypothesize actions and algorithms usually need to process very long video sequences and output the starting and ending times of each potential action in each video suffering from high computation cost. To address this, based on boundary sensitive network we propose a new temporal convolution network called Multipath Temporal ConvNet (MTN), which consists of two parts i.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high performance ring parallel architecture based on Message Passing Interface (MPI) is further introduced into temporal action proposal generation, which is a reliable communication protocol, in order to respond to the requirements of large memory occupation and a large number of videos. Remarkably, the total data transmission is reduced by adding a connection between multiple computing load in the newly developed architecture. It is found that, compared to the traditional Parameter Server architecture, our parallel architecture has higher efficiency on temporal action detection task with multiple GPUs, which is suitable for dealing with the tasks of temporal action proposal generation, especially for large datasets of millions of videos. We conduct experiments on ActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art temporal action detection methods with high recall and high temporal precision. In addition, a time metric is further proposed here to evaluate the speed performance in the distributed training process.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1906.06496
AN  - PPRN:12113989
Y2  - 2023-03-27
ER  -

TY  - JOUR
AU  - Ramazanova, Merey
AU  - Escorcia, Victor
AU  - Heilbron, Fabian Caba
AU  - Zhao, Chen
AU  - Ghanem, Bernard
TI  - OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Egocentric videos capture sequences of human activities from a first-person perspective and can provide rich multimodal signals. However, most current localization methods use third-person videos and only incorporate visual information. In this work, we take a deep look into the effectiveness of audiovisual context in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL leverages audiovisual information and context for egocentric temporal action localization (TAL). We validate our approach in two large-scale datasets, EPIC-Kitchens, and HOMAGE. Extensive experiments demonstrate the relevance of the audiovisual temporal context. Namely, we boost the localization performance (mAP) over visual-only models by +2.23% and +3.35% in the above datasets.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2202.04947
AN  - PPRN:22170491
AD  - King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia
AD  - Samsung Ctr Cambridge, Cambridge, England
AD  - Adobe Res, San Jose, CA 95110, USA
M2  - King Abdullah Univ Sci & Technol
M2  - Samsung Ctr Cambridge
M2  - Adobe Res
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Ignat, Oana
AU  - Castro, Santiago
AU  - Zhou, Yuhang
AU  - Bao, Jiajun
AU  - Shan, Dandan
AU  - Mihalcea, Rada
TI  - When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previousmethods, and leads to improvements over previous work for the task of temporal action localization.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2022 OCT
PY  - 2022
VL  - 18
IS  - 3
C7  - 142
DO  - 10.1145/3495211
AN  - WOS:000951829800012
AD  - Univ Michigan, 500 S State St, Ann Arbor, MI 48109 USA
Y2  - 2023-04-02
ER  -

TY  - BOOK
AU  - Gao, Jiyang
Z2  -  
TI  - Temporal Perception and Reasoning in Videos
M3  - Dissertation/Thesis
SN  - 9781658407045
DA  - 2018 
PY  - 2018
AN  - PQDT:67825436
AD  - University of Southern California, California, United States
M2  - University of Southern California
ER  -

TY  - CPAPER
AU  - Arnab, Anurag
AU  - Sun, Chen
AU  - Schmid, Cordelia
A1  - IEEE
TI  - Unified Graph Structured Models for Video Understanding
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Accurate video understanding involves reasoning about the relationships between actors, objects and their environment, often over long temporal intervals. In this paper, we propose a message passing graph neural network that explicitly models these spatio-temporal relations and can use explicit representations of objects, when supervision is available, and implicit representations otherwise. Our formulation generalises previous structured models for video understanding, and allows us to study how different design choices in graph structure and representation affect the model's performance. We demonstrate our method on two different tasks requiring relational reasoning in videos - spatio-temporal action detection on AVA and UCF10124, and video scene graph classification on the recent Action Genome dataset - and achieve state-of-the-art results on all three datasets. Furthermore, we show quantitatively and qualitatively how our method is able to more effectively model relationships between relevant entities in the scene.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 8097
EP  - 8106
DO  - 10.1109/ICCV48922.2021.00801
AN  - WOS:000798743206074
AD  - Google Res, Mountain View, CA 94043 USA
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Wang, Chuanxu
AU  - Wang, Jing
AU  - Liu, Peng
TI  - Complementary adversarial mechanisms for weakly-supervised temporal action localization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Weakly supervised Temporal Action Localization (WTAL) aims to locate the start and end boundaries of action instances and recognize their corresponding categories. Classical methods mainly rely on random erasure mechanisms, attention mechanisms, or imposing loss constraints. Despite their great progress, there are still two challenges of incomplete positioning and context confusion. Therefore, we propose a framework with complementary adversarial mechanisms to address these issues. In the adversarial learning stage, for an input snippet, we roughly determine its proper duration, by matching it with the specified multi-scaled anchors based on CAS score loss; then, it undergoes a frame-level iterative regression to precisely figure out its boundary, which can reject none closely related frames merged in, and ensures no overlapping between different action proposals. Subsequently, the GCN module explicitly enhances the feature representation for this fine localized snippet, aiming to strengthen the exclusiveness between different action snippets. Afterward, our complementary learning module calculates the similarity between the original input video V g and the video V r reconstructed with the above localization refined snippets, aiming to ensure no closely relevant frames missing, this checking mechanism works as feedback to guide the regression module for more accurate localization regression. Finally, each refined snippet undergoes multi-instance learning to obtain its classification score, and the top-k strategy is used to aggregate temporally adjacent snippets based on their content similarity, which can avoid fragmentation of an action proposal. This method is tested on datasets of THUMOS14 and ActivityNet1.2, and their average accuracy is 64.68% and 32.94% respectively, its comparisons with other articles prove its effectiveness.(c) 2023 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2023 JUL
PY  - 2023
VL  - 139
C7  - 109426
DO  - 10.1016/j.patcog.2023.109426
AN  - WOS:000947922400001
C6  - FEB 2023
AD  - Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266001, Peoples R China
Y2  - 2023-03-29
ER  -

TY  - JOUR
AU  - Wang, Houlin
AU  - You, Dianlong
AU  - Zhang, Shihui
TI  - Exploiting relation of video segments for temporal action detection
T2  - ADVANCED ENGINEERING INFORMATICS
M3  - Article
AB  - With the development of urban intelligence, intelligent identification of human activities has become imminent. Temporal Action Detection (TAD) is designed to identify real human social activities, which is a challenging task in video understanding. The current methods mainly use global features for boundary matching or predefine all possible proposals, while ignoring the interference of background and the causal relevance of the temporal action, resulting in the generation of more redundant proposals and the decline of detection accuracy. To fill this gap, we propose a novel Dilated Convolution Locate and Action Relevant Score model called DCAR. Specifically, DCAR includes a Dilated Location Network (DL -Net) and Action Relevance Calculation (ARC) block. For the DL -Net, we design a Boundary Feature Enhancement (BFE) block, which enhances the boundary feature of actions and fuses the similar features of different channels by pooling and channel squeeze to reduce the interference of the background. Meanwhile, we also design multiple dilated convolutional structures to aggregate long contextual information in time point/interval after boundary enhancement. For the ARC block, we use the hyperbolic space distance and cross attention to calculate the causal correlation of action proposals classification, which removes the misclassification of action proposals. We conduct extensive experiments on Thumos14 and ActivityNet-1.3, which shows our method significantly improves the performance and achieves state-of-the-art results. On Thumos14, at tIoU 0.3, 0.4 and 0.5, it achieves mAP of 63.2%, 57.0% and 48.5%, respectively; On ActivityNet-1.3 it reaches 9.42% at tIoU 0.95.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 1474-0346
SN  - 1873-5320
DA  - 2024 OCT
PY  - 2024
VL  - 62
C7  - 102585
DO  - 10.1016/j.aei.2024.102585
AN  - WOS:001262083100001
C6  - MAY 2024
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao, Hebei, Peoples R China
AD  - Key Lab Comp Virtual Technol & Syst Integrat Hebei, Qinhuangdao, Hebei, Peoples R China
AD  - Key Lab Software Engn Hebei Prov, Qinhuangdao, Hebei, Peoples R China
M2  - Key Lab Comp Virtual Technol & Syst Integrat Hebei
M2  - Key Lab Software Engn Hebei Prov
Y2  - 2024-07-12
ER  -

TY  - CPAPER
AU  - Li, Tianyu
AU  - Wu, Xinxiao
ED  - Peng, Y
ED  - Liu, Q
ED  - Lu, H
ED  - Sun, Z
ED  - Liu, C
ED  - Chen, X
ED  - Zha, H
ED  - Yang, J
TI  - Hierarchical Matching and Reasoning for Action Localization via Language Query
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2020, PT III
M3  - Proceedings Paper
CP  - 3rd Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Nanjing Univ Sci & Tech, Nanjing, PEOPLES R CHINA
AB  - This paper strives for temporal localization of actions in untrimmed videos via natural language queries. Prevailing methods represent both query sentence and video as a whole and perform sentence-video matching via global features, which neglects local correspondence between sentence and video. In this work, we aim to move beyond this limitation by delving into the fine-grained local sentence-video matching, such as phrase-motion matching and word-object matching. We propose a hierarchical matching and reasoning method based on deep conditional random field to integrate hierarchical matching between visual concepts and textual semantics for temporal action localization via query sentence. Our method decomposes each sentence into textual semantics (i.e., phrases and words), obtains multi-level matching results between the textual semantics and the visual concepts in a video (i.e., results of phrase-motion matching and word-object matching), and then reasons relations between multi-level matching via pairwise potentials of conditional random field to achieve coherence in hierarchical matching. By minimizing the overall potential, the final matching score between a sentence and a video is computed as the conditional probability of the conditional random field. Our proposed method is evaluated on public Charades-STA dataset and the experimental results verify its superiority over the state-of-the-art methods.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-60635-0
SN  - 978-3-030-60636-7
DA  - 2020 
PY  - 2020
VL  - 12307
SP  - 137
EP  - 148
DO  - 10.1007/978-3-030-60636-7_12
AN  - WOS:001425865600012
AD  - Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - JOUR
AU  - Choi, Wangyu
AU  - Chen, Jiasi
AU  - Yoon, Jongwon
TI  - Step by Step: A Gradual Approach for Dense Video Captioning
T2  - IEEE ACCESS
M3  - Article
AB  - Dense video captioning aims to localize and describe events for storytelling in untrimmed videos. It is a conceptually very challenging task that requires concise, relevant, and coherent captioning based on high-quality event localization. Unlike simple temporal action localization tasks without overlapping events, dense video captioning requires detecting multiple/overlapping regions in order to branch out the video story. Most existing methods generate numerous candidate event proposals and then eliminate duplicate ones using a event proposal selection algorithm (e.g., non-maximum suppression) or generate event proposals directly through box prediction and binary classification mechanisms, similar to object detection tasks. Despite these efforts, the aforementioned approaches tend to fail to localize overlapping events into different stories, hindering high-quality captioning. In this paper, we propose SBS, a dense video captioning framework with a gradual approach that addresses the challenge of localizing overlapping events and eventually constructs high-quality captioning. SBS accurately estimates the number of explicit events for each video snippet and then detects the boundaries context/activities, which are the details for generating the event proposals. Based on both the number of events and boundaries, SBS generates the event proposals. SBS encodes the context of the event sequence and finally generates sentences describing the event proposals. Our framework is fairly effective in localizing multiple/overlapping events, thus experimental results show the state-of-the-art performance compared to the existing methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2023 
PY  - 2023
VL  - 11
SP  - 51949
EP  - 51959
DO  - 10.1109/ACCESS.2023.3279816
AN  - WOS:001006225900001
AD  - Hanyang Univ, Dept Comp Sci & Engn, Major Bio Artificial Intelligence, Ansan 15588, South Korea
AD  - Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA
Y2  - 2023-06-27
ER  -

TY  - CPAPER
AU  - Monteiro, Carlos
AU  - Duraes, Dalila
ED  - Rocha, A
ED  - Adeli, H
ED  - Dzemyda, G
ED  - Moreira, F
TI  - Modelling a Framework to Obtain Violence Detection with Spatial-Temporal Action Localization
T2  - INFORMATION SYSTEMS AND TECHNOLOGIES, WORLDCIST 2022, VOL 1
M3  - Proceedings Paper
CP  - World Conference on Information Systems and Technologies (WorldCIST)
CL  - ELECTR NETWORK
AB  - A system that detects and identifies human activities are named Human Action Recognition. In the video approach, human activity can be classified into four different categories, depending on the complexity of the steps and the number of body parts involved in the action. The categories are gestures, actions, interactions, and activities. This type of classification is quite challenging since systems must capture valuable and discriminating characteristics caused by variations in the human body. Thus, deep learning techniques have provided the development of practical applications in various fields of signal processing, generally surpassing traditional large-scale signal processing. Recently, several applications, namely surveillance, human-computer interaction and content-based video retrieval, have studied the detection and recognition of violence. This paper presents an experiment to identify and detect violence with spatial action location, adapting a public dataset for this purpose. The idea was to use an annotated dataset of recognition of general actions and adapted to detect violence. The results show that it is possible to adapt a general public dataset of action recognition to detect and recognize violence.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 2367-3370
SN  - 2367-3389
SN  - 978-3-031-04826-5
SN  - 978-3-031-04825-8
DA  - 2022 
PY  - 2022
VL  - 468
SP  - 630
EP  - 639
DO  - 10.1007/978-3-031-04826-5_62
AN  - WOS:000873519600062
AD  - Univ Minho, Algorithm Ctr, Braga, Portugal
Y2  - 2022-11-09
ER  -

TY  - CPAPER
AU  - Ding, Guanchen
AU  - Han, Wenwei
AU  - Wang, Chenglong
AU  - Cui, Mingpeng
AU  - Zhou, Lin
AU  - Pan, Dianbo
AU  - Wang, Jiayi
AU  - Zhang, Junxi
AU  - Chen, Zhenzhong
A1  - IEEE
TI  - A Coarse-to-Fine Boundary Localization method for Naturalistic Driving Action Recognition
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Naturalistic driving action recognition plays an important role in understanding drivers' distracted behaviors in the traffic environment. The main challenge of this task is the accurate localization of the temporal boundary for each distracted driving behavior in the video. Although many temporal action localization methods can identify action categories, it is difficult to predict accurate temporal boundaries for this task since the driving actions of the same category usually present large intra-class variation. In this paper, we introduce a Coarse-to-Fine Boundary Localization method called CFBL, which obtains fine-grained temporal boundaries progressively through three stages. Concretely, in the first coarse boundary generation stage, we adopt a modified anchor-free model Anchor-Free Saliency-based Detector (AFSD) to make an interval estimation of the temporal boundaries of distracted behaviors. In the second boundary refinement stage, we use the Dense Boundary Generation (DBG) model to adjust the estimated interval of the temporal boundaries. In the final boundary decision stage, we build a Localization Boundary Refinement Module to determine the final boundaries of different actions. Besides, we adopt a voting strategy to combine the results of different camera views to enhance the model's distracted driving action classification ability. The experiments conducted on the Track 3 validation set of the 2022 AI City Challenge demonstrate competitive performance of the proposed method.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3233
EP  - 3240
DO  - 10.1109/CVPRW56347.2022.00365
AN  - WOS:000861612703038
AD  - Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan, Peoples R China
AD  - Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China
AD  - Wuhan Univ, Sch Resource & Environm Sci, Wuhan, Peoples R China
Y2  - 2022-12-07
ER  -

TY  - CPAPER
AU  - Huy-Hung Nguyen
AU  - Chi Dai Tran
AU  - Long Hoang Pham
AU  - Duong Nguyen-Ngoc Tran
AU  - Tai Huu-Phuong Tran
AU  - Duong Khac Vu
AU  - Quoc Pham-Nam Ho
AU  - Ngoc Doan-Minh Huynh
AU  - Hyung-Min Jeon
AU  - Hyung-Joon Jeon
AU  - Jae Wook Jeon
A1  - IEEE
TI  - Multi-View Spatial-Temporal Learning for Understanding Unusual Behaviors in Untrimmed Naturalistic Driving Videos
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - The task of Naturalistic Driving Action Recognition aims to detect and temporally localize distracting driving behavior in untrimmed videos. In this paper, we introduce our framework for Track 3 of the 8th AI City Challenge in 2024. The approach is primarily based on large model fine-tuning and ensemble techniques to train a set of action recognition models on a small-scale dataset. Starting with raw videos, we segment them into individual action sequences based on their annotation. We then fine-tune four different action recognition models, with K-fold cross-validation applied to the segmented data. Following this, we execute a multi-view ensemble, selecting the most visible camera views for each action class to generate clip-level classification results for each video. Finally, a multi-step post-processing algorithm, which is designed for the AI City Challenge dataset's specific features, is employed to perform temporal action localization and produce temporal segments for the actions. Our solution achieves a final mOS score of 0.7798 and attains the 5th rank on the public leaderboard for the test set A2 of the challenge. The source code will be publicly available at https://github.com/SKKUAutoLab/AIC24-Track03.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 979-8-3503-6547-4
DA  - 2024 
PY  - 2024
SP  - 7144
EP  - 7152
DO  - 10.1109/CVPRW63382.2024.00709
AN  - WOS:001327781707033
AD  - Sungkyunkwan Univ, Dept Elect & Comp Engn, Suwon, South Korea
Y2  - 2025-03-05
ER  -

TY  - CPAPER
AU  - Xu, Yuehuan
AU  - Jiang, Shuai
AU  - Cui, Zhe
AU  - Su, Fei
A1  - IEEE
TI  - Multi-View Action Recognition for Distracted Driver Behavior Localization
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - The detection and recognition of distracted driving behaviors has emerged as a new vision task with the rapid development of computer vision, which is considered as a challenging temporal action localization (TAL) problem in computer vision. The primary goal of temporal localization is to determine the start and end time of actions in untrimmed videos. Currently, most state-of-the-art temporal localization methods adopt complex architectures, which are cumbersome and time-consuming. In this paper, we propose a robust and efficient two-stage framework for distracted behavior classification-localization based on the sliding window approach, which is suitable for untrimmed naturalistic driving videos. To address the issues of high similarity among different behaviors and interference from background classes, we propose a multi-view fusion and adaptive thresholding algorithm, which effectively reduces missing detections. To address the problem of fuzzy behavior boundary localization, we design a post-processing procedure that achieves fine localization from coarse localization through post connection and candidate behavior merging criteria. In the AICITY2024 Task3 TestA, our method performs well, achieving Average Intersection over Union(AIOU) of 0.6080 and ranking eighth in AICITY2024 Task3. Our code will be released in the near future.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 979-8-3503-6547-4
DA  - 2024 
PY  - 2024
SP  - 7172
EP  - 7179
DO  - 10.1109/CVPRW63382.2024.00712
AN  - WOS:001327781707036
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
AD  - Beijing Key Lab Network Syst & Network Culture, Beijing, Peoples R China
M2  - Beijing Key Lab Network Syst & Network Culture
Y2  - 2025-03-05
ER  -

TY  - CPAPER
AU  - Ando, Ryuhei
AU  - Babazaki, Yasunori
AU  - Takahashi, Katsuhiko
ED  - Bebis, G
ED  - Ghiasi, G
ED  - Fang, Y
ED  - Sharf, A
ED  - Dong, Y
ED  - Weaver, C
ED  - Leo, Z
ED  - LaViola, JJ
ED  - Kohli, L
TI  - Local and Global Context Reasoning for Spatio-Temporal Action Localization
T2  - ADVANCES IN VISUAL COMPUTING, ISVC 2023, PT I
M3  - Proceedings Paper
CP  - 18th International Symposium on Visual Computing (ISVC)
CL  - NV
AB  - Localizing persons and recognizing their actions from videos is an essential task in video understanding. Recent advances have been made by reasoning the relationships between the actor and another actor, as well as between the actor and the environment. However, reasoning the relationships globally over the image is not always the efficient way, and there are cases that locally searching for the relative clues is more suitable. In this paper, we move one step further and model the relationship between an actor and the actor's relevant surrounding context. We developed a pipeline that observes over the full image to collect context information globally and around the actor to collect context information locally. This is achieved by implementing a Near-Actor Relation Network (NARN) that focuses on reasoning the context information locally. Two key components of our NARN enable the effective accumulation of the local context information: pose encoding, which encodes the human pose information as an additional feature, and spatial attention, which discriminates the relative context information from the others. Our pipeline accumulates the global and local relation information and gathers them for the final action classification. Experimental results on the JHMDB21 and AVA datasets demonstate that our proposed pipeline outperforms a baseline approach that only reasons about the global context. Visualization of the learned attention map indicates that our pipeline is able to focus on spatial areas that contains relative context information for each action.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-47968-7
SN  - 978-3-031-47969-4
DA  - 2023 
PY  - 2023
VL  - 14361
SP  - 147
EP  - 159
DO  - 10.1007/978-3-031-47969-4_12
AN  - WOS:001159734600012
AD  - NEC Visual Intelligence Res Labs, Kawasaki, Japan
M2  - NEC Visual Intelligence Res Labs
Y2  - 2024-03-02
ER  -

TY  - JOUR
AU  - Indris, Christopher
AU  - Ibrahim, Fady
AU  - Ibrahem, Hatem
AU  - Bramesfeld, Gotz
AU  - Huo, Jie
AU  - Ahmad, Hafiz Mughees
AU  - Hayat, Syed Khizer
AU  - Wang, Guanghui
TI  - Supervised and Self-Supervised Learning for Assembly Line Action Recognition.
T2  - Journal of imaging
M3  - Journal Article
AB  - The safety and efficiency of assembly lines are critical to manufacturing, but human supervisors cannot oversee all activities simultaneously. This study addresses this challenge by performing a comparative study to construct an initial real-time, semi-supervised temporal action recognition setup for monitoring worker actions on assembly lines. Various feature extractors and localization models were benchmarked using a new assembly dataset, with the I3D model achieving an average mAP@IoU=0.1:0.7 of 85% without optical flow or fine-tuning. The comparative study was extended to self-supervised learning via a modified SPOT model, which achieved a mAP@IoU=0.1:0.7 of 65% with just 10% of the data labeled using extractor architectures from the fully-supervised portion. Milestones include high scores for both fully and semi-supervised learning on this dataset and improved SPOT performance on ANet1.3. This study identified the particularities of the problem, which were leveraged and referenced to explain the results observed in semi-supervised scenarios. The findings highlight the potential for developing a scalable solution in the future, providing labour efficiency and safety compliance for manufacturers.
SN  - 2313-433X
DA  - 2025 Jan 10
PY  - 2025
VL  - 11
IS  - 1
DO  - 10.3390/jimaging11010017
AN  - MEDLINE:39852330
AD  - Department of Computer Science, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada.
AD  - Department of Aerospace Engineering, Toronto Metropolitan University, Toronto, ON M5B 2K3, Canada.
AD  - IFIVEO Canada Inc., Windsor, ON N8W 0A6, Canada.
Y2  - 2025-01-27
ER  -

TY  - JOUR
AU  - Liu, Yuan
AU  - Chen, Jingyuan
AU  - Chen, Xinpeng
AU  - Deng, Bing
AU  - Huang, Jianqiang
AU  - Hua, Xian-Sheng
TI  - Centerness-Aware Network for Temporal Action Proposal
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Temporal action proposal generation aims at localizing the temporal segments containing human actions in a video. This work proposes a centerness-aware network (CAN), which is a novel one-stage approach intended to generate action proposals as keypoint triplets. A keypoint triplet contains two boundary points (starting and ending) and one center point. Specifically, we evaluate the probabilities of each temporal location in the video whether it is at the boundaries or the center region of ground truth action proposals. CAN optimizes the predicted boundary points interactively in a bidirectional adaptation form by exploiting the dependencies among them. Furthermore, to accurately locate the center points of action proposals with different time spans, temporal feature pyramids are utilized to incorporate multi-scale information explicitly. Using the generated three keypoints, CAN efficiently retrieves temporal proposals by grouping keypoints into triplets if they are geometrically aligned. Experiments show that CAN achieves the state-of-the-art performance on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, further experiments demonstrate that by applying action classifiers on proposals generated by CAN, our method achieves the state-of-the-art performance in temporal action localization.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2022 JAN
PY  - 2022
VL  - 32
IS  - 1
SP  - 5
EP  - 16
DO  - 10.1109/TCSVT.2021.3075607
AN  - WOS:000742183600005
AD  - Alibaba DAMO Acad, Alibaba Grp, Hangzhou 310052, Peoples R China
Y2  - 2022-01-21
ER  -

TY  - JOUR
AU  - Zeng, Runhao
AU  - Gan, Chuang
AU  - Chen, Peihao
AU  - Huang, Wenbing
AU  - Wu, Qingyao
AU  - Tan, Mingkui
TI  - Breaking Winner-Takes-All: Iterative-Winners-Out Networks for Weakly Supervised Temporal Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - We address the challenging problem of weakly supervised temporal action localization from unconstrained web videos, where only the video-level action labels are available during training. Inspired by the adversarial erasing strategy in weakly supervised semantic segmentation, we propose a novel iterative-winners-out network. Specifically, we make two technical contributions: we propose an iterative training strategy, namely, winners-out, to select the mast discriminative action instances in each training iteration and remove them in the next training iteration. This iterative process alleviates the "winner-takes-all" phenomenon that existing approaches tend to choose the video segments that strongly correspond to the video label but neglects other less discriminative video segments. With this strategy, our network is able to localize not only the most discriminative instances but also the less discriminative ones. To better select the target action instances in winners-out, we devise a class-discriminative localization technique. By employing the attention mechanism and the information learned from data, our technique is able to identify the most discriminative action instances effectively. The two key components are integrated into an end-to-end network to localize actions without using the frame-level annotations. Extensive experimental results demonstrate that our method outperforms the state-of-the-art weakly supervised approaches on ActivityNet1.3 and improves mAP from 16.9% to 20.5% on THUMOS14. Notably, even with weak video-level supervision, our method attains comparable accuracy to those employing frame-level supervisions.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2019 DEC
PY  - 2019
VL  - 28
IS  - 12
SP  - 5797
EP  - 5808
DO  - 10.1109/TIP.2019.2922108
AN  - WOS:000484306000004
AD  - South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China
AD  - MIT IBM Watson AI Lab, Cambridge, MA 02142 USA
AD  - Tencent AI Lab, Shenzhen 518000, Peoples R China
M2  - MIT IBM Watson AI Lab
Y2  - 2019-12-01
ER  -

TY  - CPAPER
AU  - Shi, Baifeng
AU  - Dai, Qi
AU  - Hoffman, Judy
AU  - Saenko, Kate
AU  - Darrell, Trevor
AU  - Xu, Huijuan
A1  - IEEE
TI  - Temporal Action Detection with Multi-level Supervision
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification literature. Identifying that the main source of error is action incompleteness (i.e., missing parts of actions), we alleviate it by designing an unsupervised foreground attention (UFA) module utilizing the conditional independence between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. To overcome the accompanying action-context confusion problem in OSAD baselines, an information bottleneck (IB) is designed to suppress the scene information in non-action frames while preserving the action information. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data. (1)
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 8002
EP  - 8012
DO  - 10.1109/ICCV48922.2021.00792
AN  - WOS:000797698908024
AD  - Univ Calif Berkeley, Berkeley, CA 94720 USA
AD  - Microsoft Res Asia, Beijing, Peoples R China
AD  - Georgia Tech, Atlanta, GA USA
AD  - Boston Univ, Boston, MA 02215 USA
AD  - MIT IBM Watson AI Lab, Cambridge, MA USA
AD  - Penn State Univ, University Pk, PA 16802 USA
M2  - MIT IBM Watson AI Lab
Y2  - 2022-07-08
ER  -

TY  - JOUR
AU  - Zhong, Fujin
AU  - Wu, Yini
AU  - Yu, Hong
AU  - Wang, Guoyin
AU  - Lu, Zhantao
TI  - A benchmark dataset and semantics-guided detection network for spatial-temporal human actions in urban driving scenes
T2  - PATTERN RECOGNITION
M3  - Article
AB  - In real urban driving scenes, human actions are very complex and have the characteristic of multiple concurrent actions. It has a great significance to detect human actions in urban traffic scenes for auxiliary or autonomous driving systems. In this view, we introduce the TITAN-Human Action dataset for the task of multi-person spatial-temporal action detection in urban driving scenes. TITAN-Human Action provides the fine-grained action labels and location coordinates for 17,574 persons in the processed frames from the TITAN dataset. Furthermore, we propose a semantics-guided detection network (SGDNet) based on a semantic inference module (SIM) for spatial-temporal human action detection in urban driving scenes. SIM encodes the category labels into sentence vectors at the semantic level with prompting and embedding, utilizes graphs to represent the directed co-occurrence relations between categories, and adopts the graph convolutional network for semantic inference. SGDNet exploits the inference results of SIM to guide the visual branch in better performing human action detection, thereby achieving the integration of visual and linguistic information. We conducted experiments to evaluate SGDNet and several baseline methods on the TITAN-Human Action dataset, and reveal the generalizability of SIM in spatial-temporal human action detection. The source code and annotation files will be available at https://github.com/yyhbswyn/SGDNet.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2025 FEB
PY  - 2025
VL  - 158
C7  - 111035
DO  - 10.1016/j.patcog.2024.111035
AN  - WOS:001327839900001
C6  - SEP 2024
AD  - Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Key Lab Big Data Intelligent Comp, Chongqing, Peoples R China
AD  - Minist Educ, Key Lab Cyberspace Big Data Intelligent Secur, Chongqing, Peoples R China
Y2  - 2024-10-11
ER  -

TY  - JOUR
AU  - Xing, Kai
AU  - Li, Tao
AU  - Wang, Xuanhan
TI  - ProposalVLAD with Proposal-Intra Exploring for Temporal Action Proposal Generation
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Temporal action proposal generation aims to localize temporal segments of human activities in videos. Current boundary-based proposal generation methods can generate proposals with precise boundary but often suffer from the inferior quality of confidence scores used for proposal retrieving. In this article, we propose an effective and end-to-end action proposal generation method, named ProposalVLAD, with Proposal-Intra Exploring Network (PVPI-Net). We first propose a ProposalVLAD module to dynamically generate global features of the entire video, then we combine the global features and proposal local features to generate the final feature representations for all candidate proposals. Then, we design a novel Proposal-Intra Loss function (PI-Loss) to generate more reliable proposal confidence scores. Extensive experiments on large-scale and challenging datasets demonstrate the effectiveness of our proposed method. Experimental results show that our PVPI-Net achieves significant improvements on two benchmark datasets (i.e., THUMOS'14 and ActivityNet-1.3) and sets new records for temporal action detection task.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2023 MAY
PY  - 2023
VL  - 19
IS  - 3
C7  - 118
DO  - 10.1145/3571747
AN  - WOS:001011930300018
AD  - Univ Elect Sci & Technol China, Chengdu 611731, Sichuan, Peoples R China
Y2  - 2023-07-27
ER  -

TY  - JOUR
AU  - Wu, Tao
AU  - Cao, Mengqi
AU  - Gao, Ziteng
AU  - Wu, Gangshan
AU  - Wang, Limin
TI  - STMixer: A One-Stage Sparse Action Detector
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Traditional video action detectors typically adopt the two-stage pipeline, where a person detector is first employed to generate actor boxes and then 3D RoIAlign is used to extract actor-specific features for classification. This detection paradigm requires multi-stage training and inference, and the feature sampling is constrained inside the box, failing to effectively leverage richer context information outside. Recently, a few query-based action detectors have been proposed to predict action instances in an end-to-end manner. However, they still lack adaptability in feature sampling and decoding, thus suffering from the issues of inferior performance or slower convergence. In this paper, we propose two core designs for a more flexible one-stage sparse action detector. First, we present a query-based adaptive feature sampling module, which endows the detector with the flexibility of mining a group of discriminative features from the entire spatio-temporal domain. Second, we devise a decoupled feature mixing module, which dynamically attends to and mixes video features along the spatial and temporal dimensions respectively for better feature decoding. Based on these designs, we instantiate two detection pipelines, that is, STMixer-K for keyframe action detection and STMixer-T for action tubelet detection. Without bells and whistles, our STMixer detectors obtain state-of-the-art results on five challenging spatio-temporal action detection benchmarks for keyframe action detection or action tube detection.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.09842
AN  - PPRN:88529582
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
AD  - Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China
M2  - Shanghai Artificial Intelligence Lab
Y2  - 2024-04-25
ER  -

TY  - CPAPER
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
A1  - IEEE
TI  - Post-Processing Temporal Action Detection
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - Existing Temporal Action Detection (TAD) methods typically take a pre-processing step in converting an input varying-length video into a fixed-length snippet representation sequence, before temporal boundary estimation and action classification. This pre-processing step would temporally downsample the video, reducing the inference resolution and hampering the detection performance in the original temporal resolution. In essence, this is due to a temporal quantization error introduced during resolution downsampling and recovery. This could negatively impact the TAD performance, but is largely ignored by existing methods. To address this problem, in this work we introduce a novel model-agnostic post-processing method without model redesign and retraining. Specifically, we model the start and end points of action instances with a Gaussian distribution for enabling temporal boundary inference at a sub-snippet level. We further introduce an efficient Taylor-expansion based approximation, dubbed as Gaussian Approximated Post-processing (GAP). Extensive experiments demonstrate that our GAP can consistently improve a wide variety of pre-trained off-the-shelf TAD models on the challenging ActivityNet (+0.2%similar to 0.7% in average mAP) and THUMOS (+0.2%similar to 0.5% in average mAP) benchmarks. Such performance gains are already significant and highly comparable to those achieved by novel model designs. Also, GAP can be integrated with model training for further performance gain. Importantly, GAP enables lower temporal resolutions for more efficient inference, facilitating low-resource application. The code is available at https://github.com/sauradip/GAP
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 18837
EP  - 18845
DO  - 10.1109/CVPR52729.2023.01806
AN  - WOS:001062531303015
AD  - Univ Surrey, CVSSP, Guildford, Surrey, England
AD  - iFlyTek Surrey Joint Res Ctr Artificial Intellige, Guildford, Surrey, England
AD  - Surrey Inst People Centred Artificial Intelligenc, Guildford, Surrey, England
M2  - iFlyTek Surrey Joint Res Ctr Artificial Intellige
M2  - Surrey Inst People Centred Artificial Intelligenc
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
TI  - Post-Processing Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing Temporal Action Detection (TAD) methods typically take a pre-processing step in converting an input varying-length video into a fixed-length snippet representation sequence, before temporal boundary estimation and action classification. This pre-processing step would temporally downsample the video, reducing the inference resolution and hampering the detection performance in the original temporal resolution. In essence, this is due to a temporal quantization error introduced during the resolution downsampling and recovery. This could negatively impact the TAD performance, but is largely ignored by existing methods. To address this problem, in this work we introduce a novel model-agnostic post-processing method without model redesign and retraining. Specifically, we model the start and end points of action instances with a Gaussian distribution for enabling temporal boundary inference at a sub-snippet level. We further introduce an efficient Taylor-expansion based approximation, dubbed as Gaussian Approximated Post-processing (GAP). Extensive experiments demonstrate that our GAP can consistently improve a wide variety of pre-trained off-the-shelf TAD models on the challenging ActivityNet (+0.2% -0.7% in average mAP) and THUMOS (+0.2% -0.5% in average mAP) benchmarks. Such performance gains are already significant and highly comparable to those achieved by novel model designs. Also, GAP can be integrated with model training for further performance gain. Importantly, GAP enables lower temporal resolutions for more efficient inference, facilitating low-resource applications. The code will be available in https://github.com/sauradip/GAP
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2211.14924
AN  - PPRN:43048048
AD  - Univ Surrey, CVSSP, Guildford, England
AD  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence, Guildford, England
AD  - Surrey Inst People Centred Artificial Intelligence, Guildford, England
M2  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence
M2  - Surrey Inst People Centred Artificial Intelligence
Y2  - 2023-03-17
ER  -

TY  - JOUR
AU  - Wang, Zheng
AU  - Chen, Kai
AU  - Zhang, Mingxing
AU  - He, Peilin
AU  - Wang, Yajie
AU  - Zhu, Ping
AU  - Yang, Yang
TI  - Multi-scale aggregation network for temporal action proposals
T2  - PATTERN RECOGNITION LETTERS
M3  - Article
AB  - Temporal action detection is a very challenging and valuable task for video analysis and applications. The detection results, to a great extent, rely on the quality of temporal action proposals. However, temporal actions in videos vary dramatically, e.g. from a fraction of a second to minutes, which causes much difficulties for accurate temporal action proposals. In this paper, we propose a multi-scale aggregation network to overcome those variations for temporal action proposals. Our proposed network generates an actionness score sequence for the input video to automatically perceive the duration of actions, and thus can dynamically generate corresponding lengths of action proposals for them. For more reliable actionness prediction, we propose to adaptively explore the intrinsic short and long dependencies in action by two multi-scale aggregation strategies: unit level multi-scale aggregation and proposal level multi-scale aggregation. We also propose to take the soft labelling to facilitate the actionness prediction for the units near the action boundaries. Extensive experiments on THUMOS14 dataset have demonstrated the effectiveness of our proposed method. (C) 2019 Published by Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0167-8655
SN  - 1872-7344
DA  - 2019 MAY 1
PY  - 2019
VL  - 122
SP  - 60
EP  - 65
DO  - 10.1016/j.patrec.2019.02.007
AN  - WOS:000462025800009
AD  - Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China
AD  - Guizhou Food Safety Inspect & Applicat Engn Techn, Changling South Rd, Guiyang 550002, Guizhou, Peoples R China
M2  - Guizhou Food Safety Inspect & Applicat Engn Techn
Y2  - 2019-04-05
ER  -

TY  - JOUR
AU  - Vo, Khoa
AU  - Yamazaki, Kashu
AU  - Truong, Sang
AU  - Tran, Minh-Triet
AU  - Sugimoto, Akihiro
AU  - Le, Ngan
TI  - ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation
T2  - IEEE ACCESS
M3  - Article
AB  - Temporal action proposal generation (TAPG) aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet plays an important role in many tasks of video analysis and understanding. Despite the great achievement in TAPG, most existing works ignore the human perception of interaction between agents and the surrounding environment by applying a deep learning model as a black-box to the untrimmed videos to extract video visual representation. Therefore, it is beneficial and potentially improves the performance of TAPG if we can capture these interactions between agents and the environment. In this paper, we propose a novel framework named Agent-Aware Boundary Network (ABN), which consists of two sub-networks: (1) an Agent-Aware Representation Network to obtain both agent-agent and agents-environment relationships in the video representation; and (2) a Boundary Generation Network to estimate the confidence score of temporal intervals. In the Agent-Aware Representation Network, the interactions between agents are expressed through local pathway, which operates at a local level to focus on the motions of agents whereas the overall perception of the surroundings are expressed through global pathway, which operates at a global level to perceive the effects of agents-environment. Comprehensive evaluations on 20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone networks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly outperforms state-of-the-art methods regardless of the employed backbone network on TAPG. We further examine the proposal quality by leveraging proposals generated by our method onto temporal action detection (TAD) frameworks and evaluate their detection performances.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2021 
PY  - 2021
VL  - 9
SP  - 126431
EP  - 126445
DO  - 10.1109/ACCESS.2021.3110973
AN  - WOS:000696661300001
AD  - Univ Arkansas, AICV Lab, Fayetteville, AR 72703 USA
AD  - Univ Sci, VNU HCM, Ho Chi Minh City 700000, Vietnam
AD  - Natl Inst Informat NII, Tokyo 1018430, Japan
AD  - Vietnam Natl Univ, Ho Chi Minh City 700000, Vietnam
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Liu, Pengyu
AU  - Wang, Fei
AU  - Li, Kun
AU  - Chen, Guoliang
AU  - Wei, Yanyan
AU  - Tang, Shengeng
AU  - Wu, Zhiliang
AU  - Guo, Dan
TI  - Micro-gesture Online Recognition using Learnable Query Points
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the Micro-gesture Online Recognition track in the MiGA challenge at IJCAI 2024. The Micro-gesture Online Recognition task involves identifying the category and locating the start and end times of micro-gestures in video clips. Compared to the typical Temporal Action Detection task, the Micro-gesture Online Recognition task focuses more on distinguishing between micro-gestures and pinpointing the start and end times of actions. Our solution ranks 2nd in the Micro-gesture Online Recognition track.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.04490
AN  - PPRN:90733645
AD  - Hefei Univ Technol HFUT, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, Hefei, Peoples R China
AD  - Minist Educ, Key Lab Knowledge Engn Big Data HFUT, Hefei, Peoples R China
AD  - Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
AD  - Anhui Zhonghuitong Technol Co Ltd, Hefei, Peoples R China
M2  - Minist Educ
M2  - Hefei Comprehens Natl Sci Ctr
M2  - Zhejiang Univ
M2  - Anhui Zhonghuitong Technol Co Ltd
Y2  - 2024-07-20
ER  -

TY  - JOUR
AU  - Biswas, Sovan
AU  - Souri, Yaser
AU  - Gall, Juergen
TI  - Hierarchical Graph-RNNs for Action Detection of Multiple Activities
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we propose an approach that spatially localizes the activities in a video frame where each person can perform multiple activities at the same time. Our approach takes the temporal scene context as well as the relations of the actions of detected persons into account. While the temporal context is modeled by a temporal recurrent neural network (RNN), the relations of the actions are modeled by a graph RNN. Both networks are trained together and the proposed approach achieves state of the art results on the AVA dataset.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2101.08581
AN  - PPRN:11474082
AD  - Univ Bonn, Bonn, Germany
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Singh, Gurkirt
AU  - Saha, Suman
AU  - Cuzzolin, Fabio
ED  - Jawahar, CV
ED  - Li, H
ED  - Mori, G
ED  - Schindler, K
TI  - TraMNet - Transition Matrix Network for Efficient Action Tube Proposals
T2  - COMPUTER VISION - ACCV 2018, PT VI
M3  - Proceedings Paper
CP  - 14th Asian Conference on Computer Vision (ACCV)
CL  - Perth, AUSTRALIA
AB  - Current state-of-the-art methods solve spatio-temporal action localisation by extending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate sets of temporally connected bounding boxes called action micro-tubes. However, they fail to consider that the underlying anchor proposal hypotheses should also move (transition) from frame to frame, as the actor or the camera do. Assuming we evaluate n 2D anchors in each frame, then the number of possible transitions from each 2D anchor to the next, for a sequence of f consecutive frames, is in the order of O(n(f)), expensive even for small values of f.To avoid this problem we introduce a Transition-Matrix-based Network (TraMNet) which relies on computing transition probabilities between anchor proposals while maximising their overlap with ground truth bounding boxes across frames, and enforcing sparsity via a transition threshold. As the resulting transition matrix is sparse and stochastic, this reduces the proposal hypothesis search space from O(n(f)) to the cardinality of the thresholded matrix. At training time, transitions are specific to cell locations of the feature maps, so that a sparse (efficient) transition matrix is used to train the network. At test time, a denser transition matrix can be obtained either by decreasing the threshold or by adding to it all the relative transitions originating from any cell location, allowing the network to handle transitions in the test data that might not have been present in the training data, thus making detection translation-invariant. We show that our network is able to handle sparse annotations such as those available in the DALY dataset, while allowing for both dense (accurate) or sparse (efficient) evaluation within a single model. We report extensive experiments on the DALY, UCF101-24 and Transformed-UCF101-24 datasets to support our claims.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-20876-9
SN  - 978-3-030-20875-2
DA  - 2019 
PY  - 2019
VL  - 11366
SP  - 420
EP  - 437
DO  - 10.1007/978-3-030-20876-9_27
AN  - WOS:000492905500027
AD  - Oxford Brookes Univ, Visual Artificial Intelligence Lab VAIL, Oxford, England
Y2  - 2019-11-07
ER  -

TY  - CPAPER
AU  - Cai, Ting
AU  - Xiong, Yu
AU  - He, ChengYang
AU  - Wu, Chao
AU  - Zhou, Song
A1  - IEEE
TI  - TBU: A Large-scale Multi-mask Video Dataset for Teacher Behavior Understanding
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME 2024
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Niagra Falls, CANADA
AB  - The study of classroom teachers' teaching behavior aims to track the development process of teacher behavior, which provides evidence for teachers' reflection on classroom teaching. Although the recent attempts propose several promising directions for the analysis of teaching behavior, the existing public datasets are still insufficient to meet the need for these potential solutions due to lacking of varied classroom environment, fine-grained teaching scene behavior data. In this paper, we construct a large-scale, diverse, scenario-specific, and multi-task teacher behavior dataset named TBU. On top of it, we systematically investigate representative methods on multiple tasks on TBU, which can serve as a benchmark for the research towards a more comprehensive understanding of teaching video data. The dataset can be available at: https://github.com/cai-KU/TBU.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 979-8-3503-9015-5
SN  - 979-8-3503-9016-2
DA  - 2024 
PY  - 2024
DO  - 10.1109/ICME57554.2024.10687642
AN  - WOS:001364925201023
AD  - Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing, Peoples R China
AD  - Chongqing Univ Posts & Telecommun, Chongqing, Peoples R China
Y2  - 2025-04-16
ER  -

TY  - BOOK
AU  - Shou, Zheng
Z2  -  
TI  - Deep Learning for Action Understanding in Video
M3  - Dissertation/Thesis
SN  - 978-1-392-07873-0
DA  - 2019 
PY  - 2019
AN  - PQDT:61115043
AD  - Columbia University, Electrical Engineering, New York, United States
M2  - Columbia University
ER  -

TY  - JOUR
AU  - Cheng, Sheng-Tzong
AU  - Hsu, Chih-Wei
AU  - Horng, Gwo-Jiun
AU  - Jiang, Ci-Ruei
TI  - Video reasoning for conflict events through feature extraction
T2  - JOURNAL OF SUPERCOMPUTING
M3  - Article
AB  - The rapid growth of multimedia data and the improvement of deep learning technology has allowed high-accuracy models to be trained for various fields. Video tools such as video classification, temporal action detection, and video summary are now available for the understanding of videos. In daily life, many social events start with a small conflict event. If conflicts and the subsequent dangers can be learned about from a video, we can prevent social incidents from occurring early on. This research presents a video and audio reasoning network that infers possible conflict events through video and audio features. To make the respective model more generalizable to other tasks, we have also added a predictive network to predict the risk of conflict events. We use multitasking to render the characteristics of movies and voices more generalizable to other similar tasks. We also propose several methods to integrate video features and audio features, improving the reasoning performance of the model. There's a model we proposed is called the video and audio reasoning Network (VARN) which is more accurate than other models. Compared with RandomNet, it achieves a 2.9 times greater accuracy.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-8542
SN  - 1573-0484
DA  - 2021 JUN
PY  - 2021
VL  - 77
IS  - 6
SP  - 6435
EP  - 6455
DO  - 10.1007/s11227-020-03514-5
AN  - WOS:000604201000002
C6  - JAN 2021
AD  - Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan
AD  - Southern Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Tainan, Taiwan
Y2  - 2021-01-19
ER  -

TY  - CPAPER
AU  - Ben-Ari, Rami
AU  - Nacson, Mor Shpigel
AU  - Azulai, Ophir
AU  - Barzelay, Udi
AU  - Rotman, Daniel
A1  - IEEE Comp Soc
TI  - TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Classification of new class entities requires collecting and annotating hundreds or thousands of samples that is often prohibitively costly. Few-shot learning suggests learning to classify new classes using just a few examples. Only a small number of studies address the challenge of few-shot learning on spatio-temporal patterns such as videos. In this paper, we present the Temporal Aware Embedding Network (TAEN) for few-shot action recognition, that learns to represent actions, in a metric space as a trajectory, conveying both short term semantics and longer term connectivity between action parts. We demonstrate the effectiveness of TAEN on two few shot tasks, video classification and temporal action detection and evaluate our method on the Kinetics-400 and on ActivityNet 1.2 few-shot benchmarks. With training of just a few fully connected layers we reach comparable results to prior art on both few shot video classification and temporal detection tasks, while reaching state-of-the-art in certain scenarios.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 978-1-6654-4899-4
DA  - 2021 
PY  - 2021
SP  - 2780
EP  - 2788
DO  - 10.1109/CVPRW53098.2021.00313
AN  - WOS:000705890202098
AD  - OriginAI, Haifa, Israel
AD  - Technion, Haifa, Israel
AD  - IBM Res AI, Haifa, Israel
M2  - OriginAI
M2  - IBM Res AI
Y2  - 2021-11-13
ER  -

TY  - JOUR
AU  - Zou, Songshang
AU  - Xiao, Guangyi
AU  - Liu, Kun
AU  - Peng, Shun
AU  - Wei, Jianglin
AU  - Gao, Yining
AU  - Chen, Hao
TI  - Real-Time Cargo Loading Identification Based on Self-Powered Camera
T2  - IEEE SENSORS JOURNAL
M3  - Article
AB  - In practice, the statistics of cargo loading capacity needs to be realized by counting the loading actions. Object detection algorithm may not meet the requirements of such tasks under the influence of environmental factors. And the current temporal action localization methods may ignore the spatial information of objects and identify the inaccurate action time boundary of multiple objects in the cargo loading task. To fill this technical gap, this paper designs a cargo loading identification framework. The core of this framework is the Target Area Rise-descend (TAR) algorithm, which is based on the object detection model and uses the regularity of object location and object size to realize real-time recognition of the cargo loading process. We ensure the real-time and robustness of the algorithm by some means like nap mechanism and time regularity. The garbage truck recycling task is taken as an example to complete the elaboration of the proposed method. Experiments show that the accuracy of the proposed method is higher than 99%, and the realtime performance is higher than 20fps on Neural Network Computing Unit (NPU). Considering the limitations of intelligent application scenarios and resources, self-powered camera is applied. For mechanical motion similar to the scene in this paper, the proposed algorithm can use extremely low computing power to capture the motion process and achieve data statistics.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1530-437X
SN  - 1558-1748
DA  - 2023 SEP 15
PY  - 2023
VL  - 23
IS  - 18
SP  - 20794
EP  - 20804
DO  - 10.1109/JSEN.2022.3169566
AN  - WOS:001090399700032
AD  - Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China
Y2  - 2023-11-11
ER  -

TY  - JOUR
AU  - Guo, Dashan
AU  - Li, Wei
AU  - Fang, Xiangzhong
TI  - Fully Convolutional Network for Multiscale Temporal Action Proposals
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Similar to the function of object proposals in localizing objects within images, temporal action proposals can facilitate the extraction of semantic segments and simplify the computations required for temporal action localization in untrimmed videos. In this paper, we propose a fully convolutional network to identify multistale temporal action proposals (FCN-TAP) that utilizes only the temporal convolutions to retrieve accurate action proposals for video sequences. Using gated linear units, our network enables simple but powerful inferences, and by parallelizing the computations, it significantly improves performances compared with previous recurrent models. To capture more temporal contexts with fewer parameters, we apply dilated convolutions to expand the receptive fields of our network. Moreover, we divide the receptive fields into multiple scale ranges and then refine the corresponding temporal boundaries using duration regression at each scale. To generate suitable segments with arbitrary durations for training, we design a new strategy to select sampled candidates within the corresponding scale range. The power of our method is demonstrated through experiments on the THUMOS'14 and ActivityNet datasets, where FCN-TAP performs better and achieves a remarkable speedup compared to other state-of-the-art methods. Additional experiments show that our method generates high-quality proposals and improves the localization stage of existing action detection pipelines.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2018 DEC
PY  - 2018
VL  - 20
IS  - 12
SP  - 3428
EP  - 3438
DO  - 10.1109/TMM.2018.2839534
AN  - WOS:000450212600020
AD  - Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China
Y2  - 2018-12-28
ER  -

TY  - JOUR
AU  - Lan, Xiaohan
AU  - Yuan, Yitian
AU  - Wang, Xin
AU  - Wang, Zhi
AU  - Zhu, Wenwu
TI  - A Survey on Temporal Sentence Grounding in Videos
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Temporal sentence grounding in videos (TSGV), which aims at localizing one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities (i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which (i) summarizes the taxonomy of existing methods, (ii) provides a detailed description of the evaluation protocols (i.e., datasets and metrics) to be used in TSGV, and (iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, single-stage methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting-edge research in TSGV. Besides, we also share our insights on several promising directions, including four typical tasks with new and practical settings based on TSGV.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2023 MAR
PY  - 2023
VL  - 19
IS  - 2
C7  - 51
DO  - 10.1145/3532626
AN  - WOS:001011190000001
AD  - Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China
AD  - Meituan, 4 Wangjing East St, Beijing 100020, Peoples R China
AD  - Tsinghua Univ, 30 Shuangqing Rd, Beijing 100084, Peoples R China
M2  - Meituan
Y2  - 2023-07-27
ER  -

TY  - JOUR
AU  - Yudistira, Novanto
AU  - Kavitha, Muthu Subash
AU  - Kurita, Takio
TI  - Weakly-Supervised Action Localization, and Action Recognition Using Global-Local Attention of 3D CNN
T2  - INTERNATIONAL JOURNAL OF COMPUTER VISION
M3  - Article
AB  - 3D convolutional neural network (3D CNN) captures spatial and temporal information on 3D data such as video sequences. However, due to the convolution and pooling mechanism, the information loss that occurs seems unavoidable. To improve the visual explanations and classification in 3D CNN, we propose two approaches; (i) aggregate layer-wise global to local (global-local) discrete gradient using trained 3DResNext network, and (ii) implement attention gating network to improve the accuracy of the action recognition. The proposed approach intends to show the usefulness of every layer termed as global-local attention in 3D CNN via visual attribution, weakly-supervised action localization, and action recognition. Firstly, the 3DResNext is trained and applied for action classification using backpropagation concerning the maximum predicted class. The gradient and activation of every layer are then up-sampled. Later, aggregation is used to produce more nuanced attention, which points out the most critical part of the predicted class's input videos. We use contour thresholding of final attention for final localization. We evaluate spatial and temporal action localization in trimmed videos using fine-grained visual explanation via 3DCAM. Experimental results show that the proposed approach produces informative visual explanations and discriminative attention. Furthermore, the action recognition via attention gating of each layer produces better classification results than the baseline model.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-5691
SN  - 1573-1405
DA  - 2022 OCT
PY  - 2022
VL  - 130
IS  - 10
SP  - 2349
EP  - 2363
DO  - 10.1007/s11263-022-01649-x
AN  - WOS:000833995800001
C6  - AUG 2022
AD  - Brawijaya Univ, Fac Comp Sci, Informat Engn, Vet St 8, Malang 65145, East Java, Indonesia
AD  - Nagasaki Univ, Sch Informat & Data Sci, 1-14 Bunkyo Machi, Nagasaki, Japan
AD  - Hiroshima Univ, Grad Sch Adv Sci & Engn, Higashihiroshima, Hiroshima 7398521, Japan
Y2  - 2022-08-08
ER  -

TY  - JOUR
AU  - Zhang, Zijian
AU  - Zhao, Zhou
AU  - Zhang, Zhu
AU  - Lin, Zhijie
AU  - Wang, Qi
AU  - Hong, Richang
TI  - Temporal Textual Localization in Video via Adversarial Bi-Directional Interaction Networks
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - Given a natural language description, temporal textual localization aims to localize the most relevant segment in an untrimmed video, which is a natural and imperative extension of temporal action localization. Most existing temporal textual localization works neglect the long-range semantic modeling in video contents and lack accurate textual understanding. Moreover, they remain in single-task learning and fail to exploit multi-view supervised information. Based on these observations, we introduce a novel adversarial bi-directional interaction network, which is a global framework to retrieve the target segment directly. Specifically, we propose a bi-directional attention mechanism to build bi-directional information interaction, which captures long-range semantic dependencies from video context and enhances textual representation learning. After localization, we further advise an auxiliary discriminator network to verify the localization result and boost the performance by adversarial training process. We adopt multi-task learning approach to train our model, including: (1) predicting coordinate probability distribution task, which selects start and end frame to localize target segment; (2) predicting frame-level correlation distribution task, which calculates the correlation between frame and description; (3) auxiliary adversarial learning task, which calculates matched score between localization and description to boost the performance. The extensive experiments on ActivityNet Captions and TACoS show the significant effectiveness and efficiency of our method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2021 
PY  - 2021
VL  - 23
SP  - 3306
EP  - 3317
DO  - 10.1109/TMM.2020.3023339
AN  - WOS:000698902000028
AD  - Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China
AD  - Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China
AD  - Alibaba Inc, Hangzhou 310027, Peoples R China
AD  - Hefei Univ Technol, Dept Comp Sci & Technol, Hefei 230009, Peoples R China
Y2  - 2022-09-06
ER  -

TY  - JOUR
AU  - Fu, Jie
AU  - Gao, Junyu
AU  - Xu, Changsheng
TI  - Compact Representation and Reliable Classification Learning for Point-Level Weakly-Supervised Action Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Point-level weakly-supervised temporal action localization (P-WSTAL) aims to localize temporal extents of action instances and identify the corresponding categories with only a single point label for each action instance for training. Due to the sparse frame-level annotations, most existing models are in the localization-by-classification pipeline. However, there exist two major issues in this pipeline: large intra-action variation due to task gap between classification and localization and noisy classification learning caused by unreliable pseudo training samples. In this paper, we propose a novel framework CRRC-Net, which introduces a co-supervised feature learning module and a probabilistic pseudo label mining module, to simultaneously address the above two issues. Specifically, the co-supervised feature learning module is applied to exploit the complementary information in different modalities for learning more compact feature representations. Furthermore, the probabilistic pseudo label mining module utilizes the feature distances from action prototypes to estimate the likelihood of pseudo samples and rectify their corresponding labels for more reliable classification learning. Comprehensive experiments are conducted on different benchmarks and the experimental results show that our method achieves favorable performance with the state-of-the-art.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 7363
EP  - 7377
DO  - 10.1109/TIP.2022.3222623
AN  - WOS:000892917400003
AD  - Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518066, Peoples R China
Y2  - 2022-12-18
ER  -

TY  - CPAPER
AU  - Zeng, Runhao
AU  - Chen, Xiaoyong
AU  - Liang, Jiaming
AU  - Wu, Huisi
AU  - Cao, Guangzhong
AU  - Guo, Yong
A1  - IEEE
TI  - Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the Frame-Drop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18263
EP  - 18274
DO  - 10.1109/CVPR52733.2024.01729
AN  - WOS:001342515501058
AD  - Shenzhen MSU BIT Univ, Artificial Intelligence Res Inst, Shenzhen, Peoples R China
AD  - Guangdong Hong Kong Macao Joint Lab Emot Intellig, Hong Kong, Peoples R China
AD  - Shenzhen Univ, Shenzhen, Peoples R China
AD  - South China Univ Technol, Guangzhou, Peoples R China
M2  - Guangdong Hong Kong Macao Joint Lab Emot Intellig
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Gupta, Pranav
AU  - Krishnan, Advith
AU  - Nanda, Naman
AU  - Eswar, Ananth
AU  - Agarwal, Deeksha
AU  - Gohil, Pratham
AU  - Goel, Pratyush
TI  - ViDAS: Vision-based Danger Assessment and Scoring
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We present a novel dataset aimed at advancing danger analysis and assessment by addressing the challenge of quantifying danger in video content and identifying how human-like a Large Language Model (LLM) evaluator is for the same. This is achieved by compiling a collection of 100 YouTube videos featuring various events. Each video is annotated by human participants who provided danger ratings on a scale from 0 (no danger to humans) to 10 (life-threatening), with precise timestamps indicating moments of heightened danger. Additionally, we leverage LLMs to independently assess the danger levels in these videos using video summaries. We introduce Mean Squared Error (MSE) scores for multimodal meta-evaluation of the alignment between human and LLM danger assessments. Our dataset not only contributes a new resource for danger assessment in video content but also demonstrates the potential of LLMs in achieving human-like evaluations.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2410.00477
AN  - PPRN:100751262
AD  - SRM Inst Sci & Technol, Chennai, India
AD  - Vellore Inst Technol, Vellore, India
M2  - SRM Inst Sci & Technol
Y2  - 2024-10-12
ER  -

TY  - JOUR
AU  - Li, Ping
AU  - Cao, Jiachen
AU  - Yuan, Li
AU  - Ye, Qinghao
AU  - Xu, Xianghua
TI  - Truncated attention-aware proposal networks with multi-scale dilation for temporal action detection
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Detecting actions temporally in untrimmed videos is very challenging, and it accomplishes action clas-sification and localization simultaneously. Capturing the relations among action proposals (i.e., candidate video segments) is of vital importance. While there have been several attempts to encode such relations, they neglect the adverse effects of those irrelevant or negative relations among proposals. Besides, there is a crucial fact that action durations are flexible in videos, which has not been well explored. For the former, we develop a truncated attention mechanism that learns positive proposal relations by dynam-ically adjusting edge weights of proposal nodes in a graph, and construct the proposal network model using graph convolution networks to suppress disadvantageous relations of proposal pairs by truncating negative attention scores. For the latter, we devise a light multi-scale dilation module shared by all pro-posals to handle different action durations by enlarging temporal receptive field, thus capturing temporal context to increase the representation capacity of proposals. Unifying these considerations, we present the Multi-scale Dilation based Truncated Attention Proposal Network (MD-TAPN) model for temporal ac-tion detection. Our model achieves state-of-the-art performances of detecting actions on two benchmark databases, and especially it outperforms the most competitive method by a significant gain of 3.6% mAP at tIoU0.5 on THUMOS14. (c) 2023 Elsevier Ltd. All rights reserved.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2023 OCT
PY  - 2023
VL  - 142
C7  - 109684
DO  - 10.1016/j.patcog.2023.109684
AN  - WOS:001001784800001
C6  - MAY 2023
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China
AD  - Univ Calif San Diego, San Diego, CA USA
Y2  - 2023-06-15
ER  -

TY  - JOUR
AU  - Chen, Eli
AU  - Haik, Oren
AU  - Yitzhaky, Yitzhak
TI  - Online Spatio-Temporal Action Detection in Long-Distance Imaging Affected by the Atmosphere
T2  - IEEE ACCESS
M3  - Article
AB  - Current state-of-the-art approaches for spatio-temporal action detection deal with stable videos and quite sterilized environments, as seen in the UCF-101 benchmark. In addition, the objects of interest are typically relatively close to the camera, and therefore fairly clear and easily distinguished. This study presents an approach method for online human action detection in long-distance imaging affected by atmospheric distortions. We created a unique dataset of typical actions in long-range imaging. Various CNN frameworks were examined for the initial moving object detection phase, including 2D, 3D, one stream, and two-stream (RGB frames and optical flow). The basic object detection methods examined within these frameworks include the YOLOv3 and an extension of the inflated 3D ConvNet with a Feature-Fused Single Shot Multibox Detector (FFSSD) to improve small object detection. To cope with the harmful effect of the spatio-temporal random movements induced by atmospheric effects on motion estimation, we first fit the optical flow stream characteristics to a temporally noisy turbulent environment. A significant improvement of the action detection quality under such noisy conditions was obtained by constructing an online tracking algorithm that incrementally constructs and labels the objects' tracks from the network's frame-level detections. Experimental results show that our approach outperforms the state-of-the-art on our dataset in terms of the mAP measure.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2021 
PY  - 2021
VL  - 9
SP  - 24531
EP  - 24545
DO  - 10.1109/ACCESS.2021.3057172
AN  - WOS:000617354000001
AD  - Ben Gurion Univ Negev, Sch Elect & Comp Engn, Dept Electroopt Engn, IL-84105 Beer Sheva, Israel
AD  - HP Inc, Res & Dev Dept, IL-76101 Ness Ziona, Israel
M2  - HP Inc
Y2  - 2021-03-02
ER  -

TY  - JOUR
AU  - Chen, Brian
AU  - Rouditchenko, Andrew
AU  - Duarte, Kevin
AU  - Kuehne, Hilde
AU  - Thomas, Samuel
AU  - Boggust, Angie
AU  - Panda, Rameswar
AU  - Kingsbury, Brian
AU  - Feris, Rogerio
AU  - Harwath, David
AU  - Glass, James
AU  - Picheny, Michael
AU  - Chang, Shih-Fu
TI  - Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a self-supervised training framework that learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2104.12671
AN  - PPRN:11941746
AD  - Columbia Univ, New York City, NY 10027, USA
AD  - MIT CSAIL, Cambridge, MA 02139, USA
AD  - Univ Cent Florida, Orlando, FL 32816, USA
AD  - Goethe Univ Frankfurt, Frankfurt, Germany
AD  - IBM Res AI, Armonk, NY 10504, USA
AD  - IBM Watson Lab, MIT, Cambridge, MA 02142, USA
AD  - UT Austin, Austin, TX 78712, USA
AD  - NYU Courant CS & CDS, New York City, NY 10012, USA
M2  - Columbia Univ
M2  - MIT CSAIL
M2  - Goethe Univ Frankfurt
M2  - IBM Res AI
M2  - IBM Watson Lab
M2  - UT Austin
M2  - NYU Courant CS & CDS
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Arun, B.
AU  - Samanta, Debasis
A1  - IEEE
TI  - Impact of Fragmentation on Temporal Event Localization for Speech EEG
T2  - 2024 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS, SPCOM 2024
M3  - Proceedings Paper
CP  - 15th International Conference on Signal Processing and Communications (SPCOM)
CL  - Indian Inst Sci, Bangalore, INDIA
AB  - This study investigates the effect of temporal event localization of speech-imagery in electroencephalogram (EEG) signals. Temporal event localization is required for speech-imagery EEG signals as they are often recorded over long segments with the actual event being a fraction of the recorded duration. It is achieved by fragmenting the EEG epochs into smaller components and identifying those fragments from different signals that are common to the same class. After extracting these fragments and merging them into a new set of signals, they are validated using a set of classifiers. The results show a significant improvement in the classifier performance when compared to using signals without fragmentation. Extracting these fragments would also allow for extracting features of syllables (and eventually, phonemes), as the same algorithm used to fragment the signals may be used to extract features of those word components.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2474-9168
SN  - 2474-915X
SN  - 979-8-3503-5046-3
SN  - 979-8-3503-5045-6
DA  - 2024 
PY  - 2024
DO  - 10.1109/SPCOM60851.2024.10631625
AN  - WOS:001327674900049
AD  - Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur, W Bengal, India
Y2  - 2024-11-14
ER  -

TY  - CPAPER
AU  - Masuda, Tsuyoshi
AU  - Togo, Ren
AU  - Ogawa, Takahiro
AU  - Haseyama, Miki
ED  - Nakajima, M
ED  - Kim, JG
ED  - Lie, WN
ED  - Kemao, Q
TI  - A Note on Detection of Sports Action Based on Temporal Cycle Consistency Learning
T2  - INTERNATIONAL WORKSHOP ON ADVANCED IMAGING TECHNOLOGY (IWAIT) 2021
M3  - Proceedings Paper
CP  - International Workshop on Advanced Imaging Technology (IWAIT)
CL  - ELECTR NETWORK
AB  - This paper presents a method for action detection based on Temporal Cycle Consistency(TCC) Learning. The proposed method realizes the action detection of flexible length segments based on a frame-level action prediction technique. We enable calculation of similarities for spatio-temporal features based on TCC to detect target actions from input videos. Finally, our method determines temporal segments by smoothing the frame-level action detection result. Experimental results show the validity of the proposed method.
PU  - SPIE-INT SOC OPTICAL ENGINEERING
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN  - 0277-786X
SN  - 1996-756X
SN  - 978-1-5106-4365-9
DA  - 2021 
PY  - 2021
VL  - 11766
C7  - 117660P
DO  - 10.1117/12.2590987
AN  - WOS:000671837000024
AD  - Hokkaido Univ, Grad Sch Informat Sci & Technol, Hokkaido, Japan
AD  - Hokkaido Univ, Educat & Res Ctr Math & Data Sci, Hokkaido, Japan
AD  - Hokkaido Univ, Fac Informat Sci & Technol, Hokkaido, Japan
Y2  - 2021-07-25
ER  -

TY  - JOUR
AU  - Pardo, Alejandro
AU  - Alwassel, Humam
AU  - Caba Heilbron, Fabian
AU  - Thabet, Ali
AU  - Ghanem, Bernard
TI  - RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video action detectors are usually trained using datasets with fully-supervised temporal annotations. Building such datasets is an expensive task. To alleviate this problem, recent methods have tried to leverage weak labeling, where videos are untrimmed and only a video-level label is available. In this paper, we propose RefineLoc, a novel weakly-supervised temporal action localization method. RefineLoc uses an iterative refinement approach by estimating and training on snippet-level pseudo ground truth at every iteration. We show the benefit of this iterative approach and present an extensive analysis of five different pseudo ground truth generators. We show the effectiveness of our model on two standard action datasets, ActivityNet v1.2 and THUMOS14. RefineLoc shows competitive results with the state-of-the-art in weakly-supervised temporal localization. Additionally, our iterative refinement process is able to significantly improve the performance of two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1904.00227
AN  - PPRN:15021800
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Yang, Min
AU  - Chen, Guo
AU  - Zheng, Yin-Dong
AU  - Lu, Tong
AU  - Wang, Limin
TI  - BasicTAD: An astounding RGB-Only baseline for temporal action detection
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - Temporal action detection (TAD) is extensively studied in the video understanding community by generally following the object detection pipeline in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling, and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low detection efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We extensively investigate the existing techniques in each component for this baseline and, more importantly, perform end-to -end training over the entire pipeline thanks to the simplicity of design. As a result, this simple BasicTAD yields an astounding and real-time RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as PlusTAD). Empirical results demonstrate that our PlusTAD is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Meanwhile, we also perform in-depth visualization and error analysis on our proposed method and try to provide more insights into the TAD problem. Our approach can serve as a strong baseline for future TAD research. The code and model are released at https://github.com/MCG-NJU/BasicTAD.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2023 JUL
PY  - 2023
VL  - 232
C7  - 103692
DO  - 10.1016/j.cviu.2023.103692
AN  - WOS:000982605900001
C6  - APR 2023
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
Y2  - 2023-05-25
ER  -

TY  - JOUR
AU  - Gao, Jiyang
AU  - Yang, Zhenheng
AU  - Sun, Chen
AU  - Chen, Kan
AU  - Nevatia, Ram
TI  - TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1703.06189
AN  - PPRN:12836696
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Wang, Xiangyang
AU  - Yang, Kun
AU  - Ding, Qiang
AU  - Wang, Rui
AU  - Sun, Jinhua
TI  - TQRFormer: Tubelet query recollection transformer for action detection
T2  - IMAGE AND VISION COMPUTING
M3  - Article
AB  - Spatial and temporal action detection aims to precisely locate actions while predicting their respective categories. The existing solution, TubeR (Zhao et al., 2022), is designed to directly detect action tubes in videos by recognizing and localizing actions using a unified representation. However, a potential challenge arises during the decoding stage, leading to a gradual decrease in the model's performance in action detection, specifically in terms of the confidence associated with detected actions. In this paper, we propose TQRFormer: Tubelet Query Recollection Transformer, enabling the subsequent decoder to obtain information from the previous stage. Specifically, we designed Query Recollection Attention to correct errors and output the synthesized results, effectively breaking the limitations of sequential decoding. During the training stage, TubeR (Zhao et al., 2022) generates a limited number of positive sample queries through a one-to-one matching strategy, potentially impacting the effectiveness of training with positive samples. To enhance the quantity of positive samples, we propose a stage matching approach that combines both one -to -many matching and one-to-one matching without additional queries. This approach serves to boost the overall number of positive samples for improved training outcomes. We also propose a more elegant classification head that contains the start and end frames of the small tubes information, eliminating the necessity for a separate action switch. The performance of TQRFormer is superior to previous state-of-the-art technologies on public action detection datasets, including AVA, UCF101 -24, JHMDB-21 and MultiSports. The code will available at https://github.com/ykyk000/TQRFormer.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0262-8856
SN  - 1872-8138
DA  - 2024 JUL
PY  - 2024
VL  - 147
C7  - 105059
DO  - 10.1016/j.imavis.2024.105059
AN  - WOS:001242618400001
C6  - MAY 2024
AD  - Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China
AD  - Fudan Univ, Natl Childrens Med Ctr, Dept Psychol Med, Childrens Hosp, Shanghai 201102, Peoples R China
Y2  - 2024-06-15
ER  -

TY  - JOUR
AU  - Arnab, Anurag
AU  - Xiong, Xuehan
AU  - Gritsenko, Alexey
AU  - Romijnders, Rob
AU  - Djolonga, Josip
AU  - Dehghani, Mostafa
AU  - Sun, Chen
AU  - Lucic, Mario
AU  - Schmid, Cordelia
TI  - Beyond Transfer Learning: Co-finetuning for Action Localisation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Transfer learning is the predominant paradigm for training deep networks on small target datasets. Models are typically pretrained on large ``upstream&#39;&#39; datasets for classification, as such labels are easy to collect, and then finetuned on ``downstream&#39;&#39; tasks such as action localisation, which are smaller due to their finer-grained annotations. In this paper, we question this approach, and propose co-finetuning -- simultaneously training a single model on multiple "upstream&#39;&#39; and "downstream&#39;&#39; tasks. We demonstrate that co-finetuning outperforms traditional transfer learning when using the same total amount of data, and also show how we can easily extend our approach to multiple ``upstream&#39;&#39; datasets to further improve performance. In particular, co-finetuning significantly improves the performance on rare classes in our downstream task, as it has a regularising effect, and enables the network to learn feature representations that transfer between different datasets. Finally, we observe how co-finetuning with public, video classification datasets, we are able to achieve state-of-the-art results for spatio-temporal action localisation on the challenging AVA and AVA-Kinetics datasets, outperforming recent works which develop intricate models.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2207.03807
AN  - PPRN:10389618
AD  - Google Res, New York City, NY 10011, USA
M2  - Google Res
Y2  - 2023-03-17
ER  -

TY  - CPAPER
AU  - Gupta, Pranav
AU  - Krishnan, Advith
AU  - Nanda, Naman
AU  - Eswar, Ananth
AU  - Agrawal, Deeksha
AU  - Gohil, Pratham
AU  - Goel, Pratyush
A1  - ASSOC COMPUTING MACHINERY
TI  - ViDAS: Vision-based Danger Assessment and Scoring
T2  - PROCEEDINGS OF FIFTEENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING, ICVGIP 2024
M3  - Proceedings Paper
CP  - 15th Indian Conference on Computer Vision Graphics and Image Processing
CL  - Bengaluru, INDIA
AB  - We present a novel dataset aimed at advancing danger analysis and assessment by addressing the challenge of quantifying danger in video content and identifying how human-like a Large Language Model (LLM) evaluator is for the same. This is achieved by compiling a collection of 100 YouTube videos featuring various events. Each video is annotated by human participants who provided danger ratings on a scale from 0 (no danger to humans) to 10 (life-threatening), with precise timestamps indicating moments of heightened danger. Additionally, we leverage LLMs to independently assess the danger levels in these videos using video summaries. We introduce Mean Squared Error (MSE) scores for multimodal meta-evaluation of the alignment between human and LLM danger assessments. Our dataset not only contributes a new resource for danger assessment in video content but also demonstrates the potential of LLMs in achieving human-like evaluations.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-1075-9
DA  - 2024 
PY  - 2024
C7  - 29
DO  - 10.1145/3702250.3702279
AN  - WOS:001450046600029
AD  - SRM Inst Sci & Technol, Chennai, Tamil Nadu, India
AD  - VIT, Chennai, Tamil Nadu, India
Y2  - 2025-04-24
ER  -

TY  - JOUR
AU  - Aboah, Armstrong
AU  - Bagci, Ulas
AU  - Mussah, Abdul Rashid
AU  - Owor, Neema Jakisa
AU  - Adu-Gyamfi, Yaw
TI  - DeepSegmenter: Temporal Action Localization for Detecting Anomalies in Untrimmed Naturalistic Driving Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Identifying unusual driving behaviors exhibited by drivers during driving is essential for understanding driver behavior and the underlying causes of crashes. Previous studies have primarily approached this problem as a classification task, assuming that naturalistic driving videos come discretized. However, both activity segmentation and classification are required for this task due to the continuous nature of naturalistic driving videos. The current study therefore departs from conventional approaches and introduces a novel methodological framework, DeepSegmenter, that simultaneously performs activity segmentation and classification in a single framework. The proposed framework consists of four major modules namely Data Module, Activity Segmentation Module, Classification Module and Postprocessing Module. Our proposed method won 8th place in the 2023 AI City Challenge, Track 3, with an activity overlap score of 0.5426 on experimental validation data. The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.08261
AN  - PPRN:63560990
AD  - Northwestern Univ, Dept Radiol, Chicago, IL 60611, USA
AD  - Univ Missouri Columbia, Dept Civil Engn, Columbia, MO 65211, USA
M2  - Univ Missouri Columbia
Y2  - 2023-04-29
ER  -

TY  - JOUR
AU  - Nguyen, Phuc
AU  - Liu, Ting
AU  - Prasad, Gautam
AU  - Han, Bohyung
TI  - Weakly Supervised Action Localization by Sparse Temporal Pooling Network
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1712.05080
AN  - PPRN:12872437
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Hu, Xin
AU  - Wu, Zhenyu
AU  - Miao, Hao-Yu
AU  - Fan, Siqi
AU  - Long, Taiyu
AU  - Hu, Zhenyu
AU  - Pi, Pengcheng
AU  - Wu, Yi
AU  - Ren, Zhou
AU  - Wang, Zhangyang
AU  - Hua, Gang
TI  - E2TAD: An Energy-Efficient Tracking-based Action Detector
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video action detection (spatio-temporal action localization) is usually the starting point for human-centric intelligent analysis of videos nowadays. It has high practical impacts for many applications across robotics, security, healthcare, etc. The two-stage paradigm of Faster R-CNN in object detection inspires a standard paradigm of video action detection, i.e., firstly generating person proposals and then classifying their actions. However, none of the existing solutions could provide fine-grained action detection to the &ldquo;who-when-where-what&rdquo; level. This paper presents a tracking-based solution to accurately and efficiently localize predefined key actions spatially (by predicting the associated target IDs and locations) and temporally (by predicting the time in exact frame indices). This solution won the first place in the UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC). The code is available at https://github.com/VITA-Group/21LPCV-UAV-Solution.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2204.04416
AN  - PPRN:22447530
AD  - UT Austin, Austin, TX 78712, USA
AD  - Texas A&M Univ, College Stn, TX 77843, USA
AD  - NYU, New York City, NY 10012, USA
AD  - Wormpex Res, Bellevue, WA 98004, USA
M2  - UT Austin
M2  - Texas A&M Univ
M2  - NYU
M2  - Wormpex Res
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Zhao, Hangyue
AU  - Xiao, Yuchao
AU  - Zhao, Yanyun
A1  - IEEE
TI  - PAND: Precise Action Recognition on Naturalistic Driving
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Temporal action localization for untrimmed videos is a difficult problem in computer vision. It is challenge to infer the start and end of activity instances on small-scale datasets covering multi-view information accurately. In this paper, we propose an effective activity temporal localization and classification method to localize the temporal boundaries and predict the class label of activities for naturalistic driving. Our approach includes (i) a distraction behavior recognition and localization method in naturalistic driving videos on small-scale data sets, (ii) a strategy that uses multi-branch network to make full use of information from different channels, (iii)a post-processing method for selecting and correcting temporal range to ensure that our system finds accurate boundaries. In addition, the frame-level object detection information is also utilized. Extensive experiments prove the effectiveness of our method and we rank the 6th on the Test-A2 of the 6th AI City Challenge track 3.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3290
EP  - 3298
DO  - 10.1109/CVPRW56347.2022.00372
AN  - WOS:000861612703045
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
AD  - Beijing Key Lab Network Syst & Network Culture, Beijing, Peoples R China
M2  - Beijing Key Lab Network Syst & Network Culture
Y2  - 2022-12-07
ER  -

TY  - JOUR
AU  - Sun, Chen
AU  - Shrivastava, Abhinav
AU  - Vondrick, Carl
AU  - Murphy, Kevin
AU  - Sukthankar, Rahul
AU  - Schmid, Cordelia
TI  - Actor-Centric Relation Network
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level and model temporal context with 3D ConvNets. Here, we go one step further and model spatio-temporal relations to capture the interactions between human actors, relevant objects and scene elements essential to differentiate similar human actions. Our approach is weakly supervised and mines the relevant elements automatically with an actor-centric relational network (ACRN). ACRN computes and accumulates pair-wise relation information from actor and global scene features, and generates relation features for action classification. It is implemented as neural networks and can be trained jointly with an existing action detection system. We show that ACRN outperforms alternative approaches which capture relation information, and that the proposed framework improves upon the state-of-the-art performance on JHMDB and AVA. A visualization of the learned relation features confirms that our approach is able to attend to the relevant relations for each action.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.10982
AN  - PPRN:19354648
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Wake, Naoki
AU  - Kanehira, Atsushi
AU  - Sasabuchi, Kazuhiro
AU  - Takamatsu, Jun
AU  - Ikeuchi, Katsushi
TI  - Open-vocabulary Temporal Action Localization using VLMs
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.17422
AN  - PPRN:91642655
AD  - Microsoft, Appl Robot Res, Redmond, WA 98052, USA
M2  - Microsoft
Y2  - 2024-09-26
ER  -

TY  - JOUR
AU  - Murtaza, Fiza
AU  - Yousaf, Muhammad Haroon
AU  - Velastin, Sergio A.
AU  - Qian, Yu
TI  - Vectors of temporally correlated snippets for temporal action detection
T2  - COMPUTERS & ELECTRICAL ENGINEERING
M3  - Article
AB  - Detection of human actions in long untrimmed videos is an important but challenging task due to the unconstrained nature of actions present in untrimmed videos. We argue that untrimmed videos contain multiple snippets from actions and the background classes having significant correlation with each other, which results in imprecise detection of start-end times for action regions. In this work, we propose Vectors of Temporally Correlated Snippets (VTCS) which addresses this problem by finding the snippet-centroids from each class which are discriminant for their own class. For each untrimmed video, non-overlapping snippets are temporally correlated with the snippet-centroids using VTCS encoding to find the action proposals. We evaluate the performance of VTCS on the Thumos14 and ActivityNet datasets. For Thumos14, VTCS achieves a significant gain in mean Average Precision (mAP) at temporal Intersection over Union (tIoU) threshold 0.5, improving from 41.5% to 44.3%. For the sports-subset of ActivityNet dataset, VTCS obtains 38.5% mAP @0.5 tIoU threshold. (C) 2020 Elsevier Ltd. All rights reserved.
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN  - 0045-7906
SN  - 1879-0755
DA  - 2020 JUL
PY  - 2020
VL  - 85
C7  - 106654
DO  - 10.1016/j.compeleceng.2020.106654
AN  - WOS:000579056100008
AD  - Univ Engn & Technol Taxila, Dept Comp Engn, Taxila 47080, Pakistan
AD  - Natl Ctr Robot & Automat, Swarm Robot Lab, Rawalpindi, Pakistan
AD  - Zebra Technol Corp, London SE1 9LQ, England
AD  - Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England
AD  - Univ Carlos III Madrid, Dept Comp Sci, Appl Artificial Intelligence Res Grp, Madrid 28270, Spain
M2  - Natl Ctr Robot & Automat
M2  - Zebra Technol Corp
Y2  - 2020-10-30
ER  -

TY  - JOUR
AU  - Liao, Xiaoxin
AU  - Yuan, Jingyi
AU  - Cai, Zemin
AU  - Lai, Jian-huang
TI  - An attention-based bidirectional GRU network for temporal action proposals generation
T2  - JOURNAL OF SUPERCOMPUTING
M3  - Article
AB  - Temporal action detection is an important yet challenging task in video understanding task. Temporal action proposals generation is a common module in action detection, and it effects the performance of action detection greatly. The module requires methods not only generating proposals with accurate temporal boundaries, but also retrieving proposals to cover action instances with high recall using relative fewer proposals. To address these difficulties, we propose an Actionness Score Optimization Model to improve the accuracy of generated proposals by capturing global contextual information of untrimmed videos. Firstly, a deconvolution layer is utilized to learn a nonlinear upsampling for the extracted features, in both spatial and temporal domains. In order to reveal the contextual information, then we introduce the bidirectional gated recurrent unit to the network. Moreover, an attention mechanism is applied to the network so that it can focus on the most relevant parts of the information to obtain more reliable actionness scores. Finally, we validate the effectiveness of our proposed network on three challenging benchmark datasets, ActivityNet v1.2, ActivityNet v1.3, and THUMOS'14.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-8542
SN  - 1573-0484
DA  - 2023 MAY
PY  - 2023
VL  - 79
IS  - 8
SP  - 8322
EP  - 8339
DO  - 10.1007/s11227-022-04973-8
AN  - WOS:000899808500002
C6  - DEC 2022
AD  - Shantou Univ, Dept Elect Engn, Shantou 515063, Guangdong, Peoples R China
AD  - Key Lab Digital Signal & Image Proc Guangdong Prov, Shantou 515063, Guangdong, Peoples R China
AD  - Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China
AD  - Key Lab Machine Intelligent & Adv Comp, Minist Educ, Guangzhou 510006, Guangdong, Peoples R China
M2  - Key Lab Digital Signal & Image Proc Guangdong Prov
Y2  - 2023-01-26
ER  -

TY  - CPAPER
AU  - Wu, Yutang
AU  - Wang, Hanli
AU  - Wang, Shuheng
AU  - Li, Qinyu
A1  - IEEE
TI  - ENHANCED ACTION TUBELET DETECTOR FOR SPATIO-TEMPORAL VIDEO ACTION DETECTION
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING
M3  - Proceedings Paper
CP  - IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
CL  - Barcelona, SPAIN
AB  - Current spatio-temporal action detection methods usually employ a two-stream architecture, a RGB stream for raw images and an auxiliary motion stream for optical flow. Training is required individually for each stream and more efforts are necessary to improve the precision of RGB stream. To this end, a single stream network named enhanced action tubelet (EAT) detector is proposed in this work based on RGB stream. A modulation layer is designed to modulate RGB features with conditional information from the visual clues of optical flow and human pose. This network is end-to-end and the proposed layer can be easily applied into other action detectors. Experiments show that EAT detector outperforms traditional RGB stream and is competitive to existing two-stream methods while free from the trouble of training streams separately. By being embedded in a new three-stream architecture, the resulting three-stream EAT detector achieves impressive performances among the best competitors on UCF-Sports, J-HMDB and UCF-101.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1520-6149
SN  - 978-1-5090-6631-5
DA  - 2020 
PY  - 2020
SP  - 2388
EP  - 2392
DO  - 10.1109/icassp40776.2020.9054394
AN  - WOS:000615970402126
AD  - Tongji Univ, Dept Comp Sci & Technol, Shanghai, Peoples R China
AD  - Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai, Peoples R China
Y2  - 2021-03-02
ER  -

TY  - CPAPER
AU  - Chuong Nguyen
AU  - Ngoc Nguyen
AU  - Su Huynh
AU  - Vinh Nguyen
AU  - Son Nguyen
A1  - IEEE
TI  - Learning Generalized Feature for Temporal Action Detection: Application for Natural Driving Action Recognition Challenge
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW 2022
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - This paper reports our approach for the 2022 AI City Challenge - Naturalistic Driving Action Recognition (Track 3), where the objective is to detect when and what kinds of actions that a driver performs in a long, untrimmed video. Our solution is built upon the single stage ActionFormer detector, in which temporal location and classification are predicted simultaneously for efficiency. The input feature for the detector is extracted offline using our proposed backbone, which we named "ConvNext-Video". However, due to the small size of the dataset, training the model to avoid over-fitting becomes challenging. To address this problem, we focus on training techniques that can improve the generalization of underlying features. Specifically, we utilize two methods: "learning without forgetting" and semi-weak supervised learning on the unlabeled data A2. Finally, we also add a second-stage classifier (SSC) using our ConvNeXt-Video backbone. The SSC Classifer is designed to combine information from multi-clips and multi-view cameras to improve the prediction precision. Our best result achieves 29.1 F1 score on the public test set. Our source code is released at link.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2160-7508
SN  - 978-1-6654-8739-9
DA  - 2022 
PY  - 2022
SP  - 3248
EP  - 3255
DO  - 10.1109/CVPRW56347.2022.00367
AN  - WOS:000861612703040
AD  - CyberCore AI, Morioka, Iwate, Japan
M2  - CyberCore AI
Y2  - 2022-12-07
ER  -

TY  - CPAPER
AU  - Singh, Dinesh
A1  - IEEE
TI  - Graph Representation for Weakly-Supervised Spatio-Temporal Action Detection
T2  - 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
M3  - Proceedings Paper
CP  - International Joint Conference on Neural Networks (IJCNN)
CL  - Broadbeach, AUSTRALIA
AB  - Spatio-temporal action recognition and localization are crucial in several computer vision applications including video surveillance, video captioning to name a few. However, most of the existing action recognition and localization approaches are for offline use, perform well only on trimmed action clips. Also, they need precise annotations at the clip, frame, and pixel levels which is labor-intensive and thus undermines their usage for real-world large-scale scenarios. In this paper, we propose a weakly-supervised spatio-temporal action recognition and localization based on graph representation in untrimmed videos. More specifically, we propose an efficient graph representation of videos using only the clip level annotations, while existing approaches are either supervised or unsupervised learning approach. For graph construction, the local actions are determined based on the key interesting demeanor in an action clip and assigned the class label the same as that of the clip. This weak annotation impacts both action recognition and localization significantly because the local actions have considerable intra-class variability and inter-class similarity. To handle the intra-class variability and inter-class similarity, we use a weakly-supervised deep multiple instance ranking framework on the local action descriptors. To classify a graph of local actions into one of the action classes, we use a support vector machine along with a graph kernel and then localize the recognized action as a non-cubic shaped-portion of the video based on local actions in the graph. The experimental results show that the proposed approach outperforms the state-of-the-art methods on the three benchmark datasets, namely, THUMOS14, UCF-Sports, and JHMDB-21.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2161-4393
SN  - 978-1-6654-8867-9
DA  - 2023 
PY  - 2023
DO  - 10.1109/IJCNN54540.2023.10192033
AN  - WOS:001046198707098
AD  - Indian Inst Technol Mandi, Mandi, Himachal Prades, India
Y2  - 2023-09-30
ER  -

TY  - CPAPER
AU  - Huang, Qiubo
AU  - Liu, Zixuan
AU  - Lu, Ting
A1  - IEEE
TI  - Joint model for learning state recognition with combining action detection and object detection
T2  - 2022 IEEE INTERNATIONAL CONFERENCE ON IMAGING SYSTEMS AND TECHNIQUES (IST 2022)
M3  - Proceedings Paper
CP  - IEEE International Conference on Imaging Systems and Techniques (IST) / IEEE International School on Imaging
CL  - ELECTR NETWORK
AB  - How younger students, such as primary and secondary school students, can improve their concentration when studying alone has been the subject of research by education experts. In this paper, we develop a learning-state-based tomato-clock system that can help improve concentration. We propose a joint model to detect students' learning states and thus control whether the tasks of the tomato-clock can be completed properly, which in turn motivates students to focus on their learning. In the joint model, the temporal action detection model SlowFast detects the video and identifies the base action category and the state action category. In cases where the actions are similar, such as the states of reading and watching video on mobile phone, we calculate the student's head pose information to determine his or her field of view and use the FPN-Faster RCNN model to detect the key items within his or her field of view to detect the real action. Finally, their learning state was identified based on the duration of the action, with mAP of 85.28%.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2471-6162
SN  - 978-1-6654-8102-1
DA  - 2022 
PY  - 2022
DO  - 10.1109/IST55454.2022.9827718
AN  - WOS:000853020700015
AD  - Donghua Univ Shanghai, Sch Comp Sci & Technol, Shanghai, Peoples R China
Y2  - 2022-09-21
ER  -

TY  - JOUR
AU  - Dai, Rui
AU  - Das, Srijan
AU  - Ryoo, Michael S.
AU  - Bremond, Francois
TI  - AAN: Attributes-Aware Network for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The challenge of long-term video understanding remains constrained by the efficient extraction of object semantics and the modelling of their relationships for downstream tasks. Although the CLIP visual features exhibit discriminative properties for various vision tasks, particularly in object encoding, they are suboptimal for long-term video understanding. To address this issue, we present the Attributes-Aware Network (AAN), which consists of two key components: the Attributes Extractor and a Graph Reasoning block. These components facilitate the extraction of object-centric attributes and the modelling of their relationships within the video. By leveraging CLIP features, AAN outperforms state-of-the-art approaches on two popular action detection datasets: Charades and Toyota Smarthome Untrimmed datasets.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.00696
AN  - PPRN:84730312
AD  - Inria, Paris, France
AD  - UNC Charlotte, Charlotte, NC 28223, USA
AD  - SUNY Stony Brook, Stony Brook, NY 11794, USA
M2  - Inria
M2  - UNC Charlotte
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Rodin, Ivan
AU  - Furnari, Antonino
AU  - Mavroeidis, Dimitrios
AU  - Farinella, Giovanni Maria
ED  - Sclaroff, S
ED  - Distante, C
ED  - Leo, M
ED  - Farinella, GM
ED  - Tombari, F
TI  - Untrimmed Action Anticipation
T2  - IMAGE ANALYSIS AND PROCESSING, ICIAP 2022, PT III
M3  - Proceedings Paper
CP  - 21st International Conference on Image Analysis and Processing (ICIAP)
CL  - Lecce, ITALY
AB  - Egocentric action anticipation consists in predicting a future action the camera wearer will perform from egocentric video. While the task has recently attracted the attention of the research community, current approaches assume that the input videos are "trimmed", meaning that a short video sequence is sampled a fixed time before the beginning of the action. We argue that, despite the recent advances in the field, trimmed action anticipation has a limited applicability in real-world scenarios where it is important to deal with "untrimmed" video inputs and it cannot be assumed that the exact moment in which the action will begin is known at test time. To overcome such limitations, we propose an untrimmed action anticipation task, which, similarly to temporal action detection, assumes that the input video is untrimmed at test time, while still requiring predictions to be made before the actions actually take place. We propose an evaluation procedure for methods designed to address this novel task, and compare several baselines on the EPICKITCHENS-100 dataset. Experiments show that the performance of current models designed for trimmed action anticipation is very limited and more research on this task is required.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-06433-3
SN  - 978-3-031-06432-6
DA  - 2022 
PY  - 2022
VL  - 13233
SP  - 337
EP  - 348
DO  - 10.1007/978-3-031-06433-3_29
AN  - WOS:000870308100029
AD  - Univ Catania, Viale Andrea Doria 6, I-95128 Catania, Italy
AD  - Philips Res, High Tech Campus 34, NL-5656 AE Eindhoven, Netherlands
AD  - Univ Catania, Next Vis Srl Spinoff, Catania, Italy
Y2  - 2022-11-04
ER  -

TY  - CPAPER
AU  - Li, Juncheng
AU  - Xie, Junlin
AU  - Zhu, Linchao
AU  - Qian, Long
AU  - Tang, Siliang
AU  - Zhang, Wenqiao
AU  - Shi, Haochen
AU  - Zhang, Shengyu
AU  - Wei, Longhui
AU  - Tian, Qi
AU  - Zhuang, Yueting
ED  - ACM
TI  - Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
T2  - PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022
M3  - Proceedings Paper
CP  - 30th ACM International Conference on Multimedia (MM)
CL  - Lisboa, PORTUGAL
AB  - Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos (TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and laborintensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https://github.com/YYJMJC/Temporal- Emotion-Localization- inVideos.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 978-1-4503-9203-7
DA  - 2022 
PY  - 2022
SP  - 5083
EP  - 5092
DO  - 10.1145/3503161.3547886
AN  - WOS:001150372705014
AD  - Zhejiang Univ, Hangzhou, Peoples R China
AD  - Univ Technol Sydney, Sydney, NSW, Australia
AD  - Natl Univ Singapore, Singapore, Singapore
AD  - Univ Montreal, Montreal, PQ, Canada
AD  - Huawei Cloud, Beijing, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Lan, Bo
AU  - Li, Pei
AU  - Yin, Jiaxi
AU  - Song, Yunpeng
AU  - Wang, Ge
AU  - Ding, Han
AU  - Han, Jinsong
AU  - Wang, Fei
TI  - XRF V2: A Dataset for Action Summarization with Wi-Fi Signals, and IMUs in Phones, Watches, Earbuds, and Glasses
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Human Action Recognition (HAR) plays a crucial role in applications such as health monitoring, smart home automation, and human-computer interaction. While HAR has been extensively studied, action summarization, which involves identifying and summarizing continuous actions, remains an emerging task. This paper introduces the novel XRF V2 dataset, designed for indoor daily activity Temporal Action Localization (TAL) and action summarization. XRF V2 integrates multimodal data from Wi-Fi signals, IMU sensors (smartphones, smartwatches, headphones, and smart glasses), and synchronized video recordings, offering a diverse collection of indoor activities from 16 volunteers across three distinct environments. To tackle TAL and action summarization, we propose the XRFMamba neural network, which excels at capturing long-term dependencies in untrimmed sensory sequences and outperforms state-of-the-art methods, such as ActionFormer and WiFiTAD. We envision XRF V2 as a valuable resource for advancing research in human action localization, action forecasting, pose estimation, multimodal foundation models pre-training, synthetic data generation, and more.
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2501.19034
AN  - PPRN:121080075
AD  - Xian Jiaotong Univ, Sch Software Engn, Xian, Shaanxi, Peoples R China
AD  - Xian Jiaotong Univ, MOE, KLINNS Lab, Xian, Shaanxi, Peoples R China
AD  - Xian Jiaotong Univ, Sch Comp Sci & Technol, Xian, Shaanxi, Peoples R China
AD  - Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China
M2  - Xian Jiaotong Univ
M2  - Xian Jiaotong Univ
M2  - Xian Jiaotong Univ
Y2  - 2025-03-22
ER  -

TY  - CPAPER
AU  - Afham, Mohamed
AU  - Shukla, Satya Narayan
AU  - Poursaeed, Omid
AU  - Zhang, Pengchuan
AU  - Shah, Ashish
AU  - Lim, Sernam
A1  - IEEE
TI  - Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS, ICCVW
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - While most modern video understanding models operate on short-range clips, real-world videos are often several minutes long with semantically-consistent segments of variable length. A common approach to process long videos is applying a short-form video model over uniformly sampled clips of fixed temporal length and aggregating the outputs. This approach neglects the underlying nature of long videos since fixed-length clips are often redundant or uninformative. In this paper, we aim to provide a generic and adaptive sampling approach for long-form videos in lieu of the de facto uniform sampling. Viewing videos as semantically-consistent segments, we formulate a task-agnostic, unsupervised, and scalable approach based on Kernel Temporal Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our method on long-form video understanding tasks such as video classification and temporal action localization, showing consistent gains over existing approaches and achieving state-of-the-art performance on long-form video modeling.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2473-9936
SN  - 979-8-3503-0744-3
DA  - 2023 
PY  - 2023
SP  - 1181
EP  - 1186
DO  - 10.1109/ICCVW60793.2023.00128
AN  - WOS:001156680301029
AD  - Meta AI, Menlo Pk, CA 94025 USA
M2  - Meta AI
Y2  - 2024-03-21
ER  -

TY  - JOUR
AU  - Afham, Mohamed
AU  - Shukla, Satya Narayan
AU  - Poursaeed, Omid
AU  - Zhang, Pengchuan
AU  - Shah, Ashish
AU  - Lim, Sernam
TI  - Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - While most modern video understanding models operate on short-range clips, real-world videos are often several minutes long with semantically consistent segments of variable length. A common approach to process long videos is applying a short-form video model over uniformly sampled clips of fixed temporal length and aggregating the outputs. This approach neglects the underlying nature of long videos since fixed-length clips are often redundant or uninformative. In this paper, we aim to provide a generic and adaptive sampling approach for long-form videos in lieu of the de facto uniform sampling. Viewing videos as semantically consistent segments, we formulate a task-agnostic, unsupervised, and scalable approach based on Kernel Temporal Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our method on long-form video understanding tasks such as video classification and temporal action localization, showing consistent gains over existing approaches and achieving state-of-the-art performance on long-form video modeling.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.11569
AN  - PPRN:85083401
AD  - Meta AI, Menlo Pk, CA 94025, USA
M2  - Meta AI
Y2  - 2024-05-25
ER  -

TY  - JOUR
AU  - Liu, Xiaolong
AU  - Hu, Yao
AU  - Bai, Song
AU  - Ding, Fei
AU  - Bai, Xiang
AU  - Torr, Philip H.s.
TI  - Multi-shot Temporal Event Localization: a Benchmark
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Current developments in temporal event or action localization usually target actions captured by a single camera. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions. In this paper, we propose a new and challenging task called multi-shot temporal event localization, and accordingly, collect a large scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which induces large intrainstance variations. Our comprehensive evaluations show that the state-of-the-art method in temporal action localization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution, we present a simple baseline approach for handling the intra-instance variations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2012.09434
AN  - PPRN:11631162
AD  - Huazhong Univ Sci & Technol, Wuhan, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - Univ Oxford, Oxford, England
M2  - Huazhong Univ Sci & Technol
M2  - Alibaba Grp
Y2  - 2023-03-17
ER  -

TY  - CPAPER
AU  - Kim, Jinhyung
AU  - Kim, Taeoh
AU  - Shim, Minho
AU  - Han, Dongyoon
AU  - Wee, Dongyoon
AU  - Kim, Junmo
ED  - Williams, B
ED  - Chen, Y
ED  - Neville, J
TI  - Frequency Selective Augmentation for Video Representation Learning
T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 1
M3  - Proceedings Paper
CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence
CL  - Washington, DC
AB  - Recent self-supervised video representation learning methods focus on maximizing the similarity between multiple augmented views from the same video and largely rely on the quality of generated views. However, most existing methods lack a mechanism to prevent representation learning from bias towards static information in the video. In this paper, we propose frequency augmentation (FreqAug), a spatio-temporal data augmentation method in the frequency domain for video representation learning. FreqAug stochastically removes specific frequency components from the video so that learned representation captures essential features more from the remaining information for various downstream tasks. Specifically, FreqAug pushes the model to focus more on dynamic features rather than static features in the video via dropping spatial or temporal low-frequency components. To verify the generality of the proposed method, we experiment with FreqAug on multiple self-supervised learning frameworks along with standard augmentations. Transferring the improved representation to five video action recognition and two temporal action localization downstream tasks shows consistent improvements over baselines.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-880-0
DA  - 2023 
PY  - 2023
SP  - 1124
EP  - 1132
AN  - WOS:001243759700125
AD  - LG AI Res, Seoul, South Korea
AD  - NAVER CLOVA Video, Seongnam, South Korea
AD  - NAVER AI Lab, Seongnam, South Korea
AD  - Korea Adv Inst Sci & Technol, Daejeon, South Korea
M2  - LG AI Res
M2  - NAVER CLOVA Video
Y2  - 2024-09-15
ER  -

TY  - CPAPER
AU  - Liu, Yuan
AU  - Chen, Jingyuan
AU  - Chen, Zhenfang
AU  - Deng, Bing
AU  - Huang, Jianqiang
AU  - Zhang, Hanwang
A1  - IEEE COMP SOC
TI  - The Blessings of Unlabeled Background in Untrimmed Videos
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Weakly-supervised Temporal Action Localization (WTAL) aims to detect the action segments with only video-level action labels in training. The key challenge is how to distinguish the action of interest segments from the background, which is unlabelled even on the video-level. While previous works treat the background as "curses", we consider it as "blessings". Specifically, we first use causal analysis to point out that the common localization errors are due to the unobserved confounder that resides ubiquitously in visual recognition. Then, we propose a Temporal Smoothing PCA-based (TS-PCA) deconfounder, which exploits the unlabelled background to model an observed substitute for the unobserved confounder, to remove the confounding effect. Note that the proposed deconfounder is model-agnostic and non-intrusive, and hence can be applied in any WTAL method without model re-designs. Through extensive experiments on four state-of-the-art WTAL methods, we show that the deconfounder can improve all of them on the public datasets: THUMOS-14 and ActivityNet-1.3(1).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 6172
EP  - 6181
DO  - 10.1109/CVPR46437.2021.00611
AN  - WOS:000739917306038
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - Univ Hong Kong, Hong Kong, Peoples R China
AD  - Nanyang Technol Univ, Singapore, Singapore
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Zeng, Runhao
AU  - Huang, Wenbing
AU  - Tan, Mingkui
AU  - Rong, Yu
AU  - Zhao, Peilin
AU  - Huang, Junzhou
AU  - Gan, Chuang
A1  - IEEE
TI  - Graph Convolutional Networks for Temporal Action Localization
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using Graph Convolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1550-5499
SN  - 978-1-7281-4803-8
DA  - 2019 
PY  - 2019
SP  - 7093
EP  - 7102
DO  - 10.1109/ICCV.2019.00719
AN  - WOS:000548549202020
AD  - South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China
AD  - Tencent AI Lab, Shenzhen, Peoples R China
AD  - MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
AD  - Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing, Peoples R China
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Jiang, Yudong
AU  - Cui, Kaixu
AU  - Chen, Leilei
AU  - Wang, Canjin
AU  - Xu, Changliang
TI  - SoccerDB: A Large-Scale Database for Comprehensive Video Understanding
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Soccer videos can serve as a perfect research object for video understanding because soccer games are played under well-defined rules while complex and intriguing enough for researchers to study. In this paper, we propose a new soccer video database named SoccerDB, comprising 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. To our knowledge, it is the largest database for comprehensive sports video understanding on various aspects. We further survey a collection of strong baselines on SoccerDB, which have demonstrated state-of-the-art performances on independent tasks. Our evaluation suggests that we can benefit significantly when jointly considering the inner correlations among those tasks. We believe the release of SoccerDB will tremendously advance researches around comprehensive video understanding. {\itshape Our dataset and code published on https://github.com/newsdata/SoccerDB.}
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1912.04465
AN  - PPRN:22630315
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Alwassel, Humam
AU  - Giancola, Silvio
AU  - Ghanem, Bernard
A1  - IEEE Comp Soc
TI  - TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent state-of-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2473-9936
SN  - 978-1-6654-0191-3
DA  - 2021 
PY  - 2021
SP  - 3166
EP  - 3176
DO  - 10.1109/ICCVW54120.2021.00356
AN  - WOS:000739651103032
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
Y2  - 2022-02-24
ER  -

TY  - CPAPER
AU  - Dong, Yiting
AU  - Li, Yang
AU  - Zhao, Dongcheng
AU  - Shen, Guobin
AU  - Zeng, Yi
ED  - Oh, A
ED  - Neumann, T
ED  - Globerson, A
ED  - Saenko, K
ED  - Hardt, M
ED  - Levine, S
TI  - Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 36 (NEURIPS 2023)
M3  - Proceedings Paper
CP  - 37th Conference on Neural Information Processing Systems (NeurIPS)
CL  - New Orleans, LA
AB  - The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment. To address the problem, we leverage Dynamic Vision Sensors (DVS) camera to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.
PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI  - LA JOLLA
PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN  - 1049-5258
SN  - *****************
DA  - 2023 
PY  - 2023
AN  - WOS:001220600001003
AD  - Univ Chinese Acad Sci, Sch Future Technol, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China
AD  - Chinese Acad Sci, Brain Inspired Cognit Intelligence Lab, Inst Automat, Beijing, Peoples R China
AD  - Ctr Long Term Artificial Intelligence, Beijing, Peoples R China
M2  - Ctr Long Term Artificial Intelligence
Y2  - 2024-07-10
ER  -

TY  - JOUR
AU  - Liu, Yuan
AU  - Chen, Jingyuan
AU  - Chen, Zhenfang
AU  - Deng, Bing
AU  - Huang, Jianqiang
AU  - Zhang, Hanwang
TI  - The Blessings of Unlabeled Background in Untrimmed Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised Temporal Action Localization (WTAL) aims to detect the action segments with only video-level action labels in training. The key challenge is how to distinguish the action of interest segments from the background, which is unlabelled even on the video-level. While previous works treat the background as "curses", we consider it as "blessings". Specifically, we first use causal analysis to point out that the common localization errors are due to the unobserved confounder that resides ubiquitously in visual recognition. Then, we propose a Temporal Smoothing PCA-based (TS-PCA) deconfounder, which exploits the unlabelled background to model an observed substitute for the unobserved confounder, to remove the confounding effect. Note that the proposed deconfounder is model-agnostic and non-intrusive, and hence can be applied in any WTAL method without model re-designs. Through extensive experiments on four state-of-the-art WTAL methods, we show that the deconfounder can improve all of them on the public datasets: THUMOS-14 and ActivityNet-1.3.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2103.13183
AN  - PPRN:11680619
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - Univ Hong Kong, Hong Kong, Peoples R China
AD  - Nanyang Technol Univ, Singapore, Singapore
M2  - Univ Hong Kong
M2  - Nanyang Technol Univ
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Liang, Morgan
AU  - Li, Xun
AU  - Onie, Sandersan
AU  - Larsen, Mark
AU  - Sowmya, Arcot
A1  - IEEE
TI  - Improved Spatio-temporal Action Localization for Surveillance Videos
T2  - 2021 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2021)
M3  - Proceedings Paper
CP  - International Conference on Digital Image Computing - Techniques and Applications (DICTA)
CL  - ELECTR NETWORK
AB  - We present an improved spatiotemporal action localization framework that operates in an online manner. Current state of the art approaches have achieved remarkable results mainly due to the advancements in action recognition models. These approaches have commonly followed a two-stage pipeline consisting of a region proposal stage and an action classification stage. Recently, the improvement in spatiotemporal action localization models have focused on improving the action classification stage. As a result, the outputs generated in the region proposal stage are suboptimal. We believe that the proposal stage remains a crucial component in determining the overall model performance. As a result, we adopt a tracking model in place of the existing proposal models to generate more accurate and robust regions of interest (RoI). We evaluate our approach on a private CCTV surveillance dataset and on the challenging JHMDB-21 benchmark. We are able to achieve promising results on our private dataset and achieve good results for the JHMDB-21 benchmark.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-1709-9
DA  - 2021 
PY  - 2021
SP  - 147
EP  - 154
DO  - 10.1109/DICTA52665.2021.9647106
AN  - WOS:000824642300020
AD  - Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia
AD  - Univ New South Wales, Black Dog Inst, Sydney, NSW, Australia
Y2  - 2022-07-30
ER  -

TY  - JOUR
AU  - Kang, Hyolim
AU  - Hyun, Jeongseok
AU  - An, Joungbin
AU  - Yu, Youngjae
AU  - Kim, Seon Joo
TI  - ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Online Temporal Action Localization (On-TAL) is a critical task that aims to instantaneously identify action instances in untrimmed streaming videos as soon as an action concludes -- a major leap from frame-based Online Action Detection (OAD). Yet, the challenge of detecting overlapping actions is often overlooked even though it is a common scenario in streaming videos. Current methods that can address concurrent actions depend heavily on class information, limiting their flexibility. This paper introduces ActionSwitch, the first class-agnostic On-TAL framework capable of detecting overlapping actions. By obviating the reliance on class information, ActionSwitch provides wider applicability to various situations, including overlapping actions of the same class or scenarios where class information is unavailable. This approach is complemented by the proposed "conservativeness loss", which directly embeds a conservative decision-making principle into the loss function for On-TAL. Our ActionSwitch achieves state-of-the-art performance in complex datasets, including Epic-Kitchens 100 targeting the challenging egocentric view and FineAction consisting of fine-grained actions.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.12987
AN  - PPRN:90882204
AD  - Yonsei Univ, Seoul, South Korea
Y2  - 2024-07-26
ER  -

TY  - JOUR
AU  - Qiu, Haonan
AU  - Zheng, Yingbin
AU  - Ye, Hao
AU  - Lu, Yao
AU  - Wang, Feng
AU  - He, Liang
TI  - Precise Temporal Action Localization by Evolving Temporal Proposals
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1804.04803
AN  - PPRN:22705454
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Kim, Jinhyung
AU  - Kim, Taeoh
AU  - Shim, Minho
AU  - Han, Dongyoon
AU  - Wee, Dongyoon
AU  - Kim, Junmo
TI  - Frequency Selective Augmentation for Video Representation Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Recent self-supervised video representation learning methods focus on maximizing the similarity between multiple augmented views from the same video and largely rely on the quality of generated views. However, most existing methods lack a mechanism to prevent representation learning from bias towards static information in the video. In this paper, we propose frequency augmentation (FreqAug), a spatio-temporal data augmentation method in the frequency domain for video representation learning. FreqAug stochastically removes specific frequency components from the video so that learned representation captures essential features more from the remaining information for various downstream tasks. Specifically, FreqAug pushes the model to focus more on dynamic features rather than static features in the video via dropping spatial or temporal low-frequency components. To verify the generality of the proposed method, we experiment with FreqAug on multiple self-supervised learning frameworks along with standard augmentations. Transferring the improved representation to five video action recognition and two temporal action localization downstream tasks shows consistent improvements over baselines.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2204.03865
AN  - PPRN:25541249
AD  - LG AI Res, Daejeon, South Korea
AD  - NAVER CLOVA Video, Daejeon, South Korea
AD  - NAVER AI Lab, Daejeon, South Korea
AD  - KAIST, Daejeon, South Korea
M2  - LG AI Res
M2  - NAVER CLOVA Video
M2  - KAIST
Y2  - 2022-12-22
ER  -

TY  - JOUR
AU  - Zhang, Xiao-Yu
AU  - Shi, Haichao
AU  - Li, Changsheng
AU  - Shi, Xinchu
TI  - Action Shuffling for Weakly Supervised Temporal Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly supervised action localization is a challenging task with extensive applications, which aims to identify actions and the corresponding temporal intervals with only video-level annotations available. This paper analyzes the order-sensitive and location-insensitive properties of actions, and embodies them into a self-augmented learning framework to improve the weakly supervised action localization performance. To be specific, we propose a novel two-branch network architecture with intra/inter-action shuffling, referred to as ActShufNet. The intra-action shuffling branch lays out a self-supervised order prediction task to augment the video representation with inner-video relevance, whereas the inter-action shuffling branch imposes a reorganizing strategy on the existing action contents to augment the training set without resorting to any external resources. Furthermore, the global-local adversarial training is presented to enhance the model&#39;s robustness to irrelevant noises. Extensive experiments are conducted on three benchmark datasets, and the results clearly demonstrate the efficacy of the proposed method.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2105.04208
AN  - PPRN:11391147
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China
AD  - Beijing Inst Technol, Beijing, Peoples R China
AD  - Meituan Grp, Beijing, Peoples R China
M2  - Chinese Acad Sci
M2  - Univ Chinese Acad Sci
M2  - Meituan Grp
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Dong, Yiting
AU  - Li, Yang
AU  - Zhao, Dongcheng
AU  - Shen, Guobin
AU  - Zeng, Yi
TI  - Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment. To address the problem, we leverage Dynamic Vision Sensors (DVS) cameras to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2306.11546
AN  - PPRN:73443065
AD  - Univ Chinese Acad Sci, Sch Future Technol, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China
AD  - Chinese Acad Sci, Inst Automat, Brain inspired Cognit Intelligence Lab, Beijing, Peoples R China
AD  - Ctr Long term Artificial Intelligence, Beijing, Peoples R China
M2  - Ctr Long term Artificial Intelligence
Y2  - 2023-11-17
ER  -

TY  - JOUR
AU  - Alwassel, Humam
AU  - Caba Heilbron, Fabian
AU  - Escorcia, Victor
AU  - Ghanem, Bernard
TI  - Diagnosing Error in Temporal Action Detectors
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Despite the recent progress in video understanding and the continuous rate of improvement in temporal action localization throughout the years, it is still unclear how far (or close?) we are to solving the problem. To this end, we introduce a new diagnostic tool to analyze the performance of temporal action detectors in videos and compare different methods beyond a single scalar metric. We exemplify the use of our tool by analyzing the performance of the top rewarded entries in the latest ActivityNet action localization challenge. Our analysis shows that the most impactful areas to work on are: strategies to better handle temporal context around the instances, improving the robustness w.r.t. the instance absolute and relative size, and strategies to reduce the localization errors. Moreover, our experimental analysis finds the lack of agreement among annotator is not a major roadblock to attain progress in the field. Our diagnostic tool is publicly available to keep fueling the minds of other researchers with additional insights about their algorithms.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.10706
AN  - PPRN:19317861
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Tong, Haoran
AU  - Liu, Xinyan
AU  - Li, Guorong
AU  - Qing, Laiyun
A1  - ASSOC COMPUTING MACHINERY
TI  - Directly Locating Actions in Video with Single Frame Annotation
T2  - PROCEEDINGS OF THE 4TH ANNUAL ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2024
M3  - Proceedings Paper
CP  - 4th Annual International Conference on Multimedia Retrieval (ICMR)
CL  - Phuket, THAILAND
AB  - We propose a novel method for point-supervised action localization.Differs from the common practice of locating actions by first categorizing each video frame, our method directly predicts actions' positions and length. Specifically, point-supervised action localization is achieved by a series of fully supervised action location iteratively. In each iteration, the input video are used as input tokens and fed into a transformer, where the encoder extracts global context of the clips, and the decoder generates queries containing information for action localization. Three MLP heads are built on each query to obtain the probability, the center, and the length of each action instance respectively. Experiments on three popular datasets prove the potential of our method.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-0602-8
DA  - 2024 
PY  - 2024
SP  - 1135
EP  - 1139
DO  - 10.1145/3652583.3657617
AN  - WOS:001282078400129
AD  - Univ Chinese Acad Sci, Beijing, Peoples R China
Y2  - 2024-09-15
ER  -

TY  - JOUR
AU  - Huang, Zhu
AU  - Pan, Gang
AU  - Kang, Chao
AU  - Lv, Yaozhi
TI  - Semi-supervised pipe video temporal defect interval localization
T2  - COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING
M3  - Article
M3  - Early Access
AB  - In sewer pipe closed-circuit television inspection, accurate temporal defect localization is essential for effective pipe assessment. Industry standards typically do not require time interval annotations, which are more informative but lead to additional costs for fully supervised methods. Additionally, differences in scene types and camera motion patterns between pipe inspections and temporal action localization (TAL) hinder the effective transfer of point-supervised TAL methods. Therefore, this study presents a semi-supervised multi-prototype-based method incorporating visual odometry for enhanced attention guidance (PipeSPO). The semi-supervised multi-prototype-based method effectively leverages both unlabeled data and time-point annotations, which enhances performance and reduces annotation costs. Meanwhile, visual odometry features exploit the camera's unique motion patterns in pipe videos, offering additional insights to inform the model. Experiments on real-world datasets demonstrate that PipeSPO achieves 41.89% AP across intersection over union thresholds of 0.1-0.7, improving by 8.14% over current state-of-the-art methods.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 1093-9687
SN  - 1467-8667
DA  - 2025 JAN 9
PY  - 2025
DO  - 10.1111/mice.13403
AN  - WOS:001392544000001
C6  - JAN 2025
AD  - Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China
AD  - Tianjin Zhongli Intelligent Technol Co Ltd, Tianjin, Peoples R China
AD  - WSP Canada Inc, Edmonton, AB, Canada
AD  - Tianjin Municipal Engn Design & Res Inst, Key Lab Infrastructure Durabil, Tianjin, Peoples R China
M2  - Tianjin Zhongli Intelligent Technol Co Ltd
M2  - WSP Canada Inc
M2  - Tianjin Municipal Engn Design & Res Inst
Y2  - 2025-01-13
ER  -

TY  - JOUR
AU  - Mettes, Pascal
AU  - G. M. Snoek, Cees
TI  - Spatio-Temporal Instance Learning: Action Tubes from Class Supervision
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The goal of this work is spatio-temporal action localization in videos, using only the supervision from video-level class labels. The state-of-the-art casts this weakly-supervised action localization regime as a Multiple Instance Learning problem, where instances are a priori computed spatio-temporal proposals. Rather than disconnecting the spatio-temporal learning from the training, we propose Spatio-Temporal Instance Learning, which enables action localization directly from box proposals in video frames. We outline the assumptions of our model and propose a max-margin objective and optimization with latent variables that enable spatio-temporal learning of actions from video labels. We also provide an efficient linking algorithm and two reranking strategies to facilitate and further improve the action localization. Experimental evaluation on four action datasets demonstrate the effectiveness of our approach for localization from weak supervision. Moreover, we show how to incorporate other supervision levels and mixtures, as a step towards determining optimal supervision strategies for action localization.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1807.02800
AN  - PPRN:22156379
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Khoa Vo
AU  - Truong, Sang
AU  - Yamazaki, Kashu
AU  - Raj, Bhiksha
AU  - Minh-Triet Tran
AU  - Ngan Le
TI  - AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation
T2  - INTERNATIONAL JOURNAL OF COMPUTER VISION
M3  - Article
AB  - Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous state-of-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Our source code is publicly available at .
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-5691
SN  - 1573-1405
DA  - 2023 JAN
PY  - 2023
VL  - 131
IS  - 1
SP  - 302
EP  - 323
DO  - 10.1007/s11263-022-01702-9
AN  - WOS:000875589000001
C6  - OCT 2022
AD  - Univ Arkansas, AICV Lab, Fayetteville, AR 72701 USA
AD  - Carnegie Mellon Univ, Pittsburgh, PA 15213 USA
AD  - Univ Sci, Ho Chi Minh City, Vietnam
AD  - Vietnam Natl Univ, Ho Chi Minh City, Vietnam
Y2  - 2023-03-29
ER  -

TY  - JOUR
AU  - Huang, Jingjia
AU  - Li, Nannan
AU  - Li, Thomas
AU  - Liu, Shan
AU  - Li, Ge
TI  - Spatial-Temporal Context-Aware Online Action Detection and Prediction
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Spatial-temporal action detection in videos is a challenging problem that has attracted considerable attention in recent years. Most current approaches address action detection as an object detection problem, which utilizes successful object detection frameworks such as Faster R-CNN to operate action detection at every single frame first, and then generates action tubes by linking bounding boxes across the whole video in an offline fashion. However, unlike object detection in static images, temporal context information is vital for action detection in videos. Therefore, we propose an online action detection model that leverages the spatial-temporal context information existing in videos to perform action inference and localization. More specifically, we try to depict the spatial-temporal context pattern of actions via an encoder-decoder model that is based on a convolutional recurrent neural network. The model accepts a video snippet as input and encodes the dynamic information inside the snippet in the forward pass. During the backward pass, the decoder resolves the information for action detection with the current appearance or motion cue at each time stamp. In addition, we devise an incremental action-tube construction algorithm that enables our model to accomplish action prediction ahead of time and performs action detection in an online fashion. To evaluate the performance of our method, we conduct experiments on three popular public datasets UCF-101, UCF-Sports, and J-HMDB-21. The experimental results demonstrate that our method can achieve competitive or superior performance when compared to the state-of-the-art methods. To encourage further research, we release our project on "https://github.com.hjjpku.OATD."
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2020 AUG
PY  - 2020
VL  - 30
IS  - 8
SP  - 2650
EP  - 2662
DO  - 10.1109/TCSVT.2019.2923712
AN  - WOS:000557386300028
AD  - Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Peoples R China
AD  - Peking Univ, AIIT, Hangzhou 310000, Peoples R China
AD  - Tencent Media Lab, Palo Alto, CA 94301 USA
M2  - Tencent Media Lab
Y2  - 2020-08-21
ER  -

TY  - JOUR
AU  - Li, Nan-Nan
AU  - Guo, Hui-Wen
AU  - Zhao, Yang
AU  - Li, Thomas
AU  - Li, Ge
TI  - Active Temporal Action Detection in Untrimmed Videos via Deep Reinforcement Learning
T2  - IEEE ACCESS
M3  - Article
AB  - Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions in the video should be naturally one of observation and refinement: observe the current temporal window and refine the span of attended window to cover true action regions. In this paper, we propose an active action detection model that learns to search actions through continuously adjusting the bounds of temporal attended window in a self-adaptive way. The whole process can be deemed as an exploring procedure, where an agent is first placed at the beginning of the video and then traverses the whole video by adopting a sequence of transformations on the current attended window to discover actions according to a learned policy. We utilize reinforcement learning, especially the deep Q-learning algorithm to learn the agent's decision policy. Actually, we construct an end-to-end trainable framework for the action detection task, which includes a proposal generation network based on deep Q-learning, and the classification and regression networks responsible for the action category prediction and the action location adjustment, respectively. In addition, we design a long short-term memory structure upon extracted convolutional neural network features of sparsely sampled frames to generate the effective feature representations for video sequences of various durations. We evaluate the action proposal performance of our approach on THUMOS'14 and assess the generalization ability for unseen action categories on ActivityNet. We also compare the action detection performance of ours with other state-of-the-art methods on both data sets. Experiment results validate the effectiveness of the proposed approach, which can achieve comparative or superior performance than other action detection methods via much fewer proposals.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2018 
PY  - 2018
VL  - 6
SP  - 59126
EP  - 59140
DO  - 10.1109/ACCESS.2018.2872759
AN  - WOS:000449549300001
AD  - Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China
AD  - Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China
AD  - Gpower Semicond Inc, Suzhou 215000, Peoples R China
M2  - Gpower Semicond Inc
Y2  - 2018-11-22
ER  -

TY  - JOUR
AU  - Chen, Yaosen
AU  - Guo, Bing
AU  - Shen, Yan
AU  - Wang, Wei
AU  - Lu, Weichen
AU  - Suo, Xinhua
TI  - Boundary graph convolutional network for temporal action detection
T2  - IMAGE AND VISION COMPUTING
M3  - Article
AB  - Temporal action proposal generation is a fundamental yet challenging to locate the temporal action in untrimmed videos. Although current proposal generation methods can generate the precise boundary of actions, few focus on considering the relation of proposals. In this paper, we propose a unified framework to generate the temporal boundary proposals with a graph convolution network based on the boundary proposals' feature named Boundary Graph Convolutional Network (BGCN). BGCN draws inspiration from boundary methods and uses edge graph convolution relay on the boundary proposals' feature. First, we use a base layer to fusion the two-stream video features to get two-branches of base features. Then the two-branches of base features enter into the same structure of Proposal Features Graph Convolutional Network (PFGCN): Action PFGCN to extract the action classification score and Boundary PFGCN to extract the ending score and staring score. In proposal features graph convolutional network, we first densely sampled the proposals' feature from the video features. We construct a proposal feature graph, where each proposal feature as a node and their relations between proposals' features as an edge with edge convolution for graph convolution. After that, map the relations into a 2D map score. Experiments on popular benchmarks THUMOS14 demonstrate the superiority of BGCN over (44.8% versus 42.8% at tIoU 0.5) the state-of-the-art proposal generator (e.g., G-TAD, TAL-Net, and BMN) at any of tIoU thresholds from 0.3 to 0.7. On ActivityNet1.3, BGCN also got better results. Moreover, BGCN has high efficiency for action detection with less than 2 MB model size and fast inference time.GCN based on boundary generation for densely produce the action proposals Efficient and novel BGCN model has a great capability to learn the proposal features Has a lower model size for temporal action proposals generation Has fast inference time for temporal action proposals generation.(c) 2021 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0262-8856
SN  - 1872-8138
DA  - 2021 MAY
PY  - 2021
VL  - 109
C7  - 104144
DO  - 10.1016/j.imavis.2021.104144
AN  - WOS:000648892600007
C6  - MAR 2021
AD  - Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China
AD  - ChengDu Sobey Digital Technol Co Ltd, Chengdu 610041, Sichuan, Peoples R China
AD  - Chengdu Univ Informat Technol, Sch Comp Sci, Chengdu 610225, Sichuan, Peoples R China
AD  - Peng Cheng Lab, Shenzhen 518055, Peoples R China
M2  - ChengDu Sobey Digital Technol Co Ltd
Y2  - 2021-06-01
ER  -

TY  - CPAPER
AU  - Ji, Xiangli
AU  - Luo, Guibo
AU  - Zhu, Yuesheng
ED  - Jawahar, CV
ED  - Li, H
ED  - Mori, G
ED  - Schindler, K
TI  - A New Temporal Deconvolutional Pyramid Network for Action Detection
T2  - COMPUTER VISION - ACCV 2018, PT IV
M3  - Proceedings Paper
CP  - 14th Asian Conference on Computer Vision (ACCV)
CL  - Perth, AUSTRALIA
AB  - Temporal action detection is a challenging task for detecting various action instances in untrimmed videos. Existing detection approaches are unable to localize the start and end time of action instances precisely. To address this issue, we propose a novel Temporal Deconvolutional Pyramid Network (TDPN), in which a Temporal Deconvolution Fusion (TDF) module in each pyramidal hierarchy is developed to construct strong semantic features of multiple temporal scales for detecting action instances with various durations. In the TDF module, the temporal resolution of high-level feature is expanded by a temporal deconvolution. The expanded high-level features and low-level features are fused by a fusion strategy to form strong semantic features. The fused semantic features with multiple temporal scales are used to predict action categories and boundary offsets simultaneously, which significantly improves the detection performance. Besides, a strict strategy for assigning label is proposed during training to improve the precision of temporal boundaries learned by model. We evaluate our detection approach on two public datasets, i.e., THUMOS14 and MEXaction2. The experimental results have demonstrated that our TDPN model can achieve competitive performance on THUMOS14 and best performance on MEXaction2 compared with the other approaches.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-20870-7
SN  - 978-3-030-20869-1
DA  - 2019 
PY  - 2019
VL  - 11364
SP  - 696
EP  - 711
DO  - 10.1007/978-3-030-20870-7_43
AN  - WOS:000490895400043
AD  - Peking Univ, Commun & Informat Secur Lab, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China
Y2  - 2019-10-29
ER  -

TY  - CPAPER
AU  - Wang, Xiang
AU  - Gao, Changxin
AU  - Zhang, Shiwei
AU  - Sang, Nong
ED  - Peng, Y
ED  - Liu, Q
ED  - Lu, H
ED  - Sun, Z
ED  - Liu, C
ED  - Chen, X
ED  - Zha, H
ED  - Yang, J
TI  - Multi-level Temporal Pyramid Network for Action Detection
T2  - PATTERN RECOGNITION AND COMPUTER VISION, PRCV 2020, PT II
M3  - Proceedings Paper
CP  - 3rd Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
CL  - Nanjing Univ Sci & Tech, Nanjing, PEOPLES R CHINA
AB  - Currently, one-stage frameworks have been widely applied for temporal action detection, but they still suffer from the challenge that the action instances span a wide range of time. The reason is that these one-stage detectors, e.g., Single Shot Multi-Box Detector (SSD), extract temporal features only applying a single-level layer for each head, which is not discriminative enough to perform classification and regression. In this paper, we propose a Multi-Level Temporal Pyramid Network (MLTPN) to improve the discrimination of the features. Specially, we first fuse the features from multiple layers with different temporal resolutions, to encode multi-layer temporal information. We then apply a multi-level feature pyramid architecture on the features to enhance their discriminative abilities. Finally, we design a simple yet effective feature fusion module to fuse the multi-level multi-scale features. By this means, the proposed MLTPN can learn rich and discriminative features for different action instances with different durations. We evaluate MLTPN on two challenging datasets: THUMOS'14 and Activitynet v1.3, and the experimental results show that MLTPN obtains competitive performance on Activitynet v1.3 and outperforms the state-of-the-art approaches on THUMOS'14 significantly.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-60638-1
SN  - 978-3-030-60639-8
DA  - 2020 
PY  - 2020
VL  - 12306
SP  - 41
EP  - 54
DO  - 10.1007/978-3-030-60639-8_4
AN  - WOS:001425845400004
AD  - Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Wuhan, Peoples R China
AD  - Alibaba Grp, DAMO Acad, Hangzhou, Peoples R China
Y2  - 2020-01-01
ER  -

TY  - CPAPER
AU  - Khoa Vo
AU  - Yamazaki, Kashu
AU  - Nguyen, Phong X.
AU  - Phat Nguyen
AU  - Khoa Luu
AU  - Ngan Le
ED  - Matthews, MB
TI  - Contextual Explainable Video Representation: Human Perception-based Understanding
T2  - 2022 56TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS, AND COMPUTERS
M3  - Proceedings Paper
CP  - 56th Asilomar Conference on Signals, Systems, and Computers
CL  - ELECTR NETWORK
AB  - Video understanding is a growing field and a subject of intense research, which includes many interesting tasks to understanding both spatial and temporal information, e.g., action detection, action recognition, video captioning, video retrieval. One of the most challenging problems in video understanding is dealing with feature extraction, i.e. extract contextual visual representation from given untrimmed video due to the long and complicated temporal structure of unconstrained videos. Different from existing approaches, which apply a pre-trained backbone network as a black-box to extract visual representation, our approach aims to extract the most contextual information with an explainable mechanism. As we observed, humans typically perceive a video through the interactions between three main factors, i.e., the actors, the relevant objects, and the surrounding environment. Therefore, it is very crucial to design a contextual explainable video representation extraction that can capture each of such factors and model the relationships between them. In this paper, we discuss approaches, that incorporate the human perception process into modeling actors, objects, and the environment. We choose video paragraph captioning and temporal action detection to illustrate the effectiveness of human perception based-contextual representation in video understanding. Source code is publicly available at https://github.com/UARK-AICV/Video Representation.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1058-6393
SN  - 978-1-6654-5906-8
DA  - 2022 
PY  - 2022
SP  - 1326
EP  - 1333
DO  - 10.1109/IEEECONF56349.2022.10052051
AN  - WOS:000976687600244
AD  - Univ Arkansas, Dept CSCE, Fayetteville, AR 72701 USA
AD  - FPT Software, AI Lab, Ho Chi Minh City, Vietnam
M2  - FPT Software
Y2  - 2023-06-03
ER  -

TY  - BOOK
AU  - Lee, Jiyong
Z2  -  
TI  - Anchor-Free Pipeline Temporal Action Localisation
M3  - Dissertation/Thesis
SN  - 9798382637242
DA  - 2023 
PY  - 2023
AN  - PQDT:89145012
AD  - The University of Manchester (United Kingdom), England
M2  - The University of Manchester (United Kingdom)
ER  -

TY  - JOUR
AU  - Rahman, Mohammed Shaiqur
AU  - Shihab, Ibne Farabi
AU  - Chu, Lynna
AU  - Sharma, Anuj
TI  - DeepLocalization: Using change point detection for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this study, we introduce DeepLocalization, an innovative framework devised for the real-time localization of actions tailored explicitly for monitoring driver behavior. Utilizing the power of advanced deep learning methodologies, our objective is to tackle the critical issue of distracted driving-a significant factor contributing to road accidents. Our strategy employs a dual approach: leveraging Graph-Based Change-Point Detection for pinpointing actions in time alongside a Video Large Language Model (Video-LLM) for precisely categorizing activities. Through careful prompt engineering, we customize the Video-LLM to adeptly handle driving activities' nuances, ensuring its classification efficacy even with sparse data. Engineered to be lightweight, our framework is optimized for consumer-grade GPUs, making it vastly applicable in practical scenarios. We subjected our method to rigorous testing on the SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it demonstrated commendable performance-achieving 57.5% accuracy in event classification and 51% in event detection. These outcomes underscore the substantial promise of DeepLocalization in accurately identifying diverse driver behaviors and their temporal occurrences, all within the bounds of limited computational resources.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.12258
AN  - PPRN:88561875
AD  - Iowa State Univ, Ames, IA 50011, USA
Y2  - 2024-04-27
ER  -

TY  - CPAPER
AU  - Liu, Xiaolong
AU  - Hu, Yao
AU  - Bai, Song
AU  - Ding, Fei
AU  - Bai, Xiang
AU  - Torr, Philip H. S.
A1  - IEEE COMP SOC
TI  - Multi-shot Temporal Event Localization: a Benchmark
T2  - 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, CVPR 2021
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Current developments in temporal event or action localization usually target actions captured by a single camera. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions. In this paper, we propose a new and challenging task called multi-shot temporal event localization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which induces large intra-instance variations. Our comprehensive evaluations show that the state-of-the-art method in temporal action localization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution, we present a simple baseline approach for handling the intra-instance variations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5. To facilitate research in this direction, we release the dataset and the project code at https://songbai.site/muses/.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-4509-2
DA  - 2021 
PY  - 2021
SP  - 12591
EP  - 12601
DO  - 10.1109/CVPR46437.2021.01241
AN  - WOS:000742075002078
AD  - Huazhong Univ Sci & Technol, Wuhan, Peoples R China
AD  - Alibaba Grp, Hangzhou, Peoples R China
AD  - Univ Oxford, Oxford, England
Y2  - 2021-01-01
ER  -

TY  - JOUR
AU  - Huang, Zhu
AU  - Pan, Gang
AU  - Kang, Chao
AU  - Lv, Yaozhi
TI  - Semi-Supervised Pipe Video Temporal Defect Interval Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In sewer pipe Closed-Circuit Television (CCTV) inspection, accurate temporal defect localization is essential for effective defect classification, detection, segmentation and quantification. Industry standards typically do not require time-interval annotations, even though they are more informative than time-point annotations for defect localization, resulting in additional annotation costs when fully supervised methods are used. Additionally, differences in scene types and camera motion patterns between pipe inspections and Temporal Action Localization (TAL) hinder the effective transfer of point-supervised TAL methods. Therefore, this study introduces a Semi-supervised multi-Prototype-based method incorporating visual Odometry for enhanced attention guidance (PipeSPO). PipeSPO fully leverages unlabeled data through unsupervised pretext tasks and utilizes time-point annotated data with a weakly supervised multi-prototype-based method, relying on visual odometry features to capture camera pose information. Experiments on real-world datasets demonstrate that PipeSPO achieves 41.89% average precision across Intersection over Union (IoU) thresholds of 0.1-0.7, improving by 8.14% over current state-ofthe-art methods.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.15170
AN  - PPRN:91020425
AD  - Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China
AD  - Univ Alberta, Edmonton, AB, Canada
AD  - Tianjin Municipal Engn Design & Res Inst, Key Lab Infrastructure Durabil, Tianjin, Peoples R China
AD  - Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China
M2  - Univ Alberta
M2  - Tianjin Municipal Engn Design & Res Inst
Y2  - 2024-07-27
ER  -

TY  - JOUR
AU  - Artham, Sainithin
AU  - Shaikh, Soharab Hossain
TI  - A transformer-based convolutional local attention (ConvLoA) method for temporal action localization
T2  - INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS
M3  - Article
M3  - Early Access
AB  - In the realm of temporal localization in videos, our research introduces a novel framework that achieves significant results in event localization in videos. We depart from conventional approaches that rely heavily only on global context encoding for sequence analysis from video. We propose a novel framework that leverages an encoder-decoder mechanism powered by VidSwin to extract global features, which are subsequently combined with the local context. To achieve this, we designed ConvLoA, a convolutional local attention mechanism dedicated to computing contextual focus within localized areas in video frames. ConvLoA extends beyond localization, providing a pathway for generative models to create novel, unseen instructional videos. Extensive experiments on the YouCook2 and ActivityNet datasets were performed. The results affirm that the proposed approach is on par with other state-of-the-art alternatives, validating its competitiveness. This research not only highlights the importance of local context for precise localization but also sets the stage for enhanced video understanding, offering a versatile solution for event localization within videos.
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN  - 1868-8071
SN  - 1868-808X
DA  - 2024 DEC 10
PY  - 2024
DO  - 10.1007/s13042-024-02476-x
AN  - WOS:001374579100001
C6  - DEC 2024
AD  - BML Munjal Univ, Gurgaon 122413, Haryana, India
Y2  - 2024-12-18
ER  -

TY  - JOUR
AU  - Mavroudi, Effrosyni
AU  - Haro, Benjamín Béjar
AU  - Vidal, René
TI  - Representation Learning on Visual-Symbolic Graphs for Video Understanding
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:1905.07385
AN  - PPRN:48230273
Y2  - 2023-03-27
ER  -

TY  - JOUR
AU  - Shi, Baifeng
AU  - Dai, Qi
AU  - Mu, Yadong
AU  - Wang, Jingdong
TI  - Weakly-Supervised Action Localization by Generative Attention Modeling
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Weakly-supervised temporal action localization is a problem of learning an action localization model with only video-level action labeling available. The general framework largely relies on the classification activation, which employs an attention model to identify the action-related frames and then categorizes them into different classes. Such method results in the action-context confusion issue: context frames near action clips tend to be recognized as action frames themselves, since they are closely related to the specific classes. To solve the problem, in this paper we propose to model the class-agnostic frame-wise probability conditioned on the frame attention using conditional Variational Auto-Encoder (VAE). With the observation that the context exhibits notable difference from the action at representation level, a probabilistic model, i.e., conditional VAE, is learned to model the likelihood of each frame given the attention. By maximizing the conditional probability with respect to the attention, the action and non-action frames are well separated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of our method and effectiveness in handling action-context confusion problem. Code is now available on GitHub.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2003.12424
AN  - PPRN:13084009
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Zeng, Runhao
AU  - Huang, Wenbing
AU  - Tan, Mingkui
AU  - Rong, Yu
AU  - Zhao, Peilin
AU  - Huang, Junzhou
AU  - Gan, Chuang
TI  - Graph Convolutional Networks for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using Graph Convolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships. Codes are available at https://github.com/Alvin-Zeng/PGCN.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1909.03252
AN  - PPRN:12910816
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Wang, Huiyu
AU  - Singh, Mitesh Kumar
AU  - Torresani, Lorenzo
TI  - Ego-Only: Egocentric Action Detection without Exocentric Pretraining
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We present Ego-Only, the first training pipeline that enables state-of-the-art action detection on egocentric (first-person) videos without any form of exocentric (third-person) pretraining. Previous approaches found that egocentric models cannot be trained effectively from scratch and that exocentric representations transfer well to first-person videos. In this paper we revisit these two observations. Motivated by the large content and appearance gap separating the two domains, we propose a strategy that enables effective training of egocentric models without exocentric pretraining. Our Ego-Only pipeline is simple. It trains the video representation with a masked autoencoder finetuned for temporal segmentation. The learned features are then fed to an off-the-shelf temporal action localization method to detect actions. We evaluate our approach on two established egocentric video datasets: Ego4D and EPIC-Kitchens-100. On Ego4D, our Ego-Only is on-par with exocentric pretraining methods that use an order of magnitude more labels. On EPIC-Kitchens-100, our Ego-Only even outperforms exocentric pretraining (by 2.1% on verbs and by 1.8% on nouns), setting a new state-of-the-art.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2301.01380
AN  - PPRN:35870538
AD  - Meta AI, Menlo Pk, CA 94025, USA
M2  - Meta AI
Y2  - 2023-02-01
ER  -

TY  - JOUR
AU  - Grutschus, Till
AU  - Karrar, Ola
AU  - Esenov, Emir
AU  - Vats, Ekta
TI  - Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: "Fall", "Lying" and "Other/Activities of daily living (ADL)". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2401.16280
AN  - PPRN:87390964
AD  - Tech Univ Munich, Sch Computat Informat & Technol, Munich, Germany
AD  - Uppsala Univ, Dept Informat Technol, Uppsala, Sweden
M2  - Uppsala Univ
Y2  - 2024-02-15
ER  -

TY  - JOUR
AU  - Kumar Singh, Krishna
AU  - Yu, Hao
AU  - Sarmasi, Aron
AU  - Pradeep, Gautam
AU  - Jae Lee, Yong
TI  - Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We propose 'Hide-and-Seek' a general purpose data augmentation technique, which is complementary to existing data augmentation techniques and is beneficial for various visual recognition tasks. The key idea is to hide patches in a training image randomly, in order to force the network to seek other relevant content when the most discriminative content is hidden. Our approach only needs to modify the input image and can work with any network to improve its performance. During testing, it does not need to hide any patches. The main advantage of Hide-and-Seek over existing data augmentation techniques is its ability to improve object localization accuracy in the weakly-supervised setting, and we therefore use this task to motivate the approach. However, Hide-and-Seek is not tied only to the image localization task, and can generalize to other forms of visual input like videos, as well as other recognition tasks like image classification, temporal action localization, semantic segmentation, emotion recognition, age/gender estimation, and person re-identification. We perform extensive experiments to showcase the advantage of Hide-and-Seek on these various visual recognition problems.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1811.02545
AN  - PPRN:12940084
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Chen, Shimin
AU  - Li, Wei
AU  - Chen, Chen
AU  - Gu, Jianyang
AU  - Chu, Jiaming
AU  - Tao, Xunqiang
AU  - Guo, Yandong
TI  - MM-SEAL: A Large-scale Video Dataset of Multi-person Multi-grained Spatio-temporally Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we introduce a novel large-scale video dataset dubbed MM-SEAL for multi-person multi-grained spatio-temporal action localization among human daily life. We are the first to propose a new benchmark for multi-person spatio-temporal complex activity localization, where complex semantic and long duration bring new challenges to localization tasks. We observe that limited atomic actions can be combined into many complex activities. MM-SEAL provides both atomic action and complex activity annotations, producing 111.7k atomic actions spanning 172 action categories and 17.7k complex activities spanning 200 activity categories. We explore the relationship between atomic actions and complex activities, finding that atomic action features can improve the complex activity localization performance. Also, we propose a new network which generates temporal proposals and labels simultaneously, termed Faster-TAD. Finally, our evaluations show that visual features pretrained on MM-SEAL can improve the performance on other action localization benchmarks. We will release the dataset and the project code upon publication of the paper.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2204.02688
AN  - PPRN:119466150
AD  - OPPO Res Inst, Shenzhen, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
AD  - Beijing Univ Posts & Telecommun, Beijing, Peoples R China
M2  - OPPO Res Inst
M2  - Beijing Univ Posts & Telecommun
Y2  - 2025-01-08
ER  -

TY  - JOUR
AU  - Xiong, Yuanhao
AU  - Zhao, Long
AU  - Gong, Boqing
AU  - Yang, Ming-Hsuan
AU  - Schroff, Florian
AU  - Liu, Ting
AU  - Hsieh, Cho-Jui
AU  - Yuan, Liangzhe
TI  - STRUCTURED VIDEO-LANGUAGE MODELING WITH TEMPORAL GROUPING AND SPATIAL GROUNDING
T2  - Arxiv
M3  - preprint
C8  - 3
AB  - Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal localization and semantic reasoning. A powerful model is expected to be capable of capturing region-object correspondences and recognizing scene changes in a video clip, reflecting spatial and temporal granularity, respectively. To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, S-ViLM, by exploiting the intrinsic structures of these two modalities. It includes two novel designs, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features, simultaneously. Comprehensive evaluations demonstrate that S-ViLM performs favorably against existing approaches in learning more expressive representations. Specifically, S-ViLM surpasses the state-of-the-art methods substantially on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition, and temporal action localization.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2303.16341
AN  - PPRN:56452429
AD  - Google Res, New York City, NY 10011, USA
AD  - Google, Mt View, CA, USA
AD  - UCLA, Los Angeles, CA 90095, USA
M2  - Google Res
M2  - Google
Y2  - 2024-09-23
ER  -

TY  - CPAPER
AU  - Peng, Fan
AU  - Li, Kun
AU  - Liu, Xueliang
AU  - Guo, Dan
A1  - IEEE
TI  - AOPNet: Anchor Offset Prediction Network for Temporal Action Proposal Generation
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS AND COMPUTING (IEEE ICSPCC 2020)
M3  - Proceedings Paper
CP  - 10th IEEE International Conference on Signal Processing, Communications and Computing (IEEE ICSPCC)
CL  - Univ Macau, Macau, PEOPLES R CHINA
AB  - Temporal action proposal generation is a hot topic in the field of video understanding, aiming to extract action instances in long untrimmed videos. Although various approaches have been proposed, the challenge of proposal generation with reliable confidence scores and precise boundaries still exists. To solve this challenge, we propose a novel proposal generation framework called Anchor Offset Prediction Network (AOPNet), which utilizes multi-layer Bidirectional RNN (BiRNN) to predict multi-scale action proposals and fine-tune temporal boundaries with the predicted anchor offsets. Experimental results demonstrate that though with the use of anchor boundary offset prediction module, the AOPNet can remarkably improve the classification accuracy of temporal anchors and finally generate temporal proposals with high recall. Ablation studies on the THUMOS-14 dataset demonstrated the effectiveness of the proposed approach.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-7201-9
DA  - 2020 
PY  - 2020
DO  - 10.1109/icspcc50002.2020.9259546
AN  - WOS:000651422600094
AD  - Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei, Peoples R China
Y2  - 2021-06-04
ER  -

TY  - JOUR
AU  - Ye, Yuancheng
AU  - Yang, Xiaodong
AU  - Tian, Yingli
TI  - Discovering Spatio-Temporal Action Tubes
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1811.12248
AN  - PPRN:12962406
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Tan, Jing
AU  - Tang, Jiaqi
AU  - Wang, Limin
AU  - Wu, Gangshan
A1  - IEEE
TI  - Relaxed Transformer Decoders for Direct Action Proposal Generation
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action instances of interest. The existing proposal generation approaches are generally based on pre-defined anchor windows or heuristic bottom-up boundary matching strategies. This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer detection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer encoder with a boundary attentive module to better capture long-range temporal information. Second, due to the ambiguous temporal boundary and relatively sparse annotations, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Finally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net, on both tasks of temporal action proposal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more efficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/MCG-NJU/RTD-Action.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13506
EP  - 13515
DO  - 10.1109/ICCV48922.2021.01327
AN  - WOS:000798743203068
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Wang, Shengbo
AU  - Miao, Zhenjiang
AU  - Zhou, Tianyu
AU  - Li, Miaomiao
AU  - Zhang, Ruyi
A1  - IOP
TI  - Action Keyframe Connection Network for Temporal Action Proposal Generation
T2  - 2019 3RD INTERNATIONAL CONFERENCE ON MACHINE VISION AND INFORMATION TECHNOLOGY (CMVIT 2019)
M3  - Proceedings Paper
CP  - 3rd International Conference on Machine Vision and Information Technology (CMVIT)
CL  - Guangzhou, PEOPLES R CHINA
AB  - Temporal action detection is an important research topic in computer vision, of which Temporal Action Proposal (TAP) generation is a key step for finding candidate action segments. Our paper provides an action proposal generation network for temporally untrimmed videos in which a new effective and efficient deep architecture named action keyframe connection network for temporal action proposal Generation. Firstly, a two-stream network is adopted to extract frame-level features which inclued appearance feature and optical flow feature. The temporal information helps the subsequent network to determine whether a frame is the beginning or the ending of the action. Secondly, a position discrimination network is designed to infer the probability of each frame being starting frame or ending frame. The network outputs a starting probability sequence and an ending probability sequence which indicates the start of the action and the end of the action respectively. Finally, our network generates a proposal by a specific threshold rule combining the points in the starting probability sequence and the ending probability sequence. We carry out experiments on ActivityNet dataset to compare our proposed method with the state-of-the-art methods. Experiment results show that our method achieves superior performance over other methods.
PU  - IOP PUBLISHING LTD
PI  - BRISTOL
PA  - DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND
SN  - 1742-6588
SN  - 1742-6596
SN  - *****************
DA  - 2019 
PY  - 2019
VL  - 1229
C7  - 012035
DO  - 10.1088/1742-6596/1229/1/012035
AN  - WOS:000492959200035
AD  - Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing, Peoples R China
Y2  - 2019-11-08
ER  -

TY  - JOUR
AU  - Idrees, Haroon
AU  - Zamir, Amir R.
AU  - Jiang, Yu-Gang
AU  - Gorban, Alex
AU  - Laptev, Ivan
AU  - Sukthankar, Rahul
AU  - Shah, Mubarak
TI  - The THUMOS challenge on action recognition for videos "in the wild"
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - Automatically recognizing and localizing wide ranges of human actions are crucial for video understanding. Towards this goal, the THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition. Until then, video action recognition, including THUMOS challenge, had focused primarily on the classification of pre-segmented(i.e., trimmed) videos, which is an artificial task. In THUMOS 2014, we elevated action recognition to a more practical level by introducing temporally untrimmed videos. These also include 'background videos' which share similar scenes and backgrounds as action videos, but are devoid of the specific actions. The three editions of the challenge organized in 2013-2015 have made THUMOS a common benchmark for action classification and detection and the annual challenge is widely attended by teams from around the world.In this paper we describe the THUMOS benchmark in detail and give an overview of data collection and annotation procedures. We present the evaluation protocols used to quantify results in the two THUMOS tasks of action classification and temporal action detection. We also present results of submissions to the THUMOS 2015 challenge and review the participating approaches. Additionally, we include a comprehensive empirical study evaluating the differences in action recognition between trimmed and untrimmed videos, and how well methods trained on trimmed videos generalize to untrimmed videos. We conclude by proposing several directions and improvements for future THUMOS challenges. (C) 2016 Elsevier Inc. All rights reserved.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2017 FEB
PY  - 2017
VL  - 155
SP  - 1
EP  - 23
DO  - 10.1016/j.cviu.2016.10.018
AN  - WOS:000393724500001
AD  - Univ Cent Florida, Ctr Comp Vis Res, Orlando, FL 32816 USA
AD  - Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA
AD  - Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China
AD  - INRIA Paris, WILLOW Project Team, CNRS UMR 8548, ENS, Paris, France
AD  - Google Res, Mountain View, CA USA
Y2  - 2017-02-01
ER  -

TY  - JOUR
AU  - Xiao, Junrui
AU  - Jiang, He
AU  - Li, Zhikai
AU  - Gu, Qingyi
TI  - Rethinking prediction alignment in one-stage object detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Owing to their excellent performance and efficiency, one-stage detectors have been widely used in mul-timedia tasks, such as temporal action detection, object tracking, and video detection. However, misalign-ment between classification and regression branches limits the accuracy of the detector. Most existing works add an auxiliary branch or adopt a specific sample assignment strategy to alleviate this problem, but with little effect. In this paper, we attribute this to incomplete branch interactions and propose a comprehensive Predictive Aligned Object Detector (PAOD), which can better correlate two subtasks. Specifically, our proposed PAOD achieves a better trade-off between prediction-interactive and prediction-specific by adopting an Iterative Aggregation Module (IAM) and a Mutual Constraint Module (MCM). We also design an aligned label assignment with an adaptive metric and re-weighting mechanism to further narrow the misalignment between prediction heads. With negligible additional overhead, PAOD achieves 50.4 AP at single-model single-scale testing on the MS-COCO branch, which demonstrates the effectiveness of our proposal. Notably, PAOD consistently outperforms previous sota such as ATSS (47.7 AP), BorderDet (48.0 AP) and GFL (48.2 AP) by a large margin on COCO test-dev data -set, and achieves better performance than various dense detectors on Pascal VOC and CrowdHuman data -sets. Code is available at https://github.com/JunruiXiao/PAOD.(c) 2022 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2022 DEC 1
PY  - 2022
VL  - 514
SP  - 58
EP  - 69
DO  - 10.1016/j.neucom.2022.09.132
AN  - WOS:000869405900004
C6  - OCT 2022
AD  - Chinese Acad Sci, Inst Automat, East Zhongguancun Rd, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Artificial Intelligence, Jingjia Rd, Beijing, Peoples R China
Y2  - 2022-10-31
ER  -

TY  - CPAPER
AU  - Hamann, Friedhelm
AU  - Ghosh, Suman
AU  - Mart Inez, Ignacio Jua Rez
AU  - Hart, Tom
AU  - Kacelnik, Alex
AU  - Gallego, Guillermo
A1  - IEEE
TI  - Low-power, Continuous Remote Behavioral Localization with Event Cameras
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica for several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras' natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The low-power capabilities of the event camera allow it to record significantly longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.ioleventpenguins/
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18612
EP  - 18621
DO  - 10.1109/CVPR52733.2024.01761
AN  - WOS:001342515501090
AD  - Tech Univ Berlin, Berlin, Germany
AD  - Univ Oxford, Oxford, England
AD  - Oxford Brookes Univ, Oxford, England
AD  - Einstein Ctr Digital Future, Berlin, Germany
AD  - Sci Intelligence Excellence Cluster, Berlin, Germany
M2  - Einstein Ctr Digital Future
M2  - Sci Intelligence Excellence Cluster
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Lin, Tianwei
AU  - Zhao, Xu
AU  - Su, Haisheng
TI  - Joint Learning of Local and Global Context for Temporal Action Proposal Generation
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover ground truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective and efficient proposal generation method, named Local-Global Network (LGN), by which local and global contexts are jointly learned to generate high quality proposals. Locally, LGN first locates temporal boundaries with high starting and ending probabilities separately, then directly combines these boundaries as proposals. Globally, LGN evaluates the actionness probability of multiple-durations temporal regions simultaneously using temporal convolutional layers and anchor mechanism. Finally, we combine the boundary probabilities of each proposal with actionness probability of matched temporal regions as the confidence score, which is used for retrieving proposals. We conduct experiments on two datasets: ActivityNet-1.3 and THUMOS-14, where LGN outperforms other state-of-the-art methods with both high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2020 DEC
PY  - 2020
VL  - 30
IS  - 12
SP  - 4899
EP  - 4912
DO  - 10.1109/TCSVT.2019.2962063
AN  - WOS:000597751000040
AD  - Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China
AD  - Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China
Y2  - 2020-12-24
ER  -

TY  - JOUR
AU  - Gao, Haojie
AU  - Liu, Peishun
AU  - Ma, Xiaolong
AU  - Yan, Zikang
AU  - Ma, Ningning
AU  - Liu, Wenqiang
AU  - Wang, Xuefang
AU  - Tang, Ruichun
TI  - TP-LSM: visual temporal pyramidal time modeling network to multi-label action detection in image-based AI
T2  - VISUAL COMPUTER
M3  - Article
AB  - Dense multi-label action detection is a challenging task in the field of visual action, where multiple actions occur simultaneously in different time spans, hence accurately assessing the short-term and long-term temporal dependencies between actions is crucial for action detection. There is an urgent need for an effective temporal modeling technology to detect the temporal dependence of actions in videos and efficiently learn long-term and short-term action information. This paper proposes a new method based on temporal pyramid and long short-term time modeling for multi-label action detection, which combines hierarchical structure with pyramid feature hierarchy for dense multi-label temporal action detection. By using the expansion and compression convolution module (SEC) and external attention for time modeling, we focus on the temporal relationships of long and short-term actions at each stage. We then integrate hierarchical pyramid features to achieve accurate detection of actions at different temporal resolution scales. We evaluated the performance of the model on dense multi-label benchmark datasets, and achieved mAP of 47.3% and 36.0% on the MultiTHUMOS and TSU datasets, which outperforms 2.7% and 2.3% on the current state-of-the-art results. The code is available at https://github.com/Yoona6371/TP-LSM.
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN  - 0178-2789
SN  - 1432-2315
DA  - 2025 MAR 1
PY  - 2025
VL  - 41
IS  - 5
SP  - 3263
EP  - 3281
DO  - 10.1007/s00371-024-03601-1
AN  - WOS:001302271900001
C6  - AUG 2024
AD  - Ocean Univ China, Sch Comp Sci & Engn, Qing Dao 266100, Peoples R China
AD  - Ocean Univ China, Sch Math Sci, Qing Dao 266100, Peoples R China
Y2  - 2024-09-06
ER  -

TY  - JOUR
AU  - Denize, Julien
AU  - Liashuha, Mykola
AU  - Rabarisoa, Jaonary
AU  - Orcesi, Astrid
AU  - Herault, Romain
TI  - COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence compared to non-pretrained models.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2309.01270
AN  - PPRN:84765018
AD  - Univ Paris Saclay, CEA, LIST, F-91120 Palaiseau, France
AD  - Normandie Univ, INSA Rouen, LITIS, F-76801 St Etienne Du Rouvray, France
M2  - Normandie Univ
Y2  - 2024-05-25
ER  -

TY  - CPAPER
AU  - Denize, Julien
AU  - Liashuha, Mykola
AU  - Rabarisoa, Jaonary
AU  - Orcesi, Astrid
AU  - Herault, Romain
A1  - IEEE Comp Soc
TI  - COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers
T2  - 2024 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WORKSHOPS, WACVW 2024
M3  - Proceedings Paper
CP  - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence compared to non-pretrained models. Source code is available here: https://github.com/juliendenize/eztorch.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2572-4398
SN  - 979-8-3503-7028-7
SN  - 979-8-3503-7071-3
DA  - 2024 
PY  - 2024
SP  - 518
EP  - 528
DO  - 10.1109/WACVW60836.2024.00060
AN  - WOS:001223022200057
AD  - Univ Paris Saclay, CEA, List, F-91120 Palaiseau, France
AD  - Normandie Univ, INSA Rouen, LITIS, F-76801 St Etienne Du Rouvray, France
AD  - UNICAEN, Normandie Univ, CNRS, ENSICAEN,GREYC, F-14000 Caen, France
Y2  - 2024-07-24
ER  -

TY  - JOUR
AU  - Zhao, Yue
AU  - Xiong, Yuanjun
AU  - Wang, Limin
AU  - Wu, Zhirong
AU  - Tang, Xiaoou
AU  - Lin, Dahua
TI  - Temporal Action Detection with Structured Segment Networks
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1704.06228
AN  - PPRN:12722951
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Wang, Hanyuan
AU  - Mirmehdi, Majid
AU  - Damen, Dima
AU  - Perrett, Toby
TI  - Centre Stage: Centricity-based Audio-Visual Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Previous one-stage action detection approaches have modelled temporal dependencies using only the visual modality. In this paper, we explore different strategies to incorporate the audio modality, using multi-scale cross-attention to fuse the two modalities. We also demonstrate the correlation between the distance from the timestep to the action centre and the accuracy of the predicted boundaries. Thus, we propose a novel network head to estimate the closeness of timesteps to the action centre, which we call the centricity score. This leads to increased confidence for proposals that exhibit more precise boundaries. Our method can be integrated with other one-stage anchor-free architectures and we demonstrate this on three recent baselines on the EPIC-Kitchens-100 action detection benchmark where we achieve state-of-the-art performance. Detailed ablation studies showcase the benefits of fusing audio and our proposed centricity scores. 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2311.16446
AN  - PPRN:86311279
AD  - Univ Bristol, Fac Engn, Dept Comp Sci, Bristol, England
Y2  - 2023-12-12
ER  -

TY  - BOOK
AU  - Sun, Hao
Z2  -  
TI  - Human Behavior Understanding and Intention Prediction
M3  - Dissertation/Thesis
SN  - 9798834023586
DA  - 2020 
PY  - 2020
AN  - PQDT:68545795
AD  - University of Missouri - Columbia, Electrical and Computer Engineering, Missouri, United States
M2  - University of Missouri - Columbia
ER  -

TY  - JOUR
AU  - Nag, Sauradip
AU  - Zhu, Xiatian
AU  - Deng, Jiankang
AU  - Song, Yi-Zhe
AU  - Xiang, Tao
TI  - DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We propose a new formulation of temporal action de-tection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield ac-tion proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/-denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by intro-ducing a temporal location query design with faster con-vergence in training. We further propose a cross-step selec-tive conditioning algorithm for inference acceleration. Ex-tensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previ-ous art alternatives. The code will be made available at https://github.com/sauradip/DiffusionTAD.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.14863
AN  - PPRN:49550799
AD  - Univ Surrey, CVSSP, Guildford, England
AD  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence, London, England
AD  - Surrey Inst People Centred Artificial Intelligence, Guildford, England
AD  - Imperial Coll London, London, England
M2  - Univ Surrey
M2  - iFlyTek Surrey Joint Res Ctr Artificial Intelligence
M2  - Surrey Inst People Centred Artificial Intelligence
Y2  - 2023-04-07
ER  -

TY  - JOUR
AU  - Wu, Baoyuan
AU  - Zhou, Jiali
TI  - Video-Based Martial Arts Combat Action Recognition and Position Detection Using Deep Learning
T2  - IEEE ACCESS
M3  - Article
AB  - Action recognition in martial arts can offer valuable insights for technicians, athletes, and coaches. Accurate action recognition can enhance performance analysis, inform training strategies, and improve decision-making processes by providing detailed evaluations of technique execution, movement patterns, and match dynamics. This can lead to more effective coaching, better athlete preparation, and a deeper understanding of competitive outcomes. Existing methods in human action recognition often struggle with challenges such as background clutter, occlusion, and variations in appearance and speed, particularly in dynamic combat scenarios. In this study, we proposed a novel Spatio-Temporal Hierarchical Keypoint Aggregation (ST-HKA) framework for martial arts combat action recognition and localization. The ST-HKA model effectively leverages a deep learning-based approach that treats human skeleton keypoints as 3D point clouds. Unlike conventional methods that use graph convolutional networks or appearance-based techniques, our approach adopts a point cloud paradigm to treat human keypoints as a 3D point cloud, significantly improving scalability and robustness against occlusion and variations in appearance. Additionally, we incorporate a weakly supervised spatio-temporal action localization mechanism using a Context-Aware Pooling Mechanism. The proposed model was evaluated on both the Kinetics Human-Action and Taekwondo datasets, demonstrating superior performance in recognizing complex martial arts actions. The ST-HKA model achieves a Top-1 Accuracy of 88.6% and an F1-score of 83.9% on the Kinetics dataset, and 88.7% accuracy and an F1-score of 84.4% on the Taekwondo dataset. The proposed model also exhibits higher precision in detecting precise temporal boundaries, as reflected by its strong performance in action localization tasks. These results highlight the effectiveness of ST-HKA in handling complex martial arts actions with high accuracy and robustness.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2024 
PY  - 2024
VL  - 12
SP  - 161357
EP  - 161374
DO  - 10.1109/ACCESS.2024.3487289
AN  - WOS:001349751800001
AD  - Chengdu Sport Univ, Sch Wushu, Chengdu 610093, Peoples R China
AD  - Chengdu Sport Univ, Chinese Guoshu Acad, Chengdu 610093, Peoples R China
AD  - Adamson Univ, Coll Educ, Manila 1000, Philippines
AD  - Sichuan Technol & Business Univ, Sch Phys Educ, Chengdu 611745, Peoples R China
M2  - Sichuan Technol & Business Univ
Y2  - 2024-11-15
ER  -

TY  - CPAPER
AU  - Yang, Zichen
AU  - Huang, Di
AU  - Qin, Jie
AU  - Wang, Yunhong
A1  - IEEE
TI  - HUMAN-AWARE COARSE-TO-FINE ONLINE ACTION DETECTION
T2  - 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021)
M3  - Proceedings Paper
CP  - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
CL  - ELECTR NETWORK
AB  - In this work, we propose a two-stage framework to efficiently and effectively detect actions on-the-fly. An action location network (ALN) is developed in the first stage to judge whether the current frame is action-related, while the second stage involves an action classification network (ACN) to further identify the action category. In this way, irrelevant negative frames are quickly discarded and actions are detected as early as they occur. Moreover, we highlight human areas at both the stages by respectively incorporating a human detector and a human mask layer. As a result, more accurate spatial-temporal windows of actions are detected, based on which more robust features are extracted for classification. Experimental results on two popular benchmarks demonstrate the superior performance of the proposed approach.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-7281-7605-5
DA  - 2021 
PY  - 2021
SP  - 2455
EP  - 2459
DO  - 10.1109/ICASSP39728.2021.9413368
AN  - WOS:000704288402141
AD  - Beihang Univ, Sch Comp Sci & Engn, IRIP Lab, Beijing, Peoples R China
AD  - Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates
M2  - Incept Inst Artificial Intelligence
Y2  - 2021-11-24
ER  -

TY  - JOUR
AU  - Xu, Jinglin
AU  - Chen, Guangyi
AU  - Lu, Jiwen
AU  - Zhou, Jie
TI  - Unintentional Action Localization via Counterfactual Examples
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - How do humans localize unintentional action like " A boy falls down while playing skateboard "? Cognitive science shows that an 18-month-old baby understands the intention by observing the actions and comparing the feedback. Motivated by this evidence, we propose a causal inference approach that constructs a video pool containing intentional knowledge, conducts the counterfactual intervention to observe intentional action, and compares the unintentional action with intentional action to achieve localization. Specifically, we first build a video pool, where each video contains the same action content as an original unintentional action video. Then we conduct the counterfactual intervention to generate counterfactual examples. We further maximize the difference between the predictions of factual unintentional action and counterfactual intentional action to train the model. By disentangling the effects of different clues on the model prediction, we encourage the model to highlight the intention clue and alleviate the negative effect brought by the training bias of the action content clue. We evaluate our approach on a public unintentional action dataset and achieve consistent improvements on both unintentional action recognition and localization tasks.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 3281
EP  - 3294
DO  - 10.1109/TIP.2022.3166278
AN  - WOS:000794186400007
AD  - Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China
AD  - Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China
Y2  - 2022-05-24
ER  -

TY  - JOUR
AU  - Lee, Pilhyeon
AU  - Byun, Hyeran
TI  - Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action instance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel framework, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we first select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respectively. Experimental results demonstrate that our completeness guidance indeed helps the model to locate complete action instances, leading to large performance gains especially under high IoU thresholds. Moreover, we demonstrate the superiority of our method over existing state-of-the-art methods on four benchmarks: THUMOS&#39;14, GTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to recent fully-supervised methods, at the 6 times cheaper annotation cost. Our code is available at https://github.com/Pilhyeon.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2108.05029
AN  - PPRN:11882073
AD  - Yonsei Univ, Dept Comp Sci, Seoul, South Korea
AD  - Yonsei Univ, Grad Sch AI, Seoul, South Korea
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Yang, Haosen
AU  - Wu, Wenhao
AU  - Wang, Lining
AU  - Jin, Sheng
AU  - Xia, Boyang
AU  - Yao, Hongxun
AU  - Huang, Hujie
A1  - Assoc Advancement Artificial Intelligence
TI  - Temporal Action Proposal Generation with Background Constraint
T2  - THIRTY-SIXTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FOURTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE / THE TWELVETH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 36th AAAI Conference on Artificial Intelligence / 34th Conference on Innovative Applications of Artificial Intelligence / 12th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (e.g., BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, i.e., ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-876-3
DA  - 2022 
PY  - 2022
SP  - 3054
EP  - 3062
AN  - WOS:000893636203016
AD  - Harbin Inst Technol, Harbin, Peoples R China
AD  - Baidu Inc, Dept Comp Vis Technol VIS, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Beijing, Peoples R China
Y2  - 2023-02-17
ER  -

TY  - JOUR
AU  - Zhang, Haiping
AU  - Zhou, Fuxing
AU  - Wang, Dongjing
AU  - Zhang, Xinhao
AU  - Yu, Dongjin
AU  - Guan, Liming
TI  - LGAFormer: transformer with local and global attention for action detection
T2  - JOURNAL OF SUPERCOMPUTING
M3  - Article
AB  - Temporal action detection is a very important task in video understanding, aiming at predicting the start and end time boundaries of all action instances in an unedited video and their action classification. This task has been widely studied, especially after the transformer has been widely used in the field of vision. However, transformer model brings massive computational resource consumption when processing long sequence input data. At the same time, due to the ambiguity of the video action boundary, many proposal and instance-based methods cannot predict the video boundary accurately. Based on the above two points, we propose LGAFormer: a very concise model that combines the local self-attention mechanism with the global self-attention mechanism, using local self-attention in the shallow layer of the network to process short-range temporal data to model local representations while reducing a large amount of computational consumption, and using global self-attention in the deep layer of the network to model long-range temporal context. This allows our model to achieve a good balance between effectiveness and efficiency. And in terms of detection head, we combine the advantages of the segment feature and instance feature to predict action boundaries more accurately. Thanks to these two points, our method achieves comparable results on all three datasets (THUMOS14, ActivityNet 1.3, and EPIC-Kitchens 100). On THUMOS14, an average mAP of 67.7% was obtained. On ActivityNet 1.3, the best performance is obtained with an average mAP of 36.6%. On EPIC-Kitchens 100, an average mAP of 24.6% was achieved.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0920-8542
SN  - 1573-0484
DA  - 2024 AUG
PY  - 2024
VL  - 80
IS  - 12
SP  - 17952
EP  - 17979
DO  - 10.1007/s11227-024-06138-1
AN  - WOS:001214867400002
C6  - MAY 2024
AD  - Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou 310018, Zhejiang, Peoples R China
AD  - Hangzhou Dianzi Univ, Sch Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China
Y2  - 2024-05-12
ER  -

TY  - CPAPER
AU  - Yang, Min
AU  - Gao, Huan
AU  - Guo, Ping
AU  - Wang, Limin
A1  - IEEE
TI  - Adapting Short-Term Transformers for Action Detection in Untrimmed Videos
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - Vision Transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pretraining. Yet, it remains unclear how to adapt these pretrained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short-trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective crosssnippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone. For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.5 average mAP on THUMOS14, 37.40 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-5300-6
DA  - 2024 
PY  - 2024
SP  - 18570
EP  - 18579
DO  - 10.1109/CVPR52733.2024.01757
AN  - WOS:001342515501086
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Inchitech, Beijing, Peoples R China
AD  - Intel Labs China, Hillsboro, OR USA
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Inchitech
M2  - Intel Labs China
Y2  - 2025-02-01
ER  -

TY  - JOUR
AU  - Khan, Salman
AU  - Teeti, Izzeddin
AU  - Bradley, Andrew
AU  - Elhoseiny, Mohamed
AU  - Cuzzolin, Fabio
TI  - A Hybrid Graph Network for Complex Activity Detection in Video
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Interpretation and understanding of video presents a challenging computer vision task in numerous fields -e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video. We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal ‘tubes’ for the active elements (‘agents’) in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time. The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2310.17493
AN  - PPRN:85823099
AD  - Oxford Brookes Univ, Oxford, England
AD  - KAUST, Thuwal, Saudi Arabia
M2  - Oxford Brookes Univ
Y2  - 2023-11-10
ER  -

TY  - CPAPER
AU  - Khan, Salman
AU  - Teeti, Izzeddin
AU  - Bradley, Andrew
AU  - Elhoseiny, Mohamed
AU  - Cuzzolin, Fabio
A1  - IEEE Comp Soc
TI  - A Hybrid Graph Network for Complex Activity Detection in Video
T2  - 2024 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION, WACV 2024
M3  - Proceedings Paper
CP  - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Interpretation and understanding of video presents a challenging computer vision task in numerous fields - e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video.We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal 'tubes' for the active elements ('agents') in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time.The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 979-8-3503-1892-0
SN  - 979-8-3503-1893-7
DA  - 2024 
PY  - 2024
SP  - 6748
EP  - 6758
DO  - 10.1109/WACV57701.2024.00662
AN  - WOS:001222964606086
AD  - Oxford Brookes Univ, Oxford OX3 0BP, England
AD  - KAUST, Thuwal, Saudi Arabia
Y2  - 2025-03-05
ER  -

TY  - CPAPER
AU  - Lee, Pilhyeon
AU  - Byun, Hyeran
A1  - IEEE
TI  - Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action instance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel framework, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we first select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respectively. Experimental results demonstrate that our completeness guidance indeed helps the model to locate complete action instances, leading to large performance gains especially under high IoU thresholds. Moreover, we demonstrate the superiority of our method over existing state-of-the-art methods on four benchmarks: THUMOS'14, GTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to recent fully-supervised methods, at the 6x cheaper annotation cost. Our code is available at https://github.com/Pilhyeon.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 13628
EP  - 13637
DO  - 10.1109/ICCV48922.2021.01339
AN  - WOS:000798743203080
AD  - Yonsei Univ, Dept Comp Sci, Seoul, South Korea
AD  - Yonsei Univ, Grad Sch AI, Seoul, South Korea
Y2  - 2022-06-24
ER  -

TY  - JOUR
AU  - Veesam, Sai Babu
AU  - Satish, Aravapalli Rama
TI  - Design of an Integrated Model for Video Summarization Using Multimodal Fusion and YOLO for Crime Scene Analysis
T2  - IEEE ACCESS
M3  - Article
AB  - The scenario of crime scene analysis in video summarization is more demanding in that it involves the accurate and efficient extraction of critical key events from multi-camera footage, which may include the identification of persons of interest, weapons and complicated environments. The current approaches suffer from occlusions, cross-camera person re ID and small-scale weapon detection, leading to the lack of a complete and inaccurate summary. Moreover, these methods are not very robust against changing environments and do not incorporate much feedback for continuous improvement operations. To overcome these limitations, this paper presents a comprehensive system for video summarization through multimodal fusion and spatiotemporal analysis across multiple camera streams. The system integrates the following advanced technologies: AGMFN for person detection and identity matching which employs multi-head attention to fuse RGB frames, motion data and optical flow. YOLOv8 with Feature Pyramid Networks is used for multiple Scale weapon detection in order to capture smaller, partially occluded objects within cluttered scenes. Spatio-temporal action localization is achieved with the help of 3D Convolutional Neural Networks, along with Temporal Attention Networks that capture all weapon-related actions with the best set of critical frames. Finally, a feedback-driven reinforcement learning framework named RL-HITL allows continuous improvement based on human input, which enhances the adaptability of the system over temporal instance sets. This integrated system has good accuracy in person detection ranging from 95-98%, weapon detection at 92-95% and even action localization ranging from 88-91%. At the same time, it reduces the length of video by 70-80%. Real-time learning through RL-HITL ensures model refinement and hereby gives long-term benefits in security and surveillance application, hence analysis at a crime scene for different scenarios.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2169-3536
DA  - 2025 
PY  - 2025
VL  - 13
SP  - 25008
EP  - 25025
DO  - 10.1109/ACCESS.2025.3538282
AN  - WOS:001420279300018
AD  - VIT AP Univ, Sch Comp Sci & Engn, Amaravati 522241, India
Y2  - 2025-02-23
ER  -

TY  - JOUR
AU  - Sooksatra, Sorn
AU  - Watcharapinchai, Sitapa
TI  - A Comprehensive Review on Temporal-Action Proposal Generation.
T2  - Journal of imaging
M3  - Journal Article
M3  - Review
AB  - Temporal-action proposal generation (TAPG) is a well-known pre-processing of temporal-action localization and mainly affects localization performance on untrimmed videos. In recent years, there has been growing interest in proposal generation. Researchers have recently focused on anchor- and boundary-based methods for generating action proposals. The main purpose of this paper is to provide a comprehensive review of temporal-action proposal generation with network architectures and empirical results. The pre-processing step for input data is also discussed for network construction. The content of this paper was obtained from the research literature related to temporal-action proposal generation from 2012 to 2022 for performance evaluation and comparison. From several well-known databases, we used specific keywords to select 71 related studies according to their contributions and evaluation criteria. The contributions and methodologies are summarized and analyzed in a tabular form for each category. The result from state-of-the-art research was further analyzed to show its limitations and challenges for action proposal generation. TAPG performance in average recall ranges from 60% up to 78% in two TAPG benchmarks. In addition, several future potential research directions in this field are suggested based on the current limitations of the related studies.
SN  - 2313-433X
DA  - 2022 Jul 23
PY  - 2022
VL  - 8
IS  - 8
DO  - 10.3390/jimaging8080207
AN  - MEDLINE:35893085
AD  - National Electronic and Computer Technology Center, National Science and Technology Development Agency, Pathum Thani 12120, Thailand.
Y2  - 2022-07-28
ER  -

TY  - JOUR
AU  - Li, Wei
AU  - Yuan, Zehuan
AU  - Guo, Dashan
AU  - Huang, Lei
AU  - Fang, Xiangzhong
AU  - Wang, Changhu
TI  - Deformable Tube Network for Action Detection in Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We address the problem of spatio-temporal action detection in videos. Existing methods commonly either ignore temporal context in action recognition and localization, or lack the modelling of flexible shapes of action tubes. In this paper, we propose a two-stage action detector called Deformable Tube Network (DTN), which is composed of a Deformation Tube Proposal Network (DTPN) and a Deformable Tube Recognition Network (DTRN) similar to the Faster R-CNN architecture. In DTPN, a fast proposal linking algorithm (FTL) is introduced to connect region proposals across frames to generate multiple deformable action tube proposals. To perform action detection, we design a 3D convolution network with skip connections for tube classification and regression. Modelling action proposals as deformable tubes explicitly considers the shape of action tubes compared to 3D cuboids. Moreover, 3D convolution based recognition network can learn temporal dynamics sufficiently for action detection. Our experimental results show that we significantly outperform the methods with 3D cuboids and obtain the state-of-the-art results on both UCF-Sports and AVA datasets.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1907.01847
AN  - PPRN:22061471
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Ben-Ari, Rami
AU  - Nacson, Mor Shpigel
AU  - Azulai, Ophir
AU  - Barzelay, Udi
AU  - Rotman, Daniel
TI  - TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Classification of new class entities requires collecting and annotating hundreds or thousands of samples that is often prohibitively costly. Few-shot learning suggests learning to classify new classes using just a few examples. Only a small number of studies address the challenge of few-shot learning on spatio-temporal patterns such as videos. In this paper, we present the Temporal Aware Embedding Network (TAEN) for few-shot action recognition, that learns to represent actions, in a metric space as a trajectory, conveying both short term semantics and longer term connectivity between action parts. We demonstrate the effectiveness of TAEN on two few shot tasks, video classification and temporal action detection and evaluate our method on the Kinetics-400 and on ActivityNet 1.2 few-shot benchmarks. With training of just a few fully connected layers we reach comparable results to prior art on both few shot video classification and temporal detection tasks, while reaching state-of-the-art in certain scenarios.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2004.10141
AN  - PPRN:11862522
AD  - OriginAI, Boulder, CO 80304, USA
AD  - Technion, Haifa, Israel
AD  - IBM Res AI, Haifa, Israel
M2  - OriginAI
M2  - Technion
M2  - IBM Res AI
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Lu, Chongkai
AU  - Li, Ruimin
AU  - Fu, Hong
AU  - Fu, Bin
AU  - Wang, Yihao
AU  - Lo, Wai-Lun
AU  - Chi, Zheru
A1  - IEEE COMP SOC
TI  - Precise Temporal Localization for Complete Actions with Quantified Temporal Structure
T2  - 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
M3  - Proceedings Paper
CP  - 25th International Conference on Pattern Recognition (ICPR)
CL  - ELECTR NETWORK
AB  - Existing temporal action detection algorithms cannot distinguish complete and incomplete actions while this property is essential in many applications. To tackle this challenge, we proposed the action progression networks (APN), a novel model that predicts action progression of video frames with continuous numbers. Using the progression sequence of test video, on the top of the APN, a complete action searching algorithm (CAS) was designed to detect complete actions only. With the usage of frame-level fine-grained temporal structure modeling and detecting actions according to their whole temporal context, our framework can locate actions precisely and is good at avoiding incomplete action detection. We evaluated our framework on a new dataset (DFMAD-70) collected by ourselves which contains both complete and incomplete actions. Our framework got good temporal localization results with 95.77% average precision when the IoU threshold is 0.5. On the benchmark THUMOS14, an incomplete-ignostic dataset, our framework still obtain competitive performance. The code is available online at https://github.com/MakeCent/Action-Progression-Network
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1051-4651
SN  - 978-1-7281-8808-9
DA  - 2021 
PY  - 2021
SP  - 4781
EP  - 4788
DO  - 10.1109/ICPR48806.2021.9412081
AN  - WOS:000678409204120
AD  - Hong Kong Polytech Univ, Hong Kong, Peoples R China
AD  - Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian, Peoples R China
AD  - Educ Univ Hong Kong, Hong Kong, Peoples R China
AD  - Chu Hai Coll Higher Educ, Hong Kong, Peoples R China
Y2  - 2021-08-25
ER  -

TY  - CPAPER
AU  - Pehlivan, Selen
AU  - Laaksonen, Jorma
A1  - IEEE
TI  - Anchor-Free Action Proposal Network with Uncertainty Estimation
T2  - 2023 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, ICME
M3  - Proceedings Paper
CP  - IEEE International Conference on Multimedia and Expo (ICME)
CL  - Brisbane, AUSTRALIA
AB  - Proposal generation is a fundamental yet challenging task for two-stage temporal action detection pipelines. The task aims at predicting starting and ending boundaries of segments in realistic video sequences and action recognition methods cannot be directly applied to such videos due to their untrimmed nature. Most state-of-the-art models rely on temporal convolutional neural networks with pre-defined anchor segments. By eliminating anchors, we propose a lighter end-to-end trainable Anchor-Free Multiscale Transformer-based Generator (AMTG) model using local clues via video snippets. To improve effectiveness for temporal evaluation, we apply multiscale Transformer encoders to sequences with a bi-directional mask extension that simultaneously predicts boundary distances with uncertainties and various snippet-based local scores. Later, our model integrates local predictions to generate proposal candidates using the proposed scoring function. Experiments on the THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of AMTG for the temporal proposal generation task.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1945-7871
SN  - 978-1-6654-6891-6
DA  - 2023 
PY  - 2023
SP  - 1853
EP  - 1858
DO  - 10.1109/ICME55011.2023.00318
AN  - WOS:001062707300301
AD  - Aalto Univ, Sch Sci, Dept Comp Sci, Espoo, Finland
Y2  - 2023-11-05
ER  -

TY  - JOUR
AU  - Xu, Mingze
AU  - Gao, Mingfei
AU  - Chen, Yi-Ting
AU  - S. Davis, Larry
AU  - J. Crandall, David
TI  - Temporal Recurrent Networks for Online Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, Temporal Recurrent Network (TRN), to model greater temporal context of a video frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1811.07391
AN  - PPRN:13615994
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Yang, Xitong
AU  - Yang, Xiaodong
AU  - Liu, Ming-Yu
AU  - Xiao, Fanyi
AU  - Davis, Larry
AU  - Kautz, Jan
TI  - STEP: Spatio-Temporal Progressive Learning for Video Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this paper, we propose Spatio-TEmporal Progressive (STEP) action detector---a progressive learning framework for spatio-temporal action detection in videos. Starting from a handful of coarse-scale proposal cuboids, our approach progressively refines the proposals towards actions over a few steps. In this way, high-quality proposals (i.e., adhere to action movements) can be gradually obtained at later steps by leveraging the regression outputs from previous steps. At each step, we adaptively extend the proposals in time to incorporate more related temporal context. Compared to the prior work that performs action detection in one run, our progressive learning framework is able to naturally handle the spatial displacement within action tubes and therefore provides a more effective way for spatio-temporal modeling. We extensively evaluate our approach on UCF101 and AVA, and demonstrate superior detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two datasets with 3 progressive steps and using respectively only 11 and 34 initial proposals.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1904.09288
AN  - PPRN:12950691
Y2  - 2023-04-06
ER  -

TY  - CPAPER
AU  - Yang, Xitong
AU  - Yang, Xiaodong
AU  - Liu, Ming-Yu
AU  - Xiao, Fanyi
AU  - Davis, Larry
AU  - Kautz, Jan
A1  - IEEE Comp Soc
TI  - STEP: Spatio-Temporal Progressive Learning for Video Action Detection
T2  - 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)
M3  - Proceedings Paper
CP  - 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Long Beach, CA
AB  - In this paper, we propose Spatio-TEmporal Progressive (STEP) action detector-a progressive learning framework for spatio-temporal action detection in videos. Starting from a handful of coarse-scale proposal cuboids, our approach progressively refines the proposals towards actions over a few steps. In this way, high-quality proposals (i.e., adhere to action movements) can be gradually obtained at later steps by leveraging the regression outputs from previous steps. At each step, we adaptively extend the proposals in time to incorporate more related temporal context. Compared to the prior work that performs action detection in one run, our progressive learning framework is able to naturally handle the spatial displacement within action tubes and therefore provides a more effective way for spatio-temporal modeling. We extensively evaluate our approach on UCF101 and AVA, and demonstrate superior detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two datasets with 3 progressive steps and using respectively only 11 and 34 initial proposals.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-7281-3293-8
DA  - 2019 
PY  - 2019
SP  - 264
EP  - 272
DO  - 10.1109/CVPR.2019.00035
AN  - WOS:000529484000027
AD  - Univ Maryland, College Pk, MD 20742 USA
AD  - NVIDIA, Santa Clara, CA USA
AD  - Univ Calif Davis, Davis, CA 95616 USA
AD  - NVIDIA Res, Santa Clara, CA USA
M2  - NVIDIA Res
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Chéron, Guilhem
AU  - Alayrac, Jean-Baptiste
AU  - Laptev, Ivan
AU  - Schmid, Cordelia
TI  - A flexible model for training action localization with varying levels of supervision
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1806.11328
AN  - PPRN:50389741
Y2  - 2023-03-31
ER  -

TY  - CPAPER
AU  - Yu, Zefang
AU  - Xie, Mingye
AU  - Gao, Jingsheng
AU  - Liu, Ting
AU  - Fu, Yuzhuo
ED  - Wooldridge, M
ED  - Dy, J
ED  - Natarajan, S
TI  - From Raw Video to Pedagogical Insights: A Unified Framework for Student Behavior Analysis
T2  - THIRTY-EIGTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 38 NO 21
M3  - Proceedings Paper
CP  - 38th AAAI Conference on Artificial Intelligence (AAAI) / 36th Conference on Innovative Applications of Artificial Intelligence / 14th Symposium on Educational Advances in Artificial Intelligence
CL  - Vancouver, CANADA
AB  - Understanding student behavior in educational settings is critical in improving both the quality of pedagogy and the level of student engagement. While various AI-based models exist for classroom analysis, they tend to specialize in limited tasks and lack generalizability across diverse educational environments. Additionally, these models often fall short in ensuring student privacy and in providing actionable insights accessible to educators. To bridge this gap, we introduce a unified, end-to-end framework by leveraging temporal action detection techniques and advanced large language models for a more nuanced student behavior analysis. Our proposed framework provides an end-to-end pipeline that starts with raw classroom video footage and culminates in the autonomous generation of pedagogical reports. It offers a comprehensive and scalable solution for student behavior analysis. Experimental validation confirms the capability of our framework to accurately identify student behaviors and to produce pedagogically meaningful insights, thereby setting the stage for future AI-assisted educational assessments.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - *****************
DA  - 2024 
PY  - 2024
SP  - 23241
EP  - 23249
AN  - WOS:001239989100067
AD  - Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China
Y2  - 2024-08-25
ER  -

TY  - CPAPER
AU  - Choi, Jinwoo
AU  - Gao, Chen
AU  - Messou, Joseph C. E.
AU  - Huang, Jia-Bin
ED  - Wallach, H
ED  - Larochelle, H
ED  - Beygelzimer, A
ED  - d'Alche-Buc, F
ED  - Fox, E
ED  - Garnett, R
TI  - Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
T2  - ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)
M3  - Proceedings Paper
CP  - 33rd Conference on Neural Information Processing Systems (NeurIPS)
CL  - Vancouver, CANADA
AB  - Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.
PU  - NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI  - LA JOLLA
PA  - 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN  - 1049-5258
SN  - *****************
DA  - 2019 
PY  - 2019
VL  - 32
AN  - WOS:000534424300077
AD  - Virginia Tech, Blacksburg, VA 24061 USA
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Choi, Jinwoo
AU  - Gao, Chen
AU  - C. E. Messou, Joseph
AU  - Huang, Jia-Bin
TI  - Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1912.05534
AN  - PPRN:12929109
Y2  - 2022-11-19
ER  -

TY  - CPAPER
AU  - Xu, Mingze
AU  - Gao, Mingfei
AU  - Chen, Yi-Ting
AU  - Davis, Larry S.
AU  - Crandall, David J.
A1  - IEEE
TI  - Temporal Recurrent Networks for Online Action Detection
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, the Temporal Recurrent Network (TRN), to model greater temporal context of each frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1550-5499
SN  - 978-1-7281-4803-8
DA  - 2019 
PY  - 2019
SP  - 5531
EP  - 5540
DO  - 10.1109/ICCV.2019.00563
AN  - WOS:000548549200052
AD  - Indiana Univ, Bloomington, IN 47405 USA
AD  - Univ Maryland, College Pk, MD 20742 USA
AD  - Honda Res Inst, San Jose, CA 95134 USA
Y2  - 2020-08-05
ER  -

TY  - JOUR
AU  - Yang, Le
AU  - Peng, Houwen
AU  - Zhang, Dingwen
AU  - Fu, Jianlong
AU  - Han, Junwei
TI  - Revisiting Anchor Mechanisms for Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.09837
AN  - PPRN:13153780
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Ma, Fan
AU  - Jin, Xiaojie
AU  - Wang, Heng
AU  - Huang, Jingjia
AU  - Zhu, Linchao
AU  - Feng, Jiashi
AU  - Yang, Yi
TI  - Temporal Perceiving Video-Language Pre-training
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video-Language Pre-training models have recently signif-icantly improved various multi-modal downstream tasks. Previous dominant works mainly adopt contrastive learn-ing to achieve global feature alignment across modalities. However, the local associations between videos and texts are not modeled, restricting the pre-training models&rsquo; generality, especially for tasks requiring the temporal video boundary for certain query texts. This work introduces a novel text-video localization pre-text task to enable fine-grained temporal and semantic alignment such that the trained model can accurately perceive temporal bound-aries in videos given the text description. Specifically, text-video localization consists of moment retrieval, which predicts start and end boundaries in videos given the text description, and text localization which matches the subset of texts with the video features. To produce temporal boundaries, frame features in several videos are manually merged into a long video sequence that interacts with a text sequence. With the localization task, our method connects the fine-grained frame representations with the word representations and implicitly distinguishes representations of different instances in the single modality. Notably, comprehensive experimental results show that our method significantly improves the state-of-the-art performance on various benchmarks, covering text-to-video retrieval, video question answering, video captioning, temporal action localization and temporal moment retrieval. Codes will be released.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2301.07463
AN  - PPRN:35899335
AD  - ByteDance Inc, Beijing, Peoples R China
AD  - Zhejiang Univ, Hangzhou, Peoples R China
M2  - ByteDance Inc
Y2  - 2023-02-08
ER  -

TY  - JOUR
AU  - Wang, Xiang
AU  - Ma, Baiteng
AU  - Qing, Zhiwu
AU  - Sang, Yongpeng
AU  - Gao, Changxin
AU  - Zhang, Shiwei
AU  - Sang, Nong
TI  - CBR-Net: Cascade Boundary Refinement Network for Action Detection: Submission to ActivityNet Challenge 2020 (Task 1)
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - In this report, we present our solution for the task of temporal action localization (detection) (task 1) in ActivityNet Challenge 2020. The purpose of this task is to temporally localize intervals where actions of interest occur and predict the action categories in a long untrimmed video. Our solution mainly includes three components: 1) feature encoding: we apply three kinds of backbones, including TSN [7], Slowfast[3] and I3d[1], which are both pretrained on Kinetics dataset[2]. Applying these models, we can extract snippet-level video representations; 2) proposal generation: we choose BMN [5] as our baseline, base on which we design a Cascade Boundary Refinement Network (CBR-Net) to conduct proposal detection. The CBR-Net mainly contains two modules: temporal feature encoding, which applies BiLSTM to encode long-term temporal information; CBR module, which targets to refine the proposal precision under different parameter settings; 3) action localization: In this stage, we combine the video-level classification results obtained by the fine tuning networks to predict the category of each proposal. Moreover, we also apply to different ensemble strategies to improve the performance of the designed solution, by which we achieve 42.788% on the testing set of ActivityNet v1.3 dataset in terms of mean Average Precision metrics.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2006.07526
AN  - PPRN:22868407
Y2  - 2022-11-23
ER  -

TY  - CPAPER
AU  - Su, Haisheng
AU  - Gan, Weihao
AU  - Wu, Wei
AU  - Qiao, Yu
AU  - Yan, Junjie
A1  - Assoc Advancement Artificial Intelligence
TI  - BSN plus plus : Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation
T2  - THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 35th AAAI Conference on Artificial Intelligence / 33rd Conference on Innovative Applications of Artificial Intelligence / 11th Symposium on Educational Advances in Artificial Intelligence
CL  - ELECTR NETWORK
AB  - Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-866-4
DA  - 2021 
PY  - 2021
VL  - 35
SP  - 2602
EP  - 2610
AN  - WOS:000680423502078
AD  - SenseTime Res, Beijing, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - SenseTime Res
Y2  - 2021-01-01
ER  -

TY  - CPAPER
AU  - Wang, Huiyu
AU  - Singh, Mitesh Kumar
AU  - Torresani, Lorenzo
A1  - IEEE
TI  - Ego-Only: Egocentric Action Detection without Exocentric Transferring
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION, ICCV
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - We present Ego-Only, the first approach that enables state-of-the-art action detection on egocentric (first-person) videos without any form of exocentric (third-person) transferring. Despite the content and appearance gap separating the two domains, large-scale exocentric transferring has been the default choice for egocentric action detection. This is because prior works found that egocentric models are difficult to train from scratch and that transferring from exocentric representations leads to improved accuracy. However, in this paper, we revisit this common belief. Motivated by the large gap separating the two domains, we propose a strategy that enables effective training of egocentric models without exocentric transferring. Our Ego-Only approach is simple. It trains the video representation with a masked autoencoder finetuned for temporal segmentation. The learned features are then fed to an off-the-shelf temporal action localization method to detect actions. We find that this renders exocentric transferring unnecessary by showing remarkably strong results achieved by this simple Ego-Only approach on three established egocentric video datasets: Ego4D, EPIC-Kitchens-100, and Charades-Ego. On both action detection and action recognition, Ego-Only outperforms previous best exocentric transferring methods that use orders of magnitude more labels. Ego-Only sets new state-of- the-art results on these datasets and benchmarks without exocentric data.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 5227
EP  - 5238
DO  - 10.1109/ICCV51070.2023.00484
AN  - WOS:001159644305047
AD  - Meta AI, New York, NY 10003 USA
M2  - Meta AI
Y2  - 2024-04-14
ER  -

TY  - BOOK
AU  - Wang, Danxu
Z2  -  
TI  - Automated Measurement of the Water Drop Penetration Time for the Analysis of Soil Water Repellency
M3  - Dissertation/Thesis
SN  - 9798384438854
DA  - 2024 
PY  - 2024
AN  - PQDT:100655799
AD  - University of Nevada, Las Vegas, Electrical and Computer Engineering, Nevada, United States
M2  - University of Nevada, Las Vegas
ER  -

TY  - JOUR
AU  - Hall, Ashley
AU  - Victor, Brandon
AU  - He, Zhen
AU  - Langer, Matthias
AU  - Elipot, Marc
AU  - Nibali, Aiden
AU  - Morgan, Stuart
TI  - The detection, tracking, and temporal action localisation of swimmers for automated analysis
T2  - NEURAL COMPUTING & APPLICATIONS
M3  - Article
AB  - It is very important for swimming coaches to analyse a swimmer's performance at the end of each race, since the analysis can then be used to change strategies for the next round. Coaches rely heavily on statistics, such as stroke length and instantaneous velocity, when analysing performance. These statistics are usually derived from time-consuming manual video annotations. To automatically obtain the required statistics from swimming videos, we need to solve the following four challenging computer vision tasks: swimmer head detection; tracking; stroke detection; and camera calibration. We collectively solve these problems using a two-phased deep learning approach, we call Deep Detector for Actions and Swimmer Heads (DeepDASH). DeepDASH achieves a 20.8% higher F1 score for swimmer head detection and operates 6 times faster than the popular Faster R-CNN object detector. We also propose a hierarchical tracking algorithm based on the existing SORT algorithm which we call HISORT. HISORT produces significantly longer tracks than SORT by preserving swimmer identities for longer periods of time. Finally, DeepDASH achieves an overall F1 score of 97.5% for stroke detection across all four swimming stroke styles.
PU  - SPRINGER LONDON LTD
PI  - LONDON
PA  - 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN  - 0941-0643
SN  - 1433-3058
DA  - 2021 JUN
PY  - 2021
VL  - 33
IS  - 12
SP  - 7205
EP  - 7223
DO  - 10.1007/s00521-020-05485-3
AN  - WOS:000590534800004
C6  - NOV 2020
AD  - La Trobe Univ, Dept Comp Sci, Bundoora, Vic, Australia
AD  - BOSS ZhiPin, Career Sci Lab CSL, Metzingen, Germany
AD  - Swimming Australia, Canberra, ACT, Australia
AD  - Australian Inst Sport, Canberra, ACT, Australia
M2  - BOSS ZhiPin
M2  - Swimming Australia
Y2  - 2020-12-03
ER  -

TY  - JOUR
AU  - Hwang, Pin-Jui
AU  - Hsu, Chen-Chien
AU  - Chou, Po-Yung
AU  - Wang, Wei-Yen
AU  - Lin, Cheng-Hung
TI  - Vision-Based Learning from Demonstration System for Robot Arms
T2  - SENSORS
M3  - Article
AB  - Robotic arms have been widely used in various industries and have the advantages of cost savings, high productivity, and efficiency. Although robotic arms are good at increasing efficiency in repetitive tasks, they still need to be re-programmed and optimized when new tasks are to be deployed, resulting in detrimental downtime and high cost. It is therefore the objective of this paper to present a learning from demonstration (LfD) robotic system to provide a more intuitive way for robots to efficiently perform tasks through learning from human demonstration on the basis of two major components: understanding through human demonstration and reproduction by robot arm. To understand human demonstration, we propose a vision-based spatial-temporal action detection method to detect human actions that focuses on meticulous hand movement in real time to establish an action base. An object trajectory inductive method is then proposed to obtain a key path for objects manipulated by the human through multiple demonstrations. In robot reproduction, we integrate the sequence of actions in the action base and the key path derived by the object trajectory inductive method for motion planning to reproduce the task demonstrated by the human user. Because of the capability of learning from demonstration, the robot can reproduce the tasks that the human demonstrated with the help of vision sensors in unseen contexts.
PU  - MDPI
PI  - BASEL
PA  - MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND
SN  - 1424-8220
DA  - 2022 APR
PY  - 2022
VL  - 22
IS  - 7
C7  - 2678
DO  - 10.3390/s22072678
AN  - WOS:000781101100001
AD  - Natl Taiwan Normal Univ, Dept Elect Engn, Taipei 106, Taiwan
Y2  - 2022-04-24
ER  -

TY  - JOUR
AU  - Xie, Ting-Ting
Z2  -  
TI  - Learning to Localize the Temporal Extent of Human Activities in Videos
M3  - Dissertation/Thesis
DA  - 2021 
PY  - 2021
AN  - PQDT:81837698
AD  - Queen Mary University of London (United Kingdom), England
M2  - Queen Mary University of London (United Kingdom)
ER  -

TY  - CPAPER
AU  - Rahman, Mohammed Shaiqur
AU  - Shihab, Ibne Farabi
AU  - Chu, Lynna
AU  - Sharma, Anuj
A1  - IEEE
TI  - DeepLocalization: Using change point detection for Temporal Action Localization
T2  - 2024 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, CVPRW
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Seattle, WA
AB  - In this study, we introduce DeepLocalization, an innovative framework devised for the real-time localization of actions tailored explicitly for monitoring driver behavior. Utilizing the power of advanced deep learning methodologies, our objective is to tackle the critical issue of distracted driving-a significant factor contributing to road accidents. Our strategy employs a dual approach: leveraging Graph-Based Change-Point Detection for pinpointing actions in time alongside a Video Large Language Model (Video-LLM) for precisely categorizing activities. Through careful prompt engineering, we customize the Video-LLM to adeptly handle driving activities' nuances, ensuring its classification efficacy even with sparse data. Engineered to be lightweight, our framework is optimized for consumergrade GPUs, making it vastly applicable in practical scenarios. We subjected our method to rigorous testing on the SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it demonstrated commendable performance-achieving 57.5% accuracy in event classification and 51% in event detection. These outcomes underscore the substantial promise of DeepLocalization in accurately identifying diverse driver behaviors and their temporal occurrences, all within the bounds of limited computational resources.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2160-7508
SN  - 979-8-3503-6547-4
DA  - 2024 
PY  - 2024
SP  - 7252
EP  - 7260
DO  - 10.1109/CVPRW63382.2024.00721
AN  - WOS:001327781707045
AD  - Iowa State Univ, Ames, IA 50011 USA
Y2  - 2025-03-05
ER  -

TY  - JOUR
AU  - Lin, Tianwei
AU  - Zhao, Xu
AU  - Su, Haisheng
AU  - Wang, Chongjing
AU  - Yang, Ming
TI  - BSN: Boundary Sensitive Network for Temporal Action Proposal Generation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts "local to global" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.
PU  - CORNELL UNIV
DA  - 2018 
PY  - 2018
DO  - arXiv:1806.02964
AN  - PPRN:22452251
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Lepsien, Arvid
AU  - Koschmider, Agnes
AU  - Kratsch, Wolfgang
ED  - DiFrancescomarino, C
ED  - Burattin, A
ED  - Janiesch, C
ED  - Sadiq, S
TI  - Analytics Pipeline for Process Mining on Video Data
T2  - BUSINESS PROCESS MANAGEMENT FORUM, BPM 2023 FORUM
M3  - Proceedings Paper
CP  - 21st International Conference on Business Process Management (BPM)
CL  - Utrecht, NETHERLANDS
AB  - Process mining has shown that it provides valuable insights in terms of uncovering bottlenecks and inefficiencies in processes or identifying tasks for automation. However, process mining techniques expect structured input data that is at a high (business) level of abstraction. Recently, the benefits of process mining for unstructured data which is at a much lower level of abstraction have been demonstrated, e.g., for IoT data or time series data. It can be expected that the demand for methods efficiently processing these kinds of data for process mining will continuously increase. Hence, in this paper, we present an approach that allows the translation of video data into higher-level, discrete event data, thus enabling existing process mining techniques to work on data tracked in videos. Particularly, we used a combination of object tracking, spatio-temporal action detection, and techniques for raising the abstraction level of events. The evaluation results show that meaningful event logs can be extracted from an unlabeled video dataset, validating both the implementation and the feasibility of our approach.
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN  - 1865-1348
SN  - 1865-1356
SN  - 978-3-031-41622-4
SN  - 978-3-031-41623-1
DA  - 2023 
PY  - 2023
VL  - 490
SP  - 196
EP  - 213
DO  - 10.1007/978-3-031-41623-1_12
AN  - WOS:001278528000012
AD  - Univ Kiel, Dept Comp Sci, Kiel, Germany
AD  - Univ Bayreuth, Chair Business Informat & Proc Analyt, Bayreuth, Germany
AD  - Fraunhofer FIT, Bayreuth, Germany
AD  - Tech Univ Appl Sci Augsburg, Augsburg, Germany
AD  - Fraunhofer FIT, Augsburg, Germany
M2  - Tech Univ Appl Sci Augsburg
Y2  - 2024-09-04
ER  -

TY  - JOUR
AU  - Wang, Chenhao
AU  - Cai, Hongxiang
AU  - Zou, Yuxin
AU  - Xiong, Yichao
TI  - RGB Stream Is Enough for Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and opti-cal flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a hand-designed representation which not only requires heavy com-putation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action de-tection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when opti-cal flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal ac-tion detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at&nbsp;
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2107.04362
AN  - PPRN:11838106
AD  - Media Intelligence Technol Co Ltd, Bangkok, Thailand
M2  - Media Intelligence Technol Co Ltd
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Lin, Tianwei
AU  - Liu, Xiao
AU  - Li, Xin
AU  - Ding, Errui
AU  - Wen, Shilei
TI  - BMN: Boundary-Matching Network for Temporal Action Proposal Generation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1907.09702
AN  - PPRN:22456026
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Wen, Boge
AU  - Chen, Siyuan
AU  - Shao, Chenhui
TI  - Temporal action proposal for online driver action monitoring using Dilated Convolutional Temporal Prediction Network
T2  - COMPUTERS IN INDUSTRY
M3  - Article
AB  - This paper presents a new approach for temporal detection of short human activities in untrimmed videos. Most present methods for temporal action detection, to our best knowledge, are trained on public action datasets that feature actions spanning up to tens and hundreds of seconds. However, it is often desired in manufacturing, transportation, and other safety-critical scenes that fine-grained actions be automatically detected, classified, and monitored. We propose a new Dilated Convolutional Temporal Prediction Network that features 1-D dilated convolution operation in a Residual network (ResNet)-like architecture for the generation of action proposals on orders of fractions of a second. The new architecture is used as a part of the action monitoring pipeline in subway cars. Experiments demonstrate that the proposed model outperforms the state-of-the-art on the task of temporal action proposal generation on a real-world video dataset, while achieving a fast processing speed suitable for online monitoring. (C) 2020 Published by Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0166-3615
SN  - 1872-6194
DA  - 2020 OCT
PY  - 2020
VL  - 121
C7  - 103255
DO  - 10.1016/j.compind.2020.103255
AN  - WOS:000551630900006
AD  - CRRC Acad, 126 S 4th Ring Rd W, Beijing 100166, Peoples R China
AD  - Univ Illinois, Dept Mech Sci & Engn, 1206 W Green St, Urbana, IL 61801 USA
M2  - CRRC Acad
Y2  - 2020-10-01
ER  -

TY  - CPAPER
AU  - Lin, Tianwei
AU  - Liu, Xiao
AU  - Li, Xin
AU  - Ding, Errui
AU  - Wen, Shilei
A1  - IEEE
TI  - BMN: Boundary-Matching Network for Temporal Action Proposal Generation
T2  - 2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Seoul, SOUTH KOREA
AB  - Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 978-1-7281-4803-8
DA  - 2019 
PY  - 2019
SP  - 3888
EP  - 3897
DO  - 10.1109/ICCV.2019.00399
AN  - WOS:000531438104004
AD  - Baidu Inc, Dept Comp Vis Technol VIS, Beijing, Peoples R China
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Bai, Yueran
AU  - Wang, Yingying
AU  - Tong, Yunhai
AU  - Yang, Yang
AU  - Liu, Qiyue
AU  - Liu, Junhui
TI  - Boundary Content Graph Neural Network for Temporal Action Proposal Generation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-of-the-art methods in both temporal action proposal and temporal action detection tasks.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.01432
AN  - PPRN:13131332
Y2  - 2022-11-14
ER  -

TY  - CPAPER
AU  - Li, Yuxi
AU  - Lin, Weiyao
AU  - Wang, Tao
AU  - See, John
AU  - Qian, Rui
AU  - Xu, Ning
AU  - Wang, Limin
AU  - Xu, Shugong
A1  - Assoc Advancement Artificial Intelligence
TI  - Finding Action Tubes with a Sparse-to-Dense Framework
T2  - THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
M3  - Proceedings Paper
CP  - 34th AAAI Conference on Artificial Intelligence / 32nd Innovative Applications of Artificial Intelligence Conference / 10th AAAI Symposium on Educational Advances in Artificial Intelligence
CL  - New York, NY
AB  - The task of spatial-temporal action detection has attracted increasing attention among researchers. Existing dominant methods solve this problem by relying on short-term information and dense serial-wise detection on each individual frames or clips. Despite their effectiveness, these methods showed inadequate use of long-term information and are prone to inefficiency. In this paper, we propose for the first time, an efficient framework that generates action tube proposals from video streams with a single forward pass in a sparse-to-dense manner. There are two key characteristics in this framework: (1) Both long-term and short-term sampled information are explicitly utilized in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS) is designed to effectively approximate the tube output while keeping the system tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and UCFSports benchmark datasets, achieving promising results that are competitive to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our framework about 7.6 times more efficient than the nearest competitor.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - 978-1-57735-835-0
DA  - 2020 
PY  - 2020
VL  - 34
SP  - 11466
EP  - 11473
AN  - WOS:000668126803113
AD  - Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China
AD  - Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China
AD  - Multimedia Univ, Fac Comp & Informat, Cyberjaya, Malaysia
AD  - Adobe Res, San Jose, CA USA
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2021-08-18
ER  -

TY  - JOUR
AU  - Geng, Tiantian
AU  - Wang, Teng
AU  - Duan, Jinming
AU  - Zhang, Yanfu
AU  - Guan, Weili
AU  - Zheng, Feng
AU  - Shao, Ling
TI  - UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in taskspecific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2404.03179
AN  - PPRN:88414159
AD  - Southern Univ Sci & Technol, Shenzhen, Peoples R China
AD  - Univ Birmingham, Birmingham, England
AD  - Univ Hong Kong, Hong Kong, Peoples R China
AD  - William & Mary, Williamsburg, VA, USA
AD  - Harbin Inst Technol Shenzhen, Shenzhen, Peoples R China
AD  - Univ Chinese Acad Sci, Beijing, Peoples R China
M2  - Southern Univ Sci & Technol
M2  - Univ Birmingham
M2  - Univ Hong Kong
M2  - William & Mary
M2  - Univ Chinese Acad Sci
Y2  - 2024-08-22
ER  -

TY  - JOUR
AU  - Su, Haisheng
AU  - Gan, Weihao
AU  - Wu, Wei
AU  - Qiao, Yu
AU  - Yan, Junjie
TI  - BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2009.07641
AN  - PPRN:10855684
AD  - SenseTime Res, Beijing, Peoples R China
AD  - Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - SenseTime Res
M2  - Chinese Acad Sci
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Yudistira, Novanto
AU  - Kavitha, Muthu Subash
AU  - Kurita, Takio
TI  - Weakly-Supervised Action Localization and Action Recognition using Global-Local Attention of 3D CNN
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - 3D Convolutional Neural Network (3D CNN) captures spatial and temporal information on 3D data such as video sequences. However, due to the convolution and pooling mechanism, the information loss seems unavoidable. To improve the visual explanations and classification in 3D CNN, we propose two approaches; i) aggregate layer-wise global to local (global-local) discrete gradients using trained 3DResNext network, and ii) implement attention gating network to improve the accuracy of the action recognition. The proposed approach intends to show the usefulness of every layer termed as global-local attention in 3D CNN via visual attribution, weakly-supervised action localization, and action recognition. Firstly, the 3DResNext is trained and applied for action classification using backpropagation concerning the maximum predicted class. The gradients and activations of every layer are then up-sampled. Later, aggregation is used to produce more nuanced attention, which points out the most critical part of the predicted class&#39;s input videos. We use contour thresholding of final attention for final localization. We evaluate spatial and temporal action localization in trimmed videos using fine-grained visual explanation via 3DCam. Experimental results show that the proposed approach produces informative visual explanations and discriminative attention. Furthermore, the action recognition via attention gating on each layer produces better classification results than the baseline model.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2012.09542
AN  - PPRN:11958347
AD  - Brawijaya Univ, Fac Comp Sci, Informat Engn, Vet st 8, Malang 65145, East Java, Indonesia
AD  - Nagasaki Univ, Sch Informat & Data Sci, 1-14 Bunkyo-machi, Nagasaki, Japan
AD  - Hiroshima Univ, Grad Sch Adv Sci & Engn, Hiroshima 7398521, Japan
M2  - Nagasaki Univ
M2  - Hiroshima Univ
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Ghamsarian, Negin
AU  - Taschwer, Mario
AU  - Putzgruber-Adamitsch, Doris
AU  - Sarny, Stephanie
AU  - Schoeffmann, Klaus
A1  - IEEE COMP SOC
TI  - Relevance Detection in Cataract Surgery Videos by Spatio-Temporal Action Localization
T2  - 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
M3  - Proceedings Paper
CP  - 25th International Conference on Pattern Recognition (ICPR)
CL  - ELECTR NETWORK
AB  - In cataract surgery, the operation is performed with the help of a microscope. Since the microscope enables watching real-time surgery by up to two people only, a major part of surgical training is conducted using the recorded videos. To optimize the training procedure with the video content, the surgeons require an automatic relevance detection approach. In addition to relevance-based retrieval, these results can be further used for skill assessment and irregularity detection in cataract surgery videos. In this paper, a three-module framework is proposed to detect and classify the relevant phase segments in cataract videos. Taking advantage of an idle frame recognition network, the video is divided into idle and action segments. To boost the performance in relevance detection, the cornea where the relevant surgical actions are conducted is detected in all frames using Mask R-CNN. The spatiotemporally localized segments containing higher-resolution information about the pupil texture and actions, and complementary temporal information from the same phase are fed into the relevance detection module. This module consists of four parallel recurrent CNNs being responsible to detect four relevant phases that have been defined with medical experts. The results will then be integrated to classify the action phases as irrelevant or one of four relevant phases. Experimental results reveal that the proposed approach outperforms static CNNs and different configurations of feature-based and end-to-end recurrent networks.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1051-4651
SN  - 978-1-7281-8808-9
DA  - 2021 
PY  - 2021
SP  - 10720
EP  - 10727
DO  - 10.1109/ICPR48806.2021.9412525
AN  - WOS:000681331403032
AD  - Alpen Adria Univ Klagenfurt, Dept Informat Technol, Klagenfurt, Austria
AD  - Klinikum Klagenfurt, Dept Ophthalmol, Klagenfurt, Austria
M2  - Klinikum Klagenfurt
Y2  - 2021-09-04
ER  -

TY  - CPAPER
AU  - Gong, Guoqiang
AU  - Wang, Xinghan
AU  - Mu, Yadong
AU  - Tian, Qi
A1  - IEEE
TI  - Learning Temporal Co-Attention Models for Unsupervised Video Action Localization
T2  - 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2020)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - ELECTR NETWORK
AB  - Temporal action localization (TAL) in untrimmed videos recently receives tremendous research enthusiasm. To our best knowledge, this is the first attempt in the literature to explore this task under an unsupervised setting, hereafter referred to as action co-localization (ACL), where only the total count of unique actions that appear in the video set is known. To solve ACL, we propose a two-step "clustering + localization" iterative procedure. The clustering step provides noisy pseudo-labels for the localization step, and the localization step provides temporal co-attention models that in turn improve the clustering performance. Using such two-step procedure, weakly-supervised TAL can be regarded as a direct extension of our ACL model. Technically, our contributions are two-folds: 1) temporal co-attention models, either class-specific or class-agnostic, learned from video-level labels or pseudo-labels in an iterative reinforced fashion; 2) new losses specially designed for ACL, including action-background separation loss and cluster-based triplet loss. Comprehensive evaluations are conducted on 20-action THUMOS14 and 100-action ActivityNet-1.2. On both benchmarks, the proposed model for ACL exhibits strong performances, even surprisingly comparable with state-of-the-art weakly-supervised methods. For example, previous best weakly-supervised model achieves 26.8% under mAP@0.5 on THUMOS14, our new records are 30.1% (weakly-supervised) and 25.0% (unsupervised).
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-7281-7168-5
DA  - 2020 
PY  - 2020
SP  - 9816
EP  - 9825
DO  - 10.1109/CVPR42600.2020.00984
AN  - WOS:001309199902069
AD  - Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China
AD  - Huawei, Noahs Ark Lab, Shenzhen, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Wang, Jinding
AU  - Hu, Haifeng
TI  - Complementary Boundary Estimation Network for Temporal Action Proposal Generation
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - Temporal Action Detection is an important yet challenging task, in which temporal action proposal generation plays an important part. Since the temporal boundaries of action instances in videos are often ambiguous, it's difficult to locate them precisely. Boundary Sensitive Network (BSN) (Lin et al. in ECCV, 2018) is a state-of-the-art corner-based method that can generate high-quality proposals with high recall rate. It contains a temporal evaluation network and a proposal evaluation network to generate and evaluate proposals separately, which can find the temporal boundaries of action instances directly to produce proposals with flexible temporal intervals and evaluate the quality of proposals. But BSN still has some issues: (1) Due to the small reception field of temporal evaluation network, it often generates many false temporal boundaries. (2) Evaluating the quality of proposals is a difficult task and not well solved in the paper. To address these issues, we propose Complementary Boundary Estimation Network (CBEN), an improved approach to temporal action proposal generation based on the framework of BSN. Specifically, we improve BSN in two aspects: Firstly, considering the temporal evaluation network of BSN can only capture local information and tends to have high response at background segments, we combine it with a new network with larger reception field to better identify false temporal action boundaries. Secondly, to evaluate the quality of temporal action proposals more accurately, we propose a class-based proposal evaluation network and combine it with a tIoU-based proposal evaluation network to filter out low-quality proposals. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets indicate that CBEN can achieve better performance than current mainstream methods on temporal action proposal generation. We further combine CBEN with an off-the-shelf action classifier, and show consistent performance improvements on THUMOS14 dataset.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2020 DEC
PY  - 2020
VL  - 52
IS  - 3
SP  - 2275
EP  - 2295
DO  - 10.1007/s11063-020-10349-x
AN  - WOS:000570470800001
C6  - SEP 2020
AD  - Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China
Y2  - 2020-09-30
ER  -

TY  - CPAPER
AU  - Ehsanpour, Mahsa
AU  - Saleh, Fatemeh
AU  - Savarese, Silvio
AU  - Reid, Ian
AU  - Rezatofighi, Hamid
A1  - IEEE COMP SOC
TI  - JRDB-Act: A Large-scale Dataset for Spatio-temporal Action, Social Group and Activity Detection
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - The availability of large-scale video action understanding datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numerous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a significant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple (optional) interaction-based action labels. Moreover JRDB-Act provides social group annotation, conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities (common activities in each social group). Each annotated label in JRDB-Act is tagged with the annotators' confidence level which contributes to the development of reliable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 20951
EP  - 20960
DO  - 10.1109/CVPR52688.2022.02031
AN  - WOS:000870783006075
AD  - Univ Adelaide, Adelaide, SA, Australia
AD  - Samsung AI Ctr, New York, NY USA
AD  - Stanford Univ, Stanford, CA 94305 USA
AD  - Monash Univ, Melbourne, Vic, Australia
AD  - Australian Natl Univ ANU, Canberra, ACT, Australia
M2  - Samsung AI Ctr
Y2  - 2023-01-05
ER  -

TY  - CPAPER
AU  - Gao, Junyu
AU  - Chen, Mengyuan
AU  - Xu, Changsheng
A1  - IEEE COMP SOC
TI  - Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - We target at the task of weakly-supervised action localization (WSAL), where only video-level action labels are available during model training. Despite the recent progress, existing methods mainly embrace a localization-by-classification paradigm and overlook the fruitful fine-grained temporal distinctions between video sequences, thus suffering from severe ambiguity in classification learning and classification-to-localization adaption. This paper argues that learning by contextually comparing sequence-to-sequence distinctions offers an essential inductive bias in WSAL and helps identify coherent action instances. Specifically, under a differentiable dynamic programming formulation, two complementary contrastive objectives are designed, including Fine-grained Sequence Distance (FSD) contrasting and Longest Common Subsequence (LCS) contrasting, where the first one considers the relations of various action/background proposals by using match, insert, and delete operators and the second one mines the longest common subsequences between two videos. Both contrasting modules can enhance each other and jointly enjoy the merits of discriminative action-background separation and alleviated task gap between classification and localization. Extensive experiments show that our method achieves state-of-the-art performance on two popular benchmarks.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 19967
EP  - 19977
DO  - 10.1109/CVPR52688.2022.01937
AN  - WOS:000870783005078
AD  - Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China
AD  - Peng Cheng Lab, Shenzhen, Peoples R China
M2  - Univ Chinese Acad Sci UCAS
Y2  - 2023-01-05
ER  -

TY  - CPAPER
AU  - Wang, Qiang
AU  - Zhang, Yanhao
AU  - Zheng, Yun
AU  - Pan, Pan
A1  - IEEE COMP SOC
TI  - RCL: Recurrent Continuous Localization for Temporal Action Detection
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Temporal representation is the cornerstone of modern action detection techniques. State-of-the-art methods mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the temporal domain with a discretized grid, and then regress the accurate boundaries. In this paper, we revisit this foundational stage and introduce Recurrent Continuous Localization (RCL), which learns a fully continuous anchoring representation. Specifically, the proposed representation builds upon an explicit model conditioned with video embeddings and temporal coordinates, which ensure the capability of detecting segments with arbitrary length. To optimize the continuous representation, we develop an effective scale-invariant sampling strategy and recurrently refine the prediction in subsequent iterations. Our continuous anchoring scheme is fully differentiable, allowing to be seamlessly integrated into existing detectors, e.g., BMN [20] and G-TAD [41]. Extensive experiments on two benchmarks demonstrate that our continuous representation steadily surpasses other discretized counterparts by similar to 2% mAP. As a result, RCL achieves 52.92% mAP@0.5 on THUMOS14 and 37.65% mAP on ActivtiyNet v1.3, outperforming all existing single-model detectors.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 13556
EP  - 13565
DO  - 10.1109/CVPR52688.2022.01320
AN  - WOS:000870759106063
AD  - Alibaba Grp, DAMO Acad, Hangzhou, Peoples R China
Y2  - 2022-12-17
ER  -

TY  - JOUR
AU  - Zhang, Yongqiang
AU  - Ding, Mingli
AU  - Bai, Yancheng
AU  - Liu, Dandan
AU  - Ghanem, Bernard
TI  - Learning a strong detector for action localization in videos
T2  - PATTERN RECOGNITION LETTERS
M3  - Article
AB  - We address the problem of spatio-temporal action localization in videos in this paper. Current state-of-the-art methods for this challenging task rely on an object detector to localize actors at frame-level firstly, and then link or track the detections across time. Most of these methods commonly pay more attention to leveraging the temporal context of videos for action detection while ignoring the importance of the object detector itself. In this paper, we prove the importance of the object detector in the pipeline of action localization, and propose a strong object detector for better action localization in videos, which is based on the single shot multibox detector (SSD) framework. Different from SSD, we introduce an anchor refine branch at the end of the backbone network to refine the input anchors, and add a batch normalization layer before concatenating the intermediate feature maps at frame-level and after stacking feature maps at clip-level. The proposed strong detector have two contributions: (1) reducing the phenomenon of missing target objects at frame-level; (2) generating deformable anchor cuboids for modeling temporal dynamic actions. Extensive experiments on UCF-Sports, J-HMDB and UCF-101 validate our claims, and we outperform the previous state-of-the-art methods by a large margin in terms of frame-mAP and video-mAP, especially at a higher overlap threshold. (C) 2019 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0167-8655
SN  - 1872-7344
DA  - 2019 DEC 1
PY  - 2019
VL  - 128
SP  - 407
EP  - 413
DO  - 10.1016/j.patrec.2019.10.005
AN  - WOS:000498398400058
AD  - Harbin Inst Technol, Sch Instrument Sci & Engn, Harbin 150001, Heilongjiang, Peoples R China
AD  - Chinese Acad Sci, Inst Software, Beijing 100190, Peoples R China
AD  - Harbin Huade Univ HD, Sch Foreign Language Studies, Harbin 150025, Heilongjiang, Peoples R China
AD  - King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia
M2  - Harbin Huade Univ HD
Y2  - 2019-12-06
ER  -

TY  - JOUR
AU  - Abeywardena, Kalana
AU  - Sumanthiran, Shechem
AU  - Jayasundara, Sakuna
AU  - Karunasena, Sachira
AU  - Rodrigo, Ranga
AU  - Jayasekara, Peshala
TI  - KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires the utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for real-time, online applications. To accomplish activity localization under highly challenging real-time constraints, we propose utilizing fast and efficient key-point based bounding box prediction to spatially localize actions. We then introduce a tube-linking algorithm that maintains the continuity of action tubes temporally in the presence of occlusions. Further, we eliminate the need for a two-stream architecture by combining temporal and spatial information into a cascaded input to a single network, allowing the network to learn from both types of information. Temporal information is efficiently extracted using a structural similarity index map as opposed to computationally intensive optical flow. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame-mAP of 74.7% on the challenging UCF101-24 dataset, demonstrating a performance gain of 6.4% over the previous best online methods. We also achieve state-of-the-art video-mAP results compared to both online and offline methods. Moreover, our model achieves a frame rate of 41.8 FPS, which is a 10.7% improvement over contemporary real-time methods.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2111.03319
AN  - PPRN:11949396
AD  - Univ Moratuwa, Dept Elect & Telecommun Engn, Moratuwa, Sri Lanka
Y2  - 2022-11-23
ER  -

TY  - BOOK
AU  - Cheng, Feng
Z2  -  
TI  - Efficient Unimodal and Multimodal Video Understanding
M3  - Dissertation/Thesis
SN  - 9798346872443
DA  - 2024 
PY  - 2024
AN  - PQDT:120143203
AD  - The University of North Carolina at Chapel Hill, Computer Science, North Carolina, United States
M2  - The University of North Carolina at Chapel Hill
ER  -

TY  - CPAPER
AU  - Yu, Qing
AU  - Fujiwara, Kent
ED  - Williams, B
ED  - Chen, Y
ED  - Neville, J
TI  - Frame-Level Label Refinement for Skeleton-Based Weakly-Supervised Action Recognition
T2  - THIRTY-SEVENTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 37 NO 3
M3  - Proceedings Paper
CP  - 37th AAAI Conference on Artificial Intelligence (AAAI) / 35th Conference on Innovative Applications of Artificial Intelligence / 13th Symposium on Educational Advances in Artificial Intelligence
CL  - Washington, DC
AB  - In recent years, skeleton-based action recognition has achieved remarkable performance in understanding human motion from sequences of skeleton data, which is an important medium for synthesizing realistic human movement in various applications. However, existing methods assume that each action clip is manually trimmed to contain one specific action, which requires a significant amount of effort for an-notation. To solve this problem, we consider a novel problem of skeleton-based weakly-supervised temporal action localization (S-WTAL), where we need to recognize and localize human action segments in untrimmed skeleton videos given only the video-level labels. Although this task is challenging due to the sparsity of skeleton data and the lack of contextual clues from interaction with other objects and the environment, we present a frame-level label refinement frame-work based on a spatio-temporal graph convolutional network (ST-GCN) to overcome these difficulties. We use multiple instance learning (MIL) with video-level labels to generate the frame-level predictions. Inspired by advances in handling the noisy label problem, we introduce a label cleaning strategy of the frame-level pseudo labels to guide the learning pro-cess. The network parameters and the frame-level predictions are alternately updated to obtain the final results. We extensively evaluate the effectiveness of our learning approach on skeleton-based action recognition benchmarks. The state-of-the-art experimental results demonstrate that the proposed method can recognize and localize action segments of the skeleton data.
PU  - ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE
PI  - PALO ALTO
PA  - 2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA
SN  - 2159-5399
SN  - 2374-3468
SN  - *****************
DA  - 2023 
PY  - 2023
SP  - 3322
EP  - 3330
AN  - WOS:001243762100076
AD  - Univ Tokyo, Tokyo, Japan
AD  - LINE Corp, Tokyo, Japan
M2  - LINE Corp
Y2  - 2024-10-02
ER  -

TY  - JOUR
AU  - Zhao, Chen
AU  - Liu, Shuming
AU  - Mangalam, Karttikeya
AU  - Ghanem, Bernard
TI  - Re2TAL: Rewiring Pretrained Video Backbones for Reversible Action Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and complex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a large variety of reversible networks are easily obtained from existing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9% at tIoU=0.5 on THUMOS14, outperforming all other RGB-only methods. 
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2211.14053
AN  - PPRN:50015507
AD  - King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia
AD  - UC Berkeley, Berkeley, CA 94720, USA
M2  - King Abdullah Univ Sci & Technol KAUST
M2  - UC Berkeley
Y2  - 2023-04-03
ER  -

TY  - CPAPER
AU  - Abeywardena, Kalana
AU  - Sumanthiran, Shechem
AU  - Jayasundara, Sakuna
AU  - Karunasena, Sachira
AU  - Rodrigo, Ranga
AU  - Jayasekara, Peshala
A1  - IEEE
TI  - KORSAL: Key-Point Based Online Real-Time Spatio-Temporal Action Localization
T2  - 2023 IEEE CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, CCECE
M3  - Proceedings Paper
CP  - Annual IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
CL  - Regina, CANADA
AB  - Real-time and online action localization in videos poses a critical and formidable challenge. Achieving accurate action localization necessitates the integration of both temporal and spatial information. However, existing approaches rely on computationally intensive 3D convolutional neural network (CNN) architectures or redundant two-stream architectures with optical flow, rendering them unsuitable for real-time, online applications. To address this, we propose a novel approach that leverages fast and efficient key-point-based bounding box prediction for spatial action localization. Additionally, we introduce a tube-linking algorithm that ensures the temporal continuity of action tubes even in the presence of occlusions. By combining temporal and spatial information into a cascaded input for a single network, we eliminate the need for a two-stream architecture, enabling the network to effectively learn from both types of information. Instead of using computationally demanding optical flow, we extract temporal information efficiently using a structural similarity index map. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame mean average precision (mAP) of 74.7% on the challenging UCF101-24 dataset, demonstrating a notable performance gain of 6.4% over previous online methods. Moreover, we achieve state-of-the-art video mAP results compared to both online and offline methods. Furthermore, our model achieves a frame rate of 41.8 FPS (Frames per second), representing a 10.7% improvement over contemporary real-time methods.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 0840-7789
SN  - 979-8-3503-2397-9
DA  - 2023 
PY  - 2023
DO  - 10.1109/CCECE58730.2023.10288973
AN  - WOS:001103161900050
AD  - Univ Moratuwa, Dept Elect & Telecommun Engn, Moratuwa, Sri Lanka
Y2  - 2023-12-23
ER  -

TY  - CPAPER
AU  - Hachiume, Ryo
AU  - Sato, Fumiaki
AU  - Sekii, Taiki
A1  - IEEE
TI  - Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling
T2  - 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Vancouver, CANADA
AB  - This paper simultaneously addresses three limitations associated with conventional skeleton-based action recognition; skeleton detection and tracking errors, poor variety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a unified framework along with a novel deep neural network architecture called Structured Keypoint Pooling is proposed. The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less constrained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman object contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Furthermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels. This trick enables our training scheme to naturally introduce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 979-8-3503-0129-8
DA  - 2023 
PY  - 2023
SP  - 22962
EP  - 22971
DO  - 10.1109/CVPR52729.2023.02199
AN  - WOS:001062531307028
AD  - Konica Minolta Inc, Tokyo, Japan
Y2  - 2023-11-22
ER  -

TY  - JOUR
AU  - Hachiuma, Ryo
AU  - Sato, Fumiaki
AU  - Sekii, Taiki
TI  - Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - This paper simultaneously addresses three limitations associated with conventional skeleton-based action recognition; skeleton detection and tracking errors, poor variety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a unified framework along with a novel deep neural network architecture called Structured Keypoint Pooling is proposed. The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less constrained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman object contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Furthermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels. This trick enables our training scheme to naturally introduce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2303.15270
AN  - PPRN:49674376
AD  - Kon Minolta Inc, Tokyo, Japan
Y2  - 2023-04-08
ER  -

TY  - BOOK
AU  - Vaishnavi, Pratik
Z2  -  
TI  - Generating Temporal Action Proposals in Long Untrimmed Videos
M3  - Dissertation/Thesis
SN  - 978-0-438-20699-1
DA  - 2018 
PY  - 2018
AN  - PQDT:60870160
AD  - State University of New York at Stony Brook, Computer Science, New York, United States
M2  - State University of New York at Stony Brook
ER  -

TY  - JOUR
AU  - Sinha, Arkaprava
AU  - Raj, Monish Soundar
AU  - Wang, Pu
AU  - Helmy, Ahmed
AU  - Das, Srijan
TI  - MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Action detection in real-world scenarios is particularly challenging due to densely distributed actions in hour-long untrimmed videos. It requires modeling both short- and long-term temporal relationships while handling significant intra-class temporal variations. Previous state-of-the-art (SOTA) Transformer-based architectures, though effective, are impractical for real-world deployment due to their high parameter count, GPU memory usage, and limited throughput, making them unsuitable for very long videos. In this work, we innovatively adapt the Mamba architecture for action detection and propose Multi-scale Temporal Mamba (MS-Temba), comprising two key components: Temporal Mamba (Temba) Blocks and the Temporal Mamba Fuser. Temba Blocks include the Temporal Local Module (TLM) for short-range temporal modeling and the Dilated Temporal SSM (DTS) for long-range dependencies. By introducing dilations, a novel concept for Mamba, TLM and DTS capture local and global features at multiple scales. The Temba Fuser aggregates these scale-specific features using Mamba to learn comprehensive multi-scale representations of untrimmed videos. MS-Temba is validated on three public datasets, outperforming SOTA methods on long videos and matching prior methods on short videos while using only one-eighth of the parameters.1
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2501.06138
AN  - PPRN:120484984
AD  - Univ North Carolina Charlotte, Charlotte, NC 28223, USA
M2  - Univ North Carolina Charlotte
Y2  - 2025-03-15
ER  -

TY  - JOUR
AU  - Lei, Yongsheng
AU  - Ding, Meng
AU  - Lu, Tianliang
AU  - Li, Juhao
AU  - Zhao, Dongyue
AU  - Chen, Fushi
TI  - A novel approach for enhanced abnormal action recognition via coarse and precise detection stage
T2  - ELECTRONIC RESEARCH ARCHIVE
M3  - Article
AB  - With the proliferation of urban video surveillance systems, the abundance of surveillance video data has emerged as a pivotal asset for enhancing public safety. Within these video archives, the identification of abnormal human actions carries profound implications for security incidents. Nevertheless, existing surveillance systems primarily rely on conventional algorithms, leading to both missed incidents and false alarms. To address the challenge of automating multi-object surveillance video analysis, this study introduces a comprehensive method for the detection and recognition of multi-object abnormal actions. This study comprises a two-stage framework: the coarse detection stage employs an enhanced YOWOv2E model for spatio-temporal action detection, while the precise detection stage utilizes a two-stream network for precise action classification. In parallel, this paper presents the PSA-Dataset to address the current limitations in the field of abnormal action detection. Experimental results, collected from both public datasets and a self-built dataset, illustrate the effectiveness of the proposed method in identifying a wide spectrum of abnormal actions. This work offers valuable insights for automating the analysis of human actions in videos pertaining to public security.
PU  - AMER INST MATHEMATICAL SCIENCES-AIMS
PI  - SPRINGFIELD
PA  - PO BOX 2604, SPRINGFIELD, MO 65801-2604, UNITED STATES
SN  - 2688-1594
DA  - 2024 
PY  - 2024
VL  - 32
IS  - 2
SP  - 874
EP  - 896
DO  - 10.3934/era.2024042
AN  - WOS:001148009500001
AD  - Peoples Publ Secur Univ China, Sch Criminal Invest, Beijing 100038, Peoples R China
AD  - Peoples Publ Secur Univ China, Publ Secur Behav Sci Lab, Beijing 100038, Peoples R China
AD  - Peoples Publ Secur Univ China, Sch Informat & Cyber Secur, Beijing 100038, Peoples R China
Y2  - 2024-02-04
ER  -

TY  - JOUR
AU  - Singh, Gurkirt
AU  - Choutas, Vasileios
AU  - Saha, Suman
AU  - Yu, Fisher
AU  - Van Gool, Luc
TI  - Spatio-Temporal Action Detection Under Large Motion
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Current methods for spatiotemporal action tube detection often extend a bounding box proposal at a given keyframe into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatiotemporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to the cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2209.02250
AN  - PPRN:13701506
AD  - ETH Zurich, Comp Vis Lab, Zurich, Switzerland
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Rodin, Ivan
AU  - Furnari, Antonino
AU  - Farinella, Giovanni Maria
TI  - Egocentric action anticipation from untrimmed videos
T2  - IET COMPUTER VISION
M3  - Article
AB  - Egocentric action anticipation involves predicting future actions performed by the camera wearer from egocentric video. Although the task has recently gained attention in the research community, current approaches often assume that input videos are 'trimmed', meaning that a short video sequence is sampled a fixed time before the beginning of the action. However, trimmed action anticipation has limited applicability in real-world scenarios, where it is crucial to deal with 'untrimmed' video inputs and the exact moment of action initiation cannot be assumed at test time. To address these limitations, an untrimmed action anticipation task is proposed, which, akin to temporal action detection, assumes that the input video is untrimmed at test time, while still requiring predictions to be made before actions take place. The authors introduce a benchmark evaluation procedure for methods designed to address this novel task and compare several baselines on the EPIC-KITCHENS-100 dataset. Through our experimental evaluation, testing a variety of models, the authors aim to better understand their performance in untrimmed action anticipation. Our results reveal that the performance of current models designed for trimmed action anticipation is limited, emphasising the need for further research in this area.
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN  - 1751-9632
SN  - 1751-9640
DA  - 2025 JAN
PY  - 2025
VL  - 19
IS  - 1
C7  - e12342
DO  - 10.1049/cvi2.12342
AN  - WOS:001420842600001
AD  - Univ Catania, Catania, Italy
AD  - Univ Catania, Next Vis srl Spinoff, Catania, Italy
Y2  - 2025-02-20
ER  -

TY  - JOUR
AU  - Vo, Khoa
AU  - Yamazaki, Kashu
AU  - Nguyen, Phong X
AU  - Nguyen, Phat
AU  - Luu, Khoa
AU  - Le, Ngan
TI  - Contextual Explainable Video Representation: Human Perception-based Understanding
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video understanding is a growing field and a subject of intense research, which includes many interesting tasks to understanding both spatial and temporal information, e.g., action detection, action recognition, video captioning, video retrieval. One of the most challenging problems in video understanding is dealing with feature extraction, i.e. extract contextual visual representation from given untrimmed video due to the long and complicated temporal structure of unconstrained videos. Different from existing approaches, which apply a pre-trained backbone network as a black-box to extract visual representation, our approach aims to extract the most contextual information with an explainable mechanism. As we observed, humans typically perceive a video through the interactions between three main factors, i.e., the actors, the relevant objects, and the surrounding environment. Therefore, it is very crucial to design a contextual explainable video representation extraction that can capture each of such factors and model the relationships between them. In this paper, we discuss approaches, that incorporate the human perception process into modeling actors, objects, and the environment. We choose video paragraph captioning and temporal action detection to illustrate the effectiveness of human perception based-contextual representation in video understanding.&nbsp;
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2212.06206
AN  - PPRN:35894771
AD  - Univ Arkansas, Dept CSCE, Fayetteville, AR 72701, USA
AD  - FPT Software, AI Lab, Ho Chi Minh City, Vietnam
M2  - FPT Software
Y2  - 2023-02-07
ER  -

TY  - CPAPER
AU  - Singh, Gurkirt
AU  - Choutas, Vasileios
AU  - Saha, Suman
AU  - Yu, Fisher
AU  - Van Gool, Luc
A1  - IEEE
TI  - Spatio-Temporal Action Detection Under Large Motion
T2  - 2023 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
M3  - Proceedings Paper
CP  - 23rd IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CL  - Waikoloa, HI
AB  - Current methods for spatio-temporal action tube detection often extend a bounding box proposal at a given key-frame into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatio-temporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 2472-6737
SN  - 978-1-6654-9346-8
DA  - 2023 
PY  - 2023
SP  - 5998
EP  - 6007
DO  - 10.1109/WACV56688.2023.00595
AN  - WOS:000971500206013
AD  - Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland
Y2  - 2023-07-22
ER  -

TY  - JOUR
AU  - Kaufman, Dotan
AU  - Levi, Gil
AU  - Hassner, Tal
AU  - Wolf, Lior
TI  - Temporal Tessellation: A Unified Approach for Video Analysis
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - We present a general approach to video understanding, inspired by semantic transfer techniques that have been successfully used for 2D image analysis. Our method considers a video to be a 1D sequence of clips, each one associated with its own semantics. The nature of these semantics -- natural language captions or other labels -- depends on the task at hand. A test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics, following which, reference semantics can be transferred to the test video. We describe two matching methods, both designed to ensure that (a) reference clips appear similar to test clips and (b), taken together, the semantics of the selected reference clips is consistent and maintains temporal coherence. We use our method for video captioning on the LSMDC'16 benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal Action Detection on the Thumos2014 benchmark, and sound prediction on the Greatest Hits benchmark. Our method not only surpasses the state of the art, in four out of five benchmarks, but importantly, it is the only single method we know of that was successfully applied to such a diverse range of tasks.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1612.06950
AN  - PPRN:12424951
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Wang, Xiang
AU  - Gao, Changxin
AU  - Zhang, Shiwei
AU  - Sang, Nong
TI  - Multi-Level Temporal Pyramid Network for Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Currently, one-stage frameworks have been widely applied for temporal action detection, but they still suffer from the challenge that the action instances span a wide range of time. The reason is that these one-stage detectors, e.g., Single Shot Multi-Box Detector (SSD), extract temporal features only applying a single-level layer for each head, which is not discriminative enough to perform classification and regression. In this paper, we propose a Multi-Level Temporal Pyramid Network (MLTPN) to improve the discrimination of the features. Specially, we first fuse the features from multiple layers with different temporal resolutions, to encode multi-layer temporal information. We then apply a multi-level feature pyramid architecture on the features to enhance their discriminative abilities. Finally, we design a simple yet effective feature fusion module to fuse the multi-level multi-scale features. By this means, the proposed MLTPN can learn rich and discriminative features for different action instances with different durations. We evaluate MLTPN on two challenging datasets: THUMOS'14 and Activitynet v1.3, and the experimental results show that MLTPN obtains competitive performance on Activitynet v1.3 and outperforms the state-of-the-art approaches on THUMOS'14 significantly.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2008.03270
AN  - PPRN:13116160
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Shen, Qi
AU  - Zhao, Shengjie
AU  - Zhang, Rongqing
AU  - Zhang, Bin
TI  - Robust Two-Stream Multi-Feature Network for Driver Drowsiness Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Drowsiness driving is a major cause of traffic accidents and thus numerous previous researches have focused on driver drowsiness detection. Many drive relevant factors have been taken into consideration for fatigue detection and can lead to high precision, but there are still several serious constraints, such as most existing models are environmentally susceptible. In this paper, fatigue detection is considered as temporal action detection problem instead of image classification. The proposed detection system can be divided into four parts: (1) Localize the key patches of the detected driver picture which are critical for fatigue detection and calculate the corresponding optical flow. (2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our system to reduce the impact of different light conditions. (3) Three individual two-stream networks combined with attention mechanism are designed for each feature to extract temporal information. (4) The outputs of the three sub-networks will be concatenated and sent to the fully-connected network, which judges the status of the driver. The drowsiness detection system is trained and evaluated on the famous Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%, which outperforms most existing fatigue detection models.
PU  - CORNELL UNIV
DA  - 2020 
PY  - 2020
DO  - arXiv:2010.06235
AN  - PPRN:15423995
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Hu, Xuejiao
AU  - Dai, Jingzhao
AU  - Li, Ming
AU  - Peng, Chenglei
AU  - Li, Yang
AU  - Du, Sidan
TI  - Online human action detection and anticipation in videos: A survey
T2  - NEUROCOMPUTING
M3  - Article
AB  - To meet the demand for powerful models for practical applications in real time, the focus of research on human actions has shifted from offline detection to online and real-time understanding, such as driver-assistance systems, surveillance analysis, and robot services. In recent years, with the development of video recording acquisition technology and deep learning, online action analysis has made significant progress. However, there is a lack of comprehensive online surveys for online human action detection. In this survey, we discuss two hot real-time concerns: online action detection and action anticipation. Online action/activity detection aims to determine whether an action is currently taking place and what kind of action it is in untrimmed videos. Action anticipation aims to anticipate human actions under lim-ited observation of videos. Online action detection and anticipation require accuracy and low latency of detection when the video is partly observed. We present a comprehensive study that includes the defi-nition, taxonomy, comparison of state-of-the-art techniques, datasets, metrics, challenges, and future directions. We hope that it will provide readers with a detailed understanding of the topic and inspiration for new research directions.(c) 2022 Published by Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2022 JUN 28
PY  - 2022
VL  - 491
SP  - 395
EP  - 413
DO  - 10.1016/j.neucom.2022.03.069
AN  - WOS:000806853000014
C6  - APR 2022
AD  - Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China
AD  - Nanjing Inst Adv Artificial Intelligence, Nanjing 210000, Peoples R China
M2  - Nanjing Inst Adv Artificial Intelligence
Y2  - 2022-06-22
ER  -

TY  - CPAPER
AU  - Rachavarapu, Kranthi Kumar
AU  - Rajagopalan, A. N.
A1  - IEEE
TI  - Boosting Positive Segments for Weakly-Supervised Audio-Visual Video Parsing
T2  - 2023 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2023)
M3  - Proceedings Paper
CP  - IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - Paris, FRANCE
AB  - In this paper, we address the problem of weakly supervised Audio-Visual Video Parsing (AVVP), where the goal is to temporally localize events that are audible or visible and simultaneously classify them into known event categories. This is a challenging task, as we only have access to the video-level event labels during training but need to predict event labels at the segment level during evaluation. Existing multiple-instance learning (MIL) based methods use a form of attentive pooling over segment-level predictions. These methods only optimize for a subset of most discriminative segments that satisfy the weak-supervision constraints, which miss identifying positive segments. To address this, we focus on improving the proportion of positive segments detected in a video. To this end, we model the number of positive segments in a video as a latent variable and show that it can be modeled as Poisson binomial distribution over segment-level predictions, which can be computed exactly. Given the absence of fine-grained supervision, we propose an Expectation-Maximization approach to learn the model parameters by maximizing the evidence lower bound (ELBO). We iteratively estimate the minimum positive segments in a video and refine them to capture more positive segments. We conducted extensive experiments on AVVP tasks to evaluate the effectiveness of our proposed approach, and the results clearly demonstrate that it increases the number of positive segments captured compared to existing methods. Additionally, our experiments on Temporal Action Localization (TAL) demonstrate the potential of our method for generalization to similar MIL tasks.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1550-5499
SN  - 979-8-3503-0718-4
DA  - 2023 
PY  - 2023
SP  - 10158
EP  - 10168
DO  - 10.1109/ICCV51070.2023.00935
AN  - WOS:001169499002056
AD  - Indian Inst Technol Madras, Madras, Tamil Nadu, India
Y2  - 2024-04-06
ER  -

TY  - CPAPER
AU  - Shi, Xiaojuan
AU  - Sun, Hailin
AU  - Zhao, Ziyu
AU  - Li, Ruimin
AU  - Gou, Shuiping
AU  - Sun, Shuqing
A1  - IEEE COMPUTER SOC
TI  - Cardiopulmonary Resuscitation Skill Assessment based on Transformer with Temporal Filtering
T2  - 2024 IEEE INTERNATIONAL CONFERENCE ON MEDICAL ARTIFICIAL INTELLIGENCE, MEDAI 2024
M3  - Proceedings Paper
CP  - 2024 International Conference on Medical Artificial Intelligence
CL  - Chongqing, PEOPLES R CHINA
AB  - Cardiopulmonary resuscitation (CPR) is a crucial emergency technique that saves lives in emergencies such as cardiac arrest. The widespread dissemination of CPR skills is essential. However, current CPR skill training and evaluation largely rely on manual labor or expensive hardware equipment, making broad promotion challenging. In this paper, a two-stage CPR skills assessment framework with transformer based on visual action analysis is proposed. First, the action classification model, Video Swin Transformer, is utilized to conduct clip-level key step recognition, and indirectly obtain video-level temporal localization of key steps. Then, the localization results are optimized using three designed temporal filtering strategies. Additionally, a self-collected CPR dataset is built, and the experiments conducted on this dataset validate the superior performance of our method. The proposed approach provides an effective safeguard for the broadly dissemination of CPR skills.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 979-8-3503-7762-0
SN  - 979-8-3503-7761-3
DA  - 2024 
PY  - 2024
SP  - 38
EP  - 43
DO  - 10.1109/MedAI62885.2024.00012
AN  - WOS:001413988900005
AD  - Air Force Mil Med Univ, Dept Orthoped, Xijing Hosp, Xian, Shaanxi, Peoples R China
AD  - Xidian Univ, Sch Artificial Intelligence, Xian, Shaanxi, Peoples R China
AD  - Xidian Univ, Hangzhou Inst Technol, Xian, Shaanxi, Peoples R China
AD  - Xidian Univ, Acad Adv Interdisciplinary Res, Xian, Shaanxi, Peoples R China
AD  - Weifang Peoples Hosp, Intens Care Unit, Weifang, Shandong, Peoples R China
M2  - Weifang Peoples Hosp
Y2  - 2025-03-05
ER  -

TY  - JOUR
AU  - Ma, Chen
AU  - Chen, Li
AU  - Yong, Junhai
TI  - AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1812.05788
AN  - PPRN:22621202
Y2  - 2022-11-23
ER  -

TY  - JOUR
AU  - Huang, Jingjia
AU  - Li, Nannan
AU  - Zhang, Tao
AU  - Li, Ge
TI  - A Self-Adaptive Proposal Model for Temporal Action Detection based on Reinforcement Learning
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose an active action proposal model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at a position in the video at random, adopts a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS 2014 validate the effectiveness of the proposed approach, which can achieve competitive performance with current action detection algorithms via much fewer proposals.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1706.07251
AN  - PPRN:12542845
Y2  - 2022-11-15
ER  -

TY  - CPAPER
AU  - Cheng, Feng
AU  - Xu, Mingze
AU  - Xiong, Yuanjun
AU  - Chen, Hao
AU  - Li, Xinyu
AU  - Li, Wei
AU  - Xia, Wei
A1  - IEEE COMP SOC
TI  - Stochastic Back propagation: A Memory Efficient Strategy for Training Video Models
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - We propose a memory efficient method, named Stochastic Backpropagation (SBP), for training deep neural networks on videos. It is based on the finding that gradients from incomplete execution for backpropagation can still effectively train the models with minimal accuracy loss, which attributes to the high redundancy of video. SBP keeps all forward paths but randomly and independently removes the backward paths for each network layer in each training step. It reduces the GPU memory cost by eliminating the need to cache activation values corresponding to the dropped backward paths, whose amount can be controlled by an adjustable keep-ratio. Experiments show that SBP can be applied to a wide range of models for video tasks, leading to up to 80.0% GPU memory saving and 10% training speedup with less than 1% accuracy drop on action recognition and temporal action detection.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 8291
EP  - 8300
DO  - 10.1109/CVPR52688.2022.00812
AN  - WOS:000870759101034
AD  - Univ N Carolina, Chapel Hill, NC USA
AD  - AWS AI Labs, Palo Alto, CA 94301 USA
AD  - Amazon Internship, Palo Alto, CA USA
M2  - AWS AI Labs
M2  - Amazon Internship
Y2  - 2022-12-17
ER  -

TY  - JOUR
AU  - Xu, Mengmeng
AU  - Soldan, Mattia
AU  - Gao, Jialin
AU  - Liu, Shuming
AU  - Perez-Rua, Juan-Manuel
AU  - Ghanem, Bernard
TI  - Boundary-Denoising for Video Activity Localization
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Video activity localization aims at understanding the semantic content in long untrimmed videos and retrieving actions of interest. The retrieved action with its start and end locations can be used for highlight generation, temporal action detection, etc. Unfortunately, learning the exact boundary location of activities is highly challenging because temporal activities are continuous in time, and there are often no clear-cut transitions between actions. Moreover, the definition of the start and end of events is subjective, which may confuse the model. To alleviate the boundary ambiguity, we propose to study the video activity localization problem from a denoising perspective. Specifically, we propose an encoder-decoder model named DenoiseLoc. During training, a set of action spans is randomly generated from the ground truth with a controlled noise scale. Then we attempt to reverse this process by boundary denoising, allowing the localizer to predict activities with precise boundaries and resulting in faster convergence speed. Experiments show that DenoiseLoc advances %in several video activity understanding tasks. For example, we observe a gain of +12.36% average mAP on QV-Highlights dataset and +1.64% mAP@0.5 on THUMOS'14 dataset over the baseline. Moreover, DenoiseLoc achieves state-of-the-art performance on TACoS and MAD datasets, but with much fewer predictions compared to other current methods.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.02934
AN  - PPRN:55157368
AD  - KAUST, Thuwal, Saudi Arabia
AD  - NUS, Singapore, Singapore
AD  - Meta AI, Cambridge, England
M2  - KAUST
M2  - Meta AI
Y2  - 2023-04-26
ER  -

TY  - JOUR
AU  - Yang, Min
AU  - Zhang, Zichen
AU  - Wang, Limin
TI  - Temporal2Seq: A Unified Framework for Temporal Video Understanding Tasks
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - With the development of video understanding, there is a proliferation of tasks for clip-level temporal video analysis, including temporal action detection (TAD), temporal action segmentation (TAS), and generic event boundary detection (GEBD). While task-specific video understanding models have exhibited outstanding performance in each task, there remains a dearth of a unified framework capable of simultaneously addressing multiple tasks, which is a promising direction for the next generation of AI. To this end, in this paper, we propose a single unified framework, coined as Temporal2Seq, to formulate the output of these temporal video understanding tasks as a sequence of discrete tokens. With this unified token representation, Temporal2Seq can train a generalist model within a single architecture on different video understanding tasks. In the absence of multi-task learning (MTL) benchmarks, we compile a comprehensive co-training dataset by borrowing the datasets from TAD, TAS, and GEBD tasks. We evaluate our Temporal2Seq generalist model on the corresponding test sets of three tasks, demonstrating that Temporal2Seq can produce reasonable results on various tasks and achieve advantages compared with single-task training on this framework. We also investigate the generalization performance of our generalist model on new datasets from different tasks, which yields superior performance to the specific model.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2409.18478
AN  - PPRN:100707594
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
Y2  - 2024-10-09
ER  -

TY  - JOUR
AU  - Ma, Chen
AU  - Chen, Li
AU  - Yong, Junhai
TI  - AU R-CNN: Encoding expert prior knowledge into R-CNN for action unit detection
T2  - NEUROCOMPUTING
M3  - Article
AB  - Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Expert prior knowledge is encoded in the region and the Rol-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that only static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. Code will be made available. (C) 2019 Elsevier B.V. All rights reserved.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2019 AUG 25
PY  - 2019
VL  - 355
SP  - 35
EP  - 47
DO  - 10.1016/j.neucom.2019.03.082
AN  - WOS:000468599300004
AD  - Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China
AD  - Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China
Y2  - 2019-06-04
ER  -

TY  - BOOK
AU  - Bency, Archith John
Z2  -  
TI  - Search Strategies for Localization in Images and Videos
M3  - Dissertation/Thesis
SN  - 978-0-438-89659-8
DA  - 2018 
PY  - 2018
AN  - PQDT:68776511
AD  - University of California, Santa Barbara, Electrical and Computer Engineering, California, United States
M2  - University of California, Santa Barbara
ER  -

TY  - JOUR
AU  - Zhang, Min
AU  - Hu, Haiyang
AU  - Li, Zhongjin
AU  - Chen, Jie
TI  - Attention-based encoder-decoder networks for workflow recognition
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
M3  - Article
AB  - Behavior recognition is a fundamental yet challenging task in intelligent surveillance system, which plays an increasingly important role in the process of "Industry 4.0". However, monitoring the workflow of both workers and machines in production procedure is quite difficult in complex industrial environments. In this paper, we propose a novel workflow recognition framework to recognize the behavior of working subjects based on the well-designed encoder-decoder structure. Namely, attention-based workflow recognition framework, termed as AWR. To improve the accuracy of workflow recognition, a temporal attention cell (AttCell) is introduced to draw dynamic attention distribution in the last stage of the framework. In addition, a Rough-to-Refine phase localization model is exploited to improve localization accuracy, which can effectively identify the boundaries of a specific phase instance in long untrimmed videos. Comprehensive experiments indicate a 1.4% mAP@IoU= 0.4 boost on THUMOS'14 dataset and a 3.4% mAP@IoU= 0.4 boost on hand-crafted workflow dataset detection challenge compared to the advanced GTAN pipeline respectively. More remarkably, the effectiveness of the workflow recognition system is validated in a real-world production scenario.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1380-7501
SN  - 1573-7721
DA  - 2021 NOV
PY  - 2021
VL  - 80
IS  - 28-29
SP  - 34973
EP  - 34995
DO  - 10.1007/s11042-021-10633-5
AN  - WOS:000625713700009
C6  - MAR 2021
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China
Y2  - 2021-03-14
ER  -

TY  - JOUR
AU  - Huo, Shuwei
AU  - Zhou, Yuan
AU  - Wang, Ruolin
AU  - Xiang, Wei
AU  - Kung, Sun-Yuan
TI  - Semantic Relevance Learning for Video-Query Based Video Moment Retrieval
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - The task of video-query based video moment retrieval (VQ-VMR) aims to localize the segment in the reference video, which matches semantically with a short query video. This is a challenging task due to the rapid expansion and massive growth of online video services. With accurate retrieval of the target moment, we propose a new metric to effectively assess the semantic relevance between the query video and segments in the reference video. We also develop a new VQ-VMR framework to discover the intrinsic semantic relevance between a pair of input videos. It comprises two key components: a Fine-grained Feature Interaction (FFI) module and a Semantic Relevance Measurement (SRM) module. Together they can effectively deal with both the spatial and temporal dimensions of videos. First, the FFI module computes the semantic similarity between videos at a local frame level, mainly considering the spatial information in the videos. Subsequently, the SRM module learns the similarity between videos from a global perspective, taking into account the temporal information. We have conducted extensive experiments on two key datasets which demonstrate noticeable improvements of the proposed approach over the state-of-the-art methods.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2023 
PY  - 2023
VL  - 25
SP  - 9290
EP  - 9301
DO  - 10.1109/TMM.2023.3250088
AN  - WOS:001133324200021
AD  - Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China
AD  - La Trobe Univ, Sch Comp Engn & Math Sci, Melbourne, Vic 3086, Australia
AD  - James Cook Univ, Coll Sci & Engn, Cairns, Qld 4878, Australia
AD  - Princeton Univ, Elect Engn Dept, Princeton, NJ 08540 USA
Y2  - 2024-01-24
ER  -

TY  - CPAPER
AU  - Chen, Brian
AU  - Rouditchenko, Andrew
AU  - Duarte, Kevin
AU  - Kuehne, Hilde
AU  - Thomas, Samuel
AU  - Boggust, Angie
AU  - Panda, Rameswar
AU  - Kingsbury, Brian
AU  - Feris, Rogerio
AU  - Harwath, David
AU  - Glass, James
AU  - Picheny, Michael
AU  - Chang, Shih-Fu
A1  - IEEE
TI  - Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos
T2  - 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021)
M3  - Proceedings Paper
CP  - 18th IEEE/CVF International Conference on Computer Vision (ICCV)
CL  - ELECTR NETWORK
AB  - Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities. In this context, this paper proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar instances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action localization, showing state-of-the-art results on four different datasets.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 978-1-6654-2812-5
DA  - 2021 
PY  - 2021
SP  - 7992
EP  - 8001
DO  - 10.1109/ICCV48922.2021.00791
AN  - WOS:000797698908023
AD  - Columbia Univ, New York, NY 10027 USA
AD  - MIT, CSAIL, Cambridge, MA 02139 USA
AD  - Univ Cent Florida, Orlando, FL 32816 USA
AD  - Goethe Univ Frankfurt, Frankfurt, Germany
AD  - IBM Res AI, Yorktown Hts, NY USA
AD  - MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA
AD  - UT Austin, Austin, TX USA
AD  - NYU, Courant CS & CDS, New York, NY 10003 USA
M2  - IBM Res AI
Y2  - 2022-07-08
ER  -

TY  - JOUR
AU  - Li, Juncheng
AU  - Xie, Junlin
AU  - Zhu, Linchao
AU  - Qian, Long
AU  - Tang, Siliang
AU  - Zhang, Wenqiao
AU  - Shi, Haochen
AU  - Zhang, Shengyu
AU  - Wei, Longhui
AU  - Tian, Qi
AU  - Zhuang, Yueting
TI  - Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos~(TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and labor-intensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https://github.com/YYJMJC/Temporal-Emotion-Localization-in-Videos.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.01954
AN  - PPRN:11920903
AD  - Zhejiang Univ, Zhejiang, Peoples R China
AD  - Univ Technol Sydney, Ultimo, NSW, Australia
AD  - Natl Univ Singapore, Singapore, Singapore
AD  - Univ Montreal, Montreal, QBC, Canada
AD  - Huawei Cloud, Jakarta, Indonesia
M2  - Zhejiang Univ
M2  - Natl Univ Singapore
M2  - Univ Montreal
M2  - Huawei Cloud
Y2  - 2022-11-15
ER  -

TY  - JOUR
AU  - Qiu, Zhaofan
AU  - Yao, Ting
AU  - Ngo, Chong-Wah
AU  - Tian, Xinmei
AU  - Mei, Tao
TI  - Learning Spatio-Temporal Representation with Local and Global Diffusion
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency. Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported. Code is available at: https://github.com/ZhaofanQiu/local-and-global-diffusion-networks.
PU  - CORNELL UNIV
DA  - 2019 
PY  - 2019
DO  - arXiv:1906.05571
AN  - PPRN:22158467
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Hamann, Friedhelm
AU  - Ghosh, Suman
AU  - Martinez, Ignacio Juarez
AU  - Hart, Tom
AU  - Kacelnik, Alex
AU  - Gallego, Guillermo
TI  - Low-power, Continuous Remote Behavioral Localization with Event Cameras
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery -dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica for several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras’ natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The lowpower capabilities of the event camera allow it to record significantly longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2312.03799
AN  - PPRN:86435958
AD  - Tech Univ Berlin, Berlin, Germany
AD  - Oxford Univ, Oxford, England
AD  - Oxford Brookes Univ, Oxford, England
AD  - Einstein Ctr Digital Future, Berlin, Germany
AD  - Sci Intelligence Excellence Cluster, Berlin, Germany
M2  - Oxford Univ
M2  - Oxford Brookes Univ
M2  - Einstein Ctr Digital Future
M2  - Sci Intelligence Excellence Cluster
Y2  - 2024-04-12
ER  -

TY  - JOUR
AU  - Chang, Shuning
AU  - Wang, Pichao
AU  - Wang, Fan
AU  - Feng, Jiashi
AU  - Show, Mike Zheng
TI  - DOAD: Decoupled One Stage Action Detection Network
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Localizing people and recognizing their actions from videos is a challenging task towards high-level video understanding. Existing methods are mostly two-stage based, with one stage for person bounding box generation and the other stage for action recognition. However, such two-stage methods are generally with low efficiency. We observe that directly unifying detection and action recognition normally suffers from (i) inferior learning due to different desired properties of context representation for detection and action recognition; (ii) optimization difficulty with insufficient training data. In this work, we present a decoupled one-stage network dubbed DOAD, to mitigate above issues and improve the efficiency for spatio-temporal action detection. To achieve it, we decouple detection and action recognition into two branches. Specifically, one branch focuses on detection representation for actor detection, and the other one for action recognition. For the action branch, we design a transformer-based module (TransPC) to model pairwise relationships between people and context. Different from commonly used vector-based dot product in self-attention, it is built upon a novel matrix-based key and value for Hadamard attention to model person-context information. It not only exploits relationships between person pairs but also takes into account context and relative position information. The results on AVA and UCF101-24 datasets show that our method is competitive with two-stage state-of-the-art methods with significant efficiency improvement.
PU  - CORNELL UNIV
DA  - 2023 
PY  - 2023
DO  - arXiv:2304.00254
AN  - PPRN:54044741
AD  - Natl Univ Singapore, Showlab, Singapore, Singapore
AD  - Natl Univ Singapore, Singapore, Singapore
AD  - Alibaba Grp, Hangzhou, Peoples R China
M2  - Alibaba Grp
Y2  - 2023-04-15
ER  -

TY  - CPAPER
AU  - Hopp, Maximilian
AU  - Hartleb, Helge
AU  - Burchard, Robin
A1  - ACM
TI  - TA-DA! - Improving Activity Recognition Using Temporal Adapters and Data Augmentation
T2  - COMPANION OF THE 2024 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, UBICOMP COMPANION 2024
M3  - Proceedings Paper
CP  - ACM International Joint Conference on Pervasive and Ubiquitous Computing / ACM International Symposium on Wearable Computers (UbiComp/ISWC)
CL  - Melbourne, AUSTRALIA
AB  - In this report, we describe the technical details of our submission to the WEAR Dataset Challenge 2024. For this competition, we use two approaches to boost the performance of the official WEAR GitHub repository. 1) Integration of a Temporal-Informative adapter (TIA) into the models of the WEAR repository; 2) Data Augmentation Techniques to enrich the provided test dataset. Our method achieves roughly 4.7% improved results(1) on the test set of the WEAR Dataset Challenge 2024 compared to the baseline of the WEAR repository.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES
SN  - 979-8-4007-1058-2
DA  - 2024 
PY  - 2024
SP  - 551
EP  - 554
DO  - 10.1145/3675094.3678454
AN  - WOS:001322658600095
AD  - Univ Siegen, Siegen, Germany
Y2  - 2024-10-30
ER  -

TY  - JOUR
AU  - Katona, Zsofia
AU  - Ziabari, Seyed Sahand Mohammadi
AU  - Nejadasl, Fatemeh Karimi
TI  - MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Encounters between predator and prey play an essential role in ecosystems, but their rarity makes them difficult to detect in video recordings. Although advances in action recognition (AR) and temporal action detection (AD), especially transformer-based models and vision foundation models, have achieved high performance on human action datasets, animal videos remain relatively under-researched. This thesis addresses this gap by proposing the model MARINE, which utilizes motion-based frame selection designed for fast animal actions and DINOv2 feature extraction with a trainable classification head for action recognition. MARINE outperforms VideoMAE in identifying predator attacks in videos of fish, both on a small and specific coral reef dataset (81.53% against 52.64% accuracy), and on a subset of the more extensive Animal Kingdom dataset (94.86% against 83.14% accuracy). In a multi-label setting on a representative sample of Animal Kingdom, MARINE achieves 23.79% mAP, positioning it mid-field among existing benchmarks. Furthermore, in an AD task on the coral reef dataset, MARINE achieves 80.78% AP (against VideoMAE’s 34.89%) although at a lowered t-IoU threshold of 25%. Therefore, despite room for improvement, MARINE offers an effective starter framework to apply to AR and AD tasks on animal recordings and thus contribute to the study of natural ecosystems. 1 2 3
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2407.18289
AN  - PPRN:91120997
AD  - Univ Amsterdam, Amsterdam, Netherlands
Y2  - 2024-08-11
ER  -

TY  - JOUR
AU  - Huang, Wei-Jhe
AU  - Chen, Min-Hung
AU  - Lai, Shang-Hong
TI  - Spatio-Temporal Context Prompting for Zero-Shot Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video. Recent works aim to enhance this process by incorporating interaction modeling, which captures the relationship between people and their surrounding context. However, these approaches have primarily focused on fully-supervised learning, and the current limitation lies in the lack of generalization capability to recognize unseen action categories. In this paper, we aim to adapt the pretrained image-language models to detect unseen actions. To this end, we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction. Meanwhile, our Context Prompting module will utilize contextual information to prompt labels, thereby enhancing the generation of more representative text features. Moreover, to address the challenge of recognizing distinct actions by multiple people at the same timestamp, we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person's interest context tokens, and then these tokens will be used for prompting to generate text features tailored to each individual. To evaluate the ability to detect unseen actions, we propose a comprehensive benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos, bringing it closer to real-world applications. 
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2408.15996
AN  - PPRN:91782861
AD  - Natl Tsing Hua Univ, Hsinchu, Taiwan
AD  - NVIDIA, Santa Clara, CA, USA
M2  - NVIDIA
Y2  - 2024-09-23
ER  -

TY  - JOUR
AU  - Hou, Rui
AU  - Chen, Chen
AU  - Shah, Mubarak
TI  - An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - In this paper, we propose an end-to-end 3D CNN for action detection and segmentation in videos. The proposed architecture is a unified deep network that is able to recognize and localize action based on 3D convolution features. A video is first divided into equal length clips and next for each clip a set of tube proposals are generated based on 3D CNN features. Finally, the tube proposals of different clips are linked together and spatio-temporal action detection is performed using these linked video proposals. This top-down action detection approach explicitly relies on a set of good tube proposals to perform well and training the bounding box regression usually requires a large number of annotated samples. To remedy this, we further extend the 3D CNN to an encoder-decoder structure and formulate the localization problem as action segmentation. The foreground regions (i.e. action regions) for each frame are segmented first then the segmented foreground maps are used to generate the bounding boxes. This bottom-up approach effectively avoids tube proposal generation by leveraging the pixel-wise annotations of segmentation. The segmentation framework also can be readily applied to a general problem of video object segmentation. Extensive experiments on several video datasets demonstrate the superior performance of our approach for action detection and video object segmentation compared to the state-of-the-arts.
PU  - CORNELL UNIV
DA  - 2017 
PY  - 2017
DO  - arXiv:1712.01111
AN  - PPRN:19308193
Y2  - 2023-03-23
ER  -

TY  - CPAPER
AU  - Wang, Shuoping
AU  - Xiao, Youan
AU  - Wang, Tengfei
AU  - Li, Zhuo
A1  - AMER SOC MECHANICAL ENGINEERS
TI  - THE REAL-TIME HUMAN RELIABILITY DETECTION SYSTEM BASED ON SHIP BRIDGE VIDEOS
T2  - PROCEEDINGS OF ASME 2022 INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, IMECE2022, VOL 9
M3  - Proceedings Paper
CP  - ASME International Mechanical Engineering Congress and Exposition (IMECE)
CL  - Columbus, OH
AB  - Human reliability analysis in assessment of ship collision risk has always been a concern for shipping practitioners. The risk of ship collision is largely related to the driving state of crew. At the same time, although surveillance cameras have been deployed in key locations such as ship bridge on most of the current operating ships, the monitoring information is only used as evidence for daily evaluation or responsibility determination after an accident, further values are not discovered. How to make full use of existing ship bridge video resources to assess the real-time state of crew, and then assess the risk of collisions, is of great significance and academic value.Therefore, this paper proposes a real-time human reliability detection system based on ship bridge videos. Object detection model trained on COCO[1] dataset using Faster-RCNN [2] algorithm and spatio-temporal action detection model trained on AVA[3] dataset using SlowFast[4] algorithm are used to perform video understanding work on ship bridge videos, obtaining the actions of crew and corresponding probabilities. This paper introduces performance impact factors (PIFs) and Information, Decision and Action in a Crew content cognitive (IDAC) model to build a three-layer mapping model which clarifies how to use the results of video understanding work to calculate relevant PIF scores and explain the impact of the state of crew on various processes in IDAC model.
PU  - AMER SOC MECHANICAL ENGINEERS
PI  - NEW YORK
PA  - THREE PARK AVENUE, NEW YORK, NY 10016-5990 USA
SN  - 978-0-7918-8671-7
DA  - 2022 
PY  - 2022
AN  - WOS:001215443800103
AD  - Wuhan Univ Technol, Sch Informat Engn, Wuhan, Peoples R China
AD  - Wuhan Univ Technol, Sch Transportat & Logist Engn, Wuhan, Peoples R China
Y2  - 2024-11-01
ER  -

TY  - JOUR
AU  - Ehsanpour, Mahsa
AU  - Saleh, Fatemeh
AU  - Savarese, Silvio
AU  - Reid, Ian
AU  - Rezatofighi, Hamid
TI  - JRDB-Act: A Large-scale Dataset for Spatio-temporal Action, Social Group and Activity Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The availability of large-scale video action understanding datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numerous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a significant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple~(optional) interaction-based action labels. Moreover JRDB-Act provides social group annotation, conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities~(common activities in each social group). Each annotated label in JRDB-Act is tagged with the annotators&#39; confidence level which contributes to the development of reliable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection.&nbsp;
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2106.08827
AN  - PPRN:11968547
AD  - Univ Adelaide, Adelaide, Australia
AD  - Samsung AI Cambridge, Cambridge, England
AD  - Stanford Univ, Stanford, CA 94305, USA
AD  - Monash Univ, Melbourne, Australia
AD  - Australian Natl Univ ANU, Canberra, Australia
M2  - Univ Adelaide
M2  - Samsung AI Cambridge
M2  - Stanford Univ
M2  - Australian Natl Univ ANU
Y2  - 2022-11-14
ER  -

TY  - JOUR
AU  - Yang, Min
AU  - Gao, Huan
AU  - Guo, Ping
AU  - Wang, Limin
TI  - Adapting Short-Term Transformers for Action Detection in Untrimmed Videos
T2  - Arxiv
M3  - preprint
C8  - 2
AB  - Vision Transformer (ViT) has shown high potential in video recognition, owing to its flexible design, adaptable self-attention mechanisms, and the efficacy of masked pre-training. Yet, it remains unclear how to adapt these pre-trained short-term ViTs for temporal action detection (TAD) in untrimmed videos. The existing works treat them as off-the-shelf feature extractors for each short-trimmed snippet without capturing the fine-grained relation among different snippets in a broader temporal context. To mitigate this issue, this paper focuses on designing a new mechanism for adapting these pre-trained ViT models as a unified long-form video transformer to fully unleash its modeling power in capturing inter-snippet relation, while still keeping low computation overhead and memory consumption for efficient TAD. To this end, we design effective cross-snippet propagation modules to gradually exchange short-term video information among different snippets from two levels. For inner-backbone information propagation, we introduce a cross-snippet propagation strategy to enable multi-snippet temporal feature interaction inside the backbone.For post-backbone information propagation, we propose temporal transformer layers for further clip-level modeling. With the plain ViT-B pre-trained with VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very competitive performance to previous temporal action detectors, riching up to 69.5 average mAP on THUMOS14, 37.40 average mAP on ActivityNet-1.3 and 17.20 average mAP on FineAction.
PU  - CORNELL UNIV
DA  - 2024 
PY  - 2024
DO  - arXiv:2312.01897
AN  - PPRN:86378711
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China
AD  - Inchitech, Sri, Andhra Pradesh, India
AD  - Intel Labs China, Beijing, Peoples R China
AD  - Shanghai AI Lab, Shanghai, Peoples R China
M2  - Nanjing Univ
M2  - Inchitech
M2  - Intel Labs China
Y2  - 2024-04-25
ER  -

TY  - JOUR
AU  - Zhang, Xiao-Yu
AU  - Shi, Haichao
AU  - Li, Changsheng
AU  - Shi, Xinchu
TI  - Action Shuffling for Weakly Supervised Temporal Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Weakly supervised action localization is a challenging task with extensive applications, which aims to identify actions and the corresponding temporal intervals with only video-level annotations available. This paper analyzes the order-sensitive and location-insensitive properties of actions, and embodies them into a self-augmented learning framework to improve the weakly supervised action localization performance. To be specific, we propose a novel two-branch network architecture with intra/inter-action shuffling, referred to as ActShufNet. The intra-action shuffling branch lays out a self-supervised order prediction task to augment the video representation with inner-video relevance, whereas the inter-action shuffling branch imposes a reorganizing strategy on the existing action contents to augment the training set without resorting to any external resources. Furthermore, the global-local adversarial training is presented to enhance the model's robustness to irrelevant noises. Extensive experiments are conducted on three benchmark datasets, and the results clearly demonstrate the efficacy of the proposed method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2022 
PY  - 2022
VL  - 31
SP  - 4447
EP  - 4457
DO  - 10.1109/TIP.2022.3185485
AN  - WOS:000819824000007
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 101408, Peoples R China
AD  - Beijing Inst Technol, Beijing 100081, Peoples R China
AD  - Meituan Grp, Beijing 100102, Peoples R China
M2  - Meituan Grp
Y2  - 2022-07-08
ER  -

TY  - JOUR
AU  - Zhang, Xiao-Yu
AU  - Li, Changsheng
AU  - Shi, Haichao
AU  - Zhu, Xiaobin
AU  - Li, Peng
AU  - Dong, Jing
TI  - AdapNet: Adaptability Decomposing Encoder-Decoder Network for Weakly Supervised Action Recognition and Localization
T2  - IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
M3  - Article
AB  - The point process is a solid framework to model sequential data, such as videos, by exploring the underlying relevance. As a challenging problem for high-level video understanding, weakly supervised action recognition and localization in untrimmed videos have attracted intensive research attention. Knowledge transfer by leveraging the publicly available trimmed videos as external guidance is a promising attempt to make up for the coarse-grained video-level annotation and improve the generalization performance. However, unconstrained knowledge transfer may bring about irrelevant noise and jeopardize the learning model. This article proposes a novel adaptability decomposing encoder-decoder network to transfer reliable knowledge between the trimmed and untrimmed videos for action recognition and localization by bidirectional point process modeling, given only video-level annotations. By decomposing the original features into the domain-adaptable and domain-specific ones based on their adaptability, trimmed-untrimmed knowledge transfer can be safely confined within a more coherent subspace. An encoder-decoder-based structure is carefully designed and jointly optimized to facilitate effective action classification and temporal localization. Extensive experiments are conducted on two benchmark data sets (i.e., THUMOS14 and ActivityNet1.3), and the experimental results clearly corroborate the efficacy of our method.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2162-237X
SN  - 2162-2388
DA  - 2023 APR
PY  - 2023
VL  - 34
IS  - 4
SP  - 1852
EP  - 1863
DO  - 10.1109/TNNLS.2019.2962815
AN  - WOS:000964682100019
AD  - Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China
AD  - Beijing Inst Technol, Beijing, Peoples R China
AD  - Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing, Peoples R China
AD  - China Univ Petr East China, Coll Informat & Control Engn, Qingdao, Peoples R China
AD  - Chinese Acad Sci, Inst Automation, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit, Beijing, Peoples R China
Y2  - 2023-04-28
ER  -

TY  - BOOK
AU  - Agarwal, Nakul
Z2  -  
TI  - Learning to Recognise Objects and Actions for Intelligent Agents
M3  - Dissertation/Thesis
SN  - 9781687947031
DA  - 2019 
PY  - 2019
AN  - PQDT:68749550
AD  - University of California, Merced, Electrical Engineering and Computer Science, California, United States
M2  - University of California, Merced
ER  -

TY  - JOUR
AU  - Xie, Chi
AU  - Zhuang, Zikun
AU  - Zhao, Shengjie
AU  - Liang, Shuang
TI  - Temporal Dropout for Weakly Supervised Action Localization
T2  - ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
M3  - Article
AB  - Weakly supervised action localization is a challenging problem in video understanding and action recognition. Existing models usually formulate the training process as direct classification using video-level supervision. They tend to only locate the most discriminative parts of action instances and produce temporally incomplete detection results. A natural solution for this problem, the adversarial erasing strategy, is to remove such parts from training so that models can attend to complementary parts. Previous works do it in an offline and heuristic way. They adopt a multi-stage pipeline, where discriminative regions are determined and erased under the guidance of detection results from last stage. Such a pipeline can be both ineffective and inefficient, possibly hindering the overall performance. On the contrary, we combine adversarial erasing with dropout mechanism and propose a Temporal Dropout Module that learns where to remove in a data-driven and online manner. This plug-and-play module is trained without iterative stages, which not only simplifies the pipeline but also makes the regularization during training easier and more adaptive. Experiments show that the proposed method outperforms previous erasing-based methods by a large margin. More importantly, it achieves universal improvement when plugged into various direct classification methods and obtains state-of-the-art performance.
PU  - ASSOC COMPUTING MACHINERY
PI  - NEW YORK
PA  - 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN  - 1551-6857
SN  - 1551-6865
DA  - 2023 MAY
PY  - 2023
VL  - 19
IS  - 3
C7  - 102
DO  - 10.1145/3567827
AN  - WOS:001011930300002
AD  - Tongji Univ, 4800 Caoan Rd, Shanghai, Peoples R China
Y2  - 2023-07-27
ER  -

TY  - CPAPER
AU  - Biswas, Sovan
AU  - Souri, Yaser
AU  - Gall, Juergen
A1  - IEEE
TI  - HIERARCHICAL GRAPH-RNNS FOR ACTION DETECTION OF MULTIPLE ACTIVITIES
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
M3  - Proceedings Paper
CP  - 26th IEEE International Conference on Image Processing (ICIP)
CL  - Taipei, TAIWAN
AB  - In this paper, we propose an approach that spatially localizes the activities in a video frame where each person can perform multiple activities at the same time. Our approach takes the temporal scene context as well as the relations of the actions of detected persons into account. While the temporal context is modeled by a temporal recurrent neural network (RNN), the relations of the actions are modeled by a graph RNN. Both networks are trained together and the proposed approach achieves state of the art results on the AVA dataset.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1522-4880
SN  - 978-1-5386-6249-6
DA  - 2019 
PY  - 2019
SP  - 3686
EP  - 3690
DO  - 10.1109/icip.2019.8803650
AN  - WOS:000521828603165
AD  - Univ Bonn, Bonn, Germany
Y2  - 2020-04-15
ER  -

TY  - JOUR
AU  - Min, Sunah
AU  - Moon, Jinyoung
TI  - Information Elevation Network for Fast Online Action Detection
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Online action detection (OAD) is a task that receives video segments within a streaming video as inputs and identifies ongoing actions within them. It is important to retain past information associated with a current action. However, long short-term memory (LSTM), a popular recurrent unit for modeling temporal information from videos, accumulates past information from the previous hidden and cell states and the extracted visual features at each timestep without considering the relationships between the past and current information. Consequently, the forget gate of the original LSTM can lose the accumulated information relevant to the current action because it determines which information to forget without considering the current action. We introduce a novel information elevation unit (IEU) that lifts up and accumulate the past information relevant to the current action in order to model the past information that is especially relevant to the current action. To the best of our knowledge, our IEN is the first attempt that considers the computational overhead for the practical use of OAD. Through ablation studies, we design an efficient and effective OAD network using IEUs, called an information elevation network (IEN). Our IEN uses visual features extracted by a fast action recognition network taking only RGB frames because extracting optical flows requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows.
PU  - CORNELL UNIV
DA  - 2021 
PY  - 2021
DO  - arXiv:2109.13572
AN  - PPRN:118867333
AD  - Univ Sci & Technol, Daejoen, South Korea
AD  - Elect & Telecommun Res Inst, Daejoen, South Korea
M2  - Univ Sci & Technol
M2  - Elect & Telecommun Res Inst
Y2  - 2024-11-29
ER  -

TY  - JOUR
AU  - Li, Shuaicheng
AU  - Zhang, Feng
AU  - Zhao, Rui-Wei
AU  - Feng, Rui
AU  - Yang, Kunlin
AU  - Liu, Lingbo
AU  - Hou, Jun
TI  - Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - It has been found that temporal action proposal generation, which aims to discover the temporal action instances within the range of the start and end frames in the untrimmed videos, can largely benefit from proper temporal and semantic context exploitation. The latest efforts were dedicated to considering the temporal context and similarity-based semantic context through self-attention modules. However, they still suffer from cluttered background information and limited contextual feature learning. In this paper, we propose a novel Pyramid Region-based Slot Attention (PRSlot) modules to address these issues. Instead of using the similarity computation, our PRSlot module directly learns the local relations in an encoder-decoder manner and generates the representation of a local region enhanced based on the attention over input features called slot. Specifically, upon the input snippet-level features, PRSlot module takes the target snippet as query, its surrounding region as key and then generates slot representations for each query-key slot by aggregating the local snippet context with a parallel pyramid strategy. Based on PRSlot modules, we present a novel Pyramid Region-based Slot Attention Network termed PRSA-Net to learn a unified visual representation with rich temporal and semantic context for better proposal generation. Extensive experiments are conducted on two widely adopted THUMOS14 and ActivityNet-1.3 benchmarks. Our PRSA-Net outperforms other state-of-the-art methods. In particular, we improve the AR@100 from the previous best 50.67% to 56.12% for proposal generation and raise the mAP under 0.5 tIoU from 51.9% to 58.7% for action detection on THUMOS14.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2206.10095
AN  - PPRN:12298498
AD  - Sensetime Res, Hong Kong, Peoples R China
AD  - Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China
AD  - Fudan Univ, Acad Engn & Technol, Shanghai, Peoples R China
AD  - Fudan Zhangjiang Inst, Shanghai, Peoples R China
AD  - Hong Kong Polytech Univ, Hong Kong, Peoples R China
M2  - Sensetime Res
M2  - Fudan Zhangjiang Inst
M2  - Hong Kong Polytech Univ
Y2  - 2022-11-19
ER  -

TY  - JOUR
AU  - Chen, Keqi
AU  - Schewski, Lilien
AU  - Srivastav, Vinkle
AU  - Lavanchy, Joel
AU  - Mutter, Didier
AU  - Beldi, Guido
AU  - Keller, Sandra
AU  - Padoy, Nicolas
TI  - When do they StOP?: A First Step Towards Automatically Identifying Team Communication in the Operating Room
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - Purpose: Surgical performance depends not only on surgeons' technical skills but also on team communication within and across the different professional groups present during the operation. Therefore, automatically identifying team communication in the OR is crucial for patient safety and advances in the development of computer-assisted surgical workflow analysis and intra-operative support systems. To take the first step, we propose a new task of detecting communication briefings involving all OR team members, i.e. the team Time-out and the StOP?-protocol, by localizing their start and end times in video recordings of surgical operations. Methods: We generate an OR dataset of real surgeries, called Team-OR, with more than one hundred hours of surgical videos captured by the multi-view camera system in the OR. The dataset contains temporal annotations of 33 Time-out and 22 StOP?-protocol activities in total. We then propose a novel group activity detection approach, where we encode both scene context and action features, and use an efficient neural network model to output the results. Results: The experimental results on the Team-OR dataset show that our approach outperforms existing state-of-the-art temporal action detection approaches. It also demonstrates the lack of research on group activities in the OR, proving the significance of our dataset. Conclusion: We investigate the Team Time-Out and the StOP?-protocol in the OR, by presenting the first OR dataset with temporal annotations of group activities protocols, and introducing a novel group activity detection approach that outperforms existing approaches. Code is available at https://github.com/CAMMA-public/Team-OR.
PU  - CORNELL UNIV
DA  - 2025 
PY  - 2025
DO  - arXiv:2502.08299
AN  - PPRN:121537758
AD  - Univ Strasbourg, CNRS, INSERM, ICube,UMR7357, Strasbourg, France
AD  - IHU Strasbourg, F-67000 Strasbourg, France
AD  - Univ Bern, Dept Biomed Res DBMR, CH-3008 Bern, Switzerland
AD  - Univ Digest Hlth Care Ctr, CH-4002 Basel, Switzerland
AD  - Univ Basel, Dept Biomed Engn, CH-4123 Allschwil, Switzerland
AD  - Univ Hosp Strasbourg, F-67000 Strasbourg, France
AD  - Univ Bern, Bern Univ Hosp, Dept Visceral Surg & Med, CH-3010 Bern, Switzerland
M2  - Univ Strasbourg
M2  - IHU Strasbourg
M2  - Univ Bern
M2  - Univ Digest Hlth Care Ctr
M2  - Univ Hosp Strasbourg
M2  - Univ Bern
Y2  - 2025-04-05
ER  -

TY  - JOUR
AU  - Yang, Fan
AU  - Ukita, Norimichi
AU  - Sakti, Sakriani
AU  - Nakamura, Satoshi
TI  - Actor-identified Spatiotemporal Action Detection -- Detecting Who Is Doing What in Videos
T2  - Arxiv
M3  - preprint
C8  - 1
AB  - The success of deep learning on video Action Recognition (AR) has motivated researchers to progressively promote related tasks from the coarse level to the fine-grained level. Compared with conventional AR that only predicts an action label for the entire video, Temporal Action Detection (TAD) has been investigated for estimating the start and end time for each action in videos. Taking TAD a step further, Spatiotemporal Action Detection (SAD) has been studied for localizing the action both spatially and temporally in videos. However, who performs the action, is generally ignored in SAD, while identifying the actor could also be important. To this end, we propose a novel task, Actor-identified Spatiotemporal Action Detection (ASAD), to bridge the gap between SAD and actor identification.In ASAD, we not only detect the spatiotemporal boundary for instance-level action but also assign the unique ID to each actor. To approach ASAD, Multiple Object Tracking (MOT) and Action Classification (AC) are two fundamental elements. By using MOT, the spatiotemporal boundary of each actor is obtained and assigned to a unique actor identity. By using AC, the action class is estimated within the corresponding spatiotemporal boundary. Since ASAD is a new task, it poses many new challenges that cannot be addressed by existing methods: i) no dataset is specifically created for ASAD, ii) no evaluation metrics are designed for ASAD, iii) current MOT performance is the bottleneck to obtain satisfactory ASAD results. To address those problems, we contribute to i) annotate a new ASAD dataset, ii) propose ASAD evaluation metrics by considering multi-label actions and actor identification, iii) improve the data association strategies in MOT to boost the MOT performance, which leads to better ASAD results.
PU  - CORNELL UNIV
DA  - 2022 
PY  - 2022
DO  - arXiv:2208.12940
AN  - PPRN:12826199
AD  - Nara Inst Sci & Technol, Ikoma, Japan
AD  - Toyota Technol Inst, Nagoya, Japan
AD  - RIKEN, Ctr Adv Intelligence Project AIP, Tokyo, Japan
M2  - Nara Inst Sci & Technol
M2  - RIKEN
Y2  - 2022-11-15
ER  -

TY  - BOOK
AU  - Ignat, Oana
Z2  -  
TI  - Towards Human Action Understanding in Social Media Videos Using Multimodal Models
M3  - Dissertation/Thesis
SN  - 9798845465511
DA  - 2022 
PY  - 2022
AN  - PQDT:68525083
AD  - University of Michigan, Computer Science & Engineering, Michigan, United States
M2  - University of Michigan
ER  -

TY  - JOUR
AU  - Zhang, Peng
AU  - Dong, Lijia
AU  - Zhao, Xinlei
AU  - Lei, Weimin
AU  - Zhang, Wei
TI  - An end-to-end framework for real-time violent behavior detection based on 2D CNNs
T2  - JOURNAL OF REAL-TIME IMAGE PROCESSING
M3  - Article
AB  - Violent behavior detection (VioBD), as a special action recognition task, aims to detect violent behaviors in videos, such as mutual fighting and assault. Some progress has been made in the research of violence detection, but the existing methods have poor real-time performance and the algorithm performance is limited by the interference of complex backgrounds and the occlusion of dense crowds. To solve the above problems, we propose an end-to-end real-time violence detection framework based on 2D CNNs. First, we propose a lightweight skeletal image (SI) as the input modality, which can obtain the human body posture information and richer contextual information, and at the same time remove the background interference. As tested, at the same accuracy, the resolution of SI modality is only one-third of that of RGB modality, which greatly improves the real-time performance of model training and inference, and at the same resolution, SI modality has higher inaccuracy. Second, we also design a parallel prediction module (PPM), which can simultaneously obtain the single image detection results and the inter-frame motion information of the video, which can improve the real-time performance of the algorithm compared with the traditional "detect the image first, understand the video later" mode. In addition, we propose an auxiliary parameter generation module (APGM) with both efficiency and accuracy, APGM is a 2D CNNs-based video understanding module for weighting the spatial information of the video features, processing speed can reach 30-40 frames per second, and compared with models such as CNN-LSTM (Iqrar et al., Aamir: Cnn-lstm based smart real-time video surveillance system. In: 2022 14th International Conference on Mathematics, Actuarial, Science, Computer Science and Statistics (MACS), pages 1-5. IEEE, 2022) and Ludl et al. (Cristobal: Simple yet efficient real-time pose-based action recognition. In: 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 581-588. IEEE, 1999), the propagation effect speed can be increased by an average of 3 similar to 20\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$3 \sim 20$$\end{document} frames per second per group of clips, which further improves the video motion detection efficiency and accuracy, greatly improving real-time performance. We conducted experiments on some challenging benchmarks, and RVBDN can maintain excellent speed and accuracy in long-term interactions, and are able to meet real-time requirements in methods for violence detection and spatio-temporal action detection. Finally, we update our proposed new dataset on violence detection images (violence image dataset). Dataset is available at https://github.com/ChinaZhangPeng/Violence-Image-Dataset
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN  - 1861-8200
SN  - 1861-8219
DA  - 2024 APR
PY  - 2024
VL  - 21
IS  - 2
C7  - 57
DO  - 10.1007/s11554-024-01443-7
AN  - WOS:001190949700001
AD  - Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Liaoning, Peoples R China
AD  - Shenyang Er Yi San Elect Technol Co Ltd, Shenyang 110023, Liaoning, Peoples R China
M2  - Shenyang Er Yi San Elect Technol Co Ltd
Y2  - 2024-04-03
ER  -

TY  - CPAPER
AU  - Qiu, Zhaofan
AU  - Yao, Ting
AU  - Ngo, Chong-Wah
AU  - Tian, Xinmei
AU  - Mei, Tao
A1  - IEEE Comp Soc
TI  - Learning Spatio-Temporal Representation with Local and Global Diffusion
T2  - 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2019)
M3  - Proceedings Paper
CP  - 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - Long Beach, CA
AB  - Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency. Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 1063-6919
SN  - 978-1-7281-3293-8
DA  - 2019 
PY  - 2019
SP  - 12048
EP  - 12057
DO  - 10.1109/CVPR.2019.01233
AN  - WOS:000542649305068
AD  - Univ Sci & Technol China, Hefei, Peoples R China
AD  - JD AI Res, Beijing, Peoples R China
AD  - City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China
M2  - JD AI Res
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Li, Yixuan
AU  - Wang, Zhenzhi
AU  - Li, Zhifeng
AU  - Wang, Limin
TI  - Sparse Action Tube Detection
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Action tube detection is a challenging task as it requires not only to locate action instances in each frame, but also link them in time. Existing action tube detection methods often employ multi-stage pipelines with complex designs and time-consuming linking procedure. In this paper, we present a simple end-to-end action tube detection method, termed as Sparse Tube Detector (STDet). Unlike those dense action detectors, our core idea is to use a set of learnable tube queries and directly decode them into action tubes (i.e., a set of tracked boxes with action label) from video content. This sparse detection paradigm shares several advantages. First, the large number of hand-crafted anchor candidates in dense action detectors is greatly reduced to a small number of learnable tubes, which results in a more efficient detection framework. Second, our learnable tube queries directly attend the whole video content, which endows our method with the capacity of capturing long-range information for action detection. Finally, our action detector is an end-to-end tube detection without requiring the linking procedure, which directly and explicitly predicts the action boundary instead of depending on the linking strategy. Extensive experiments shows that our STDet outperforms the previous state-of-the-art methods on two challenging untrimmed video action detection datasets of UCF101-24 and MultiSports. We hope our method will be an simple end-to-end tube detection baseline and can inspire new ideas in this direction.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2024 
PY  - 2024
VL  - 33
SP  - 1740
EP  - 1752
DO  - 10.1109/TIP.2024.3368958
AN  - WOS:001181446500002
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
AD  - Tencent Data Platform, Shenzhen 518038, Peoples R China
AD  - Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China
M2  - Tencent Data Platform
Y2  - 2024-04-02
ER  -

TY  - JOUR
AU  - Zhang, Jipeng
AU  - Shao, Jie
AU  - Cao, Rui
AU  - Gao, Lianli
AU  - Xu, Xing
AU  - Shen, Heng Tao
TI  - Action-Centric Relation Transformer Network for Video Question Answering
T2  - IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
M3  - Article
AB  - Video question answering (VideoQA) has emerged as a popular research topic in recent years. Enormous efforts have been devoted to developing more effective fusion strategies and better intra-modal feature preparation. To explore these issues further, we identify two key problems. (1) Current works take almost no account of introducing action of interest in video representation. Additionally, there exists insufficient labeling data on where the action of interest is in many datasets. However, questions in VideoQA are usually action-centric. (2) Frame-to-frame relations, which can provide useful temporal attributes (e.g., state transition, action counting), lack relevant research. Based on these observations, we propose an action-centric relation transformer network (ACRTransformer) for VideoQA and make two significant improvements. (1) We explicitly consider the action recognition problem and present a visual feature encoding technique, action-based encoding (ABE), to emphasize the frames with high actionness probabilities (the probability that the frame has actions). (2) We better exploit the interplays between temporal frames using a relation transformer network (RTransformer). Experiments on popular benchmark datasets in VideoQA clearly establish our superiority over previous state-of-the-art models. Code could be found at https://github.com/op-multimodal/ACRTransformer.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1051-8215
SN  - 1558-2205
DA  - 2022 JAN
PY  - 2022
VL  - 32
IS  - 1
SP  - 63
EP  - 74
DO  - 10.1109/TCSVT.2020.3048440
AN  - WOS:000742183600010
AD  - Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Future Media, Chengdu 611731, Peoples R China
AD  - Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China
AD  - Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore
M2  - Sichuan Artificial Intelligence Res Inst
Y2  - 2022-01-21
ER  -

TY  - JOUR
AU  - Ma, Yunsheng
AU  - Du, Runjia
AU  - Abdelraouf, Amr
AU  - Han, Kyungtae
AU  - Gupta, Rohit
AU  - Wang, Ziran
TI  - Driver Digital Twin for Online Recognition of Distracted Driving Behaviors
T2  - IEEE TRANSACTIONS ON INTELLIGENT VEHICLES
M3  - Article
AB  - Deep learning has been widely utilized in intelligent vehicle systems, particularly in the field of driver distraction detection. However, existing methods in this application tend to focus solely on appearance or cognitive state as indicators of distraction, while neglecting the significance of temporal modeling in accurately identifying driver actions. This oversight can result in limitations such as difficulty in comprehending context, incapability to recognize gradual changes, and failure to capture complex behaviors. To address these limitations, this paper introduces a new framework based on the concept of Driver Digital Twin (DDT). The DDT framework serves as a digital replica of the driver, capturing their naturalistic driving data and behavioral models. It consists of a transformer-based driver action recognition module and a novel temporal localization module to detect distracted behaviors. Additionally, we propose a pseudo-labeled multi-task learning algorithm that includes driver emotion recognition as supplementary information for recognizing distractions. We have validated the effectiveness of our approach using three publicly available driver distraction detection benchmarks: SFDDD, AUCDD, and SynDD2. The results demonstrate that our framework achieves state-of-the-art performance in both driver action recognition and temporal localization tasks. It outperforms the leading methods by 6.5 and 0.9 percentage points on SFDDD and AUCDD, respectively. Furthermore, it ranks in the top 5% on the SynDD2 leaderboard.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 2379-8858
SN  - 2379-8904
DA  - 2024 FEB
PY  - 2024
VL  - 9
IS  - 2
SP  - 3168
EP  - 3180
DO  - 10.1109/TIV.2024.3353253
AN  - WOS:001215322100044
AD  - Purdue Univ, Coll Engn, W Lafayette, IN 47907 USA
AD  - Toyota Motor North Amer, InfoTech Labs, Mountain View, CA 94043 USA
M2  - Toyota Motor North Amer
Y2  - 2024-06-27
ER  -

TY  - JOUR
AU  - Zhang, Min
AU  - Hu, Haiyang
AU  - Li, Zhongjin
AU  - Chen, Jie
TI  - Proposal-Based Graph Attention Networks for Workflow Detection
T2  - NEURAL PROCESSING LETTERS
M3  - Article
AB  - In the process of "Industry 4.0", video analysis plays a vital role in a variety of industrial applications. Video-based action detection has obtained promising performance in computer vision community. However, in complex factory environment, how to detect workflow of both machines and workers in production process is not well resolved. To solve this issue, we propose a generic proposal based Graph Attention Networks for workflow detection. Specifically, an efficient and effective action proposal method is firstly employed to generate workflow proposals. Then, these proposals and their relations are exploited for proposal graph construction. Here, two types of relationships are considered for identifying the workflow phases, which are contextual and surrounding relations to capture context information and characterize the correlations between different workflow instances. To improve the recognition accuracy, within-category and between-category attention are incorporated to learn long-range and dynamic dependencies respectively. Thus, the capability of feature representation for workflow detection can be greatly enhanced. Experimental results verify that the proposed approach is considerably improved upon the state-of-the-arts on THUMOS'14 and a practical workflow dataset, achieving 6.7% and 3.9% absolute improvement compared to the advanced GTAN detector at tIoU threshold 0.4, respectively. Moreover, augmentation experiments are carried out on ActivityNet1.3 to prove the effectiveness of performance improvement by modeling workflow proposal relationships.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 1370-4621
SN  - 1573-773X
DA  - 2022 FEB
PY  - 2022
VL  - 54
IS  - 1
SP  - 101
EP  - 123
DO  - 10.1007/s11063-021-10622-7
AN  - WOS:000684758500001
C6  - AUG 2021
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China
AD  - Zhejiang Ind Polytech Coll, Dept Design & Art, Shaoxing, Peoples R China
M2  - Zhejiang Ind Polytech Coll
Y2  - 2021-08-20
ER  -

TY  - BOOK
AU  - Tang, Yue
Z2  -  
TI  - Efficient Hardware and Software Design for On-Device Learning of Video Streams
M3  - Dissertation/Thesis
SN  - 9798310324022
DA  - 2024 
PY  - 2024
AN  - PQDT:123174130
AD  - University of Pittsburgh, Pennsylvania, United States
M2  - University of Pittsburgh
ER  -

TY  - JOUR
AU  - Zhang, Ruipeng
AU  - Qin, Binjie
AU  - Zhao, Jun
AU  - Zhu, Yueqi
AU  - Lv, Yisong
AU  - Ding, Song
TI  - Locating X-Ray Coronary Angiogram Keyframes via Long Short-Term Spatiotemporal Attention With Image-to-Patch Contrastive Learning
T2  - IEEE TRANSACTIONS ON MEDICAL IMAGING
M3  - Article
AB  - Locating the start, apex and end keyframes of moving contrast agents for keyframe counting in X-ray coronary angiography (XCA) is very important for the diagnosis and treatment of cardiovascular diseases. To locate these keyframes from the class-imbalanced and boundary-agnostic foreground vessel actions that overlap complex backgrounds, we propose long short-term spatiotemporal attention by integrating a convolutional long short-term memory (CLSTM) network into a multiscale Transformer to learn the segment- and sequence-level dependencies in the consecutive-frame-based deep features. Image-to-patch contrastive learning is further embedded between the CLSTM-based long-term spatiotemporal attention and Transformer-based short-term attention modules. The imagewise contrastive module reuses the long-term attention to contrast image-level foreground/background of XCA sequence, while patchwise contrastive projection selects the random patches of backgrounds as convolution kernels to project foreground/background frames into different latent spaces. A new XCA video dataset is collected to evaluate the proposed method. The experimental results show that the proposed method achieves a mAP (mean average precision) of 72.45% and a F-score of 0.8296, considerably outperforming the state-of-the-art methods. The source code is available at https://github.com/Binjie-Qin/STA-IPCon.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 0278-0062
SN  - 1558-254X
DA  - 2024 JAN
PY  - 2024
VL  - 43
IS  - 1
SP  - 51
EP  - 63
DO  - 10.1109/TMI.2023.3286859
AN  - WOS:001158081600033
AD  - Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China
AD  - Shanghai Jiao Tong Univ, Shanghai Peoples Hosp 6, Dept Radiol, Sch Med, Shanghai 200233, Peoples R China
AD  - Shanghai Jiao Tong Univ, Sch Continuing Educ, Shanghai 200240, Peoples R China
AD  - Shanghai Jiao Tong Univ, Ren Ji Hosp, Sch Med, Dept Cardiol, Shanghai 200127, Peoples R China
Y2  - 2024-03-13
ER  -

TY  - JOUR
AU  - Kalogeiton, Vasiliki
Z2  -  
TI  - Localizing spatially and temporally objects and actions in videos
M3  - Dissertation/Thesis
DA  - 2018 
PY  - 2018
AN  - PQDT:67496094
AD  - The University of Edinburgh (United Kingdom), Scotland
M2  - The University of Edinburgh (United Kingdom)
ER  -

TY  - JOUR
AU  - Moniruzzaman, Md
AU  - Yin, Zhaozheng
TI  - Progressive Knowledge Distillation From Different Levels of Teachers for Online Action Detection
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - In this paper, we explore the problem of Online Action Detection (OAD), where the task is to detect ongoing actions from streaming videos without access to video frames in the future. Existing methods achieve good detection performance by capturing long-range temporal structures. However, a major challenge of this task is to detect actions at a specific time that arrive with insufficient observations. In this work, we utilize the additional future frames available at the training phase and propose a novel Knowledge Distillation (KD) framework for OAD, where a teacher network looks at more frames from the future and the student network distills the knowledge from the teacher for detecting ongoing actions from the observation up to the current frames. Usually, the conventional KD regards a high-level teacher network (i.e., the network after the last training iteration) to guide the student network throughout all training iterations, which may result in poor distillation due to the large knowledge gap between the high-level teacher and the student network at early training iterations. To remedy this, we propose a novel progressive knowledge distillation from different levels of teachers (PKD-DLT) for OAD, where in addition to a high-level teacher, we also generate several low- and middle-level teachers, and progressively transfer the knowledge (in the order of low- to high-level) to the student network throughout training iterations, for effective distillation. Evaluated on two challenging datasets THUMOS14 and TVSeries, we validate that our PKD-DLT is an effective teacher-student learning paradigm, which can be a plug-in to improve the performance of the existing OAD models and achieve a state-of-the-art.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2025 
PY  - 2025
VL  - 27
SP  - 1526
EP  - 1537
DO  - 10.1109/TMM.2024.3521772
AN  - WOS:001442981700013
AD  - SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA
AD  - SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA
Y2  - 2025-03-21
ER  -

TY  - CPAPER
AU  - Herzig, Roei
AU  - Ben-Avraham, Elad
AU  - Mangalam, Karttikeya
AU  - Bar, Amir
AU  - Chechik, Gal
AU  - Rohrbach, Anna
AU  - Darrell, Trevor
AU  - Globerson, Amir
A1  - IEEE COMP SOC
TI  - Object-Region Video Transformers
T2  - 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2022)
M3  - Proceedings Paper
CP  - IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CL  - New Orleans, LA
AB  - Recently, video transformers have shown great success in video understanding, exceeding CNN performance; yet existing video transformer models do not explicitly model objects, although objects can be essential for recognizing actions. In this work, we present Object-Region Video Transformers (ORViT), an object-centric approach that extends video transformer layers with a block that directly incorporates object representations. The key idea is to fuse object-centric representations starting from early layers and propagate them into the transformer-layers, thus affecting the spatio-temporal representations throughout the network. Our ORViT block consists of two object-level streams: appearance and dynamics. In the appearance stream, an "Object-Region Attention" module applies self-attention over the patches and object regions. In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information. We further model object dynamics via a separate "Object-Dynamics Module", which captures trajectory interactions, and show how to integrate the two streams. We evaluate our model on four tasks and five datasets: compositional and few-shot action recognition on SomethingElse, spatio-temporal action detection on AVA, and standard action recognition on Something-Something V2, Diving48 and Epic-Kitchen100. We show strong performance improvement across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a transformer architecture. For code and pretrained models, visit the project page at https://roeiherz.github.io/ORViT/
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN  - 1063-6919
SN  - 978-1-6654-6946-3
DA  - 2022 
PY  - 2022
SP  - 3138
EP  - 3149
DO  - 10.1109/CVPR52688.2022.00315
AN  - WOS:000867754203039
AD  - Tel Aviv Univ, Tel Aviv, Israel
AD  - Univ Calif Berkeley, Berkeley, CA USA
AD  - Bar Ilan Univ, NVIDIA Res, Ramat Gan, Israel
Y2  - 2022-12-17
ER  -

TY  - BOOK
AU  - Hwang, Pin-Jui
Z2  -  
TI  - Vision-Based Learning From Demonstration and Collaborative Robotic Systems
M3  - Dissertation/Thesis
SN  - 9798342736862
DA  - 2023 
PY  - 2023
AN  - PQDT:119727417
AD  - National Taiwan Normal University (Taiwan), Department of Electrical Engineering, Taiwan R.O.C.
M2  - National Taiwan Normal University (Taiwan)
ER  -

TY  - CPAPER
AU  - Wang, Shengbo
AU  - Miao, Zhenjiang
AU  - Xu, Wanru
AU  - Ma, Cong
AU  - Li, Miaomiao
A1  - IEEE
TI  - Boundary Sensitive and Category Sensitive Network for Temporal Action Proposal Generation
T2  - 2019 CHINESE AUTOMATION CONGRESS (CAC2019)
M3  - Proceedings Paper
CP  - Chinese Automation Congress (CAC)
CL  - Hangzhou, PEOPLES R CHINA
AB  - The latest research on temporal motion detection research content is an important research topic in the field of computer vision. The TAP (temporal action proposal) generation task is an essential part for quickly and accurately drawing semantically important action proposals from untrimmed video. As a separate and important research field, the proposal generated by the temporal action proposal generation network should have three attributes: (1) flexible temporal length, (2) accurate temporal boundaries and (3) reliable confidence scores. Therefore, a new type of effective and efficient universal action proposal generation network for temporally uncropped videos is provided by us, named Boundary Sensitive and Category Sensitive (BSCS) network. Firstly, We need a combination of a frame-level appearance features and a clip-level optical flow features, so a two-stream network is adopted. Secondly, a sub-network in our network is designed. This sub-network needs to outputs 203 probability sequences. These probability sequences represent the temporal distribution of the start and end frames and categories, respectively. The ending frame and starting frame belong to the same kind of action are screened through the probability sequence of 200 action categories. Finally, according to certain rules that combining the ending frames and the starting frames, our network generates proposals. We further compare our method with more experiments experimentally with the existing networks in ActivityNet-1.3 [1]. Comparing the network results with experiments proves that our method is more effective.
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
SN  - 2688-092X
SN  - 2688-0938
SN  - 978-1-7281-4094-0
DA  - 2019 
PY  - 2019
SP  - 5194
EP  - 5199
DO  - 10.1109/cac48633.2019.8996591
AN  - WOS:000679040105049
AD  - Beijing Jiaotong Univ, Sch Comp & Informat Technol, Inst Informat Sci, Beijing, Peoples R China
Y2  - 2019-01-01
ER  -

TY  - JOUR
AU  - Su, Taiyi
AU  - Wang, Hanli
AU  - Wang, Lei
TI  - Multi-Level Content-Aware Boundary Detection for Temporal Action Proposal Generation
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - It is challenging to generate temporal action proposals from untrimmed videos. In general, boundary-based temporal action proposal generators are based on detecting temporal action boundaries, where a classifier is usually applied to evaluate the probability of each temporal action location. However, most existing approaches treat boundaries and contents separately, which neglect that the context of actions and the temporal locations complement each other, resulting in incomplete modeling of boundaries and contents. In addition, temporal boundaries are often located by exploiting either local clues or global information, without mining local temporal information and temporal-to-temporal relations sufficiently at different levels. Facing these challenges, a novel approach named multi-level content-aware boundary detection (MCBD) is proposed to generate temporal action proposals from videos, which jointly models the boundaries and contents of actions and captures multi-level (i.e., frame level and proposal level) temporal and context information. Specifically, the proposed MCBD preliminarily mines rich frame-level features to generate one-dimensional probability sequences, and further exploits temporal-to-temporal proposal-level relations to produce two-dimensional probability maps. The final temporal action proposals are obtained by a fusion of the multi-level boundary and content probabilities, achieving precise boundaries and reliable confidence of proposals. The extensive experiments on the three benchmark datasets of THUMOS14, ActivityNet v1.3 and HACS demonstrate the effectiveness of the proposed MCBD compared to state-of-the-art methods. The source code of this work can be found in https://mic.tongji.edu.cn.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2023 
PY  - 2023
VL  - 32
SP  - 6090
EP  - 6101
DO  - 10.1109/TIP.2023.3328471
AN  - WOS:001130103500011
AD  - Tongji Univ, Dept Comp Sci & Technol, Minist Educ, Shanghai 200092, Peoples R China
AD  - Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China
AD  - DeepBlue Technol Shanghai Co Ltd, Shanghai 200336, Peoples R China
M2  - DeepBlue Technol Shanghai Co Ltd
Y2  - 2024-03-09
ER  -

TY  - BOOK
AU  - Yang, Xitong
Z2  -  
TI  - Long-Term Temporal Modeling for Video Action Understanding
M3  - Dissertation/Thesis
SN  - 9798460452743
DA  - 2021 
PY  - 2021
AN  - PQDT:64557566
AD  - University of Maryland, College Park, Computer Science, Maryland, United States
M2  - University of Maryland, College Park
ER  -

TY  - BOOK
AU  - Yang, Hongtao
Z2  -  
TI  - Visual Representation Learning with Progressive Data Scarcity
M3  - Dissertation/Thesis
SN  - 9798684618987
DA  - 2020 
PY  - 2020
AN  - PQDT:66916516
AD  - The Australian National University (Australia), Australia
M2  - The Australian National University (Australia)
ER  -

TY  - JOUR
AU  - Soucek, Tomas
AU  - Alayrac, Jean-Baptiste
AU  - Miech, Antoine
AU  - Laptev, Ivan
AU  - Sivic, Josef
TI  - Multi-Task Learning of Object States and State-Modifying Actions From Web Videos
T2  - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
M3  - Article
AB  - We aim to learn to temporally localize object state changes and the corresponding state-modifying actions by observing people interacting with objects in long uncurated web videos. We introduce three principal contributions. First, we develop a self-supervised model for jointly learning state-modifying actions together with the corresponding object states from an uncurated set of videos from the Internet. The model is self-supervised by the causal ordering signal, i.e., initial object state -> manipulating action -> end state. Second, we explore alternative multi-task network architectures and identify a model that enables efficient joint learning of multiple object states and actions, such as pouring water and pouring coffee, together. Third, we collect a new dataset, named ChangeIt, with more than 2600 hours of video and 34 thousand changes of object states. We report results on an existing instructional video dataset COIN as well as our new large-scale ChangeIt dataset containing tens of thousands of long uncurated web videos depicting various interactions such as hole drilling, cream whisking, or paper plane folding. We show that our multi-task model achieves a relative improvement of 40% over the prior methods and significantly outperforms both image-based and video-based zero-shot models for this problem.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 0162-8828
SN  - 1939-3539
DA  - 2024 JUL
PY  - 2024
VL  - 46
IS  - 7
SP  - 5114
EP  - 5130
DO  - 10.1109/TPAMI.2024.3362288
AN  - WOS:001240147800027
AD  - Czech Tech Univ, Czech Inst Informat Robot & Cybernet, Prague 16000, Czech Republic
AD  - DeepMind, London EC4A 3TW, England
AD  - PSL Res Univ, Inria, Ecole normale Super, CNRS,ENS, F-75005 Paris, France
AD  - PSL Res Univ, Dept informat, Ecole normale Super, CNRS,ENS, F-75005 Paris, France
Y2  - 2024-06-29
ER  -

TY  - JOUR
AU  - Zhang, Xiao-Yu
AU  - Shi, Haichao
AU  - Li, Changsheng
AU  - Li, Peng
AU  - Li, Zekun
AU  - Ren, Peng
TI  - Weakly-supervised action localization via embedding-modeling iterative optimization
T2  - PATTERN RECOGNITION
M3  - Article
AB  - Action recognition and localization in untrimmed videos in weakly supervised scenario is a challenging problem of great application prospects. Limited by the information available in video-level labels, it is a promising attempt to fully leverage the instructive knowledge learned on trimmed videos to facilitate analysis of untrimmed videos, considering that there are abundant trimmed videos which are publicly available and well segmented with semantic descriptions. In order to enforce effective trimmed untrimmed augmentation, this paper presents a novel framework of embedding-modeling iterative optimization network, referred to as IONet. In the proposed method, action classification modeling and shared subspace embedding are learned jointly in an iterative way, so that robust cross-domain knowledge transfer is achieved. With a carefully designed two-stage self-attentive representation learning workflow for untrimmed videos, irrelevant backgrounds are eliminated and fine-grained temporal relevance can be robustly explored. Extensive experiments are conducted on two benchmark datasets, i.e., THUMOS14 and ActivityNet1.3, and experimental results clearly corroborate the efficacy of our method. Source code is available on GitHub.
PU  - ELSEVIER SCI LTD
PI  - London
PA  - 125 London Wall, London, ENGLAND
SN  - 0031-3203
SN  - 1873-5142
DA  - 2021 MAY
PY  - 2021
VL  - 113
DO  - 10.1016/j.patcog.2021.107831
AN  - WOS:000626268800005
C6  - JAN 2021
AD  - Chinese Acad Sci, Inst Informat Engn, 89 Minzhuang Rd, Beijing, Peoples R China
AD  - Univ Chinese Acad Sci, Sch Cyber Secur, 19 A Yuquan Rd, Beijing, Peoples R China
AD  - Beijing Inst Technol, Sch Comp Sci & Technol, 5 Zhongguancun South St, Beijing, Peoples R China
AD  - China Univ Petr East China, Coll Informat & Control Engn, 66 West Changjiang Rd, Qingdao, Peoples R China
Y2  - 2021-03-28
ER  -

TY  - BOOK
AU  - Truong, Quang Sang
Z2  -  
TI  - Towards Multi-Modal Interpretable Video Understanding
M3  - Dissertation/Thesis
SN  - 9798381193367
DA  - 2023 
PY  - 2023
AN  - PQDT:87150088
AD  - University of Arkansas, Computer Engineering, Arkansas, United States
M2  - University of Arkansas
ER  -

TY  - JOUR
AU  - Tapaswi, Makarand
AU  - Kumar, Vijay
AU  - Laptev, Ivan
TI  - Long term spatio-temporal modeling for action detection
T2  - COMPUTER VISION AND IMAGE UNDERSTANDING
M3  - Article
AB  - Modeling person interactions with their surroundings has proven to be effective for recognizing and localizing human actions in videos. While most recent works focus on learning short term interactions, in this work, we consider long-term person interactions and jointly localize actions of multiple actors over an entire video shot. We construct a graph with nodes that correspond to keyframe actor instances and connect them with two edge types. Spatial edges connect actors within a keyframe, and temporal edges connect multiple instances of the same actor over a video shot. We propose a Graph Neural Network that explicitly models spatial and temporal states for each person instance and learns to effectively combine information from both modalities to make predictions at the same time. We conduct experiments on the AVA dataset and show that our graph-based model provides consistent improvements over several video descriptors, achieving state-of-the-art performance without any fine-tuning.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1077-3142
SN  - 1090-235X
DA  - 2021 SEP
PY  - 2021
VL  - 210
C7  - 103242
DO  - 10.1016/j.cviu.2021.103242
AN  - WOS:000691995600003
C6  - JUL 2021
AD  - Inria WILLOW, 2 Rue Simone IFF, F-75012 Paris, France
M2  - Inria WILLOW
Y2  - 2021-07-15
ER  -

TY  - BOOK
AU  - Asnani, Vishal
Z2  -  
TI  - Proactive Schemes: Adversarial Attacks for Social Good
M3  - Dissertation/Thesis
SN  - 9798310195790
DA  - 2025 
PY  - 2025
AN  - PQDT:122820913
AD  - Michigan State University, Computer Science - Doctor of Philosophy, Michigan, United States
M2  - Michigan State University
ER  -

TY  - JOUR
AU  - You, Dianlong
AU  - Xiao, Jiawei
AU  - Wang, Yang
AU  - Yan, Huigui
AU  - Wu, Di
AU  - Chen, Zhen
AU  - Shen, Limin
AU  - Wu, Xindong
TI  - Online Learning From Incomplete and Imbalanced Data Streams
T2  - IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
M3  - Article
AB  - Learning with streaming data has attracted extensive research interest in recent years. Existing online learning approaches have specific assumptions regarding data streams, such as requiring fixed or varying feature spaces with explicit patterns and balanced class distributions. While the data streams generated in many real scenarios commonly have arbitrarily incomplete feature spaces and dynamic imbalanced class distributions, making existing approaches be unsuitable for real applications. To address this issue, this paper proposes a novel Online Learning from Incomplete and Imbalanced Data Streams (OLI (2) DS) algorithm. OLI 2 DS has a two-fold main idea: 1) it follows the empirical risk minimization principle to identify the most informative features of incomplete feature spaces, and 2) it develops a dynamic cost strategy to handle imbalanced class distributions in real-time by transforming F-measure optimization into a weighted surrogate loss minimization. To evaluate OLI (2) DS, we compare it with stateof-the-art related algorithms in three kinds of experiments. First, we adopt 14 real datasets to simulate three scenarios of incomplete feature spaces, i.e., trapezoidal, feature evolvable, and capricious data streams. Second, based on a benchmark online analyzer, we generate 13 datasets to simulate incomplete data streams with different imbalance ratios. Third, we analyze concept drift in two simulated scenes, i.e., online learning and data stream mining, and verify the adaption of OLI (2) DS on repeated concept drifts and variable imbalance ratios. The results demonstrate that OLI (2) DS achieves a significantly better performance than its rivals. Besides, a real-world case study on movie review classification is conducted to elaborate on our OLI (2) DS algorithm's effectiveness. Code is released at https://github.com/youdianlong/OLI2DS.
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN  - 1041-4347
SN  - 1558-2191
DA  - 2023 OCT 1
PY  - 2023
VL  - 35
IS  - 10
SP  - 10650
EP  - 10665
DO  - 10.1109/TKDE.2023.3250472
AN  - WOS:001068964300057
AD  - Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China
AD  - Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China
AD  - Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ China, Hefei 230009, Anhui, Peoples R China
Y2  - 2023-10-18
ER  -

TY  - JOUR
AU  - Ning, Ke
AU  - Cai, Ming
AU  - Xie, Di
AU  - Wu, Fei
TI  - An Attentive Sequence to Sequence Translator for Localizing Video Clips by Natural Language
T2  - IEEE TRANSACTIONS ON MULTIMEDIA
M3  - Article
AB  - We propose a novel attentive sequence to sequence translator (ASST) for localizing video clips by natural language descriptions. We make two contributions. First, we propose an attentive mechanism that aligns natural language descriptions and video content. A bi-directional Recurrent Neural Network (RNN) parses natural language descriptions in two directions. Given a video-description pair, ASST generates a vector sequence representation. Each vector represents a video frame, conditioned by the description. The vector sequence representation not only preserves the temporal dependencies between the frames, but also provides an effective way to perform frame-level video-language matching. The attentive model then aligns words to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. The integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by 4.28% in Rank@1 on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by 13.41% in Recall@1,IoU = 0.5.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1520-9210
SN  - 1941-0077
DA  - 2020 SEPT
PY  - 2020
VL  - 22
IS  - 9
SP  - 2434
EP  - 2443
DO  - 10.1109/TMM.2019.2957854
AN  - WOS:000562310200017
AD  - Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China
AD  - Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China
AD  - Zhejiang Univ, Inst Artificial Intelligence, Hangzhou 310027, Zhejiang, Peoples R China
AD  - Hikvis Res Inst, Hangzhou 310051, Peoples R China
M2  - Hikvis Res Inst
Y2  - 2020-09-07
ER  -

TY  - BOOK
AU  - Monteiro, Carlos Filipe Batista Cardoso
Z2  -  
TI  - Spatio-Temporal Action Localization with Deep Learning
M3  - Dissertation/Thesis
SN  - 9798381495973
DA  - 2021 
PY  - 2021
AN  - PQDT:87640062
AD  - Universidade do Minho (Portugal), Portugal
M2  - Universidade do Minho (Portugal)
ER  -

TY  - JOUR
AU  - Ning, Ke
AU  - Xie, Lingxi
AU  - Liu, Jianzhuang
AU  - Wu, Fei
AU  - Tian, Qi
TI  - Interaction-Integrated Network for Natural Language Moment Localization
T2  - IEEE TRANSACTIONS ON IMAGE PROCESSING
M3  - Article
AB  - Natural language moment localization aims at localizing video clips according to a natural language description. The key to this challenging task lies in modeling the relationship between verbal descriptions and visual contents. Existing approaches often sample a number of clips from the video, and individually determine how each of them is related to the query sentence. However, this strategy can fail dramatically, in particular when the query sentence refers to some visual elements that appear outside of, or even are distant from, the target clip. In this paper, we address this issue by designing an Interaction-Integrated Network ((IN)-N-2), which contains a few Interaction-Integrated Cells ((ICs)-Cs-2). The idea lies in the observation that the query sentence not only provides a description to the video clip, but also contains semantic cues on the structure of the entire video. Based on this, (ICs)-Cs-2 go one step beyond modeling short-term contexts in the time domain by encoding long-term video content into every frame feature. By stacking a few (ICs)-Cs-2, the obtained network, (IN)-N-2, enjoys an improved ability of inference, brought by both (I) multi-level correspondence between vision and language and (II) more accurate cross-modal alignment. When evaluated on a challenging video moment localization dataset named DiDeMo, (IN)-N-2 outperforms the state-of-the-art approach by a clear margin of 1.98%. On other two challenging datasets, Charades-STA and TACoS, (IN)-N-2 also reports competitive performance.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1057-7149
SN  - 1941-0042
DA  - 2021 
PY  - 2021
VL  - 30
SP  - 2538
EP  - 2548
DO  - 10.1109/TIP.2021.3052086
AN  - WOS:000615040400009
AD  - Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China
AD  - Huawei Inc, Shenzhen 518129, Peoples R China
Y2  - 2021-02-26
ER  -

TY  - JOUR
AU  - Zheng, Chan
AU  - Yang, Xiaofan
AU  - Zhu, Xunmu
AU  - Chen, Changxin
AU  - Wang, Lina
AU  - Tu, Shuqin
AU  - Yang, Aqing
AU  - Xue, Yueju
TI  - Automatic posture change analysis of lactating sows by action localisation and tube optimisation from untrimmed depth videos
T2  - BIOSYSTEMS ENGINEERING
M3  - Article
AB  - The automatic detection of postures and posture changes in sows using a computer-vision system has substantial potential for learning their maternal abilities, enhancing their welfare and productivity, and reducing the crushing risk to piglets. The objectives of this study are to (1) detect frame-level sow postures, (2) temporally localise posture change actions, and (3) generate spatio-temporally action tubes parsed from a long-time untrimmed segment of depth video. Depth videos were recorded for five batches of lactating sows, using a Kinect from a top-view in a commercial farm. Three batches were used for training and validation, and the other two for testing. Four postures (standing, sitting, ventral lying, and lateral lying) were automatically detected, with a mean average-precision (mAP) of 0.927. The localisation performance of the clip-level mAP involved eight posture change actions, and achieved 0.774 in the temporal intersection over union (tIoU) >= 0.5. A tube optimisation algorithm was used to optimise and smooth the action tubes. When the mean IoU >= 0.8 in the tube, the performance of the video-level mAP significantly improved, from 0.313 to 0.796. The error analysis could deepen the understanding of the causes of errors in action detection. The system was applied to test two day videos of various sows, by obtaining the regularity of posture change probability, comparing the action characteristics, and discerning the maternal differences of the sows. The methodology can be applied in large-scale deployments for learning livestock action preferences and behavioural traits, thereby enhancing welfare and productivity on a farm. (C) 2020 IAgrE. Published by Elsevier Ltd. All rights reserved.
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN  - 1537-5110
SN  - 1537-5129
DA  - 2020 JUN
PY  - 2020
VL  - 194
SP  - 227
EP  - 250
DO  - 10.1016/j.biosystemseng.2020.04.005
AN  - WOS:000532810000017
AD  - South China Agr Univ, Coll Math & Informat, Guangzhou, Peoples R China
AD  - South China Agr Univ, Coll Elect Engn, Guangzhou, Peoples R China
AD  - South China Agr Univ, Coll Anim Sci, Guangzhou, Peoples R China
AD  - Guangdong Engn Res Ctr Datamat Modern Pig Prod, Guangzhou, Peoples R China
AD  - Guangdong Engn Res Ctr Informat Monitoring Agr, Guangzhou, Peoples R China
AD  - Guangdong Lab Lingnan Modern Agr, Guangzhou, Peoples R China
M2  - Guangdong Engn Res Ctr Datamat Modern Pig Prod
M2  - Guangdong Engn Res Ctr Informat Monitoring Agr
Y2  - 2020-05-28
ER  -

TY  - JOUR
AU  - Han, Tingting
AU  - Zhao, Sicheng
AU  - Sun, Xiaoshuai
AU  - Yu, Jun
TI  - Modeling long-term video semantic distribution for temporal action proposal generation
T2  - NEUROCOMPUTING
M3  - Article
AB  - Video temporal segmentation plays a vital role in video analysis since many higher-level computer vision tasks rely on it. Some recent efforts have been dedicated to generating temporal action proposals for long and untrimmed videos, which requires methods to generate accurate boundaries for video semantics. In this paper, we propose a novel and efficient Temporal Distribution Network (TDN), to model the long-term distribution of video semantic units (video dictionary). Firstly, we encode the semantics and context relations of video segments with a boundary-specified video embedding method. Then based on temporal convolutional layers, we design a Temporal Distribution Network (TDN) enumerating all the possible temporal locations in one pass and generating proposals that have high action confidence scores by capturing the long-term distributions of video semantics. We validate our method on temporal action proposal generation tasks and action detection tasks. Experimental results on two benchmark datasets, THUMOS14 and ActivityNet-1.3, show that the proposed method can significantly outperform the state-of-the-art approaches. Our model could obtain high-quality action proposals with a much faster speed. (c) 2021 Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2022 JUN 14
PY  - 2022
VL  - 490
SP  - 217
EP  - 225
DO  - 10.1016/j.neucom.2021.11.085
AN  - WOS:000796199200003
C6  - APR 2022
AD  - Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China
AD  - Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA
AD  - Xiamen Univ, Sch Informat, Xiamen, Peoples R China
Y2  - 2022-05-31
ER  -

TY  - JOUR
AU  - Wang, Lining
AU  - Yao, Hongxun
AU  - Yang, Haosen
AU  - Wang, Sibo
AU  - Jin, Sheng
TI  - MIFNet: Multiple instances focused temporal action proposal generation
T2  - NEUROCOMPUTING
M3  - Article
AB  - Temporal action proposal generation (TAPG) serves as a promising solution for video analysis. However, the performance of existing methods is still far from satisfactory for real-world applications. We attribute it to a crucial issue, i.e., hard multiple instances. In this paper, we investigate why this is the case. We discover that when processing multiple instances videos, mainstream approaches always recognize mul-tiple instances as one instance due to boundary ambiguity or ignoring insignificant backgrounds between these instances. To address this problem, we propose a Multiple Instances Focused Network(MIFNet) that improves the quality of action proposals by considering boundary correlations and fusing multi-scale proposals. In particular, we first propose a pure boundary embedding module named Boundary Constraint Module (BCM) for suppressing the generation of hard negatives proposal by evaluating bound-ary correlation. The BCM introduces a boundary contrastive learning strategy that can pull the positive boundary pairs' representation closer and push the negative pairs' representation away. Then, a Proposal Blending Module (PBM) is proposed, which augments the proposal-level representation by mod-eling information among multi-scale proposals so that proposals can be complemented with local details as well as global information. The experimental results on the ActivityNet-v1.3 and THUMOS14 bench-marks demonstrate that MIFNet outperforms the state-of-the-arts.(c) 2023 Published by Elsevier B.V.
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN  - 0925-2312
SN  - 1872-8286
DA  - 2023 JUN 14
PY  - 2023
VL  - 538
C7  - 126025
DO  - 10.1016/j.neucom.2023.01.045
AN  - WOS:000980065500001
C6  - APR 2023
AD  - Harbin Inst Technol, Fac Comp, Harbin, Peoples R China
AD  - Univ Surrey, Guildford, England
AD  - Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China
Y2  - 2023-05-16
ER  -

TY  - JOUR
AU  - Yang, Yu
AU  - Wang, Mengmeng
AU  - Mei, Jianbiao
AU  - Liu, Yong
TI  - Exploiting semantic-level affinities with a mask-guided network for temporal action proposal in videos
T2  - APPLIED INTELLIGENCE
M3  - Article
AB  - Temporal action proposal (TAP) aims to detect the action instances' starting and ending times in untrimmed videos, which is fundamental and critical for large-scale video analysis and human action understanding. The main challenge of the temporal action proposal lies in modeling representative temporal relations in long untrimmed videos. Existing state-of-the-art methods achieve temporal modeling by building local-level, proposal-level, or global-level temporal dependencies. Local methods lack a wider receptive field, while proposal and global methods lack the focalization of learning action frames and contain background distractions. In this paper, we propose that learning semantic-level affinities can capture more practical information. Specifically, by modeling semantic associations between frames and action units, action segments (foregrounds) can aggregate supportive cues from other co-occurring actions, and nonaction clips (backgrounds) can learn the discriminations between them and action frames. To this end, we propose a novel framework named the Mask-Guided Network (MGNet) to build semantic-level temporal associations for the TAP task. Specifically, we first propose a Foreground Mask Generation (FMG) module to adaptively generate the foreground mask, representing the locations of the action units throughout the video. Second, we design a Mask-Guided Transformer (MGT) by exploiting the foreground mask to guide the self-attention mechanism to focus on and calculate semantic affinities with the foreground frames. Finally, these two modules are jointly explored in a unified framework. MGNet models the intra-semantic similarities for foregrounds, extracting supportive action cues for boundary refinement; it also builds the inter-semantic distances for backgrounds, providing the semantic gaps to suppress false positives and distractions. Extensive experiments are conducted on two challenging datasets, ActivityNet-1.3 and THUMOS14, and the results demonstrate that our method achieves superior performance.
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN  - 0924-669X
SN  - 1573-7497
DA  - 2023 JUN
PY  - 2023
VL  - 53
IS  - 12
SP  - 15516
EP  - 15536
DO  - 10.1007/s10489-022-04261-1
AN  - WOS:000886437800001
C6  - NOV 2022
AD  - Zhejiang Univ, Inst Cyber Syst & Control, Hangzhou 310027, Peoples R China
Y2  - 2022-12-02
ER  -

TY  - BOOK
AU  - Wang, Xijun
Z2  -  
TI  - Deep Learning Applied to Image and Video Processing
M3  - Dissertation/Thesis
SN  - 9798381977776
DA  - 2024 
PY  - 2024
AN  - PQDT:88341751
AD  - Northwestern University, Computer Science, Illinois, United States
M2  - Northwestern University
ER  -

TY  - JOUR
AU  - Yin, Jun
AU  - Han, Jun
AU  - Xie, Ruiqi
AU  - Wang, Chenghao
AU  - Duan, Xuyang
AU  - Rong, Yitong
AU  - Zeng, Xiaoyang
AU  - Tao, Jun
TI  - MC-LSTM: Real-Time 3D Human Action Detection System for Intelligent Healthcare Applications
T2  - IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
M3  - Article
AB  - Due to the movement expressiveness and privacy assurance of human skeleton data, 3D skeleton-based action inference is becoming popular in healthcare applications. These scenarios call for more advanced performance in application-specific algorithms and efficient hardware support. Warnings on health emergencies sensitive to response speed require low latency output and action early detection capabilities. Medical monitoring that works in an always-on edge platform needs the system processor to have extreme energy efficiency. Therefore, in this paper, we propose the MC-LSTM, a functional and versatile 3D skeleton-based action detection system, for the above demands. Our system achieves state-of-the-art accuracy on trimmed and untrimmed cases of general-purpose and medical-specific datasets with early-detection features. Further, the MC-LSTM accelerator supports parallel inference on up to 64 input channels. The implementation on Xilinx ZCU104 reaches a throughput of 18 658 Frames-Per-Second (FPS) and an inference latency of 3.5 ms with the batch size of 64. Accordingly, the power consumption is 3.6 W for the whole FPGA+ARM system, which is 37.8x and 10.4x more energy-efficient than the high-end Titan X GPU and i7-9700 CPU, respectively. Meanwhile, our accelerator also keeps a 4 similar to 5x energy efficiency advantage against the low-power high-performance Firefly-RK3399 board carrying an ARM Cortex-A72+A53 CPU. We further synthesize an 8-bit quantized version on the same hardware, providing a 48.8% increase in energy efficiency under the same throughput.
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN  - 1932-4545
SN  - 1940-9990
DA  - 2021 APR
PY  - 2021
VL  - 15
IS  - 2
SP  - 259
EP  - 269
DO  - 10.1109/TBCAS.2021.3064841
AN  - WOS:000655246400007
AD  - Fudan Univ, State Key Lab ASIC & Syst, Shanghai 201203, Peoples R China
Y2  - 2021-06-18
ER  -

